{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sansith/gpt2sp/blob/bertsp/model_training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qyLoXxhlhfbk"
      },
      "source": [
        "# Model Training Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Ren7VyEyhfbl"
      },
      "source": [
        "### Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMyN0hhThfbl",
        "outputId": "bc9e260f-b571-4885-faf7-aaa3d95590f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas===1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Collecting koila\n",
            "  Downloading koila-0.1.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Collecting pynvml (from koila)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from koila) (13.7.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->koila) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Installing collected packages: pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, koila\n",
            "Successfully installed koila-0.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynvml-11.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch pandas===1.5.3 transformers numpy tokenizers koila tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb"
      ],
      "metadata": {
        "id": "O8TuUYj7h6St"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIh96xVTiuIa",
        "outputId": "ca9c9488-8b03-4a0f-cd05-f29260936686"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3R16ZxuirGo",
        "outputId": "3be31f12-4ff0-405d-9890-a1d7cedc72fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "bhcSAPbLhfbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup , BertTokenizer\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
        "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
        "from transformers import GPT2Config , BertConfig\n",
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "import torch.nn as nn\n",
        "from transformers import  BertPreTrainedModel , BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "class BertSP(BertPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.transformer = BertModel(config)\n",
        "        print(\"n_embd/hidden_size : \", config.hidden_size)\n",
        "        self.dense1 = nn.Linear(config.hidden_size, 4 * config.hidden_size, bias=False)\n",
        "        self.dense2 = nn.Linear(4 * config.hidden_size, config.hidden_size, bias=False)\n",
        "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        # MLP Layer\n",
        "        hidden_states = self.dense1(hidden_states)\n",
        "        hidden_states = self.dense2(hidden_states)\n",
        "\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        assert (\n",
        "            self.config.pad_token_id is not None or batch_size == 1\n",
        "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "                logger.warning(\n",
        "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "                )\n",
        "\n",
        "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = nn.L1Loss()\n",
        "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (pooled_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "cf6cpSnmvZcA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbQNj41Ghfbn"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "ch24eOM0hfbn"
      },
      "outputs": [],
      "source": [
        "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME , ADD_DESCRIPTION\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
        "SEQUENCE_LEN = 100\n",
        "LEARNING_RATE = 5e-4\n",
        "TOKENIZER = 'bert' # available:bert, gpt2, wordlevel, sentencepiece, wordpiece\n",
        "MODEL_NAME = 'bert' # available: bert, gpt2sp, gpt2\n",
        "ADD_DESCRIPTION = False\n",
        "\n",
        "# define device\n",
        "global DEVICE\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# define files to be used\n",
        "global DATA_PATH\n",
        "DATA_PATH = './sp_dataset/marked_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH1j_lvmhfbn"
      },
      "source": [
        "### Static Methods and Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "W1FZRuJ7hfbn"
      },
      "outputs": [],
      "source": [
        "OUTPUT = '  '\n",
        "MODEL = None\n",
        "DYNAMIC_BATCH = True\n",
        "BATCH_SIZE = None\n",
        "WITHIN_PROJECT = None\n",
        "MAE_RECORDS = []\n",
        "MDAE_RECORDS = []\n",
        "\n",
        "def data_processing(file_pair):\n",
        "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
        "\n",
        "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
        "    for train_file_name in file_pair['train']:\n",
        "        fname = DATA_PATH + train_file_name + '.csv'\n",
        "        df = prepare_dataframe(fname)\n",
        "        train_data = train_data.append(df)\n",
        "\n",
        "    # data split\n",
        "    if WITHIN_PROJECT:\n",
        "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
        "    else:\n",
        "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
        "    # define batch size dynamically based on training length\n",
        "    if DYNAMIC_BATCH:\n",
        "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
        "    # tokenization\n",
        "    tokens_train = tokenization(train_text.tolist())\n",
        "    tokens_val = tokenization(val_text.tolist())\n",
        "    print(tokens_train['input_ids'][:5])\n",
        "\n",
        "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
        "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
        "\n",
        "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
        "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
        "\n",
        "    # prepare testing datasets\n",
        "    all_test_dataloader = []\n",
        "    test_file_names = []\n",
        "    if WITHIN_PROJECT:\n",
        "        tokens_test = tokenization(test_text.tolist())\n",
        "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(file_pair['test'][0])\n",
        "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "    for test_file_name in file_pair['test']:\n",
        "        fname = DATA_PATH + test_file_name + '.csv'\n",
        "        test_data = prepare_dataframe(fname)\n",
        "\n",
        "        test_text = test_data['text']\n",
        "        test_labels = test_data['label']\n",
        "\n",
        "        # tokenization\n",
        "        tokens_test = tokenization(test_text.tolist())\n",
        "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
        "\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(test_file_name)\n",
        "    print('cross project data processing!')\n",
        "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "\n",
        "def train_val_split(data, split_ratio):\n",
        "    print('cross project split!')\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_text = data['text'][:split_point]\n",
        "    train_labels = data['label'][:split_point]\n",
        "    val_text = data['text'][split_point:]\n",
        "    val_labels = data['label'][split_point:]\n",
        "    return train_text, train_labels, val_text, val_labels\n",
        "\n",
        "\n",
        "def tokenization(text_list):\n",
        "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
        "    # tokenization\n",
        "    if TOKENIZER == 'wordpiece':\n",
        "        print('using wordpiece tokenizer!')\n",
        "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
        "    elif TOKENIZER == 'sentencepiece':\n",
        "        print('using sentencepiece tokenizer!')\n",
        "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
        "    elif TOKENIZER == 'wordlevel':\n",
        "        print('using wordlevel tokenizer!')\n",
        "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
        "        encoded_sentences = {'input_ids':[]}\n",
        "        for sentence in text_list:\n",
        "            encoded = tokenizer.encode(sentence)\n",
        "            encoded = encoded.ids\n",
        "            if len(encoded) > SEQUENCE_LEN:\n",
        "                encoded = encoded[:SEQUENCE_LEN]\n",
        "            elif len(encoded) < SEQUENCE_LEN:\n",
        "                padding = SEQUENCE_LEN - len(encoded)\n",
        "                for _ in range(padding):\n",
        "                    encoded.append(3)\n",
        "            encoded_sentences['input_ids'].append(encoded)\n",
        "        return encoded_sentences\n",
        "    elif TOKENIZER == 'gpt2':\n",
        "        print('using pretrained gpt-2 tokenizer')\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
        "        tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "    elif TOKENIZER == 'bert':\n",
        "        print('usingbert tokenizer')\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        # tokenizer.pad_token = '[PAD]'\n",
        "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
        "\n",
        "\n",
        "def prepare_dataframe(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    # some rows have no description, fill blank to avoid Null\n",
        "    data = data.fillna(' ')\n",
        "\n",
        "\n",
        "    if ADD_DESCRIPTION :\n",
        "      print(\"### text : title+description\")\n",
        "      d = {'text': (data['title'] + \" : \" + data[\"description\"]).tolist(), 'label': data['storypoint']}\n",
        "    else:\n",
        "      print(\"### text : title\")\n",
        "      d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
        "    print(\"Input data feed ::: \",d['text'][0])\n",
        "    return pd.DataFrame(data=d)\n",
        "\n",
        "\n",
        "def prepare_dataloader(seq, y, sampler_type):\n",
        "    global BATCH_SIZE\n",
        "    tensor_dataset = TensorDataset(seq, y)\n",
        "    if sampler_type == 'random':\n",
        "        sampler = RandomSampler(tensor_dataset)\n",
        "    elif sampler_type == 'sequential':\n",
        "        sampler = SequentialSampler(tensor_dataset)\n",
        "    print(\"BATCH_SIZE : \",BATCH_SIZE)\n",
        "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def within_project_split(data):\n",
        "    print('within project split!')\n",
        "    train_val_split_point = int(len(data) * 0.6)\n",
        "    val_test_split_point = int(len(data) * 0.8)\n",
        "    train_text = data['text'][:train_val_split_point]\n",
        "    train_labels = data['label'][:train_val_split_point]\n",
        "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
        "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
        "    test_text = data['text'][val_test_split_point:]\n",
        "    test_labels = data['label'][val_test_split_point:]\n",
        "    return train_text, train_labels, val_text, val_labels, test_text, test_labels\n",
        "\n",
        "\n",
        "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
        "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
        "\n",
        "    # Optimizerrr -->\n",
        "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)\n",
        "    # Total number of training steps is [number of batches] x [number of epochs]\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "    # Create the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    print(\"Start training for \", file_pair, \".....\")\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    # tensorboard writer\n",
        "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
        "    writer = SummaryWriter(writer_path)\n",
        "\n",
        "    # vars for model selection\n",
        "    min_eval_loss_epoch = [10000, 0]\n",
        "\n",
        "    time_records = []\n",
        "    MAE_RECORDS = []\n",
        "    MDAE_RECORDS = []\n",
        "    start_time = time.time()\n",
        "    loss_fct = nn.L1Loss()\n",
        "    for e in range(EPOCHS):\n",
        "        # ---TRAINING---\n",
        "        # clean GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\">>> epoch \", e)\n",
        "        # set model into train mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # pdb.set_trace()\n",
        "            b_input_ids = batch[0].to(DEVICE)\n",
        "            b_labels = batch[1].to(DEVICE)\n",
        "            model.zero_grad()\n",
        "            result = model(b_input_ids,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "            total_train_loss += loss.item()\n",
        "            # Calculates the gradients\n",
        "            loss.backward()\n",
        "            # The clip_grad_norm_ function clips (limits) the norm (magnitude) of the gradients to a maximum value specified by the user.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            #updates the weights and bias accrding to the calculated gradients\n",
        "            optimizer.step()\n",
        "            # update learning rates\n",
        "            scheduler.step()\n",
        "            # clean memory\n",
        "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
        "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
        "        # clean memory\n",
        "        del avg_train_loss, total_train_loss\n",
        "\n",
        "        time_records.append(time.time() - start_time)\n",
        "\n",
        "        # ---EVAL---\n",
        "        print(\"-\")\n",
        "        # set model into eval mode\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        for batch in val_dataloader:\n",
        "            b_input_ids = batch[0].to(DEVICE)\n",
        "            b_labels = batch[1].to(DEVICE)\n",
        "            model.zero_grad()\n",
        "            result = model(b_input_ids,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "            total_eval_loss += loss.item()\n",
        "            # clean memory\n",
        "            del b_input_ids, b_labels, batch, result, loss, logits\n",
        "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
        "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
        "\n",
        "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
        "            min_eval_loss_epoch[0] = avg_eval_loss\n",
        "            min_eval_loss_epoch[1] = e\n",
        "\n",
        "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
        "        # clean memory\n",
        "        del avg_eval_loss, total_eval_loss\n",
        "        # save model state to dict\n",
        "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
        "\n",
        "        print(\"===============================\")\n",
        "\n",
        "        # testing on holdout data\n",
        "        index = 0\n",
        "        for test_dataloader in all_test_dataloader:\n",
        "            test_file_name = test_file_names[index]\n",
        "            index += 1\n",
        "            testing_start_time = time.time()\n",
        "            predictions = []\n",
        "            true_labels = []\n",
        "            for batch in test_dataloader:\n",
        "                batch = tuple(t.to(DEVICE) for t in batch)\n",
        "                b_input_ids, b_labels = batch\n",
        "                with torch.no_grad():\n",
        "                    logits = model(b_input_ids)\n",
        "                logits = logits['logits'].detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "            # calculate errors\n",
        "            distance_records = []\n",
        "            for i in range(len(predictions)):\n",
        "                for j in range(len(predictions[i])):\n",
        "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
        "                    distance_records.append(distance)\n",
        "\n",
        "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
        "            MAE = np.mean(np.array(distance_records))\n",
        "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
        "            MdAE = np.median(np.array(distance_records))\n",
        "\n",
        "            MAE_RECORDS.append(MAE)\n",
        "            MDAE_RECORDS.append(MdAE)\n",
        "\n",
        "            global OUTPUT\n",
        "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
        "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
        "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
        "            print('MAE: ', MAE)\n",
        "            print('MdAE: ', MdAE)\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "\n",
        "    # select model\n",
        "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]),\n",
        "              'models/' + str(file_pair['train'][0]) + '_'\n",
        "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
        "\n",
        "    # del unwanted models\n",
        "    for i in range(20):\n",
        "        try:\n",
        "            os.remove(\"models/epo_\" + str(i))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
        "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
        "    global BATCH_SIZE\n",
        "    OUTPUT += 'batch size: ' + str(BATCH_SIZE) + '\\n'\n",
        "    global ADD_DESCRIPTION\n",
        "    OUTPUT += 'Description added : ' + str(ADD_DESCRIPTION) + '\\n'\n",
        "\n",
        "\n",
        "    print('all done for one project')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn3yXK4lhfbo"
      },
      "source": [
        "### Within Project Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "x-uMZ1Cfhfbo"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwmBk2BPhfbo",
        "outputId": "42532499-13cb-40db-b31d-95f553d2db2b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Add Copy URL actions to right-click context menu of Remote view for S3 files\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 5587, 6100, 24471, 2140, 4506, 2000, 2157, 1011, 11562, 6123, 12183, 1997, 6556, 3193, 2005, 1055, 2509, 6764, 102], [101, 26794, 5162, 5363, 2000, 2330, 1037, 2047, 6013, 1997, 2993, 2043, 3098, 6764, 3081, 3645, 10566, 102, 0, 0], [101, 4180, 6509, 3769, 6279, 2003, 3491, 2648, 1996, 3898, 7372, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 25718, 8285, 9006, 10814, 3508, 2005, 6687, 4725, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 11920, 2019, 3746, 2046, 1996, 16129, 3559, 2323, 3443, 2019, 3746, 6415, 102, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  149\n",
            "BATCH_SIZE :  149\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  149\n",
            "Start training for  {'train': ['aptanastudio'], 'test': ['aptanastudio']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 8.20\n",
            "-\n",
            " Average eval MAE loss: 16.58\n",
            "===============================\n",
            "MAE:  17.908117\n",
            "MdAE:  19.757294\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 13.00\n",
            "-\n",
            " Average eval MAE loss: 8.72\n",
            "===============================\n",
            "MAE:  7.4582424\n",
            "MdAE:  5.241375\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 7.67\n",
            "-\n",
            " Average eval MAE loss: 8.28\n",
            "===============================\n",
            "MAE:  9.669859\n",
            "MdAE:  11.07366\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 5.79\n",
            "-\n",
            " Average eval MAE loss: 3.15\n",
            "===============================\n",
            "MAE:  3.427276\n",
            "MdAE:  1.58763\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 4.37\n",
            "-\n",
            " Average eval MAE loss: 2.92\n",
            "===============================\n",
            "MAE:  3.8963127\n",
            "MdAE:  3.4498835\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 4.54\n",
            "-\n",
            " Average eval MAE loss: 3.50\n",
            "===============================\n",
            "MAE:  4.5576105\n",
            "MdAE:  4.505416\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 4.40\n",
            "-\n",
            " Average eval MAE loss: 3.00\n",
            "===============================\n",
            "MAE:  3.4865334\n",
            "MdAE:  2.0347486\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 4.25\n",
            "-\n",
            " Average eval MAE loss: 2.88\n",
            "===============================\n",
            "MAE:  3.8516173\n",
            "MdAE:  3.378543\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 4.40\n",
            "-\n",
            " Average eval MAE loss: 3.31\n",
            "===============================\n",
            "MAE:  4.340903\n",
            "MdAE:  4.1595182\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 4.48\n",
            "-\n",
            " Average eval MAE loss: 2.79\n",
            "===============================\n",
            "MAE:  3.5688689\n",
            "MdAE:  2.6560125\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 4.49\n",
            "-\n",
            " Average eval MAE loss: 3.16\n",
            "===============================\n",
            "MAE:  4.1682515\n",
            "MdAE:  3.8839397\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 4.29\n",
            "-\n",
            " Average eval MAE loss: 3.11\n",
            "===============================\n",
            "MAE:  4.112413\n",
            "MdAE:  3.7948132\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 4.26\n",
            "-\n",
            " Average eval MAE loss: 3.05\n",
            "===============================\n",
            "MAE:  3.4637234\n",
            "MdAE:  1.8626413\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 4.54\n",
            "-\n",
            " Average eval MAE loss: 2.78\n",
            "===============================\n",
            "MAE:  3.5694654\n",
            "MdAE:  2.660513\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 4.47\n",
            "-\n",
            " Average eval MAE loss: 3.26\n",
            "===============================\n",
            "MAE:  4.2828994\n",
            "MdAE:  4.0669355\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 4.49\n",
            "-\n",
            " Average eval MAE loss: 2.79\n",
            "===============================\n",
            "MAE:  3.568495\n",
            "MdAE:  2.6531882\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 4.33\n",
            "-\n",
            " Average eval MAE loss: 2.88\n",
            "===============================\n",
            "MAE:  3.8562276\n",
            "MdAE:  3.3859034\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 4.26\n",
            "-\n",
            " Average eval MAE loss: 2.77\n",
            "===============================\n",
            "MAE:  3.5744298\n",
            "MdAE:  2.6979728\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 4.34\n",
            "-\n",
            " Average eval MAE loss: 2.77\n",
            "===============================\n",
            "MAE:  3.5750651\n",
            "MdAE:  2.7027683\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 4.17\n",
            "-\n",
            " Average eval MAE loss: 2.70\n",
            "===============================\n",
            "MAE:  3.6469924\n",
            "MdAE:  3.0519304\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Allows CVS repo to timeout and report on locking issues\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 4473, 26226, 2015, 16360, 2080, 2000, 2051, 5833, 1998, 3189, 2006, 14889, 3314, 102, 0, 0, 0, 0, 0], [101, 3499, 1037, 3857, 2000, 2022, 2872, 2012, 1996, 2132, 1997, 1996, 3857, 24240, 1012, 1012, 1012, 1006, 2030, 102], [101, 2765, 2025, 5552, 2043, 4638, 5833, 11896, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2831, 5963, 2013, 21274, 4005, 2000, 15216, 8241, 2000, 2421, 1041, 5910, 3872, 4057, 3463, 102, 0, 0, 0], [101, 5198, 2064, 2156, 1996, 5003, 8159, 11336, 1006, 2177, 3593, 1010, 20785, 3593, 1010, 2544, 1007, 2114, 2169, 102]]\n",
            "BATCH_SIZE :  93\n",
            "BATCH_SIZE :  93\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  93\n",
            "Start training for  {'train': ['bamboo'], 'test': ['bamboo']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 4.40\n",
            "-\n",
            " Average eval MAE loss: 19.79\n",
            "===============================\n",
            "MAE:  19.63551\n",
            "MdAE:  19.512028\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 15.10\n",
            "-\n",
            " Average eval MAE loss: 14.48\n",
            "===============================\n",
            "MAE:  14.633713\n",
            "MdAE:  14.757519\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 6.88\n",
            "-\n",
            " Average eval MAE loss: 1.58\n",
            "===============================\n",
            "MAE:  1.5144242\n",
            "MdAE:  1.2259631\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 2.90\n",
            "-\n",
            " Average eval MAE loss: 0.84\n",
            "===============================\n",
            "MAE:  0.92206204\n",
            "MdAE:  0.69802594\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 1.60\n",
            "-\n",
            " Average eval MAE loss: 1.04\n",
            "===============================\n",
            "MAE:  1.194781\n",
            "MdAE:  1.3185905\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 1.65\n",
            "-\n",
            " Average eval MAE loss: 1.12\n",
            "===============================\n",
            "MAE:  1.1313633\n",
            "MdAE:  0.67446065\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 1.56\n",
            "-\n",
            " Average eval MAE loss: 0.77\n",
            "===============================\n",
            "MAE:  0.87141585\n",
            "MdAE:  0.7881584\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 1.51\n",
            "-\n",
            " Average eval MAE loss: 0.92\n",
            "===============================\n",
            "MAE:  0.9851508\n",
            "MdAE:  0.5857487\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 1.46\n",
            "-\n",
            " Average eval MAE loss: 1.26\n",
            "===============================\n",
            "MAE:  1.2381518\n",
            "MdAE:  0.8645077\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 1.50\n",
            "-\n",
            " Average eval MAE loss: 0.75\n",
            "===============================\n",
            "MAE:  0.85948676\n",
            "MdAE:  0.8093884\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 1.39\n",
            "-\n",
            " Average eval MAE loss: 0.65\n",
            "===============================\n",
            "MAE:  0.8004469\n",
            "MdAE:  0.6117747\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 1.56\n",
            "-\n",
            " Average eval MAE loss: 1.32\n",
            "===============================\n",
            "MAE:  1.2814033\n",
            "MdAE:  0.9414809\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 1.62\n",
            "-\n",
            " Average eval MAE loss: 0.65\n",
            "===============================\n",
            "MAE:  0.796802\n",
            "MdAE:  0.6412151\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 1.55\n",
            "-\n",
            " Average eval MAE loss: 1.21\n",
            "===============================\n",
            "MAE:  1.1984775\n",
            "MdAE:  0.79390097\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 1.56\n",
            "-\n",
            " Average eval MAE loss: 0.62\n",
            "===============================\n",
            "MAE:  0.76421\n",
            "MdAE:  0.90445745\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 1.54\n",
            "-\n",
            " Average eval MAE loss: 0.68\n",
            "===============================\n",
            "MAE:  0.8062873\n",
            "MdAE:  0.9040656\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 1.44\n",
            "-\n",
            " Average eval MAE loss: 0.98\n",
            "===============================\n",
            "MAE:  1.0294374\n",
            "MdAE:  0.5069337\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 1.37\n",
            "-\n",
            " Average eval MAE loss: 0.62\n",
            "===============================\n",
            "MAE:  0.7693786\n",
            "MdAE:  0.862712\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 1.54\n",
            "-\n",
            " Average eval MAE loss: 0.61\n",
            "===============================\n",
            "MAE:  0.758664\n",
            "MdAE:  0.9492526\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 1.35\n",
            "-\n",
            " Average eval MAE loss: 0.73\n",
            "===============================\n",
            "MAE:  0.84661895\n",
            "MdAE:  0.83228827\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Line coverage data is inconsistent\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 2240, 6325, 2951, 2003, 20316, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2469, 10273, 2465, 15069, 2003, 16542, 2043, 5834, 2006, 1037, 15723, 1998, 1037, 3231, 1011, 15723, 2013, 1996, 102], [101, 4292, 2569, 16015, 6434, 16101, 2000, 1037, 3120, 16101, 3972, 12870, 2015, 2035, 3120, 999, 102, 0, 0, 0], [101, 5587, 4118, 1013, 4861, 2504, 16015, 4292, 1999, 2622, 5144, 3931, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2191, 3231, 2448, 10566, 4685, 2488, 1998, 2644, 14889, 1996, 21318, 11689, 102, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  69\n",
            "BATCH_SIZE :  69\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  69\n",
            "Start training for  {'train': ['clover'], 'test': ['clover']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 8.74\n",
            "-\n",
            " Average eval MAE loss: 12.15\n",
            "===============================\n",
            "MAE:  11.77961\n",
            "MdAE:  12.292812\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 8.86\n",
            "-\n",
            " Average eval MAE loss: 5.61\n",
            "===============================\n",
            "MAE:  6.29273\n",
            "MdAE:  5.540004\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 6.27\n",
            "-\n",
            " Average eval MAE loss: 5.35\n",
            "===============================\n",
            "MAE:  7.7931075\n",
            "MdAE:  4.8320665\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 5.09\n",
            "-\n",
            " Average eval MAE loss: 5.11\n",
            "===============================\n",
            "MAE:  5.917048\n",
            "MdAE:  4.9727974\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 4.51\n",
            "-\n",
            " Average eval MAE loss: 3.61\n",
            "===============================\n",
            "MAE:  6.058326\n",
            "MdAE:  3.0972872\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 5.03\n",
            "-\n",
            " Average eval MAE loss: 3.05\n",
            "===============================\n",
            "MAE:  4.422598\n",
            "MdAE:  3.4131413\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 3.82\n",
            "-\n",
            " Average eval MAE loss: 1.99\n",
            "===============================\n",
            "MAE:  3.8490348\n",
            "MdAE:  2.2379248\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 4.31\n",
            "-\n",
            " Average eval MAE loss: 1.65\n",
            "===============================\n",
            "MAE:  4.0978456\n",
            "MdAE:  1.1368061\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 3.56\n",
            "-\n",
            " Average eval MAE loss: 2.53\n",
            "===============================\n",
            "MAE:  4.1283503\n",
            "MdAE:  2.9317083\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 3.31\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  3.6529865\n",
            "MdAE:  0.948804\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 3.77\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  3.6814888\n",
            "MdAE:  1.386069\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 3.33\n",
            "-\n",
            " Average eval MAE loss: 2.09\n",
            "===============================\n",
            "MAE:  3.8985133\n",
            "MdAE:  2.3608232\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 3.75\n",
            "-\n",
            " Average eval MAE loss: 1.50\n",
            "===============================\n",
            "MAE:  3.6696901\n",
            "MdAE:  1.2851243\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 3.70\n",
            "-\n",
            " Average eval MAE loss: 1.52\n",
            "===============================\n",
            "MAE:  3.6766584\n",
            "MdAE:  1.3447461\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 3.15\n",
            "-\n",
            " Average eval MAE loss: 1.68\n",
            "===============================\n",
            "MAE:  3.7185311\n",
            "MdAE:  1.7029908\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 3.41\n",
            "-\n",
            " Average eval MAE loss: 1.84\n",
            "===============================\n",
            "MAE:  3.770555\n",
            "MdAE:  2.0429897\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 3.43\n",
            "-\n",
            " Average eval MAE loss: 1.71\n",
            "===============================\n",
            "MAE:  3.7274213\n",
            "MdAE:  1.7790489\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 3.71\n",
            "-\n",
            " Average eval MAE loss: 1.61\n",
            "===============================\n",
            "MAE:  3.6999025\n",
            "MdAE:  1.5436106\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 3.52\n",
            "-\n",
            " Average eval MAE loss: 1.58\n",
            "===============================\n",
            "MAE:  3.6908727\n",
            "MdAE:  1.4663577\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 3.53\n",
            "-\n",
            " Average eval MAE loss: 1.59\n",
            "===============================\n",
            "MAE:  3.6935754\n",
            "MdAE:  1.4894762\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Transition git repositories to Stash\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 6653, 21025, 2102, 16360, 20049, 29469, 2229, 2000, 2358, 11823, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2345, 4697, 1040, 2213, 3260, 4861, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2330, 2039, 1048, 4757, 2102, 4007, 5653, 2075, 7201, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 6653, 2000, 13693, 3980, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5587, 16942, 1011, 2241, 23569, 27605, 6290, 2000, 2033, 3022, 1035, 4800, 8873, 2102, 102, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  840\n",
            "BATCH_SIZE :  840\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  840\n",
            "Start training for  {'train': ['datamanagement'], 'test': ['datamanagement']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 10.50\n",
            "-\n",
            " Average eval MAE loss: 19.31\n",
            "===============================\n",
            "MAE:  18.517624\n",
            "MdAE:  19.25618\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 17.99\n",
            "-\n",
            " Average eval MAE loss: 12.44\n",
            "===============================\n",
            "MAE:  14.229072\n",
            "MdAE:  9.984961\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 11.77\n",
            "-\n",
            " Average eval MAE loss: 7.97\n",
            "===============================\n",
            "MAE:  8.294781\n",
            "MdAE:  6.9488077\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 9.48\n",
            "-\n",
            " Average eval MAE loss: 5.30\n",
            "===============================\n",
            "MAE:  7.060936\n",
            "MdAE:  2.697729\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 8.73\n",
            "-\n",
            " Average eval MAE loss: 5.40\n",
            "===============================\n",
            "MAE:  6.403606\n",
            "MdAE:  3.4998403\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 8.70\n",
            "-\n",
            " Average eval MAE loss: 4.91\n",
            "===============================\n",
            "MAE:  6.5784335\n",
            "MdAE:  1.7920029\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 8.69\n",
            "-\n",
            " Average eval MAE loss: 5.64\n",
            "===============================\n",
            "MAE:  6.5435896\n",
            "MdAE:  4.0876045\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 8.82\n",
            "-\n",
            " Average eval MAE loss: 4.88\n",
            "===============================\n",
            "MAE:  6.288755\n",
            "MdAE:  2.3886838\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 8.97\n",
            "-\n",
            " Average eval MAE loss: 4.92\n",
            "===============================\n",
            "MAE:  6.2139606\n",
            "MdAE:  2.8544025\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 8.50\n",
            "-\n",
            " Average eval MAE loss: 5.30\n",
            "===============================\n",
            "MAE:  6.344312\n",
            "MdAE:  3.220138\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 8.35\n",
            "-\n",
            " Average eval MAE loss: 4.87\n",
            "===============================\n",
            "MAE:  6.3206587\n",
            "MdAE:  2.190032\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 8.59\n",
            "-\n",
            " Average eval MAE loss: 4.92\n",
            "===============================\n",
            "MAE:  6.212747\n",
            "MdAE:  2.8619645\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 8.61\n",
            "-\n",
            " Average eval MAE loss: 5.03\n",
            "===============================\n",
            "MAE:  6.2266173\n",
            "MdAE:  2.6633923\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 8.95\n",
            "-\n",
            " Average eval MAE loss: 4.95\n",
            "===============================\n",
            "MAE:  6.1980195\n",
            "MdAE:  2.9305\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 8.60\n",
            "-\n",
            " Average eval MAE loss: 4.99\n",
            "===============================\n",
            "MAE:  6.2134213\n",
            "MdAE:  2.7866454\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 8.46\n",
            "-\n",
            " Average eval MAE loss: 5.00\n",
            "===============================\n",
            "MAE:  6.215355\n",
            "MdAE:  2.768578\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 8.44\n",
            "-\n",
            " Average eval MAE loss: 4.96\n",
            "===============================\n",
            "MAE:  6.2001157\n",
            "MdAE:  2.910924\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 8.96\n",
            "-\n",
            " Average eval MAE loss: 5.02\n",
            "===============================\n",
            "MAE:  6.22209\n",
            "MdAE:  2.7056794\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 8.64\n",
            "-\n",
            " Average eval MAE loss: 4.97\n",
            "===============================\n",
            "MAE:  6.2066717\n",
            "MdAE:  2.8496842\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 8.40\n",
            "-\n",
            " Average eval MAE loss: 4.97\n",
            "===============================\n",
            "MAE:  6.2054687\n",
            "MdAE:  2.8609245\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Document logging framework\n",
            "within project split!\n",
            "usingbert tokenizer\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[101, 6254, 15899, 7705, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3499, 2005, 3722, 2039, 16616, 1997, 2622, 2544, 3616, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9625, 7170, 1024, 20410, 3144, 4241, 22648, 23743, 2094, 13749, 4355, 1997, 2184, 2102, 2497, 1997, 1038, 7317, 102], [101, 16545, 13910, 2475, 2243, 1024, 3746, 7584, 2326, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 16545, 13910, 2475, 2243, 1024, 3746, 8241, 2326, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  119\n",
            "BATCH_SIZE :  119\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  119\n",
            "Start training for  {'train': ['duracloud'], 'test': ['duracloud']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 4.30\n",
            "-\n",
            " Average eval MAE loss: 15.85\n",
            "===============================\n",
            "MAE:  15.922928\n",
            "MdAE:  15.729273\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 6.73\n",
            "-\n",
            " Average eval MAE loss: 2.38\n",
            "===============================\n",
            "MAE:  2.30764\n",
            "MdAE:  2.501669\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 3.76\n",
            "-\n",
            " Average eval MAE loss: 3.73\n",
            "===============================\n",
            "MAE:  3.6529746\n",
            "MdAE:  3.8470042\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 3.76\n",
            "-\n",
            " Average eval MAE loss: 3.11\n",
            "===============================\n",
            "MAE:  3.172454\n",
            "MdAE:  2.928298\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 3.36\n",
            "-\n",
            " Average eval MAE loss: 1.04\n",
            "===============================\n",
            "MAE:  0.8404047\n",
            "MdAE:  1.10255\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 2.82\n",
            "-\n",
            " Average eval MAE loss: 2.18\n",
            "===============================\n",
            "MAE:  2.0993702\n",
            "MdAE:  2.2934\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 2.00\n",
            "-\n",
            " Average eval MAE loss: 1.70\n",
            "===============================\n",
            "MAE:  1.6314968\n",
            "MdAE:  1.7490122\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 1.83\n",
            "-\n",
            " Average eval MAE loss: 2.69\n",
            "===============================\n",
            "MAE:  2.617958\n",
            "MdAE:  2.811988\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.66\n",
            "-\n",
            " Average eval MAE loss: 0.92\n",
            "===============================\n",
            "MAE:  0.7960507\n",
            "MdAE:  0.66769946\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 1.46\n",
            "-\n",
            " Average eval MAE loss: 1.13\n",
            "===============================\n",
            "MAE:  0.9455544\n",
            "MdAE:  1.2702887\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 1.53\n",
            "-\n",
            " Average eval MAE loss: 1.53\n",
            "===============================\n",
            "MAE:  1.4548672\n",
            "MdAE:  1.6488972\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 1.75\n",
            "-\n",
            " Average eval MAE loss: 0.97\n",
            "===============================\n",
            "MAE:  0.78133\n",
            "MdAE:  0.8254441\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 1.44\n",
            "-\n",
            " Average eval MAE loss: 0.99\n",
            "===============================\n",
            "MAE:  0.7761317\n",
            "MdAE:  0.999588\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 1.35\n",
            "-\n",
            " Average eval MAE loss: 1.08\n",
            "===============================\n",
            "MAE:  1.0047805\n",
            "MdAE:  1.1988103\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 1.54\n",
            "-\n",
            " Average eval MAE loss: 0.94\n",
            "===============================\n",
            "MAE:  0.7890345\n",
            "MdAE:  0.567345\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 1.40\n",
            "-\n",
            " Average eval MAE loss: 1.09\n",
            "===============================\n",
            "MAE:  0.8944346\n",
            "MdAE:  1.1887407\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 1.46\n",
            "-\n",
            " Average eval MAE loss: 0.96\n",
            "===============================\n",
            "MAE:  0.7834754\n",
            "MdAE:  0.75357294\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 1.39\n",
            "-\n",
            " Average eval MAE loss: 0.91\n",
            "===============================\n",
            "MAE:  0.7983313\n",
            "MdAE:  0.74409795\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 1.37\n",
            "-\n",
            " Average eval MAE loss: 0.88\n",
            "===============================\n",
            "MAE:  0.8055856\n",
            "MdAE:  0.9871174\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 1.45\n",
            "-\n",
            " Average eval MAE loss: 0.89\n",
            "===============================\n",
            "MAE:  0.8038968\n",
            "MdAE:  0.9305413\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  As a JIRA Administrator I would like to be able to change the trigger of the night service\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 2004, 1037, 10147, 2527, 8911, 1045, 2052, 2066, 2000, 2022, 2583, 2000, 2689, 1996, 9495, 1997, 1996, 2305, 102], [101, 2004, 1037, 10147, 2527, 8911, 1045, 2052, 2066, 2000, 2022, 2583, 2000, 2689, 1996, 9495, 1997, 1996, 2305, 102], [101, 12391, 4773, 6198, 14593, 2229, 2089, 13249, 2007, 2060, 13354, 7076, 102, 0, 0, 0, 0, 0, 0, 0], [101, 12391, 4773, 6198, 14593, 2229, 2089, 13249, 2007, 2060, 13354, 7076, 102, 0, 0, 0, 0, 0, 0, 0], [101, 5587, 3793, 2000, 1996, 29003, 11721, 24291, 1000, 19528, 2622, 1000, 4471, 102, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  63\n",
            "BATCH_SIZE :  63\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  63\n",
            "Start training for  {'train': ['jirasoftware'], 'test': ['jirasoftware']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 6.72\n",
            "-\n",
            " Average eval MAE loss: 18.40\n",
            "===============================\n",
            "MAE:  17.674198\n",
            "MdAE:  18.32214\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 6.51\n",
            "-\n",
            " Average eval MAE loss: 2.62\n",
            "===============================\n",
            "MAE:  2.2147145\n",
            "MdAE:  1.8466048\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 5.07\n",
            "-\n",
            " Average eval MAE loss: 2.79\n",
            "===============================\n",
            "MAE:  3.5134997\n",
            "MdAE:  2.8656123\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 4.71\n",
            "-\n",
            " Average eval MAE loss: 2.84\n",
            "===============================\n",
            "MAE:  2.4256287\n",
            "MdAE:  2.1732135\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 2.89\n",
            "-\n",
            " Average eval MAE loss: 3.14\n",
            "===============================\n",
            "MAE:  2.7439935\n",
            "MdAE:  2.5563307\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 3.03\n",
            "-\n",
            " Average eval MAE loss: 2.84\n",
            "===============================\n",
            "MAE:  2.4256656\n",
            "MdAE:  2.1732578\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 3.41\n",
            "-\n",
            " Average eval MAE loss: 2.69\n",
            "===============================\n",
            "MAE:  2.2762847\n",
            "MdAE:  1.9876189\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 2.81\n",
            "-\n",
            " Average eval MAE loss: 2.03\n",
            "===============================\n",
            "MAE:  1.7729985\n",
            "MdAE:  1.2693214\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.91\n",
            "-\n",
            " Average eval MAE loss: 3.88\n",
            "===============================\n",
            "MAE:  3.5230734\n",
            "MdAE:  3.4938679\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 3.15\n",
            "-\n",
            " Average eval MAE loss: 1.66\n",
            "===============================\n",
            "MAE:  1.5827551\n",
            "MdAE:  1.019769\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 2.74\n",
            "-\n",
            " Average eval MAE loss: 3.06\n",
            "===============================\n",
            "MAE:  2.664886\n",
            "MdAE:  2.4611335\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 2.71\n",
            "-\n",
            " Average eval MAE loss: 2.65\n",
            "===============================\n",
            "MAE:  2.243744\n",
            "MdAE:  1.9130926\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 2.87\n",
            "-\n",
            " Average eval MAE loss: 2.26\n",
            "===============================\n",
            "MAE:  1.9125618\n",
            "MdAE:  1.1545787\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 2.82\n",
            "-\n",
            " Average eval MAE loss: 2.84\n",
            "===============================\n",
            "MAE:  2.4307582\n",
            "MdAE:  2.179388\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 2.75\n",
            "-\n",
            " Average eval MAE loss: 1.94\n",
            "===============================\n",
            "MAE:  1.7255731\n",
            "MdAE:  1.4465427\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 2.76\n",
            "-\n",
            " Average eval MAE loss: 2.81\n",
            "===============================\n",
            "MAE:  2.401505\n",
            "MdAE:  2.1441846\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 2.84\n",
            "-\n",
            " Average eval MAE loss: 2.57\n",
            "===============================\n",
            "MAE:  2.1785371\n",
            "MdAE:  1.7637453\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 2.89\n",
            "-\n",
            " Average eval MAE loss: 3.01\n",
            "===============================\n",
            "MAE:  2.6126673\n",
            "MdAE:  2.398294\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 2.74\n",
            "-\n",
            " Average eval MAE loss: 2.66\n",
            "===============================\n",
            "MAE:  2.2459824\n",
            "MdAE:  1.9182196\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 2.87\n",
            "-\n",
            " Average eval MAE loss: 2.61\n",
            "===============================\n",
            "MAE:  2.2105343\n",
            "MdAE:  1.837029\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Report executor terminations to framework schedulers.\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 3189, 4654, 8586, 16161, 2099, 18287, 2015, 2000, 7705, 6134, 2869, 1012, 102, 0, 0, 0, 0, 0, 0], [101, 2033, 17063, 6658, 2323, 17053, 4654, 8586, 16161, 2869, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 14451, 4708, 1035, 3478, 3114, 2000, 7705, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 13212, 7705, 11896, 2000, 2448, 2349, 2000, 2919, 9245, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2036, 4638, 1005, 21025, 2102, 4487, 4246, 1011, 1011, 9132, 29336, 1011, 1011, 9813, 1005, 1999, 2695, 1011, 102]]\n",
            "BATCH_SIZE :  302\n",
            "BATCH_SIZE :  302\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  302\n",
            "Start training for  {'train': ['mesos'], 'test': ['mesos']} .....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 8.14\n",
            "-\n",
            " Average eval MAE loss: 9.04\n",
            "===============================\n",
            "MAE:  9.300682\n",
            "MdAE:  10.043852\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 7.42\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  1.4386982\n",
            "MdAE:  1.3528974\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 3.33\n",
            "-\n",
            " Average eval MAE loss: 2.80\n",
            "===============================\n",
            "MAE:  2.9380486\n",
            "MdAE:  3.5100763\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 2.87\n",
            "-\n",
            " Average eval MAE loss: 2.02\n",
            "===============================\n",
            "MAE:  1.74456\n",
            "MdAE:  0.9899574\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 2.47\n",
            "-\n",
            " Average eval MAE loss: 1.52\n",
            "===============================\n",
            "MAE:  1.4131666\n",
            "MdAE:  1.3116541\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 1.82\n",
            "-\n",
            " Average eval MAE loss: 1.47\n",
            "===============================\n",
            "MAE:  1.2115922\n",
            "MdAE:  0.7262578\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 1.74\n",
            "-\n",
            " Average eval MAE loss: 1.40\n",
            "===============================\n",
            "MAE:  1.2616229\n",
            "MdAE:  1.0668526\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 1.74\n",
            "-\n",
            " Average eval MAE loss: 1.44\n",
            "===============================\n",
            "MAE:  1.2144455\n",
            "MdAE:  0.51342535\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 1.69\n",
            "-\n",
            " Average eval MAE loss: 1.52\n",
            "===============================\n",
            "MAE:  1.411985\n",
            "MdAE:  1.3097451\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 1.64\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  1.22843\n",
            "MdAE:  1.0132332\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 1.65\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  1.4383063\n",
            "MdAE:  1.3522642\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 1.73\n",
            "-\n",
            " Average eval MAE loss: 1.48\n",
            "===============================\n",
            "MAE:  1.2113367\n",
            "MdAE:  0.74770594\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 1.73\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  1.4324362\n",
            "MdAE:  1.342782\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 1.69\n",
            "-\n",
            " Average eval MAE loss: 1.41\n",
            "===============================\n",
            "MAE:  1.2169794\n",
            "MdAE:  0.72627187\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 1.72\n",
            "-\n",
            " Average eval MAE loss: 1.51\n",
            "===============================\n",
            "MAE:  1.4000795\n",
            "MdAE:  1.290513\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 1.67\n",
            "-\n",
            " Average eval MAE loss: 1.39\n",
            "===============================\n",
            "MAE:  1.2190531\n",
            "MdAE:  0.90047026\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 1.66\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  1.2196118\n",
            "MdAE:  0.9473846\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 1.77\n",
            "-\n",
            " Average eval MAE loss: 1.41\n",
            "===============================\n",
            "MAE:  1.267526\n",
            "MdAE:  1.0763881\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 1.66\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  1.2196981\n",
            "MdAE:  0.95463467\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 1.66\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  1.2340192\n",
            "MdAE:  1.0222616\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### text : title\n",
            "Input data feed :::  Forum: Per-discussion subscription\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 7057, 1024, 2566, 1011, 6594, 15002, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7057, 1024, 7514, 2011, 1041, 1011, 5653, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3499, 5089, 2000, 20648, 3056, 4249, 1999, 7809, 4023, 2004, 3223, 102, 0, 0, 0, 0, 0, 0, 0], [101, 7057, 1024, 10838, 2000, 2279, 11689, 4957, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2224, 6337, 1005, 1055, 2607, 2862, 2003, 12098, 16313, 8486, 2135, 3132, 2007, 2053, 2126, 1997, 3773, 102]]\n",
            "BATCH_SIZE :  209\n",
            "BATCH_SIZE :  209\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  209\n",
            "Start training for  {'train': ['moodle'], 'test': ['moodle']} .....\n",
            ">>> epoch  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average training MAE loss: 18.99\n",
            "-\n",
            " Average eval MAE loss: 15.16\n",
            "===============================\n",
            "MAE:  13.000348\n",
            "MdAE:  14.853317\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 16.77\n",
            "-\n",
            " Average eval MAE loss: 14.12\n",
            "===============================\n",
            "MAE:  9.5092\n",
            "MdAE:  10.047956\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 13.77\n",
            "-\n",
            " Average eval MAE loss: 16.25\n",
            "===============================\n",
            "MAE:  5.8497553\n",
            "MdAE:  4.27724\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 12.62\n",
            "-\n",
            " Average eval MAE loss: 16.75\n",
            "===============================\n",
            "MAE:  5.5218453\n",
            "MdAE:  3.3850212\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 12.91\n",
            "-\n",
            " Average eval MAE loss: 14.39\n",
            "===============================\n",
            "MAE:  8.758314\n",
            "MdAE:  8.803181\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 13.05\n",
            "-\n",
            " Average eval MAE loss: 14.31\n",
            "===============================\n",
            "MAE:  8.965117\n",
            "MdAE:  9.148838\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 12.97\n",
            "-\n",
            " Average eval MAE loss: 17.00\n",
            "===============================\n",
            "MAE:  5.3787727\n",
            "MdAE:  3.0458975\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 13.93\n",
            "-\n",
            " Average eval MAE loss: 14.72\n",
            "===============================\n",
            "MAE:  7.9373507\n",
            "MdAE:  7.4310036\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 12.49\n",
            "-\n",
            " Average eval MAE loss: 14.64\n",
            "===============================\n",
            "MAE:  8.141935\n",
            "MdAE:  7.7729487\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 12.56\n",
            "-\n",
            " Average eval MAE loss: 15.97\n",
            "===============================\n",
            "MAE:  6.038079\n",
            "MdAE:  4.7896547\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 13.38\n",
            "-\n",
            " Average eval MAE loss: 14.93\n",
            "===============================\n",
            "MAE:  7.406082\n",
            "MdAE:  6.543024\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 12.22\n",
            "-\n",
            " Average eval MAE loss: 15.32\n",
            "===============================\n",
            "MAE:  6.4619193\n",
            "MdAE:  5.057102\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 12.88\n",
            "-\n",
            " Average eval MAE loss: 15.20\n",
            "===============================\n",
            "MAE:  6.7231965\n",
            "MdAE:  5.4016304\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 12.67\n",
            "-\n",
            " Average eval MAE loss: 15.27\n",
            "===============================\n",
            "MAE:  6.5358906\n",
            "MdAE:  5.08856\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 13.11\n",
            "-\n",
            " Average eval MAE loss: 15.11\n",
            "===============================\n",
            "MAE:  6.929964\n",
            "MdAE:  5.747226\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 12.87\n",
            "-\n",
            " Average eval MAE loss: 15.23\n",
            "===============================\n",
            "MAE:  6.628047\n",
            "MdAE:  5.242594\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 12.50\n",
            "-\n",
            " Average eval MAE loss: 15.16\n",
            "===============================\n",
            "MAE:  6.8101892\n",
            "MdAE:  5.5470314\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 13.28\n",
            "-\n",
            " Average eval MAE loss: 15.26\n",
            "===============================\n",
            "MAE:  6.565003\n",
            "MdAE:  5.1372194\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 13.32\n",
            "-\n",
            " Average eval MAE loss: 15.16\n",
            "===============================\n",
            "MAE:  6.809097\n",
            "MdAE:  5.545204\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 12.95\n",
            "-\n",
            " Average eval MAE loss: 15.17\n",
            "===============================\n",
            "MAE:  6.7990055\n",
            "MdAE:  5.5283394\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Implement true multicast functionality for <all> processor\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 10408, 2995, 4800, 10526, 15380, 2005, 1026, 2035, 1028, 13151, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2515, 20568, 2490, 1060, 2050, 12598, 2006, 1060, 2050, 4219, 2109, 2011, 1037, 3500, 4874, 6922, 1029, 102, 0], [101, 2421, 27354, 11336, 1999, 20568, 4563, 4353, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2127, 1011, 3144, 2323, 2275, 6453, 18093, 2007, 2197, 6453, 2363, 2077, 6016, 2000, 21469, 4160, 102, 0, 0], [101, 2127, 1011, 3144, 2323, 2490, 26351, 8093, 17175, 2271, 2224, 3572, 102, 0, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  159\n",
            "BATCH_SIZE :  159\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  159\n",
            "Start training for  {'train': ['mule'], 'test': ['mule']} .....\n",
            ">>> epoch  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average training MAE loss: 6.75\n",
            "-\n",
            " Average eval MAE loss: 17.07\n",
            "===============================\n",
            "MAE:  16.703669\n",
            "MdAE:  16.327633\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 6.39\n",
            "-\n",
            " Average eval MAE loss: 9.69\n",
            "===============================\n",
            "MAE:  10.058205\n",
            "MdAE:  10.434607\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 7.59\n",
            "-\n",
            " Average eval MAE loss: 3.69\n",
            "===============================\n",
            "MAE:  3.6116757\n",
            "MdAE:  4.370677\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 3.55\n",
            "-\n",
            " Average eval MAE loss: 2.67\n",
            "===============================\n",
            "MAE:  2.8938346\n",
            "MdAE:  2.6166983\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 3.85\n",
            "-\n",
            " Average eval MAE loss: 5.79\n",
            "===============================\n",
            "MAE:  5.599339\n",
            "MdAE:  4.8498917\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 3.87\n",
            "-\n",
            " Average eval MAE loss: 2.40\n",
            "===============================\n",
            "MAE:  2.5938854\n",
            "MdAE:  1.6222594\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 2.96\n",
            "-\n",
            " Average eval MAE loss: 2.77\n",
            "===============================\n",
            "MAE:  2.8397224\n",
            "MdAE:  2.7729132\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 2.92\n",
            "-\n",
            " Average eval MAE loss: 2.36\n",
            "===============================\n",
            "MAE:  2.4987254\n",
            "MdAE:  2.93287\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.85\n",
            "-\n",
            " Average eval MAE loss: 2.39\n",
            "===============================\n",
            "MAE:  2.5276031\n",
            "MdAE:  2.8731008\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 2.69\n",
            "-\n",
            " Average eval MAE loss: 2.51\n",
            "===============================\n",
            "MAE:  2.6256475\n",
            "MdAE:  2.670171\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 2.71\n",
            "-\n",
            " Average eval MAE loss: 2.36\n",
            "===============================\n",
            "MAE:  2.497277\n",
            "MdAE:  2.9358683\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 2.77\n",
            "-\n",
            " Average eval MAE loss: 2.33\n",
            "===============================\n",
            "MAE:  2.4880302\n",
            "MdAE:  2.7236176\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 2.78\n",
            "-\n",
            " Average eval MAE loss: 2.66\n",
            "===============================\n",
            "MAE:  2.7486627\n",
            "MdAE:  2.5844407\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 2.89\n",
            "-\n",
            " Average eval MAE loss: 2.40\n",
            "===============================\n",
            "MAE:  2.5381455\n",
            "MdAE:  2.8512802\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 2.77\n",
            "-\n",
            " Average eval MAE loss: 2.38\n",
            "===============================\n",
            "MAE:  2.5209816\n",
            "MdAE:  2.8868048\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 2.78\n",
            "-\n",
            " Average eval MAE loss: 2.66\n",
            "===============================\n",
            "MAE:  2.747809\n",
            "MdAE:  2.582674\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 2.79\n",
            "-\n",
            " Average eval MAE loss: 2.33\n",
            "===============================\n",
            "MAE:  2.4919176\n",
            "MdAE:  2.6741898\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 2.80\n",
            "-\n",
            " Average eval MAE loss: 2.33\n",
            "===============================\n",
            "MAE:  2.478754\n",
            "MdAE:  2.8415565\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 2.77\n",
            "-\n",
            " Average eval MAE loss: 2.47\n",
            "===============================\n",
            "MAE:  2.5937705\n",
            "MdAE:  2.7361498\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 2.81\n",
            "-\n",
            " Average eval MAE loss: 2.51\n",
            "===============================\n",
            "MAE:  2.6257074\n",
            "MdAE:  2.6700473\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Support for request/reply\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 2490, 2005, 5227, 1013, 7514, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3685, 12324, 1037, 2996, 2622, 2013, 21025, 2102, 2302, 10697, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3431, 2000, 9262, 3642, 2079, 2025, 2131, 2980, 7333, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4039, 2000, 5587, 1037, 3433, 2043, 4526, 1037, 2117, 4834, 1999, 1996, 2168, 1049, 12314, 102, 0, 0, 0], [101, 3415, 15327, 2015, 1999, 1996, 20950, 3193, 2024, 2025, 2108, 3718, 2043, 2017, 6366, 2035, 1996, 3787, 1997, 102]]\n",
            "BATCH_SIZE :  131\n",
            "BATCH_SIZE :  131\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  131\n",
            "Start training for  {'train': ['mulestudio'], 'test': ['mulestudio']} .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 6.09\n",
            "-\n",
            " Average eval MAE loss: 13.51\n",
            "===============================\n",
            "MAE:  13.825926\n",
            "MdAE:  15.743349\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 6.65\n",
            "-\n",
            " Average eval MAE loss: 4.22\n",
            "===============================\n",
            "MAE:  3.7352636\n",
            "MdAE:  1.7850657\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 3.90\n",
            "-\n",
            " Average eval MAE loss: 6.57\n",
            "===============================\n",
            "MAE:  6.32836\n",
            "MdAE:  6.991596\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 5.17\n",
            "-\n",
            " Average eval MAE loss: 10.13\n",
            "===============================\n",
            "MAE:  9.162943\n",
            "MdAE:  7.2173653\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 5.87\n",
            "-\n",
            " Average eval MAE loss: 4.28\n",
            "===============================\n",
            "MAE:  3.865741\n",
            "MdAE:  2.2244186\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 3.89\n",
            "-\n",
            " Average eval MAE loss: 4.19\n",
            "===============================\n",
            "MAE:  3.651397\n",
            "MdAE:  2.4339266\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 3.60\n",
            "-\n",
            " Average eval MAE loss: 5.15\n",
            "===============================\n",
            "MAE:  4.493931\n",
            "MdAE:  2.0430646\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 3.15\n",
            "-\n",
            " Average eval MAE loss: 4.27\n",
            "===============================\n",
            "MAE:  3.8403313\n",
            "MdAE:  2.0278244\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.99\n",
            "-\n",
            " Average eval MAE loss: 4.64\n",
            "===============================\n",
            "MAE:  4.024625\n",
            "MdAE:  2.9904623\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 2.93\n",
            "-\n",
            " Average eval MAE loss: 4.20\n",
            "===============================\n",
            "MAE:  3.6912527\n",
            "MdAE:  2.1255693\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 3.01\n",
            "-\n",
            " Average eval MAE loss: 4.20\n",
            "===============================\n",
            "MAE:  3.6733112\n",
            "MdAE:  2.264381\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 2.77\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.5877652\n",
            "MdAE:  3.0215616\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 2.78\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6027048\n",
            "MdAE:  2.8106527\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 2.79\n",
            "-\n",
            " Average eval MAE loss: 4.22\n",
            "===============================\n",
            "MAE:  3.634401\n",
            "MdAE:  3.1270294\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 2.80\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6233633\n",
            "MdAE:  2.6508217\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 2.71\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6049588\n",
            "MdAE:  2.7932134\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 2.79\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6009388\n",
            "MdAE:  2.8243136\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 2.56\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6085217\n",
            "MdAE:  2.7656488\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 2.97\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6008475\n",
            "MdAE:  2.825019\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 2.68\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.6003788\n",
            "MdAE:  2.8286486\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  HDFS ItemWriter\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 10751, 10343, 8875, 15994, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 10751, 10343, 4563, 3015, 2393, 2121, 4280, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3149, 15584, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 10722, 10814, 2951, 3252, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 25353, 14540, 8649, 13749, 4355, 3258, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  634\n",
            "BATCH_SIZE :  634\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  634\n",
            "Start training for  {'train': ['springxd'], 'test': ['springxd']} .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 6.61\n",
            "-\n",
            " Average eval MAE loss: 13.01\n",
            "===============================\n",
            "MAE:  13.309196\n",
            "MdAE:  13.569466\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 8.72\n",
            "-\n",
            " Average eval MAE loss: 6.80\n",
            "===============================\n",
            "MAE:  7.1331015\n",
            "MdAE:  7.3575087\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 6.63\n",
            "-\n",
            " Average eval MAE loss: 7.60\n",
            "===============================\n",
            "MAE:  7.3249297\n",
            "MdAE:  7.045893\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 4.41\n",
            "-\n",
            " Average eval MAE loss: 2.58\n",
            "===============================\n",
            "MAE:  2.648204\n",
            "MdAE:  2.8017993\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 2.97\n",
            "-\n",
            " Average eval MAE loss: 3.20\n",
            "===============================\n",
            "MAE:  2.9187212\n",
            "MdAE:  2.6396847\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 2.74\n",
            "-\n",
            " Average eval MAE loss: 2.78\n",
            "===============================\n",
            "MAE:  2.8755703\n",
            "MdAE:  2.5034342\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 2.57\n",
            "-\n",
            " Average eval MAE loss: 1.93\n",
            "===============================\n",
            "MAE:  1.7502027\n",
            "MdAE:  1.3839216\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 2.21\n",
            "-\n",
            " Average eval MAE loss: 1.84\n",
            "===============================\n",
            "MAE:  1.7289528\n",
            "MdAE:  1.7589822\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.19\n",
            "-\n",
            " Average eval MAE loss: 1.80\n",
            "===============================\n",
            "MAE:  1.7373941\n",
            "MdAE:  1.9430647\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 2.26\n",
            "-\n",
            " Average eval MAE loss: 1.92\n",
            "===============================\n",
            "MAE:  1.8815957\n",
            "MdAE:  1.5715084\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 2.18\n",
            "-\n",
            " Average eval MAE loss: 1.92\n",
            "===============================\n",
            "MAE:  1.749106\n",
            "MdAE:  1.4032791\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 2.20\n",
            "-\n",
            " Average eval MAE loss: 2.13\n",
            "===============================\n",
            "MAE:  2.1303954\n",
            "MdAE:  2.0685577\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 2.34\n",
            "-\n",
            " Average eval MAE loss: 1.79\n",
            "===============================\n",
            "MAE:  1.7172084\n",
            "MdAE:  1.9662712\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 2.20\n",
            "-\n",
            " Average eval MAE loss: 1.85\n",
            "===============================\n",
            "MAE:  1.7991472\n",
            "MdAE:  1.7839484\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 2.16\n",
            "-\n",
            " Average eval MAE loss: 1.78\n",
            "===============================\n",
            "MAE:  1.7221644\n",
            "MdAE:  1.982306\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 2.21\n",
            "-\n",
            " Average eval MAE loss: 1.85\n",
            "===============================\n",
            "MAE:  1.8010174\n",
            "MdAE:  1.7791302\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 2.14\n",
            "-\n",
            " Average eval MAE loss: 1.83\n",
            "===============================\n",
            "MAE:  1.777959\n",
            "MdAE:  1.8385434\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 2.18\n",
            "-\n",
            " Average eval MAE loss: 1.80\n",
            "===============================\n",
            "MAE:  1.7475748\n",
            "MdAE:  1.9168327\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 2.12\n",
            "-\n",
            " Average eval MAE loss: 1.80\n",
            "===============================\n",
            "MAE:  1.7455398\n",
            "MdAE:  1.9220762\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 2.13\n",
            "-\n",
            " Average eval MAE loss: 1.81\n",
            "===============================\n",
            "MAE:  1.756457\n",
            "MdAE:  1.8939464\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  SQL Server Single Sign On Support doesn't work in data profiler repository connections\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 29296, 8241, 2309, 3696, 2006, 2490, 2987, 1005, 1056, 2147, 1999, 2951, 6337, 2099, 22409, 7264, 102, 0, 0], [101, 6366, 2048, 7753, 1999, 6075, 7251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4434, 4106, 1024, 2592, 1035, 8040, 28433, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3674, 4106, 20390, 2024, 1000, 3897, 2098, 2041, 1000, 1998, 3685, 2022, 2109, 102, 0, 0, 0, 0, 0], [101, 1000, 3193, 19528, 10281, 1000, 12183, 2515, 2025, 4653, 2006, 1037, 2795, 4106, 1006, 2007, 25410, 3627, 1007, 102]]\n",
            "BATCH_SIZE :  248\n",
            "BATCH_SIZE :  248\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  248\n",
            "Start training for  {'train': ['talenddataquality'], 'test': ['talenddataquality']} .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 6.99\n",
            "-\n",
            " Average eval MAE loss: 16.41\n",
            "===============================\n",
            "MAE:  19.226973\n",
            "MdAE:  20.632088\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 7.61\n",
            "-\n",
            " Average eval MAE loss: 4.35\n",
            "===============================\n",
            "MAE:  4.0921803\n",
            "MdAE:  4.086375\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 7.57\n",
            "-\n",
            " Average eval MAE loss: 4.87\n",
            "===============================\n",
            "MAE:  5.316209\n",
            "MdAE:  5.756601\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 4.14\n",
            "-\n",
            " Average eval MAE loss: 4.11\n",
            "===============================\n",
            "MAE:  2.8754363\n",
            "MdAE:  3.1370058\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 3.85\n",
            "-\n",
            " Average eval MAE loss: 4.70\n",
            "===============================\n",
            "MAE:  4.9090595\n",
            "MdAE:  5.2010326\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 3.72\n",
            "-\n",
            " Average eval MAE loss: 4.11\n",
            "===============================\n",
            "MAE:  2.9013276\n",
            "MdAE:  3.1901307\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 3.96\n",
            "-\n",
            " Average eval MAE loss: 4.61\n",
            "===============================\n",
            "MAE:  4.706889\n",
            "MdAE:  4.9251647\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 3.76\n",
            "-\n",
            " Average eval MAE loss: 4.02\n",
            "===============================\n",
            "MAE:  3.2862446\n",
            "MdAE:  3.0200748\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 3.82\n",
            "-\n",
            " Average eval MAE loss: 4.27\n",
            "===============================\n",
            "MAE:  3.902217\n",
            "MdAE:  3.8271627\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 4.08\n",
            "-\n",
            " Average eval MAE loss: 4.13\n",
            "===============================\n",
            "MAE:  3.5739825\n",
            "MdAE:  3.3792772\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 3.65\n",
            "-\n",
            " Average eval MAE loss: 4.47\n",
            "===============================\n",
            "MAE:  4.3660083\n",
            "MdAE:  4.460021\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 3.74\n",
            "-\n",
            " Average eval MAE loss: 4.13\n",
            "===============================\n",
            "MAE:  2.8168411\n",
            "MdAE:  3.0167775\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 3.94\n",
            "-\n",
            " Average eval MAE loss: 4.22\n",
            "===============================\n",
            "MAE:  3.772796\n",
            "MdAE:  3.6505642\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 3.69\n",
            "-\n",
            " Average eval MAE loss: 4.10\n",
            "===============================\n",
            "MAE:  3.5029857\n",
            "MdAE:  3.2823997\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 3.68\n",
            "-\n",
            " Average eval MAE loss: 4.22\n",
            "===============================\n",
            "MAE:  3.772992\n",
            "MdAE:  3.6508317\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 3.66\n",
            "-\n",
            " Average eval MAE loss: 4.13\n",
            "===============================\n",
            "MAE:  3.5553784\n",
            "MdAE:  3.353891\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 3.65\n",
            "-\n",
            " Average eval MAE loss: 4.08\n",
            "===============================\n",
            "MAE:  3.4494827\n",
            "MdAE:  3.2093925\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 3.72\n",
            "-\n",
            " Average eval MAE loss: 4.17\n",
            "===============================\n",
            "MAE:  3.65097\n",
            "MdAE:  3.4843292\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 3.70\n",
            "-\n",
            " Average eval MAE loss: 4.08\n",
            "===============================\n",
            "MAE:  3.4436803\n",
            "MdAE:  3.2014747\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 3.72\n",
            "-\n",
            " Average eval MAE loss: 4.08\n",
            "===============================\n",
            "MAE:  3.4407933\n",
            "MdAE:  3.1975355\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Investigation: S1 Improved user experience with TOS/TIS/ESB Studio\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 4812, 1024, 1055, 2487, 5301, 5310, 3325, 2007, 2000, 2015, 1013, 22320, 1013, 9686, 2497, 2996, 102, 0, 0], [101, 8556, 1024, 1055, 2475, 2449, 2326, 7375, 1998, 10802, 8819, 1999, 2000, 2015, 1013, 22320, 1013, 2996, 102, 0], [101, 8556, 1024, 1055, 2509, 5301, 20950, 2951, 8304, 6177, 1999, 2000, 2015, 1013, 22320, 1013, 2996, 102, 0, 0], [101, 8556, 1024, 1055, 2581, 22320, 19387, 2595, 2241, 2006, 1996, 2326, 4713, 102, 0, 0, 0, 0, 0, 0], [101, 8556, 1024, 21025, 2705, 12083, 9230, 2047, 6695, 1998, 7860, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  156\n",
            "BATCH_SIZE :  156\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  156\n",
            "Start training for  {'train': ['talendesb'], 'test': ['talendesb']} .....\n",
            ">>> epoch  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average training MAE loss: 4.52\n",
            "-\n",
            " Average eval MAE loss: 19.28\n",
            "===============================\n",
            "MAE:  19.480928\n",
            "MdAE:  19.527344\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 14.87\n",
            "-\n",
            " Average eval MAE loss: 14.73\n",
            "===============================\n",
            "MAE:  14.52593\n",
            "MdAE:  14.479952\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 6.85\n",
            "-\n",
            " Average eval MAE loss: 2.40\n",
            "===============================\n",
            "MAE:  2.2393188\n",
            "MdAE:  2.024942\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 1.97\n",
            "-\n",
            " Average eval MAE loss: 1.82\n",
            "===============================\n",
            "MAE:  1.6599326\n",
            "MdAE:  1.6655596\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 1.47\n",
            "-\n",
            " Average eval MAE loss: 1.19\n",
            "===============================\n",
            "MAE:  1.0040269\n",
            "MdAE:  0.6958624\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 1.27\n",
            "-\n",
            " Average eval MAE loss: 1.34\n",
            "===============================\n",
            "MAE:  1.1805524\n",
            "MdAE:  1.5782452\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 1.22\n",
            "-\n",
            " Average eval MAE loss: 1.09\n",
            "===============================\n",
            "MAE:  0.927117\n",
            "MdAE:  0.8617346\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 1.29\n",
            "-\n",
            " Average eval MAE loss: 1.24\n",
            "===============================\n",
            "MAE:  1.080674\n",
            "MdAE:  1.366308\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 1.14\n",
            "-\n",
            " Average eval MAE loss: 1.15\n",
            "===============================\n",
            "MAE:  0.9711895\n",
            "MdAE:  0.5422096\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 1.18\n",
            "-\n",
            " Average eval MAE loss: 1.40\n",
            "===============================\n",
            "MAE:  1.2339559\n",
            "MdAE:  1.6915646\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 1.25\n",
            "-\n",
            " Average eval MAE loss: 1.17\n",
            "===============================\n",
            "MAE:  0.9920477\n",
            "MdAE:  0.6090125\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 1.14\n",
            "-\n",
            " Average eval MAE loss: 1.28\n",
            "===============================\n",
            "MAE:  1.1184008\n",
            "MdAE:  1.4463625\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 1.16\n",
            "-\n",
            " Average eval MAE loss: 1.07\n",
            "===============================\n",
            "MAE:  0.9139669\n",
            "MdAE:  1.0125637\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 1.08\n",
            "-\n",
            " Average eval MAE loss: 1.07\n",
            "===============================\n",
            "MAE:  0.9109802\n",
            "MdAE:  0.9787265\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 1.08\n",
            "-\n",
            " Average eval MAE loss: 1.10\n",
            "===============================\n",
            "MAE:  0.93852705\n",
            "MdAE:  1.0646791\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 1.10\n",
            "-\n",
            " Average eval MAE loss: 1.10\n",
            "===============================\n",
            "MAE:  0.940784\n",
            "MdAE:  1.0694685\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 1.07\n",
            "-\n",
            " Average eval MAE loss: 1.07\n",
            "===============================\n",
            "MAE:  0.91153806\n",
            "MdAE:  1.0074098\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 1.11\n",
            "-\n",
            " Average eval MAE loss: 1.07\n",
            "===============================\n",
            "MAE:  0.91375154\n",
            "MdAE:  1.0121067\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 1.08\n",
            "-\n",
            " Average eval MAE loss: 1.13\n",
            "===============================\n",
            "MAE:  0.96868616\n",
            "MdAE:  1.1286755\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 1.07\n",
            "-\n",
            " Average eval MAE loss: 1.09\n",
            "===============================\n",
            "MAE:  0.934986\n",
            "MdAE:  1.0571651\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Android: While debugger is running, cannot back out and go back into an app\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 11924, 1024, 2096, 2139, 8569, 13327, 2003, 2770, 1010, 3685, 2067, 2041, 1998, 2175, 2067, 2046, 2019, 10439, 102], [101, 11924, 1024, 10439, 27774, 2196, 2579, 2013, 27339, 9397, 1012, 20950, 102, 0, 0, 0, 0, 0, 0, 0], [101, 11924, 1024, 3675, 5144, 2024, 3714, 2005, 3746, 8584, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 11924, 1024, 2516, 8237, 2003, 6913, 2043, 2440, 18182, 17624, 3898, 2003, 2109, 1012, 102, 0, 0, 0, 0], [101, 16380, 1024, 8011, 1998, 4530, 4949, 9231, 5754, 17287, 9285, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "BATCH_SIZE :  405\n",
            "BATCH_SIZE :  405\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  405\n",
            "Start training for  {'train': ['titanium'], 'test': ['titanium']} .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 7.35\n",
            "-\n",
            " Average eval MAE loss: 16.89\n",
            "===============================\n",
            "MAE:  17.041813\n",
            "MdAE:  17.242514\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 15.59\n",
            "-\n",
            " Average eval MAE loss: 15.26\n",
            "===============================\n",
            "MAE:  15.13311\n",
            "MdAE:  14.880337\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 8.63\n",
            "-\n",
            " Average eval MAE loss: 3.14\n",
            "===============================\n",
            "MAE:  3.0250025\n",
            "MdAE:  1.8830214\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 4.34\n",
            "-\n",
            " Average eval MAE loss: 2.96\n",
            "===============================\n",
            "MAE:  2.8406284\n",
            "MdAE:  1.532167\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 4.03\n",
            "-\n",
            " Average eval MAE loss: 2.32\n",
            "===============================\n",
            "MAE:  2.1793866\n",
            "MdAE:  1.5262527\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 4.01\n",
            "-\n",
            " Average eval MAE loss: 2.94\n",
            "===============================\n",
            "MAE:  2.816268\n",
            "MdAE:  1.5141902\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 4.22\n",
            "-\n",
            " Average eval MAE loss: 2.68\n",
            "===============================\n",
            "MAE:  2.5274434\n",
            "MdAE:  1.61954\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 3.97\n",
            "-\n",
            " Average eval MAE loss: 2.64\n",
            "===============================\n",
            "MAE:  2.5171034\n",
            "MdAE:  2.0834866\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 3.85\n",
            "-\n",
            " Average eval MAE loss: 2.28\n",
            "===============================\n",
            "MAE:  2.1404345\n",
            "MdAE:  1.6544824\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 3.89\n",
            "-\n",
            " Average eval MAE loss: 2.87\n",
            "===============================\n",
            "MAE:  2.748304\n",
            "MdAE:  1.6435223\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 4.01\n",
            "-\n",
            " Average eval MAE loss: 2.42\n",
            "===============================\n",
            "MAE:  2.2754126\n",
            "MdAE:  1.2101388\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 4.02\n",
            "-\n",
            " Average eval MAE loss: 2.39\n",
            "===============================\n",
            "MAE:  2.2605023\n",
            "MdAE:  2.4282131\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 3.87\n",
            "-\n",
            " Average eval MAE loss: 2.32\n",
            "===============================\n",
            "MAE:  2.1837924\n",
            "MdAE:  2.282238\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 3.90\n",
            "-\n",
            " Average eval MAE loss: 2.26\n",
            "===============================\n",
            "MAE:  2.1221259\n",
            "MdAE:  1.7147532\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 3.97\n",
            "-\n",
            " Average eval MAE loss: 2.47\n",
            "===============================\n",
            "MAE:  2.339879\n",
            "MdAE:  2.4207363\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 3.73\n",
            "-\n",
            " Average eval MAE loss: 2.21\n",
            "===============================\n",
            "MAE:  2.073164\n",
            "MdAE:  1.8759351\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 3.80\n",
            "-\n",
            " Average eval MAE loss: 2.33\n",
            "===============================\n",
            "MAE:  2.191272\n",
            "MdAE:  2.296471\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 3.89\n",
            "-\n",
            " Average eval MAE loss: 2.31\n",
            "===============================\n",
            "MAE:  2.1782587\n",
            "MdAE:  2.2717075\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 3.77\n",
            "-\n",
            " Average eval MAE loss: 2.24\n",
            "===============================\n",
            "MAE:  2.0991871\n",
            "MdAE:  2.1212378\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 3.73\n",
            "-\n",
            " Average eval MAE loss: 2.29\n",
            "===============================\n",
            "MAE:  2.158775\n",
            "MdAE:  2.234631\n",
            "all done for one project\n",
            "results have been written into a text file!\n",
            "n_embd/hidden_size :  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertSP were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.weight', 'bert.dense2.weight', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Asset data does not correctly obey contextual ownership like the entity\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0495575b8cdc>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "[[101, 11412, 2951, 2515, 2025, 11178, 15470, 6123, 8787, 6095, 2066, 1996, 9178, 102, 0, 0, 0, 0, 0, 0], [101, 14451, 25416, 21898, 19204, 2012, 1996, 2717, 7563, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2919, 20248, 23032, 5651, 2972, 3074, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4064, 5164, 2987, 1005, 1056, 6366, 2019, 9178, 3200, 1006, 19701, 2987, 1005, 1056, 2147, 2593, 1007, 102, 0], [101, 4840, 4748, 10020, 5310, 19204, 2180, 1005, 1056, 2147, 2006, 1013, 2968, 1013, 5198, 1013, 2033, 102, 0, 0]]\n",
            "BATCH_SIZE :  86\n",
            "BATCH_SIZE :  86\n",
            "usingbert tokenizer\n",
            "BATCH_SIZE :  86\n",
            "Start training for  {'train': ['usergrid'], 'test': ['usergrid']} .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> epoch  0\n",
            " Average training MAE loss: 4.76\n",
            "-\n",
            " Average eval MAE loss: 17.87\n",
            "===============================\n",
            "MAE:  17.96912\n",
            "MdAE:  17.91798\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 6.49\n",
            "-\n",
            " Average eval MAE loss: 0.99\n",
            "===============================\n",
            "MAE:  1.4555333\n",
            "MdAE:  1.4875145\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 2.61\n",
            "-\n",
            " Average eval MAE loss: 5.32\n",
            "===============================\n",
            "MAE:  5.4205475\n",
            "MdAE:  5.3690004\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 3.01\n",
            "-\n",
            " Average eval MAE loss: 1.70\n",
            "===============================\n",
            "MAE:  1.7143028\n",
            "MdAE:  1.5714598\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 1.73\n",
            "-\n",
            " Average eval MAE loss: 1.45\n",
            "===============================\n",
            "MAE:  1.5482124\n",
            "MdAE:  1.267483\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 1.44\n",
            "-\n",
            " Average eval MAE loss: 1.40\n",
            "===============================\n",
            "MAE:  1.788959\n",
            "MdAE:  1.1005278\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 1.15\n",
            "-\n",
            " Average eval MAE loss: 0.94\n",
            "===============================\n",
            "MAE:  1.286987\n",
            "MdAE:  0.5146842\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 1.08\n",
            "-\n",
            " Average eval MAE loss: 0.89\n",
            "===============================\n",
            "MAE:  1.2683158\n",
            "MdAE:  0.56405926\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 1.03\n",
            "-\n",
            " Average eval MAE loss: 1.11\n",
            "===============================\n",
            "MAE:  1.5537964\n",
            "MdAE:  1.3142138\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 1.02\n",
            "-\n",
            " Average eval MAE loss: 0.66\n",
            "===============================\n",
            "MAE:  1.1771702\n",
            "MdAE:  0.94845605\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 1.10\n",
            "-\n",
            " Average eval MAE loss: 0.63\n",
            "===============================\n",
            "MAE:  1.1666443\n",
            "MdAE:  0.9928479\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 0.94\n",
            "-\n",
            " Average eval MAE loss: 1.01\n",
            "===============================\n",
            "MAE:  1.4727081\n",
            "MdAE:  1.4572237\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 0.97\n",
            "-\n",
            " Average eval MAE loss: 0.66\n",
            "===============================\n",
            "MAE:  1.1760361\n",
            "MdAE:  0.9532392\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 0.93\n",
            "-\n",
            " Average eval MAE loss: 0.67\n",
            "===============================\n",
            "MAE:  1.1821533\n",
            "MdAE:  0.9274404\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 0.87\n",
            "-\n",
            " Average eval MAE loss: 0.84\n",
            "===============================\n",
            "MAE:  1.3398\n",
            "MdAE:  1.3083746\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 0.87\n",
            "-\n",
            " Average eval MAE loss: 0.63\n",
            "===============================\n",
            "MAE:  1.1653538\n",
            "MdAE:  0.99829054\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 0.86\n",
            "-\n",
            " Average eval MAE loss: 0.65\n",
            "===============================\n",
            "MAE:  1.1800805\n",
            "MdAE:  1.0266871\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 0.90\n",
            "-\n",
            " Average eval MAE loss: 0.76\n",
            "===============================\n",
            "MAE:  1.2723438\n",
            "MdAE:  1.1894064\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 0.90\n",
            "-\n",
            " Average eval MAE loss: 0.65\n",
            "===============================\n",
            "MAE:  1.1795188\n",
            "MdAE:  1.0256968\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 0.87\n",
            "-\n",
            " Average eval MAE loss: 0.70\n",
            "===============================\n",
            "MAE:  1.2217741\n",
            "MdAE:  1.10022\n",
            "all done for one project\n",
            "results have been written into a text file!\n"
          ]
        }
      ],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = True\n",
        "\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        # {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['bamboo'], 'test': ['bamboo']},\n",
        "                        {'train': ['clover'], 'test': ['clover']},\n",
        "                        {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
        "                        {'train': ['duracloud'], 'test': ['duracloud']},\n",
        "                        {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
        "                        {'train': ['mesos'], 'test': ['mesos']},\n",
        "                        {'train': ['moodle'], 'test': ['moodle']},\n",
        "                        {'train': ['mule'], 'test': ['mule']},\n",
        "                        {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
        "                        {'train': ['springxd'], 'test': ['springxd']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
        "                        {'train': ['talendesb'], 'test': ['talendesb']},\n",
        "                        {'train': ['titanium'], 'test': ['titanium']},\n",
        "                        {'train': ['usergrid'], 'test': ['usergrid']},\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        elif TOKENIZER == 'bert':\n",
        "            config = BertConfig(num_labels=1, pad_token_id=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'bert':\n",
        "            MODEL = BertSP.from_pretrained('bert-base-uncased', config=config)\n",
        "            MODEL.cuda()\n",
        "\n",
        "\n",
        "\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSte0lR_hfbo"
      },
      "source": [
        "### Cross Project Training Script - Within Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9vp2J2hfbo"
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# within repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
        "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
        "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
        "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMcxbMB7hfbp"
      },
      "source": [
        "### Cross Project Training Script - Cross Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35iJqeeNhfbp"
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# cross repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['clover'], 'test': ['usergrid']},\n",
        "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['mule'], 'test': ['titanium']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "conda-root-py",
      "name": "workbench-notebooks.m117",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}