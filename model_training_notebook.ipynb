{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1139368413.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source \"$HOME/.cargo/env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\n",
      "Collecting pyproject\n",
      "  Using cached pyproject-1.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (from pyproject) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "Installing collected packages: pyproject, transformers\n",
      "Successfully installed pyproject-1.3.1 transformers-4.39.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas transformers numpy tokenizers tensorboard pyproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
    "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece \n",
    "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
    "\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    "    print(tokens_train['input_ids'][:5])\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "    # tokenization\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'gpt2':\n",
    "        print('using pretrained gpt-2 tokenizer')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()  \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records)) \n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within Project Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 29.38 MiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 281.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m             OUTPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m MODEL_NAME \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2sp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     41\u001b[0m     MODEL \u001b[38;5;241m=\u001b[39m GPT2SP\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names \u001b[38;5;241m=\u001b[39m data_processing(file_pair\u001b[38;5;241m=\u001b[39mfile)\n\u001b[1;32m     44\u001b[0m train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2528\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2526\u001b[0m     )\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 29.38 MiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 281.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        # {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        # {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        # {'train': ['clover'], 'test': ['clover']},\n",
    "                        # {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        # {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        # {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        # {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        # {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        # {'train': ['mule'], 'test': ['mule']},\n",
    "                        # {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        # {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        # {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        # {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        # {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        # {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)    \n",
    "            \n",
    "                   \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "            \n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2136.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2SP.py                         \u001b[0m\u001b[01;34mlogo\u001b[0m/\n",
      "GPT2SP_inspection_notebook.ipynb  model_training_notebook.ipynb\n",
      "LICENSE                           \u001b[01;34mmodels\u001b[0m/\n",
      "README.md                         \u001b[01;34mresults\u001b[0m/\n",
      "\u001b[01;34m__pycache__\u001b[0m/                      \u001b[01;34msp_dataset\u001b[0m/\n",
      "\u001b[01;34mabe0\u001b[0m/                             \u001b[01;34mtb\u001b[0m/\n",
      "\u001b[01;34mall_tokenizers\u001b[0m/                   tokenizer_training_notebook.ipynb\n",
      "\u001b[01;34mcorpus_tokenization_comparison\u001b[0m/   vocab_and_tokenization_comparison.ipynb\n",
      "\u001b[01;34mcustom_transformers_interpret\u001b[0m/    \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mdata_model_analysis\u001b[0m/              \u001b[01;34mxai_tokens\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9833680987358093}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "def get_gpt2sp_pipeline(model: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    # model = \"MickyMike/0-GPT2SP-\" + model.lower()\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained('gpt2',config=config)\n",
    "    state_dict = torch.load(\"./models/gpt2sp_base_appceleratorstudio_appceleratorstudio_epo_7.pth\",map_location='cpu')\n",
    "    gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return pipeline(task=\"text-classification\", model=gpt2sp, tokenizer=tokenizer, device=DEVICE )\n",
    "\n",
    "\n",
    "def predict_sp(estimator: Pipeline, given_title: str) -> dict:\n",
    "    res= estimator(given_title)\n",
    "    # return round(.item(), 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "pipeline = get_gpt2sp_pipeline(\"\")\n",
    "story_point = predict_sp(pipeline, \"new mobile projects can'\\t find appicon.jpg resulting in ' error detected' shown on TiApp pane\")\n",
    "\n",
    "print(story_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9862019419670105}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "def get_gpt2sp_pipeline(model: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    # model = \"MickyMike/0-GPT2SP-\" + model.lower()\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained('MickyMike/0-GPT2SP-appceleratorstudio',config=config)\n",
    "    # state_dict = torch.load(\"./models/gpt2sp_base_appceleratorstudio_appceleratorstudio_epo_7.pth\",map_location='cpu')\n",
    "    # gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    return pipeline(task=\"text-classification\", model=gpt2sp, tokenizer=tokenizer, device=DEVICE )\n",
    "\n",
    "\n",
    "def predict_sp(estimator: Pipeline, given_title: str) -> dict:\n",
    "    res= estimator(given_title)\n",
    "    # return round(.item(), 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "pipeline = get_gpt2sp_pipeline(\"\")\n",
    "story_point = predict_sp(pipeline, \"new mobile projects can'\\t find appicon.jpg resulting in ' error detected' shown on TiApp pane\")\n",
    "\n",
    "print(story_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MickyMike/0-GPT2SP-appceleratorstudio were not used when initializing GPT2ForSequenceClassification: ['dense1.weight', 'dense2.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5239192247390747}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"MickyMike/0-GPT2SP-appceleratorstudio\")\n",
    "res = pipe(\"Add preference for default settings on android run/debug configurations (API & Screen size)\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum) (3.8.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.25.2)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum) (2.2.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum) (4.66.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->captum) (12.3.101)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top token : studio\n",
      "tensor([[3.7373]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "from transformers import PreTrainedTokenizer\n",
    "from custom_transformers_interpret import  SequenceClassificationExplainer\n",
    "\n",
    "\n",
    "def get_top_token(token_attributions: list) -> list:\n",
    "    # word_attributions have a shape of [('word', 0.1234), ...]\n",
    "    top_index = 0\n",
    "    top_value = None\n",
    "    for i in range(len(token_attributions)):\n",
    "        if top_value is None or token_attributions[i][1] > top_value:\n",
    "            top_value = token_attributions[i][1]\n",
    "            top_index = i\n",
    "    return [str(token_attributions[top_index][0])]\n",
    "\n",
    "\n",
    "def get_gpt2sp_pipeline(text: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    model = 'gpt2' #\"MickyMike/0-GPT2SP-appceleratorstudio\"\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained(model,config=config)\n",
    "    state_dict = torch.load(\"./models/gpt2sp_base_appceleratorstudio_appceleratorstudio_epo_7.pth\")\n",
    "    gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    # gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer:PreTrainedTokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    input_ids = tokenizer(text,return_tensors=\"pt\")\n",
    "    # input_ids.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outs = gpt2sp(**input_ids)\n",
    "\n",
    "    explainer = SequenceClassificationExplainer(gpt2sp,tokenizer)\n",
    "    word_att = explainer(text)\n",
    "    top_token = get_top_token(word_att)\n",
    "    print(\"top token :\",str(top_token[0]))\n",
    "\n",
    "    return outs\n",
    "\n",
    "outs = get_gpt2sp_pipeline(\"Update Hyperloop documentation link in the studio\")\n",
    "\n",
    "\n",
    "print(outs.logits)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# within repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# cross repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GPT2SP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
