{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1139368413.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source \"$HOME/.cargo/env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: pyproject in /opt/conda/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (from pyproject) (3.1.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.42.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas transformers numpy tokenizers tensorboard pyproject wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 392e817af43c45fef2953b58c84ebb95d7dd31b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
    "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.03 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece \n",
    "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
    "\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    "    print(tokens_train['input_ids'][:5])\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "    # tokenization\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'gpt2':\n",
    "        print('using pretrained gpt-2 tokenizer')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': (data['title'] + \" | \" + data[\"description\"] ).tolist(), 'label': data['storypoint']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global MODEL_NAME, TOKENIZER, LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE , SEQUENCE_LEN , BATCH_SIZE_RATIO\n",
    "\n",
    "    wandb.init(\n",
    "                # set the wandb project where this run will be logged\n",
    "                project = \"esti-mate\",\n",
    "                name = f\"{MODEL_NAME}_{file_pair['train'][0]}\",\n",
    "                tags = [\"RMSLELoss\",\"concat\"],\n",
    "\n",
    "                # track hyperparameters and run metadata\n",
    "                config={\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"sequence_len\": SEQUENCE_LEN,\n",
    "                \"batch_size_ratio\":BATCH_SIZE_RATIO,\n",
    "                \"tokenizer\":TOKENIZER,\n",
    "                \"model_name\":MODEL_NAME,\n",
    "                \"description_added\":True,\n",
    "                \"epochs\": EPOCHS,\n",
    "                'data_set':file_pair[\"train\"][0]\n",
    "                }\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()  \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        wandb.log({f'train_loss':avg_train_loss},step=e)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                result = model(b_input_ids, \n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        wandb.log({f'eval_loss':avg_eval_loss},step=e)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records)) \n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "\n",
    "    wandb.log({\"best_MAE\": MAE_RECORDS[min_eval_loss_epoch[1]],\"best_MdAE\": MDAE_RECORDS[min_eval_loss_epoch[1]] , \"best_MAR_train_time\":time_records[min_eval_loss_epoch[1]]  })\n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within Project Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_5009/4183117133.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[4550, 7257, 1028, 2134, 4187, 874, 287, 2163, 800, 20968, 930, 1391, 6494, 92, 27, 7146, 6927, 79, 29, 464, 2126, 994, 318, 326, 611, 674, 20150, 23007, 257, 2099, 355, 2163, 1822, 11, 356, 815, 307, 1498, 284, 2251, 281, 4554, 286, 326, 2099, 355, 281, 2134, 18875, 355, 281, 1822, 284, 257, 2163, 43219, 13, 1114, 1672, 25, 3556, 79, 29, 1279, 3866, 29, 1279, 8189, 29, 40533, 13, 10080, 13, 17953, 33986, 7, 1391, 1222, 2528, 26, 26745, 12, 6888, 12, 1456, 5, 13655, 26, 1782, 5619, 3556, 8189, 29, 7359, 3866, 12240, 7146, 29, 90, 6494], [10260, 24150, 329, 2034, 7015, 1352, 13877, 284, 2034, 7015, 1352, 11112, 220, 930, 1391, 6494, 92, 27, 7146, 6927, 79, 29, 2953, 1551, 4259, 3895, 17149, 11, 3917, 299, 6691, 13, 8673, 584, 8947, 355, 880, 25970, 79, 12240, 7146, 29, 90, 6494, 92, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 649, 19449, 32815, 329, 26144, 1074, 930, 1391, 6494, 92, 27, 7146, 6927, 79, 29, 16447, 19449, 32815, 7268, 6608, 2672, 329, 1774, 7257, 13, 7157, 32815, 284, 19193, 1074, 284, 5072, 649, 34165, 14, 40386, 3696, 355, 636, 286, 649, 26144, 10050, 13, 632, 743, 307, 326, 2427, 356, 460, 779, 262, 3691, 15390, 5794, 11, 287, 543, 1339, 356, 481, 761, 284, 7603, 284, 262, 26144, 1074, 3224, 3709, 284, 307, 2087, 284, 262, 34165, 25970, 79, 12240, 7146, 29, 90, 6494, 92, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 4935, 31458, 14161, 7873, 930, 1391, 6494, 92, 27, 7146, 6927, 79, 29, 16447, 3119, 2443, 329, 1628, 543, 3578, 17512, 286, 14553, 10288, 13, 11787, 460, 751, 393, 4781, 10288, 422, 262, 29905, 393, 10289, 357, 361, 1744, 737, 3556, 79, 29, 1279, 79, 29, 1858, 743, 880, 307, 281, 4920, 23094, 393, 4683, 2443, 287, 19599, 11, 28288, 11, 449, 10305, 51, 393, 8057, 13, 7488, 10055, 302, 3500, 428, 2443, 318, 1744, 393, 5035, 25970, 79, 12240, 7146, 29, 90, 6494, 92, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3791, 27850, 4935, 16884, 930, 1391, 6494, 92, 27, 7146, 6927, 79, 29, 36881, 357, 31227, 284, 10385, 4683, 1628, 6282, 2438, 284, 7349, 737, 2773, 3709, 994, 25, 1279, 64, 13291, 28, 366, 5450, 1378, 12567, 13, 785, 14, 1324, 7015, 1352, 14, 83, 270, 15776, 62, 16244, 263, 14, 2436, 672, 14, 9866, 14, 33236, 14, 8457, 14, 16302, 13, 8457, 2, 43, 43697, 5320, 3740, 1378, 12567, 13, 785, 14, 1324, 7015, 1352, 14, 83, 270, 15776, 62, 16244, 263, 14, 2436, 672, 14, 9866, 14, 4965, 78, 986, 3556, 64, 28401, 12691, 994, 25, 1279, 64]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseniyas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/gpt2sp/wandb/run-20240330_142519-3s6bw512</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seniyas/esti-mate/runs/3s6bw512' target=\"_blank\">gpt2sp_appceleratorstudio</a></strong> to <a href='https://wandb.ai/seniyas/esti-mate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seniyas/esti-mate' target=\"_blank\">https://wandb.ai/seniyas/esti-mate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seniyas/esti-mate/runs/3s6bw512' target=\"_blank\">https://wandb.ai/seniyas/esti-mate/runs/3s6bw512</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 6.97\n",
      "-\n",
      " Average eval MAE loss: 1.48\n",
      "===============================\n",
      "MAE:  1.512077\n",
      "MdAE:  1.0917227\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  2.020484\n",
      "MdAE:  1.8081338\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.49\n",
      "-\n",
      " Average eval MAE loss: 1.78\n",
      "===============================\n",
      "MAE:  1.7960339\n",
      "MdAE:  1.5534604\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.08\n",
      "-\n",
      " Average eval MAE loss: 1.83\n",
      "===============================\n",
      "MAE:  1.7795147\n",
      "MdAE:  1.4966741\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.90\n",
      "-\n",
      " Average eval MAE loss: 2.47\n",
      "===============================\n",
      "MAE:  2.4823313\n",
      "MdAE:  2.2366526\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.67\n",
      "-\n",
      " Average eval MAE loss: 1.75\n",
      "===============================\n",
      "MAE:  1.7886248\n",
      "MdAE:  1.4323916\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.71\n",
      "-\n",
      " Average eval MAE loss: 1.79\n",
      "===============================\n",
      "MAE:  1.844916\n",
      "MdAE:  1.4966317\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.47\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  1.9021661\n",
      "MdAE:  1.5855639\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.33\n",
      "-\n",
      " Average eval MAE loss: 1.94\n",
      "===============================\n",
      "MAE:  1.939563\n",
      "MdAE:  1.6030233\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.18\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "MAE:  2.0875823\n",
      "MdAE:  1.7745366\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.10\n",
      "-\n",
      " Average eval MAE loss: 2.29\n",
      "===============================\n",
      "MAE:  2.328578\n",
      "MdAE:  2.1963098\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.95\n",
      "-\n",
      " Average eval MAE loss: 1.80\n",
      "===============================\n",
      "MAE:  1.8649708\n",
      "MdAE:  1.6287653\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.88\n",
      "-\n",
      " Average eval MAE loss: 1.91\n",
      "===============================\n",
      "MAE:  1.9808383\n",
      "MdAE:  1.7869099\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.77\n",
      "-\n",
      " Average eval MAE loss: 1.84\n",
      "===============================\n",
      "MAE:  1.9254514\n",
      "MdAE:  1.5653241\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.78\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  2.1227906\n",
      "MdAE:  1.8528514\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.66\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  2.0004811\n",
      "MdAE:  1.7624283\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.63\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  2.1043096\n",
      "MdAE:  1.97599\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.58\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  2.0466175\n",
      "MdAE:  1.8737543\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.52\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  2.1347063\n",
      "MdAE:  1.9863216\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  2.0981262\n",
      "MdAE:  1.9393576\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m             OUTPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     MODEL\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     46\u001b[0m file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names \u001b[38;5;241m=\u001b[39m data_processing(file_pair\u001b[38;5;241m=\u001b[39mfile)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrain_eval_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_test_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_file_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m MODEL\n\u001b[1;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()            \n",
      "Cell \u001b[0;32mIn[3], line 302\u001b[0m, in \u001b[0;36mtrain_eval_test\u001b[0;34m(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# select model\u001b[39;00m\n\u001b[1;32m    298\u001b[0m os\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/epo_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(min_eval_loss_epoch[\u001b[38;5;241m1\u001b[39m]), \n\u001b[1;32m    299\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m    300\u001b[0m           \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_epo_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(min_eval_loss_epoch[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mlog_to_wandb({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_MAE\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAE_RECORDS[min_eval_loss_epoch[\u001b[38;5;241m1\u001b[39m]],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_MdAE\u001b[39m\u001b[38;5;124m\"\u001b[39m: MDAE_RECORDS[min_eval_loss_epoch[\u001b[38;5;241m1\u001b[39m]] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_MAR_train_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_records[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_eval_loss_epoch[\u001b[38;5;241m1\u001b[39m]]  })\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# del unwanted models\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        # {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        # {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        # {'train': ['clover'], 'test': ['clover']},\n",
    "                        # {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        # {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        # {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        # {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        # {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        # {'train': ['mule'], 'test': ['mule']},\n",
    "                        # {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        # {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        # {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        # {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        # {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        # {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)    \n",
    "            \n",
    "                   \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "            \n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2136.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2SP.py                         \u001b[0m\u001b[01;34mlogo\u001b[0m/\n",
      "GPT2SP_inspection_notebook.ipynb  model_training_notebook.ipynb\n",
      "LICENSE                           \u001b[01;34mmodels\u001b[0m/\n",
      "README.md                         \u001b[01;34mresults\u001b[0m/\n",
      "\u001b[01;34m__pycache__\u001b[0m/                      \u001b[01;34msp_dataset\u001b[0m/\n",
      "\u001b[01;34mabe0\u001b[0m/                             \u001b[01;34mtb\u001b[0m/\n",
      "\u001b[01;34mall_tokenizers\u001b[0m/                   tokenizer_training_notebook.ipynb\n",
      "\u001b[01;34mcorpus_tokenization_comparison\u001b[0m/   vocab_and_tokenization_comparison.ipynb\n",
      "\u001b[01;34mcustom_transformers_interpret\u001b[0m/    \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mdata_model_analysis\u001b[0m/              \u001b[01;34mxai_tokens\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9833680987358093}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "def get_gpt2sp_pipeline(model: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    # model = \"MickyMike/0-GPT2SP-\" + model.lower()\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained('gpt2',config=config)\n",
    "    state_dict = torch.load(\"./models/gpt2sp_base_appceleratorstudio_appceleratorstudio_epo_7.pth\",map_location='cpu')\n",
    "    gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return pipeline(task=\"text-classification\", model=gpt2sp, tokenizer=tokenizer, device=DEVICE )\n",
    "\n",
    "\n",
    "def predict_sp(estimator: Pipeline, given_title: str) -> dict:\n",
    "    res= estimator(given_title)\n",
    "    # return round(.item(), 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "pipeline = get_gpt2sp_pipeline(\"\")\n",
    "story_point = predict_sp(pipeline, \"new mobile projects can'\\t find appicon.jpg resulting in ' error detected' shown on TiApp pane\")\n",
    "\n",
    "print(story_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9862019419670105}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "def get_gpt2sp_pipeline(model: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    # model = \"MickyMike/0-GPT2SP-\" + model.lower()\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained('MickyMike/0-GPT2SP-appceleratorstudio',config=config)\n",
    "    # state_dict = torch.load(\"./models/gpt2sp_base_appceleratorstudio_appceleratorstudio_epo_7.pth\",map_location='cpu')\n",
    "    # gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    return pipeline(task=\"text-classification\", model=gpt2sp, tokenizer=tokenizer, device=DEVICE )\n",
    "\n",
    "\n",
    "def predict_sp(estimator: Pipeline, given_title: str) -> dict:\n",
    "    res= estimator(given_title)\n",
    "    # return round(.item(), 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "pipeline = get_gpt2sp_pipeline(\"\")\n",
    "story_point = predict_sp(pipeline, \"new mobile projects can'\\t find appicon.jpg resulting in ' error detected' shown on TiApp pane\")\n",
    "\n",
    "print(story_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MickyMike/0-GPT2SP-appceleratorstudio were not used when initializing GPT2ForSequenceClassification: ['dense1.weight', 'dense2.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5239192247390747}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"MickyMike/0-GPT2SP-appceleratorstudio\")\n",
    "res = pipe(\"Add preference for default settings on android run/debug configurations (API & Screen size)\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum) (3.8.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.25.2)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum) (2.2.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum) (4.66.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->captum) (12.3.101)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top token : additions\n",
      "tensor([[6.7372]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline,Pipeline\n",
    "from transformers import PreTrainedTokenizer\n",
    "from custom_transformers_interpret import  SequenceClassificationExplainer\n",
    "\n",
    "\n",
    "def get_top_token(token_attributions: list) -> list:\n",
    "    # word_attributions have a shape of [('word', 0.1234), ...]\n",
    "    top_index = 0\n",
    "    top_value = None\n",
    "    for i in range(len(token_attributions)):\n",
    "        if top_value is None or token_attributions[i][1] > top_value:\n",
    "            top_value = token_attributions[i][1]\n",
    "            top_index = i\n",
    "    return [str(token_attributions[top_index][0])]\n",
    "\n",
    "\n",
    "def get_gpt2sp_pipeline(text: str) -> Pipeline:\n",
    "    global DEVICE\n",
    "    model = 'gpt2' #\"MickyMike/0-GPT2SP-appceleratorstudio\"\n",
    "    config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "    gpt2sp = GPT2SP.from_pretrained(model,config=config)\n",
    "    state_dict = torch.load(\"models/epo_2\")\n",
    "    gpt2sp.load_state_dict(state_dict=state_dict ,strict=False  )\n",
    "    # gpt2sp.to(DEVICE)\n",
    "    gpt2sp.eval()\n",
    "\n",
    "    tokenizer:PreTrainedTokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    \n",
    "    input_ids = tokenizer(text,return_tensors=\"pt\")\n",
    "    # input_ids.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outs = gpt2sp(**input_ids)\n",
    "\n",
    "    explainer = SequenceClassificationExplainer(gpt2sp,tokenizer)\n",
    "    word_att = explainer(text)\n",
    "    top_token = get_top_token(word_att)\n",
    "    print(\"top token :\",str(top_token[0]))\n",
    "\n",
    "    return outs\n",
    "\n",
    "outs = get_gpt2sp_pipeline(\"Create test plan for tiapp.xml module additions\")\n",
    "\n",
    "\n",
    "print(outs.logits)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# within repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# cross repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GPT2SP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
