{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: pandas===1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: koila in /opt/conda/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas===1.5.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas===1.5.3) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from koila) (11.5.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from koila) (13.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->koila) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->koila) (2.17.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->koila) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch pandas===1.5.3 transformers numpy tokenizers koila tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
    "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME , ADD_DESCRIPTION\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.1 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece \n",
    "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
    "ADD_DESCRIPTION = False\n",
    "\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT = '  '\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    "    print(tokens_train['input_ids'][:5])\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "    # tokenization\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'gpt2':\n",
    "        print('using pretrained gpt-2 tokenizer')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    \n",
    "\n",
    "    if ADD_DESCRIPTION :\n",
    "      print(\"### text : title+description\")\n",
    "      d = {'text': (data['title'] + \" : \" + data[\"description\"]).tolist(), 'label': data['storypoint']}\n",
    "    else:\n",
    "      print(\"### text : title\")\n",
    "      d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    print(\"Input data feed ::: \",d['text'][0])\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()  \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records)) \n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE) + '\\n'\n",
    "    global ADD_DESCRIPTION\n",
    "    OUTPUT += 'Description added : ' + str(ADD_DESCRIPTION) + '\\n'\n",
    "    \n",
    "    \n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within Project Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Add CA against object literals in function invocations\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[4550, 7257, 1028, 2134, 4187, 874, 287, 2163, 800, 20968, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [10260, 24150, 329, 2034, 7015, 1352, 13877, 284, 2034, 7015, 1352, 11112, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 649, 19449, 32815, 329, 26144, 1074, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 4935, 31458, 14161, 7873, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3791, 27850, 4935, 16884, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 29.58\n",
      "-\n",
      " Average eval MAE loss: 11.26\n",
      "===============================\n",
      "MAE:  10.987148\n",
      "MdAE:  11.080142\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.32\n",
      "-\n",
      " Average eval MAE loss: 2.97\n",
      "===============================\n",
      "MAE:  2.7040088\n",
      "MdAE:  2.5397143\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.61\n",
      "-\n",
      " Average eval MAE loss: 2.85\n",
      "===============================\n",
      "MAE:  3.1570585\n",
      "MdAE:  3.041329\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.25\n",
      "-\n",
      " Average eval MAE loss: 2.52\n",
      "===============================\n",
      "MAE:  2.3036551\n",
      "MdAE:  1.8241332\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.70\n",
      "-\n",
      " Average eval MAE loss: 2.75\n",
      "===============================\n",
      "MAE:  2.4968967\n",
      "MdAE:  2.2525737\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 1.57\n",
      "===============================\n",
      "MAE:  1.4605458\n",
      "MdAE:  1.1322608\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.61\n",
      "-\n",
      " Average eval MAE loss: 2.34\n",
      "===============================\n",
      "MAE:  2.087717\n",
      "MdAE:  1.9854324\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.89\n",
      "-\n",
      " Average eval MAE loss: 1.98\n",
      "===============================\n",
      "MAE:  1.7706888\n",
      "MdAE:  1.6635478\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.04\n",
      "-\n",
      " Average eval MAE loss: 2.50\n",
      "===============================\n",
      "MAE:  2.2727308\n",
      "MdAE:  2.0818863\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.91\n",
      "-\n",
      " Average eval MAE loss: 2.38\n",
      "===============================\n",
      "MAE:  2.1817021\n",
      "MdAE:  1.9254222\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.81\n",
      "-\n",
      " Average eval MAE loss: 2.49\n",
      "===============================\n",
      "MAE:  2.277921\n",
      "MdAE:  2.0741777\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.82\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  1.8778718\n",
      "MdAE:  1.6818733\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.54\n",
      "-\n",
      " Average eval MAE loss: 1.98\n",
      "===============================\n",
      "MAE:  1.8307195\n",
      "MdAE:  1.5718935\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.48\n",
      "-\n",
      " Average eval MAE loss: 2.59\n",
      "===============================\n",
      "MAE:  2.4094853\n",
      "MdAE:  2.1362314\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.55\n",
      "-\n",
      " Average eval MAE loss: 2.52\n",
      "===============================\n",
      "MAE:  2.3517668\n",
      "MdAE:  2.0749905\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.30\n",
      "-\n",
      " Average eval MAE loss: 2.24\n",
      "===============================\n",
      "MAE:  2.055238\n",
      "MdAE:  1.6996229\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 2.12\n",
      "===============================\n",
      "MAE:  1.9251834\n",
      "MdAE:  1.6735227\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 2.47\n",
      "===============================\n",
      "MAE:  2.239064\n",
      "MdAE:  1.9286497\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.22\n",
      "-\n",
      " Average eval MAE loss: 2.23\n",
      "===============================\n",
      "MAE:  2.0333345\n",
      "MdAE:  1.7531259\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.19\n",
      "-\n",
      " Average eval MAE loss: 2.25\n",
      "===============================\n",
      "MAE:  2.0368395\n",
      "MdAE:  1.7166339\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Add Copy URL actions to right-click context menu of Remote view for S3 files\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[4550, 17393, 10289, 4028, 284, 826, 12, 12976, 4732, 6859, 286, 21520, 1570, 329, 311, 18, 3696, 50256, 50256, 50256], [32, 457, 2271, 8404, 284, 1280, 257, 649, 4554, 286, 2346, 618, 4756, 3696, 2884, 3964, 19142, 50256, 50256, 50256], [19746, 3342, 46207, 318, 3402, 2354, 262, 3159, 13215, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [11909, 47, 5231, 42829, 24547, 329, 2560, 5050, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [6187, 16406, 281, 2939, 656, 262, 11532, 5464, 815, 2251, 281, 2939, 7621, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['aptanastudio'], 'test': ['aptanastudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 12.39\n",
      "-\n",
      " Average eval MAE loss: 8.68\n",
      "===============================\n",
      "MAE:  6.8016005\n",
      "MdAE:  4.7819023\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 7.47\n",
      "-\n",
      " Average eval MAE loss: 5.06\n",
      "===============================\n",
      "MAE:  3.7691936\n",
      "MdAE:  2.853842\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 4.44\n",
      "-\n",
      " Average eval MAE loss: 3.56\n",
      "===============================\n",
      "MAE:  3.752133\n",
      "MdAE:  3.5974903\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.24\n",
      "-\n",
      " Average eval MAE loss: 3.67\n",
      "===============================\n",
      "MAE:  3.9397628\n",
      "MdAE:  3.60608\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.89\n",
      "-\n",
      " Average eval MAE loss: 5.38\n",
      "===============================\n",
      "MAE:  4.108749\n",
      "MdAE:  2.9844902\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.91\n",
      "-\n",
      " Average eval MAE loss: 3.87\n",
      "===============================\n",
      "MAE:  4.214387\n",
      "MdAE:  3.3009753\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.90\n",
      "-\n",
      " Average eval MAE loss: 4.28\n",
      "===============================\n",
      "MAE:  4.4876394\n",
      "MdAE:  3.350463\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.37\n",
      "-\n",
      " Average eval MAE loss: 4.56\n",
      "===============================\n",
      "MAE:  3.76555\n",
      "MdAE:  2.6710098\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.40\n",
      "-\n",
      " Average eval MAE loss: 4.17\n",
      "===============================\n",
      "MAE:  4.4359717\n",
      "MdAE:  3.4288063\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 2.29\n",
      "-\n",
      " Average eval MAE loss: 4.09\n",
      "===============================\n",
      "MAE:  4.633038\n",
      "MdAE:  3.8298016\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 2.35\n",
      "-\n",
      " Average eval MAE loss: 4.47\n",
      "===============================\n",
      "MAE:  3.992039\n",
      "MdAE:  2.906571\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.70\n",
      "-\n",
      " Average eval MAE loss: 4.00\n",
      "===============================\n",
      "MAE:  4.447366\n",
      "MdAE:  3.4456835\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.55\n",
      "-\n",
      " Average eval MAE loss: 3.99\n",
      "===============================\n",
      "MAE:  4.333024\n",
      "MdAE:  3.576809\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.28\n",
      "-\n",
      " Average eval MAE loss: 4.14\n",
      "===============================\n",
      "MAE:  4.3668475\n",
      "MdAE:  3.4585295\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 4.14\n",
      "===============================\n",
      "MAE:  4.1175566\n",
      "MdAE:  2.9709408\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.10\n",
      "-\n",
      " Average eval MAE loss: 3.88\n",
      "===============================\n",
      "MAE:  3.99591\n",
      "MdAE:  2.8927076\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.15\n",
      "-\n",
      " Average eval MAE loss: 3.92\n",
      "===============================\n",
      "MAE:  3.986185\n",
      "MdAE:  2.8434258\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.96\n",
      "-\n",
      " Average eval MAE loss: 4.07\n",
      "===============================\n",
      "MAE:  4.1313095\n",
      "MdAE:  2.9274545\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.93\n",
      "-\n",
      " Average eval MAE loss: 3.91\n",
      "===============================\n",
      "MAE:  3.9140184\n",
      "MdAE:  2.7444806\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.89\n",
      "-\n",
      " Average eval MAE loss: 3.95\n",
      "===============================\n",
      "MAE:  3.9838278\n",
      "MdAE:  2.8637493\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Allows CVS repo to timeout and report on locking issues\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[34934, 327, 20304, 29924, 284, 26827, 290, 989, 319, 22656, 2428, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [12154, 257, 1382, 284, 307, 4624, 379, 262, 1182, 286, 262, 1382, 16834, 986, 357, 273, 4370, 262, 16834, 1502], [23004, 407, 7448, 618, 28006, 10143, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [25685, 1891, 422, 27468, 5797, 284, 37252, 4382, 284, 2291, 412, 4462, 6115, 3817, 2482, 50256, 50256, 50256, 50256, 50256], [14490, 460, 766, 262, 285, 4005, 8265, 357, 8094, 312, 11, 24127, 312, 11, 2196, 8, 1028, 1123, 1410, 13]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['bamboo'], 'test': ['bamboo']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 14.97\n",
      "-\n",
      " Average eval MAE loss: 2.95\n",
      "===============================\n",
      "MAE:  2.9147785\n",
      "MdAE:  3.0826058\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.72\n",
      "-\n",
      " Average eval MAE loss: 3.47\n",
      "===============================\n",
      "MAE:  3.4716907\n",
      "MdAE:  3.317344\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.97\n",
      "-\n",
      " Average eval MAE loss: 1.06\n",
      "===============================\n",
      "MAE:  1.0819849\n",
      "MdAE:  0.8899708\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.45\n",
      "-\n",
      " Average eval MAE loss: 0.79\n",
      "===============================\n",
      "MAE:  0.90542656\n",
      "MdAE:  0.93209827\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.44\n",
      "-\n",
      " Average eval MAE loss: 0.67\n",
      "===============================\n",
      "MAE:  0.762667\n",
      "MdAE:  0.6214129\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.35\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  1.048762\n",
      "MdAE:  0.9692991\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.29\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  1.0059149\n",
      "MdAE:  0.9754555\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.11\n",
      "-\n",
      " Average eval MAE loss: 1.41\n",
      "===============================\n",
      "MAE:  1.2593452\n",
      "MdAE:  1.1155174\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.18\n",
      "-\n",
      " Average eval MAE loss: 1.26\n",
      "===============================\n",
      "MAE:  1.0573198\n",
      "MdAE:  0.94305325\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 0.83\n",
      "===============================\n",
      "MAE:  0.84214216\n",
      "MdAE:  0.7688656\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.47\n",
      "-\n",
      " Average eval MAE loss: 0.79\n",
      "===============================\n",
      "MAE:  0.799628\n",
      "MdAE:  0.6169369\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 1.02\n",
      "===============================\n",
      "MAE:  0.91996026\n",
      "MdAE:  0.6783936\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.73\n",
      "-\n",
      " Average eval MAE loss: 0.93\n",
      "===============================\n",
      "MAE:  0.89253\n",
      "MdAE:  0.73981476\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.74\n",
      "-\n",
      " Average eval MAE loss: 1.07\n",
      "===============================\n",
      "MAE:  0.99213\n",
      "MdAE:  0.8559737\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.65\n",
      "-\n",
      " Average eval MAE loss: 0.91\n",
      "===============================\n",
      "MAE:  0.80267566\n",
      "MdAE:  0.6561043\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.68\n",
      "-\n",
      " Average eval MAE loss: 1.15\n",
      "===============================\n",
      "MAE:  0.943521\n",
      "MdAE:  0.75875854\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.60\n",
      "-\n",
      " Average eval MAE loss: 1.02\n",
      "===============================\n",
      "MAE:  0.8784338\n",
      "MdAE:  0.7319124\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.49\n",
      "-\n",
      " Average eval MAE loss: 0.95\n",
      "===============================\n",
      "MAE:  0.8443147\n",
      "MdAE:  0.64913654\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.38\n",
      "-\n",
      " Average eval MAE loss: 0.94\n",
      "===============================\n",
      "MAE:  0.85383147\n",
      "MdAE:  0.6878743\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.36\n",
      "-\n",
      " Average eval MAE loss: 0.95\n",
      "===============================\n",
      "MAE:  0.8512246\n",
      "MdAE:  0.6728339\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Line coverage data is inconsistent\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[13949, 5197, 1366, 318, 18326, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [19457, 6495, 1398, 6978, 318, 11491, 618, 6906, 319, 257, 17379, 290, 257, 1332, 12, 9491, 422, 262, 976, 285], [34149, 2041, 8875, 341, 5072, 26672, 284, 257, 2723, 26672, 28128, 274, 477, 2723, 0, 50256, 50256, 50256, 50256, 50256], [4550, 2446, 14, 26090, 1241, 8875, 341, 4634, 287, 1628, 6608, 2443, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [12050, 6208, 5660, 19142, 1620, 1365, 290, 2245, 22656, 262, 12454, 4704, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['clover'], 'test': ['clover']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 22.71\n",
      "-\n",
      " Average eval MAE loss: 2.39\n",
      "===============================\n",
      "MAE:  4.009734\n",
      "MdAE:  2.1224427\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.06\n",
      "-\n",
      " Average eval MAE loss: 1.83\n",
      "===============================\n",
      "MAE:  3.6147738\n",
      "MdAE:  1.1553788\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.85\n",
      "-\n",
      " Average eval MAE loss: 1.77\n",
      "===============================\n",
      "MAE:  3.6172078\n",
      "MdAE:  1.2707124\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.47\n",
      "-\n",
      " Average eval MAE loss: 2.27\n",
      "===============================\n",
      "MAE:  4.109285\n",
      "MdAE:  2.2994049\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.70\n",
      "-\n",
      " Average eval MAE loss: 2.23\n",
      "===============================\n",
      "MAE:  3.9261103\n",
      "MdAE:  2.2204595\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.21\n",
      "-\n",
      " Average eval MAE loss: 1.85\n",
      "===============================\n",
      "MAE:  3.556704\n",
      "MdAE:  1.0333395\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.86\n",
      "-\n",
      " Average eval MAE loss: 3.18\n",
      "===============================\n",
      "MAE:  4.7024283\n",
      "MdAE:  3.044798\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 3.19\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  3.7563734\n",
      "MdAE:  1.5892489\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.33\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  3.6477249\n",
      "MdAE:  1.5191691\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 2.28\n",
      "-\n",
      " Average eval MAE loss: 1.80\n",
      "===============================\n",
      "MAE:  3.5550673\n",
      "MdAE:  1.2862864\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.87\n",
      "-\n",
      " Average eval MAE loss: 2.55\n",
      "===============================\n",
      "MAE:  4.5582266\n",
      "MdAE:  2.0859375\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.89\n",
      "-\n",
      " Average eval MAE loss: 1.91\n",
      "===============================\n",
      "MAE:  3.8037446\n",
      "MdAE:  1.5215392\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.64\n",
      "-\n",
      " Average eval MAE loss: 2.16\n",
      "===============================\n",
      "MAE:  4.2475624\n",
      "MdAE:  1.4998467\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.74\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  3.9426227\n",
      "MdAE:  1.2585273\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.38\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  3.9906778\n",
      "MdAE:  1.5154105\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.20\n",
      "-\n",
      " Average eval MAE loss: 2.03\n",
      "===============================\n",
      "MAE:  4.0140944\n",
      "MdAE:  1.548183\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.11\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  4.027797\n",
      "MdAE:  1.471256\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.07\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  4.0211477\n",
      "MdAE:  1.5758467\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.08\n",
      "-\n",
      " Average eval MAE loss: 2.12\n",
      "===============================\n",
      "MAE:  4.2267375\n",
      "MdAE:  1.5791392\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.06\n",
      "-\n",
      " Average eval MAE loss: 2.08\n",
      "===============================\n",
      "MAE:  4.153898\n",
      "MdAE:  1.5751863\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Transition git repositories to Stash\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[8291, 653, 17606, 38072, 284, 520, 1077, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [19006, 1096, 14848, 4365, 2643, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [11505, 510, 30948, 2257, 3788, 21898, 8341, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [8291, 653, 284, 7326, 23079, 20396, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [4550, 28486, 12, 3106, 6436, 7509, 284, 2212, 62, 16680, 361, 270, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['datamanagement'], 'test': ['datamanagement']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 19.43\n",
      "-\n",
      " Average eval MAE loss: 6.01\n",
      "===============================\n",
      "MAE:  6.4052997\n",
      "MdAE:  3.1574316\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 8.83\n",
      "-\n",
      " Average eval MAE loss: 5.89\n",
      "===============================\n",
      "MAE:  6.198457\n",
      "MdAE:  3.340217\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 8.18\n",
      "-\n",
      " Average eval MAE loss: 5.36\n",
      "===============================\n",
      "MAE:  5.6791677\n",
      "MdAE:  2.6097844\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 6.84\n",
      "-\n",
      " Average eval MAE loss: 5.08\n",
      "===============================\n",
      "MAE:  5.4207644\n",
      "MdAE:  2.1257296\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 6.33\n",
      "-\n",
      " Average eval MAE loss: 6.17\n",
      "===============================\n",
      "MAE:  6.2639666\n",
      "MdAE:  3.5367558\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 6.08\n",
      "-\n",
      " Average eval MAE loss: 6.62\n",
      "===============================\n",
      "MAE:  6.960117\n",
      "MdAE:  3.0643806\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 5.57\n",
      "-\n",
      " Average eval MAE loss: 5.39\n",
      "===============================\n",
      "MAE:  5.5899777\n",
      "MdAE:  2.3171682\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 5.11\n",
      "-\n",
      " Average eval MAE loss: 5.73\n",
      "===============================\n",
      "MAE:  5.78188\n",
      "MdAE:  2.3647475\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 4.74\n",
      "-\n",
      " Average eval MAE loss: 6.08\n",
      "===============================\n",
      "MAE:  6.2042937\n",
      "MdAE:  2.8773804\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 4.36\n",
      "-\n",
      " Average eval MAE loss: 5.33\n",
      "===============================\n",
      "MAE:  5.517885\n",
      "MdAE:  2.25099\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 4.34\n",
      "-\n",
      " Average eval MAE loss: 5.43\n",
      "===============================\n",
      "MAE:  5.597931\n",
      "MdAE:  2.6746473\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 4.05\n",
      "-\n",
      " Average eval MAE loss: 5.39\n",
      "===============================\n",
      "MAE:  5.569095\n",
      "MdAE:  2.3514428\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 3.67\n",
      "-\n",
      " Average eval MAE loss: 6.11\n",
      "===============================\n",
      "MAE:  6.382068\n",
      "MdAE:  3.0392075\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 3.40\n",
      "-\n",
      " Average eval MAE loss: 5.40\n",
      "===============================\n",
      "MAE:  5.8409195\n",
      "MdAE:  2.540657\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 3.15\n",
      "-\n",
      " Average eval MAE loss: 6.08\n",
      "===============================\n",
      "MAE:  6.47511\n",
      "MdAE:  2.9523187\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 2.93\n",
      "-\n",
      " Average eval MAE loss: 5.72\n",
      "===============================\n",
      "MAE:  6.0775185\n",
      "MdAE:  2.6560462\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 2.81\n",
      "-\n",
      " Average eval MAE loss: 5.70\n",
      "===============================\n",
      "MAE:  5.9657645\n",
      "MdAE:  2.6412373\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 2.60\n",
      "-\n",
      " Average eval MAE loss: 5.74\n",
      "===============================\n",
      "MAE:  5.9449697\n",
      "MdAE:  2.5609658\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 2.56\n",
      "-\n",
      " Average eval MAE loss: 5.76\n",
      "===============================\n",
      "MAE:  5.949089\n",
      "MdAE:  2.5550687\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 2.49\n",
      "-\n",
      " Average eval MAE loss: 5.79\n",
      "===============================\n",
      "MAE:  5.9897227\n",
      "MdAE:  2.6788235\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Document logging framework\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[24941, 18931, 9355, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [35265, 329, 2829, 19698, 286, 1628, 2196, 3146, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [33, 12171, 3440, 25, 49899, 4388, 360, 5330, 18839, 26151, 286, 838, 22737, 286, 347, 6581, 2695, 50256, 50256, 50256], [12889, 7156, 17, 42, 25, 7412, 11315, 2139, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [12889, 7156, 17, 42, 25, 7412, 4382, 2139, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['duracloud'], 'test': ['duracloud']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 24.09\n",
      "-\n",
      " Average eval MAE loss: 1.52\n",
      "===============================\n",
      "MAE:  1.4586334\n",
      "MdAE:  1.3957963\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.12\n",
      "-\n",
      " Average eval MAE loss: 1.76\n",
      "===============================\n",
      "MAE:  1.6644847\n",
      "MdAE:  1.3365394\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.68\n",
      "-\n",
      " Average eval MAE loss: 1.15\n",
      "===============================\n",
      "MAE:  1.1322147\n",
      "MdAE:  1.0121033\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.38\n",
      "-\n",
      " Average eval MAE loss: 0.89\n",
      "===============================\n",
      "MAE:  0.8252006\n",
      "MdAE:  0.5317217\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.23\n",
      "-\n",
      " Average eval MAE loss: 0.85\n",
      "===============================\n",
      "MAE:  0.7955237\n",
      "MdAE:  0.56172746\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.14\n",
      "-\n",
      " Average eval MAE loss: 0.91\n",
      "===============================\n",
      "MAE:  0.8237945\n",
      "MdAE:  0.6069233\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.09\n",
      "-\n",
      " Average eval MAE loss: 0.85\n",
      "===============================\n",
      "MAE:  0.80624837\n",
      "MdAE:  0.50868374\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.93\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  0.89340746\n",
      "MdAE:  0.5959791\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.91\n",
      "-\n",
      " Average eval MAE loss: 0.85\n",
      "===============================\n",
      "MAE:  0.79459345\n",
      "MdAE:  0.61093014\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.79\n",
      "-\n",
      " Average eval MAE loss: 1.03\n",
      "===============================\n",
      "MAE:  0.92513263\n",
      "MdAE:  0.62964547\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.71\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  0.89427686\n",
      "MdAE:  0.6733291\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.67\n",
      "-\n",
      " Average eval MAE loss: 0.90\n",
      "===============================\n",
      "MAE:  0.8558721\n",
      "MdAE:  0.5415392\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.69\n",
      "-\n",
      " Average eval MAE loss: 0.96\n",
      "===============================\n",
      "MAE:  0.8366153\n",
      "MdAE:  0.5608672\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.46\n",
      "-\n",
      " Average eval MAE loss: 0.91\n",
      "===============================\n",
      "MAE:  0.808022\n",
      "MdAE:  0.5866233\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.47\n",
      "-\n",
      " Average eval MAE loss: 0.92\n",
      "===============================\n",
      "MAE:  0.85820895\n",
      "MdAE:  0.5821831\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 0.87\n",
      "===============================\n",
      "MAE:  0.8127899\n",
      "MdAE:  0.5665065\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.41\n",
      "-\n",
      " Average eval MAE loss: 0.85\n",
      "===============================\n",
      "MAE:  0.79067105\n",
      "MdAE:  0.55003816\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.39\n",
      "-\n",
      " Average eval MAE loss: 0.85\n",
      "===============================\n",
      "MAE:  0.7913565\n",
      "MdAE:  0.56597126\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 0.88\n",
      "===============================\n",
      "MAE:  0.81141263\n",
      "MdAE:  0.54013133\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 0.88\n",
      "===============================\n",
      "MAE:  0.8052071\n",
      "MdAE:  0.5331808\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  As a JIRA Administrator I would like to be able to change the trigger of the night service\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[1722, 257, 449, 40, 3861, 22998, 314, 561, 588, 284, 307, 1498, 284, 1487, 262, 7616, 286, 262, 1755, 2139], [1722, 257, 449, 40, 3861, 22998, 314, 561, 588, 284, 307, 1498, 284, 1487, 262, 7616, 286, 262, 1755, 2139], [46189, 3992, 1818, 47217, 743, 19122, 351, 584, 20652, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [46189, 3992, 1818, 47217, 743, 19122, 351, 584, 20652, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [4550, 2420, 284, 262, 2449, 576, 39266, 366, 44651, 4935, 1, 3275, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['jirasoftware'], 'test': ['jirasoftware']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 31.09\n",
      "-\n",
      " Average eval MAE loss: 10.98\n",
      "===============================\n",
      "MAE:  10.867614\n",
      "MdAE:  11.423271\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.01\n",
      "-\n",
      " Average eval MAE loss: 1.61\n",
      "===============================\n",
      "MAE:  1.6372194\n",
      "MdAE:  1.2372532\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.26\n",
      "-\n",
      " Average eval MAE loss: 1.76\n",
      "===============================\n",
      "MAE:  1.6942437\n",
      "MdAE:  1.425348\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.92\n",
      "-\n",
      " Average eval MAE loss: 2.36\n",
      "===============================\n",
      "MAE:  2.1969442\n",
      "MdAE:  1.8757067\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.64\n",
      "-\n",
      " Average eval MAE loss: 2.10\n",
      "===============================\n",
      "MAE:  1.9119854\n",
      "MdAE:  1.3133311\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.96\n",
      "-\n",
      " Average eval MAE loss: 3.61\n",
      "===============================\n",
      "MAE:  3.4226427\n",
      "MdAE:  3.2411823\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.15\n",
      "-\n",
      " Average eval MAE loss: 1.91\n",
      "===============================\n",
      "MAE:  1.9502403\n",
      "MdAE:  1.5461559\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.77\n",
      "-\n",
      " Average eval MAE loss: 2.09\n",
      "===============================\n",
      "MAE:  1.9232363\n",
      "MdAE:  1.513504\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.98\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  2.0650945\n",
      "MdAE:  1.5784369\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.65\n",
      "-\n",
      " Average eval MAE loss: 1.98\n",
      "===============================\n",
      "MAE:  1.9133697\n",
      "MdAE:  1.4335256\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.44\n",
      "-\n",
      " Average eval MAE loss: 2.43\n",
      "===============================\n",
      "MAE:  2.1662312\n",
      "MdAE:  1.6163957\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.13\n",
      "-\n",
      " Average eval MAE loss: 2.18\n",
      "===============================\n",
      "MAE:  1.9639078\n",
      "MdAE:  1.6995199\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 2.52\n",
      "===============================\n",
      "MAE:  2.1894417\n",
      "MdAE:  1.9538648\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.21\n",
      "-\n",
      " Average eval MAE loss: 2.16\n",
      "===============================\n",
      "MAE:  1.9970028\n",
      "MdAE:  1.5766106\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.08\n",
      "-\n",
      " Average eval MAE loss: 2.15\n",
      "===============================\n",
      "MAE:  2.2380097\n",
      "MdAE:  1.787005\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.00\n",
      "-\n",
      " Average eval MAE loss: 2.20\n",
      "===============================\n",
      "MAE:  2.0477507\n",
      "MdAE:  1.6356637\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.74\n",
      "-\n",
      " Average eval MAE loss: 2.30\n",
      "===============================\n",
      "MAE:  2.1494489\n",
      "MdAE:  1.8152056\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.72\n",
      "-\n",
      " Average eval MAE loss: 2.30\n",
      "===============================\n",
      "MAE:  2.0820727\n",
      "MdAE:  1.797317\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.68\n",
      "-\n",
      " Average eval MAE loss: 2.30\n",
      "===============================\n",
      "MAE:  2.1029246\n",
      "MdAE:  1.7402875\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.60\n",
      "-\n",
      " Average eval MAE loss: 2.33\n",
      "===============================\n",
      "MAE:  2.084855\n",
      "MdAE:  1.8067241\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Report executor terminations to framework schedulers.\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[19100, 3121, 273, 5651, 602, 284, 9355, 6038, 377, 364, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [44, 274, 418, 11778, 815, 12940, 3121, 669, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3109, 3455, 309, 1921, 42, 62, 7708, 4146, 1961, 1738, 284, 15183, 19653, 13, 50256, 50256, 50256, 50256, 50256, 50256], [23410, 2049, 9355, 10143, 284, 1057, 2233, 284, 2089, 9701, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [7583, 2198, 705, 18300, 814, 1377, 19509, 14269, 1377, 301, 1886, 6, 287, 1281, 12, 19023, 82, 13, 9078, 13]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['mesos'], 'test': ['mesos']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 18.26\n",
      "-\n",
      " Average eval MAE loss: 2.69\n",
      "===============================\n",
      "MAE:  2.6786115\n",
      "MdAE:  2.8393006\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.21\n",
      "-\n",
      " Average eval MAE loss: 1.34\n",
      "===============================\n",
      "MAE:  1.203958\n",
      "MdAE:  0.6717353\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.11\n",
      "-\n",
      " Average eval MAE loss: 1.45\n",
      "===============================\n",
      "MAE:  1.3714885\n",
      "MdAE:  0.9744022\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 1.27\n",
      "===============================\n",
      "MAE:  1.1652453\n",
      "MdAE:  0.8726387\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.72\n",
      "-\n",
      " Average eval MAE loss: 2.32\n",
      "===============================\n",
      "MAE:  2.1162887\n",
      "MdAE:  2.1281857\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 1.34\n",
      "===============================\n",
      "MAE:  1.2206208\n",
      "MdAE:  1.0420841\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.69\n",
      "-\n",
      " Average eval MAE loss: 2.10\n",
      "===============================\n",
      "MAE:  1.9624155\n",
      "MdAE:  1.7620015\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.48\n",
      "-\n",
      " Average eval MAE loss: 1.58\n",
      "===============================\n",
      "MAE:  1.5695173\n",
      "MdAE:  1.2160554\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.36\n",
      "-\n",
      " Average eval MAE loss: 1.39\n",
      "===============================\n",
      "MAE:  1.3708929\n",
      "MdAE:  1.0914929\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.19\n",
      "-\n",
      " Average eval MAE loss: 1.53\n",
      "===============================\n",
      "MAE:  1.4441999\n",
      "MdAE:  1.2439755\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.12\n",
      "-\n",
      " Average eval MAE loss: 1.33\n",
      "===============================\n",
      "MAE:  1.2276999\n",
      "MdAE:  0.9120446\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 1.38\n",
      "===============================\n",
      "MAE:  1.288209\n",
      "MdAE:  0.8880161\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.86\n",
      "-\n",
      " Average eval MAE loss: 1.40\n",
      "===============================\n",
      "MAE:  1.2908945\n",
      "MdAE:  0.9299698\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.81\n",
      "-\n",
      " Average eval MAE loss: 1.43\n",
      "===============================\n",
      "MAE:  1.3421043\n",
      "MdAE:  1.0024874\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.76\n",
      "-\n",
      " Average eval MAE loss: 1.52\n",
      "===============================\n",
      "MAE:  1.378455\n",
      "MdAE:  1.0141418\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.67\n",
      "-\n",
      " Average eval MAE loss: 1.46\n",
      "===============================\n",
      "MAE:  1.3294444\n",
      "MdAE:  0.94273245\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.77\n",
      "-\n",
      " Average eval MAE loss: 1.49\n",
      "===============================\n",
      "MAE:  1.3595561\n",
      "MdAE:  0.94962865\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.66\n",
      "-\n",
      " Average eval MAE loss: 1.51\n",
      "===============================\n",
      "MAE:  1.3583887\n",
      "MdAE:  1.0076647\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.54\n",
      "-\n",
      " Average eval MAE loss: 1.49\n",
      "===============================\n",
      "MAE:  1.3457202\n",
      "MdAE:  0.97361934\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.54\n",
      "-\n",
      " Average eval MAE loss: 1.50\n",
      "===============================\n",
      "MAE:  1.3519899\n",
      "MdAE:  0.97369456\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Forum: Per-discussion subscription\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[1890, 388, 25, 2448, 12, 15410, 11956, 14569, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [1890, 388, 25, 14883, 416, 304, 12, 4529, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [12154, 7799, 284, 11986, 1728, 7032, 287, 6831, 3842, 355, 2672, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [1890, 388, 25, 5120, 284, 1306, 4704, 2792, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [32, 779, 7034, 338, 1781, 1351, 318, 9277, 3093, 3614, 351, 645, 835, 286, 4379, 1844, 1351, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['moodle'], 'test': ['moodle']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 35.59\n",
      "-\n",
      " Average eval MAE loss: 14.28\n",
      "===============================\n",
      "MAE:  8.938059\n",
      "MdAE:  9.036295\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 12.90\n",
      "-\n",
      " Average eval MAE loss: 15.02\n",
      "===============================\n",
      "MAE:  5.887364\n",
      "MdAE:  4.4773145\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 13.48\n",
      "-\n",
      " Average eval MAE loss: 14.33\n",
      "===============================\n",
      "MAE:  7.248261\n",
      "MdAE:  7.0820026\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 12.30\n",
      "-\n",
      " Average eval MAE loss: 14.44\n",
      "===============================\n",
      "MAE:  11.0118065\n",
      "MdAE:  10.478514\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 10.98\n",
      "-\n",
      " Average eval MAE loss: 14.23\n",
      "===============================\n",
      "MAE:  9.541613\n",
      "MdAE:  8.350607\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 9.48\n",
      "-\n",
      " Average eval MAE loss: 13.91\n",
      "===============================\n",
      "MAE:  8.729458\n",
      "MdAE:  6.6272516\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 8.71\n",
      "-\n",
      " Average eval MAE loss: 14.19\n",
      "===============================\n",
      "MAE:  11.955379\n",
      "MdAE:  10.179546\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 8.46\n",
      "-\n",
      " Average eval MAE loss: 13.93\n",
      "===============================\n",
      "MAE:  9.985378\n",
      "MdAE:  7.129906\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 8.50\n",
      "-\n",
      " Average eval MAE loss: 15.09\n",
      "===============================\n",
      "MAE:  12.673677\n",
      "MdAE:  8.316821\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 6.86\n",
      "-\n",
      " Average eval MAE loss: 14.04\n",
      "===============================\n",
      "MAE:  9.520509\n",
      "MdAE:  7.4617395\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 6.48\n",
      "-\n",
      " Average eval MAE loss: 16.03\n",
      "===============================\n",
      "MAE:  13.631713\n",
      "MdAE:  10.340275\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 6.05\n",
      "-\n",
      " Average eval MAE loss: 19.22\n",
      "===============================\n",
      "MAE:  16.059187\n",
      "MdAE:  9.672712\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 5.31\n",
      "-\n",
      " Average eval MAE loss: 15.30\n",
      "===============================\n",
      "MAE:  12.319148\n",
      "MdAE:  9.190386\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 4.89\n",
      "-\n",
      " Average eval MAE loss: 15.31\n",
      "===============================\n",
      "MAE:  12.53641\n",
      "MdAE:  9.902796\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 4.28\n",
      "-\n",
      " Average eval MAE loss: 15.57\n",
      "===============================\n",
      "MAE:  12.393789\n",
      "MdAE:  9.4350815\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 3.41\n",
      "-\n",
      " Average eval MAE loss: 14.01\n",
      "===============================\n",
      "MAE:  10.196951\n",
      "MdAE:  7.636986\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 3.24\n",
      "-\n",
      " Average eval MAE loss: 14.76\n",
      "===============================\n",
      "MAE:  10.669853\n",
      "MdAE:  7.604827\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 14.49\n",
      "===============================\n",
      "MAE:  11.305875\n",
      "MdAE:  8.408464\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 2.68\n",
      "-\n",
      " Average eval MAE loss: 15.37\n",
      "===============================\n",
      "MAE:  12.104521\n",
      "MdAE:  8.801563\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 2.49\n",
      "-\n",
      " Average eval MAE loss: 14.80\n",
      "===============================\n",
      "MAE:  10.991078\n",
      "MdAE:  7.790963\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Implement true multicast functionality for <all> processor\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[3546, 26908, 2081, 47368, 459, 11244, 329, 1279, 439, 29, 12649, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [13921, 337, 2261, 1104, 1395, 32, 8611, 319, 1395, 32, 4133, 973, 416, 257, 8225, 2134, 7515, 5633, 50256, 50256], [818, 9152, 21201, 8265, 287, 337, 2261, 4755, 6082, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [28446, 12, 17212, 815, 900, 6631, 21437, 351, 938, 6631, 2722, 878, 7216, 284, 23641, 48, 50256, 50256, 50256, 50256], [28446, 12, 17212, 815, 1104, 18305, 516, 779, 2663, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['mule'], 'test': ['mule']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 23.88\n",
      "-\n",
      " Average eval MAE loss: 7.08\n",
      "===============================\n",
      "MAE:  7.265451\n",
      "MdAE:  7.4112415\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.07\n",
      "-\n",
      " Average eval MAE loss: 4.16\n",
      "===============================\n",
      "MAE:  4.2881823\n",
      "MdAE:  4.134663\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.38\n",
      "-\n",
      " Average eval MAE loss: 2.91\n",
      "===============================\n",
      "MAE:  3.0736413\n",
      "MdAE:  3.046388\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.95\n",
      "-\n",
      " Average eval MAE loss: 3.36\n",
      "===============================\n",
      "MAE:  3.3327854\n",
      "MdAE:  2.5674357\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.53\n",
      "-\n",
      " Average eval MAE loss: 2.87\n",
      "===============================\n",
      "MAE:  2.8163564\n",
      "MdAE:  2.0070612\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.22\n",
      "-\n",
      " Average eval MAE loss: 2.51\n",
      "===============================\n",
      "MAE:  2.6810136\n",
      "MdAE:  2.632815\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.38\n",
      "-\n",
      " Average eval MAE loss: 2.73\n",
      "===============================\n",
      "MAE:  2.9150789\n",
      "MdAE:  2.330892\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.16\n",
      "-\n",
      " Average eval MAE loss: 2.54\n",
      "===============================\n",
      "MAE:  2.593792\n",
      "MdAE:  2.005897\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.96\n",
      "-\n",
      " Average eval MAE loss: 2.52\n",
      "===============================\n",
      "MAE:  2.6273568\n",
      "MdAE:  2.130134\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.60\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "MAE:  2.846879\n",
      "MdAE:  2.4428217\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.43\n",
      "-\n",
      " Average eval MAE loss: 2.60\n",
      "===============================\n",
      "MAE:  2.7840328\n",
      "MdAE:  2.2050323\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 2.55\n",
      "===============================\n",
      "MAE:  2.8040414\n",
      "MdAE:  2.2207096\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 2.56\n",
      "===============================\n",
      "MAE:  2.72453\n",
      "MdAE:  2.210948\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.17\n",
      "-\n",
      " Average eval MAE loss: 2.53\n",
      "===============================\n",
      "MAE:  2.689805\n",
      "MdAE:  2.1620808\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.98\n",
      "-\n",
      " Average eval MAE loss: 2.49\n",
      "===============================\n",
      "MAE:  2.6615126\n",
      "MdAE:  2.122746\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.09\n",
      "-\n",
      " Average eval MAE loss: 2.60\n",
      "===============================\n",
      "MAE:  2.8119814\n",
      "MdAE:  2.202549\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 2.49\n",
      "===============================\n",
      "MAE:  2.6697745\n",
      "MdAE:  2.0595105\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.90\n",
      "-\n",
      " Average eval MAE loss: 2.56\n",
      "===============================\n",
      "MAE:  2.758513\n",
      "MdAE:  2.1992497\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 2.53\n",
      "===============================\n",
      "MAE:  2.7075443\n",
      "MdAE:  2.105075\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 2.53\n",
      "===============================\n",
      "MAE:  2.7124527\n",
      "MdAE:  2.0929422\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Support for request/reply\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[15514, 329, 2581, 14, 47768, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [34, 34574, 1330, 257, 11733, 1628, 422, 15151, 1231, 8563, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [29238, 284, 7349, 2438, 466, 407, 651, 3024, 12380, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3118, 540, 284, 751, 257, 2882, 618, 4441, 257, 1218, 5202, 287, 262, 976, 285, 11125, 50256, 50256, 50256, 50256], [28531, 43076, 287, 262, 23735, 1570, 389, 407, 852, 4615, 618, 345, 4781, 477, 262, 4847, 286, 257, 1611, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['mulestudio'], 'test': ['mulestudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 23.79\n",
      "-\n",
      " Average eval MAE loss: 5.02\n",
      "===============================\n",
      "MAE:  4.024889\n",
      "MdAE:  3.929449\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.12\n",
      "-\n",
      " Average eval MAE loss: 4.69\n",
      "===============================\n",
      "MAE:  3.6797862\n",
      "MdAE:  2.322072\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.02\n",
      "-\n",
      " Average eval MAE loss: 4.91\n",
      "===============================\n",
      "MAE:  3.9395378\n",
      "MdAE:  3.0281963\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.40\n",
      "-\n",
      " Average eval MAE loss: 4.65\n",
      "===============================\n",
      "MAE:  3.6537077\n",
      "MdAE:  3.0081406\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 4.62\n",
      "===============================\n",
      "MAE:  3.6351418\n",
      "MdAE:  2.5594716\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.06\n",
      "-\n",
      " Average eval MAE loss: 4.69\n",
      "===============================\n",
      "MAE:  3.7014039\n",
      "MdAE:  2.552209\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.59\n",
      "-\n",
      " Average eval MAE loss: 4.82\n",
      "===============================\n",
      "MAE:  3.7711344\n",
      "MdAE:  2.8268561\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.54\n",
      "-\n",
      " Average eval MAE loss: 4.85\n",
      "===============================\n",
      "MAE:  3.7334583\n",
      "MdAE:  3.2661743\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.73\n",
      "-\n",
      " Average eval MAE loss: 4.88\n",
      "===============================\n",
      "MAE:  3.8388839\n",
      "MdAE:  2.798764\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 2.23\n",
      "-\n",
      " Average eval MAE loss: 4.81\n",
      "===============================\n",
      "MAE:  3.7064276\n",
      "MdAE:  2.8718352\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 2.11\n",
      "-\n",
      " Average eval MAE loss: 4.79\n",
      "===============================\n",
      "MAE:  3.6613674\n",
      "MdAE:  2.5862184\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 5.04\n",
      "===============================\n",
      "MAE:  3.833697\n",
      "MdAE:  2.6011605\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.66\n",
      "-\n",
      " Average eval MAE loss: 4.89\n",
      "===============================\n",
      "MAE:  3.7585325\n",
      "MdAE:  2.526311\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.80\n",
      "-\n",
      " Average eval MAE loss: 5.07\n",
      "===============================\n",
      "MAE:  3.7301972\n",
      "MdAE:  2.4138718\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.56\n",
      "-\n",
      " Average eval MAE loss: 5.04\n",
      "===============================\n",
      "MAE:  3.813915\n",
      "MdAE:  2.4055014\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.38\n",
      "-\n",
      " Average eval MAE loss: 5.07\n",
      "===============================\n",
      "MAE:  3.7292273\n",
      "MdAE:  2.4238186\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.33\n",
      "-\n",
      " Average eval MAE loss: 5.02\n",
      "===============================\n",
      "MAE:  3.707641\n",
      "MdAE:  2.4312363\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.31\n",
      "-\n",
      " Average eval MAE loss: 5.23\n",
      "===============================\n",
      "MAE:  3.928511\n",
      "MdAE:  2.3101785\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.11\n",
      "-\n",
      " Average eval MAE loss: 5.02\n",
      "===============================\n",
      "MAE:  3.6675432\n",
      "MdAE:  2.544076\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.03\n",
      "-\n",
      " Average eval MAE loss: 5.07\n",
      "===============================\n",
      "MAE:  3.773347\n",
      "MdAE:  2.3571756\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  HDFS ItemWriter\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[39, 8068, 50, 9097, 34379, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [39, 8068, 50, 7231, 3597, 31904, 6097, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [29239, 33432, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [51, 29291, 1366, 4645, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [44387, 6404, 554, 3495, 295, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['springxd'], 'test': ['springxd']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 13.94\n",
      "-\n",
      " Average eval MAE loss: 2.64\n",
      "===============================\n",
      "MAE:  2.7572246\n",
      "MdAE:  2.9196756\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 3.26\n",
      "-\n",
      " Average eval MAE loss: 1.82\n",
      "===============================\n",
      "MAE:  1.7190495\n",
      "MdAE:  1.7989675\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.64\n",
      "-\n",
      " Average eval MAE loss: 2.14\n",
      "===============================\n",
      "MAE:  2.1704345\n",
      "MdAE:  2.139295\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.18\n",
      "-\n",
      " Average eval MAE loss: 2.03\n",
      "===============================\n",
      "MAE:  2.0475707\n",
      "MdAE:  1.8719149\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.19\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  1.9996293\n",
      "MdAE:  1.7158365\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.03\n",
      "-\n",
      " Average eval MAE loss: 2.25\n",
      "===============================\n",
      "MAE:  2.3316114\n",
      "MdAE:  2.3409543\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.21\n",
      "-\n",
      " Average eval MAE loss: 1.78\n",
      "===============================\n",
      "MAE:  1.7102009\n",
      "MdAE:  1.294445\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.98\n",
      "-\n",
      " Average eval MAE loss: 1.82\n",
      "===============================\n",
      "MAE:  1.7023258\n",
      "MdAE:  1.1309569\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 1.83\n",
      "===============================\n",
      "MAE:  1.7429537\n",
      "MdAE:  1.1886984\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.63\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.9693309\n",
      "MdAE:  1.411378\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.49\n",
      "-\n",
      " Average eval MAE loss: 2.02\n",
      "===============================\n",
      "MAE:  2.0232358\n",
      "MdAE:  1.466895\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.27\n",
      "-\n",
      " Average eval MAE loss: 1.85\n",
      "===============================\n",
      "MAE:  1.7964569\n",
      "MdAE:  1.306394\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.33\n",
      "-\n",
      " Average eval MAE loss: 1.92\n",
      "===============================\n",
      "MAE:  1.9979917\n",
      "MdAE:  1.4452978\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.22\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  2.0590582\n",
      "MdAE:  1.4878149\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.14\n",
      "-\n",
      " Average eval MAE loss: 2.13\n",
      "===============================\n",
      "MAE:  2.2374668\n",
      "MdAE:  1.6024951\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.07\n",
      "-\n",
      " Average eval MAE loss: 1.92\n",
      "===============================\n",
      "MAE:  1.9420655\n",
      "MdAE:  1.3192083\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 1.85\n",
      "===============================\n",
      "MAE:  1.8880966\n",
      "MdAE:  1.3975056\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "MAE:  2.1572647\n",
      "MdAE:  1.5514178\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.06\n",
      "-\n",
      " Average eval MAE loss: 1.97\n",
      "===============================\n",
      "MAE:  2.0382435\n",
      "MdAE:  1.4366674\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.90\n",
      "-\n",
      " Average eval MAE loss: 1.98\n",
      "===============================\n",
      "MAE:  2.0692992\n",
      "MdAE:  1.4908938\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  SQL Server Single Sign On Support doesn't work in data profiler repository connections\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[17861, 9652, 14206, 5865, 1550, 7929, 1595, 470, 670, 287, 1366, 1534, 5329, 16099, 8787, 50256, 50256, 50256, 50256, 50256], [27914, 734, 15180, 287, 8373, 8893, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [32048, 3781, 1058, 1321, 62, 15952, 2611, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [48101, 3781, 21337, 389, 366, 2164, 16548, 503, 1, 290, 2314, 307, 973, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [1, 7680, 12515, 15274, 1, 6859, 857, 407, 3359, 319, 257, 3084, 3781, 357, 4480, 360, 48, 14330, 8, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['talenddataquality'], 'test': ['talenddataquality']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 27.47\n",
      "-\n",
      " Average eval MAE loss: 10.40\n",
      "===============================\n",
      "MAE:  12.592496\n",
      "MdAE:  13.384028\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.96\n",
      "-\n",
      " Average eval MAE loss: 3.66\n",
      "===============================\n",
      "MAE:  3.189951\n",
      "MdAE:  3.0844078\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.69\n",
      "-\n",
      " Average eval MAE loss: 3.66\n",
      "===============================\n",
      "MAE:  3.4299228\n",
      "MdAE:  3.3514829\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.64\n",
      "-\n",
      " Average eval MAE loss: 3.87\n",
      "===============================\n",
      "MAE:  2.5538354\n",
      "MdAE:  1.8881094\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.49\n",
      "-\n",
      " Average eval MAE loss: 3.59\n",
      "===============================\n",
      "MAE:  3.1884162\n",
      "MdAE:  2.7527297\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.10\n",
      "-\n",
      " Average eval MAE loss: 4.14\n",
      "===============================\n",
      "MAE:  4.761494\n",
      "MdAE:  4.660549\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 3.08\n",
      "-\n",
      " Average eval MAE loss: 3.50\n",
      "===============================\n",
      "MAE:  2.6498954\n",
      "MdAE:  2.0023987\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.75\n",
      "-\n",
      " Average eval MAE loss: 3.69\n",
      "===============================\n",
      "MAE:  3.398387\n",
      "MdAE:  2.7549253\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.13\n",
      "-\n",
      " Average eval MAE loss: 3.61\n",
      "===============================\n",
      "MAE:  3.8104227\n",
      "MdAE:  3.182701\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.89\n",
      "-\n",
      " Average eval MAE loss: 3.53\n",
      "===============================\n",
      "MAE:  3.6230693\n",
      "MdAE:  2.9763432\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.63\n",
      "-\n",
      " Average eval MAE loss: 3.94\n",
      "===============================\n",
      "MAE:  4.5663104\n",
      "MdAE:  4.085495\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.67\n",
      "-\n",
      " Average eval MAE loss: 3.61\n",
      "===============================\n",
      "MAE:  3.8486571\n",
      "MdAE:  3.1278906\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.45\n",
      "-\n",
      " Average eval MAE loss: 3.75\n",
      "===============================\n",
      "MAE:  3.5145576\n",
      "MdAE:  2.6777716\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.49\n",
      "-\n",
      " Average eval MAE loss: 3.81\n",
      "===============================\n",
      "MAE:  3.713247\n",
      "MdAE:  2.9042878\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 3.73\n",
      "===============================\n",
      "MAE:  3.9914613\n",
      "MdAE:  3.316533\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.36\n",
      "-\n",
      " Average eval MAE loss: 3.75\n",
      "===============================\n",
      "MAE:  4.094441\n",
      "MdAE:  3.405836\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.09\n",
      "-\n",
      " Average eval MAE loss: 3.64\n",
      "===============================\n",
      "MAE:  3.872467\n",
      "MdAE:  3.147843\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 3.76\n",
      "===============================\n",
      "MAE:  4.1282988\n",
      "MdAE:  3.4039612\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.07\n",
      "-\n",
      " Average eval MAE loss: 3.75\n",
      "===============================\n",
      "MAE:  4.219925\n",
      "MdAE:  3.53267\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.98\n",
      "-\n",
      " Average eval MAE loss: 3.65\n",
      "===============================\n",
      "MAE:  3.893\n",
      "MdAE:  3.1447496\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Investigation: S1 Improved user experience with TOS/TIS/ESB Studio\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[19070, 7065, 25, 311, 16, 24125, 2836, 1998, 351, 309, 2640, 14, 51, 1797, 14, 1546, 33, 11733, 50256, 50256], [19070, 10055, 25, 311, 17, 7320, 4809, 46333, 290, 10131, 9352, 287, 309, 2640, 14, 51, 1797, 14, 41501, 50256], [19070, 10055, 25, 311, 18, 24125, 23735, 6060, 49500, 36109, 287, 309, 2640, 14, 51, 1797, 14, 41501, 50256, 50256], [19070, 10055, 25, 311, 22, 309, 1797, 11923, 87, 1912, 319, 262, 4809, 19239, 50256, 50256, 50256, 50256, 50256, 50256], [19070, 10055, 25, 33084, 13472, 649, 6443, 290, 6459, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['talendesb'], 'test': ['talendesb']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 23.50\n",
      "-\n",
      " Average eval MAE loss: 8.62\n",
      "===============================\n",
      "MAE:  8.415205\n",
      "MdAE:  8.689943\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.79\n",
      "-\n",
      " Average eval MAE loss: 1.87\n",
      "===============================\n",
      "MAE:  1.8743789\n",
      "MdAE:  1.529362\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.29\n",
      "-\n",
      " Average eval MAE loss: 0.95\n",
      "===============================\n",
      "MAE:  0.94719523\n",
      "MdAE:  0.64888406\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.12\n",
      "-\n",
      " Average eval MAE loss: 0.95\n",
      "===============================\n",
      "MAE:  0.9285971\n",
      "MdAE:  0.641299\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.29\n",
      "-\n",
      " Average eval MAE loss: 0.88\n",
      "===============================\n",
      "MAE:  0.88481265\n",
      "MdAE:  0.6023309\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.00\n",
      "-\n",
      " Average eval MAE loss: 0.87\n",
      "===============================\n",
      "MAE:  0.89930934\n",
      "MdAE:  0.6265737\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.76\n",
      "-\n",
      " Average eval MAE loss: 1.04\n",
      "===============================\n",
      "MAE:  0.9560489\n",
      "MdAE:  0.6310651\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.72\n",
      "-\n",
      " Average eval MAE loss: 0.88\n",
      "===============================\n",
      "MAE:  0.87928915\n",
      "MdAE:  0.5157157\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.65\n",
      "-\n",
      " Average eval MAE loss: 0.93\n",
      "===============================\n",
      "MAE:  0.9175715\n",
      "MdAE:  0.6044993\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.60\n",
      "-\n",
      " Average eval MAE loss: 0.93\n",
      "===============================\n",
      "MAE:  0.92329264\n",
      "MdAE:  0.6158049\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.59\n",
      "-\n",
      " Average eval MAE loss: 1.01\n",
      "===============================\n",
      "MAE:  0.9450403\n",
      "MdAE:  0.66028816\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.55\n",
      "-\n",
      " Average eval MAE loss: 1.01\n",
      "===============================\n",
      "MAE:  0.98225105\n",
      "MdAE:  0.6706078\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.50\n",
      "-\n",
      " Average eval MAE loss: 0.97\n",
      "===============================\n",
      "MAE:  0.95650524\n",
      "MdAE:  0.63833547\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.45\n",
      "-\n",
      " Average eval MAE loss: 1.00\n",
      "===============================\n",
      "MAE:  0.9653321\n",
      "MdAE:  0.648919\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.41\n",
      "-\n",
      " Average eval MAE loss: 1.00\n",
      "===============================\n",
      "MAE:  1.0015403\n",
      "MdAE:  0.67833215\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.33\n",
      "-\n",
      " Average eval MAE loss: 1.03\n",
      "===============================\n",
      "MAE:  1.0117962\n",
      "MdAE:  0.7513763\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 1.02\n",
      "===============================\n",
      "MAE:  1.013159\n",
      "MdAE:  0.7232547\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 0.98\n",
      "===============================\n",
      "MAE:  0.96674025\n",
      "MdAE:  0.6527647\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.30\n",
      "-\n",
      " Average eval MAE loss: 1.01\n",
      "===============================\n",
      "MAE:  0.99973637\n",
      "MdAE:  0.66180384\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.26\n",
      "-\n",
      " Average eval MAE loss: 1.01\n",
      "===============================\n",
      "MAE:  1.0071715\n",
      "MdAE:  0.69882774\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Android: While debugger is running, cannot back out and go back into an app\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[25934, 25, 2893, 49518, 318, 2491, 11, 2314, 736, 503, 290, 467, 736, 656, 281, 598, 50256, 50256, 50256, 50256], [25934, 25, 2034, 9641, 1239, 2077, 422, 256, 544, 381, 13, 19875, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [25934, 25, 15443, 6608, 389, 5445, 329, 7412, 7680, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [25934, 25, 11851, 5657, 318, 9066, 618, 1336, 9612, 22870, 3159, 318, 973, 13, 50256, 50256, 50256, 50256, 50256, 50256], [35742, 25, 12697, 290, 4268, 3975, 6757, 37647, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['titanium'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 27.16\n",
      "-\n",
      " Average eval MAE loss: 5.68\n",
      "===============================\n",
      "MAE:  5.756569\n",
      "MdAE:  5.542074\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 5.64\n",
      "-\n",
      " Average eval MAE loss: 5.80\n",
      "===============================\n",
      "MAE:  5.460118\n",
      "MdAE:  5.171556\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 4.62\n",
      "-\n",
      " Average eval MAE loss: 2.43\n",
      "===============================\n",
      "MAE:  2.2119377\n",
      "MdAE:  2.1355476\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.72\n",
      "-\n",
      " Average eval MAE loss: 2.71\n",
      "===============================\n",
      "MAE:  2.430834\n",
      "MdAE:  1.6455433\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.49\n",
      "-\n",
      " Average eval MAE loss: 2.54\n",
      "===============================\n",
      "MAE:  2.4116826\n",
      "MdAE:  1.6284752\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.22\n",
      "-\n",
      " Average eval MAE loss: 2.74\n",
      "===============================\n",
      "MAE:  2.496672\n",
      "MdAE:  2.13734\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.96\n",
      "-\n",
      " Average eval MAE loss: 3.03\n",
      "===============================\n",
      "MAE:  2.76503\n",
      "MdAE:  2.0329833\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.78\n",
      "-\n",
      " Average eval MAE loss: 3.05\n",
      "===============================\n",
      "MAE:  2.8152552\n",
      "MdAE:  2.092863\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.55\n",
      "-\n",
      " Average eval MAE loss: 2.96\n",
      "===============================\n",
      "MAE:  2.7295485\n",
      "MdAE:  1.9489365\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 2.35\n",
      "-\n",
      " Average eval MAE loss: 3.47\n",
      "===============================\n",
      "MAE:  3.047246\n",
      "MdAE:  2.0743651\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 2.26\n",
      "-\n",
      " Average eval MAE loss: 2.99\n",
      "===============================\n",
      "MAE:  2.615919\n",
      "MdAE:  1.7395811\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 2.05\n",
      "-\n",
      " Average eval MAE loss: 2.89\n",
      "===============================\n",
      "MAE:  2.5920715\n",
      "MdAE:  1.7129073\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.95\n",
      "-\n",
      " Average eval MAE loss: 3.24\n",
      "===============================\n",
      "MAE:  2.9523892\n",
      "MdAE:  2.0652606\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.86\n",
      "-\n",
      " Average eval MAE loss: 3.37\n",
      "===============================\n",
      "MAE:  3.1067731\n",
      "MdAE:  2.1342797\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.75\n",
      "-\n",
      " Average eval MAE loss: 3.36\n",
      "===============================\n",
      "MAE:  3.045791\n",
      "MdAE:  2.0782545\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.55\n",
      "-\n",
      " Average eval MAE loss: 3.23\n",
      "===============================\n",
      "MAE:  2.944814\n",
      "MdAE:  1.9907091\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.55\n",
      "-\n",
      " Average eval MAE loss: 3.06\n",
      "===============================\n",
      "MAE:  2.7535386\n",
      "MdAE:  1.86165\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.48\n",
      "-\n",
      " Average eval MAE loss: 3.41\n",
      "===============================\n",
      "MAE:  3.065734\n",
      "MdAE:  2.0377822\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 3.10\n",
      "===============================\n",
      "MAE:  2.756324\n",
      "MdAE:  1.9630287\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.37\n",
      "-\n",
      " Average eval MAE loss: 3.31\n",
      "===============================\n",
      "MAE:  2.963025\n",
      "MdAE:  2.0854735\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/ipykernel_16639/1859469705.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Asset data does not correctly obey contextual ownership like the entity\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n",
      "using pretrained gpt-2 tokenizer\n",
      "[[45869, 1366, 857, 407, 9380, 22389, 38356, 9238, 588, 262, 9312, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3109, 3455, 14976, 11241, 379, 262, 30617, 14249, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [22069, 40087, 12405, 5860, 2104, 4947, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [40613, 4731, 1595, 470, 4781, 281, 9312, 3119, 357, 8423, 1595, 470, 670, 2035, 8, 50256, 50256, 50256, 50256, 50256], [35857, 32053, 2836, 11241, 1839, 470, 670, 319, 1220, 27604, 14, 18417, 14, 1326, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['usergrid'], 'test': ['usergrid']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 21.67\n",
      "-\n",
      " Average eval MAE loss: 1.72\n",
      "===============================\n",
      "MAE:  1.7347909\n",
      "MdAE:  1.4411049\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.47\n",
      "-\n",
      " Average eval MAE loss: 1.32\n",
      "===============================\n",
      "MAE:  1.4216458\n",
      "MdAE:  1.0833669\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.26\n",
      "-\n",
      " Average eval MAE loss: 0.92\n",
      "===============================\n",
      "MAE:  1.2185626\n",
      "MdAE:  0.725698\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.24\n",
      "-\n",
      " Average eval MAE loss: 2.69\n",
      "===============================\n",
      "MAE:  2.7547362\n",
      "MdAE:  2.757624\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.23\n",
      "-\n",
      " Average eval MAE loss: 1.09\n",
      "===============================\n",
      "MAE:  1.3646202\n",
      "MdAE:  1.3319106\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 0.83\n",
      "===============================\n",
      "MAE:  1.211044\n",
      "MdAE:  0.9309807\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 0.95\n",
      "===============================\n",
      "MAE:  1.296095\n",
      "MdAE:  1.0008197\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 0.91\n",
      "===============================\n",
      "MAE:  1.2604004\n",
      "MdAE:  1.1898375\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.90\n",
      "-\n",
      " Average eval MAE loss: 0.94\n",
      "===============================\n",
      "MAE:  1.2560407\n",
      "MdAE:  0.89413214\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.79\n",
      "-\n",
      " Average eval MAE loss: 0.97\n",
      "===============================\n",
      "MAE:  1.2463814\n",
      "MdAE:  0.9640336\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.70\n",
      "-\n",
      " Average eval MAE loss: 1.13\n",
      "===============================\n",
      "MAE:  1.4673387\n",
      "MdAE:  1.1867087\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.87\n",
      "-\n",
      " Average eval MAE loss: 1.40\n",
      "===============================\n",
      "MAE:  1.6924936\n",
      "MdAE:  1.2461176\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 1.13\n",
      "===============================\n",
      "MAE:  1.480952\n",
      "MdAE:  1.2475913\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.82\n",
      "-\n",
      " Average eval MAE loss: 1.20\n",
      "===============================\n",
      "MAE:  1.5596516\n",
      "MdAE:  1.4087739\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.60\n",
      "-\n",
      " Average eval MAE loss: 0.97\n",
      "===============================\n",
      "MAE:  1.3131295\n",
      "MdAE:  1.0969167\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.54\n",
      "-\n",
      " Average eval MAE loss: 1.11\n",
      "===============================\n",
      "MAE:  1.4344077\n",
      "MdAE:  1.1145334\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.53\n",
      "-\n",
      " Average eval MAE loss: 1.03\n",
      "===============================\n",
      "MAE:  1.3606025\n",
      "MdAE:  1.0283499\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.45\n",
      "-\n",
      " Average eval MAE loss: 1.08\n",
      "===============================\n",
      "MAE:  1.4194347\n",
      "MdAE:  1.1529918\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.42\n",
      "-\n",
      " Average eval MAE loss: 1.08\n",
      "===============================\n",
      "MAE:  1.3989415\n",
      "MdAE:  1.0862644\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.35\n",
      "-\n",
      " Average eval MAE loss: 1.06\n",
      "===============================\n",
      "MAE:  1.3889675\n",
      "MdAE:  1.0888608\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        {'train': ['clover'], 'test': ['clover']},\n",
    "                        {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        {'train': ['mule'], 'test': ['mule']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# within repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# cross repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)           \n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
