{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sansith/gpt2sp/blob/gpt2sp-base/model_training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyLoXxhlhfbk",
    "tags": []
   },
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ren7VyEyhfbl",
    "tags": []
   },
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMyN0hhThfbl",
    "outputId": "50d5ceea-3b3c-43f0-d343-dd1fb9c1f2a5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: pandas===1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Collecting koila\n",
      "  Downloading koila-0.1.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2023.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Collecting pynvml (from koila)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from koila) (13.7.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (2.16.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->koila) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "Installing collected packages: pynvml, koila\n",
      "Successfully installed koila-0.1.1 pynvml-11.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install torch pandas===1.5.3 transformers numpy tokenizers koila tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O8TuUYj7h6St",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIh96xVTiuIa",
    "outputId": "bfcd72e9-2a8b-4661-e10f-caf2278b6cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3R16ZxuirGo",
    "outputId": "eaafa7c2-00d4-4438-918d-0d745ee1b7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp\n"
     ]
    }
   ],
   "source": [
    "cd drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bhcSAPbLhfbm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
    "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbQNj41Ghfbn"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ch24eOM0hfbn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME , ADD_DESCRIPTION\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece\n",
    "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
    "ADD_DESCRIPTION = False\n",
    "\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH\n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH1j_lvmhfbn"
   },
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W1FZRuJ7hfbn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT = '  '\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "\n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    "    print(tokens_train['input_ids'][:5])\n",
    "\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "\n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "    # tokenization\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'gpt2':\n",
    "        print('using pretrained gpt-2 tokenizer')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "\n",
    "\n",
    "    if ADD_DESCRIPTION :\n",
    "      print(\"### text : title+description\")\n",
    "      d = {'text': (data['title'] + \" : \" + data[\"description\"]).tolist(), 'label': data['storypoint']}\n",
    "    else:\n",
    "      print(\"### text : title\")\n",
    "      d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    print(\"Input data feed ::: \",d['text'][0])\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    print(\"BATCH_SIZE : \",BATCH_SIZE)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels\n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "\n",
    "    # Optimizerrr -->\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "\n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "\n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # pdb.set_trace()\n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()\n",
    "            # Calculates the gradients\n",
    "            loss.backward()\n",
    "            # The clip_grad_norm_ function clips (limits) the norm (magnitude) of the gradients to a maximum value specified by the user.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            #updates the weights and bias accrding to the calculated gradients\n",
    "            optimizer.step()\n",
    "            # update learning rates\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "\n",
    "        time_records.append(time.time() - start_time)\n",
    "\n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                b_input_ids = batch[0].to(DEVICE)\n",
    "                b_labels = batch[1].to(DEVICE)\n",
    "                model.zero_grad()\n",
    "                result = model(b_input_ids,\n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "                loss = result.loss\n",
    "                logits = result.logits\n",
    "                total_eval_loss += loss.item()\n",
    "                # clean memory\n",
    "                del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "\n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "\n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "\n",
    "        print(\"===============================\")\n",
    "\n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records))\n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records))\n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "\n",
    "            global OUTPUT\n",
    "            print(\"Testing model\")\n",
    "            OUTPUT += 'Testing### '+ '\\n'\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]),\n",
    "              'models/' + str(file_pair['train'][0]) + '_'\n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "\n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "    OUTPUT += 'Epoch train summary---------- '\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'Batch size: ' + str(BATCH_SIZE) + '\\n'\n",
    "    OUTPUT += 'Training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Eval minimum loss value : ' + str(min_eval_loss_epoch[0]) + '\\n'\n",
    "    OUTPUT += 'Eval minimum loss epoch : ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    OUTPUT += 'Testing MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "                + 'Testing MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    \n",
    "    global ADD_DESCRIPTION\n",
    "    OUTPUT += 'Description added : ' + str(ADD_DESCRIPTION) + '\\n'\n",
    "\n",
    "\n",
    "    print('all done for one project')\n",
    "    return MAE_RECORDS[min_eval_loss_epoch[1]] , min_eval_loss_epoch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn3yXK4lhfbo"
   },
   "source": [
    "### Within Project Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x-uMZ1Cfhfbo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e42ea5b7c5cd441fae34eb3581d3539c",
      "04e1d58915c443e499307d81f03208c3",
      "9cce0a849b8c4e8e9d9bb9d15f7e156f",
      "faf919c3181e45458c81e9aa09e0982b",
      "6f322d342ce74f46ae14dcb871d852a5",
      "18062757012e4e7e82a4f05dc49e46d1",
      "1b8fa57bb0114ea5a7340b022f3893ac",
      "ae36d1a5ff674735880f71aa8642e207",
      "2f645897ff064249961881a6a0e0c46d",
      "9dca7f0c372d4ba3aa01eee4234ad5f4",
      "88f69548e4f74a56aee2cd05eeaafe73"
     ]
    },
    "id": "MwmBk2BPhfbo",
    "outputId": "33b1e290-7250-4c95-8213-60e2e518bc27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text : title\n",
      "Input data feed :::  Add CA against object literals in function invocations\n",
      "within project split!\n",
      "using pretrained gpt-2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3520/121170968.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained gpt-2 tokenizer\n",
      "[[4550, 7257, 1028, 2134, 4187, 874, 287, 2163, 800, 20968, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [10260, 24150, 329, 2034, 7015, 1352, 13877, 284, 2034, 7015, 1352, 11112, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 649, 19449, 32815, 329, 26144, 1074, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 4935, 31458, 14161, 7873, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3791, 27850, 4935, 16884, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
      "BATCH_SIZE :  525\n",
      "BATCH_SIZE :  525\n",
      "using pretrained gpt-2 tokenizer\n",
      "BATCH_SIZE :  525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 42.83\n",
      "-\n",
      " Average eval MAE loss: 46.77\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  47.07893\n",
      "MdAE:  48.416306\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 17.24\n",
      "-\n",
      " Average eval MAE loss: 3.21\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.8529825\n",
      "MdAE:  2.6550426\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 8.63\n",
      "-\n",
      " Average eval MAE loss: 5.07\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  4.5823975\n",
      "MdAE:  4.7589445\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.24\n",
      "-\n",
      " Average eval MAE loss: 2.93\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  3.4085364\n",
      "MdAE:  3.199597\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.43\n",
      "-\n",
      " Average eval MAE loss: 2.20\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.9652869\n",
      "MdAE:  1.580579\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.80\n",
      "-\n",
      " Average eval MAE loss: 1.63\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.4848595\n",
      "MdAE:  0.9865682\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.74\n",
      "-\n",
      " Average eval MAE loss: 1.51\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.4586949\n",
      "MdAE:  0.9931872\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.76\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.3351946\n",
      "MdAE:  2.0268233\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.75\n",
      "-\n",
      " Average eval MAE loss: 1.60\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.5803692\n",
      "MdAE:  1.1040106\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 2.70\n",
      "-\n",
      " Average eval MAE loss: 3.11\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.724688\n",
      "MdAE:  2.6182377\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 2.58\n",
      "-\n",
      " Average eval MAE loss: 1.71\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.5221918\n",
      "MdAE:  1.2330198\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 2.44\n",
      "-\n",
      " Average eval MAE loss: 2.53\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.1822493\n",
      "MdAE:  2.0312946\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 2.52\n",
      "-\n",
      " Average eval MAE loss: 1.79\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.6685677\n",
      "MdAE:  1.3511012\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 2.38\n",
      "-\n",
      " Average eval MAE loss: 2.45\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.1567023\n",
      "MdAE:  2.006941\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 2.14\n",
      "-\n",
      " Average eval MAE loss: 1.84\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.6846424\n",
      "MdAE:  1.4068686\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 2.05\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.8581717\n",
      "MdAE:  1.6156306\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 2.00\n",
      "-\n",
      " Average eval MAE loss: 2.22\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  2.054062\n",
      "MdAE:  1.7949696\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.91\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.8874292\n",
      "MdAE:  1.6259534\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 2.16\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.9702224\n",
      "MdAE:  1.6875737\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.87\n",
      "-\n",
      " Average eval MAE loss: 2.08\n",
      "===============================\n",
      "Testing model\n",
      "MAE:  1.8968523\n",
      "MdAE:  1.6601171\n",
      "all done for one project\n",
      "MAE value for the project :  1.4586949\n",
      "Minimum loss value for the project :  1.5088822841644287\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        # {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        # {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        # {'train': ['clover'], 'test': ['clover']},\n",
    "                        # # {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        # {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        # {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        # {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        # {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        # {'train': ['mule'], 'test': ['mule']},\n",
    "                        # {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        # {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        # {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        # {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        # {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        # {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        TRAIN_MAE_VALUE ,min_loss_value =train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        \n",
    "        print('MAE value for the project : ',TRAIN_MAE_VALUE)\n",
    "        print('Minimum loss value for the project : ',min_loss_value)\n",
    "\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()\n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSte0lR_hfbo"
   },
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ij9vp2J2hfbo"
   },
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# within repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'bbpe':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
    "        elif TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()\n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMcxbMB7hfbp"
   },
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35iJqeeNhfbp"
   },
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT\n",
    "WITHIN_PROJECT = False\n",
    "\n",
    "# cross repo\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        if TOKENIZER == 'gpt2':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
    "        elif TOKENIZER == 'wordlevel':\n",
    "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
    "        if MODEL_NAME == 'gpt2':\n",
    "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        elif MODEL_NAME == 'gpt2sp':\n",
    "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "            MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()\n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04e1d58915c443e499307d81f03208c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18062757012e4e7e82a4f05dc49e46d1",
      "placeholder": "​",
      "style": "IPY_MODEL_1b8fa57bb0114ea5a7340b022f3893ac",
      "value": "model.safetensors: 100%"
     }
    },
    "18062757012e4e7e82a4f05dc49e46d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b8fa57bb0114ea5a7340b022f3893ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f645897ff064249961881a6a0e0c46d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6f322d342ce74f46ae14dcb871d852a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88f69548e4f74a56aee2cd05eeaafe73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cce0a849b8c4e8e9d9bb9d15f7e156f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae36d1a5ff674735880f71aa8642e207",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f645897ff064249961881a6a0e0c46d",
      "value": 548105171
     }
    },
    "9dca7f0c372d4ba3aa01eee4234ad5f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae36d1a5ff674735880f71aa8642e207": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e42ea5b7c5cd441fae34eb3581d3539c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04e1d58915c443e499307d81f03208c3",
       "IPY_MODEL_9cce0a849b8c4e8e9d9bb9d15f7e156f",
       "IPY_MODEL_faf919c3181e45458c81e9aa09e0982b"
      ],
      "layout": "IPY_MODEL_6f322d342ce74f46ae14dcb871d852a5"
     }
    },
    "faf919c3181e45458c81e9aa09e0982b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dca7f0c372d4ba3aa01eee4234ad5f4",
      "placeholder": "​",
      "style": "IPY_MODEL_88f69548e4f74a56aee2cd05eeaafe73",
      "value": " 548M/548M [00:04&lt;00:00, 122MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
