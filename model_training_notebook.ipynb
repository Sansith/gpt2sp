{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qyLoXxhlhfbk"
      },
      "source": [
        "# Model Training Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Ren7VyEyhfbl"
      },
      "source": [
        "### Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMyN0hhThfbl",
        "outputId": "5a09ffe5-e96a-47fc-a7cb-2c101fb0fa96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas===1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: koila in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from koila) (11.5.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from koila) (13.7.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->koila) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch pandas===1.5.3 transformers numpy tokenizers koila tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb"
      ],
      "metadata": {
        "id": "O8TuUYj7h6St"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIh96xVTiuIa",
        "outputId": "a4875c90-5b90-4053-963b-0b777a70d6c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3R16ZxuirGo",
        "outputId": "f1f6bc95-54b7-4716-f568-47e1d8687746"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "bhcSAPbLhfbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
        "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
        "from transformers import GPT2Config\n",
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbQNj41Ghfbn"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "ch24eOM0hfbn"
      },
      "outputs": [],
      "source": [
        "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME , ADD_DESCRIPTION\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
        "SEQUENCE_LEN = 20\n",
        "LEARNING_RATE = 5e-4\n",
        "TOKENIZER = 'gpt2' # available: gpt2, wordlevel, sentencepiece, wordpiece\n",
        "MODEL_NAME = 'gpt2sp' # available: gpt2sp, gpt2\n",
        "ADD_DESCRIPTION = False\n",
        "\n",
        "# define device\n",
        "global DEVICE\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# define files to be used\n",
        "global DATA_PATH\n",
        "DATA_PATH = './sp_dataset/marked_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH1j_lvmhfbn"
      },
      "source": [
        "### Static Methods and Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "W1FZRuJ7hfbn"
      },
      "outputs": [],
      "source": [
        "OUTPUT = '  '\n",
        "MODEL = None\n",
        "DYNAMIC_BATCH = True\n",
        "BATCH_SIZE = None\n",
        "WITHIN_PROJECT = None\n",
        "MAE_RECORDS = []\n",
        "MDAE_RECORDS = []\n",
        "\n",
        "def data_processing(file_pair):\n",
        "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
        "\n",
        "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
        "    for train_file_name in file_pair['train']:\n",
        "        fname = DATA_PATH + train_file_name + '.csv'\n",
        "        df = prepare_dataframe(fname)\n",
        "        train_data = train_data.append(df)\n",
        "\n",
        "    # data split\n",
        "    if WITHIN_PROJECT:\n",
        "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
        "    else:\n",
        "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
        "    # define batch size dynamically based on training length\n",
        "    if DYNAMIC_BATCH:\n",
        "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
        "    # tokenization\n",
        "    tokens_train = tokenization(train_text.tolist())\n",
        "    tokens_val = tokenization(val_text.tolist())\n",
        "    print(tokens_train['input_ids'][:5])\n",
        "\n",
        "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
        "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
        "\n",
        "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
        "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
        "\n",
        "    # prepare testing datasets\n",
        "    all_test_dataloader = []\n",
        "    test_file_names = []\n",
        "    if WITHIN_PROJECT:\n",
        "        tokens_test = tokenization(test_text.tolist())\n",
        "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(file_pair['test'][0])\n",
        "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "    for test_file_name in file_pair['test']:\n",
        "        fname = DATA_PATH + test_file_name + '.csv'\n",
        "        test_data = prepare_dataframe(fname)\n",
        "\n",
        "        test_text = test_data['text']\n",
        "        test_labels = test_data['label']\n",
        "\n",
        "        # tokenization\n",
        "        tokens_test = tokenization(test_text.tolist())\n",
        "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
        "\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(test_file_name)\n",
        "    print('cross project data processing!')\n",
        "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "\n",
        "def train_val_split(data, split_ratio):\n",
        "    print('cross project split!')\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_text = data['text'][:split_point]\n",
        "    train_labels = data['label'][:split_point]\n",
        "    val_text = data['text'][split_point:]\n",
        "    val_labels = data['label'][split_point:]\n",
        "    return train_text, train_labels, val_text, val_labels\n",
        "\n",
        "\n",
        "def tokenization(text_list):\n",
        "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
        "    # tokenization\n",
        "    if TOKENIZER == 'wordpiece':\n",
        "        print('using wordpiece tokenizer!')\n",
        "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
        "    elif TOKENIZER == 'sentencepiece':\n",
        "        print('using sentencepiece tokenizer!')\n",
        "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
        "    elif TOKENIZER == 'wordlevel':\n",
        "        print('using wordlevel tokenizer!')\n",
        "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
        "        encoded_sentences = {'input_ids':[]}\n",
        "        for sentence in text_list:\n",
        "            encoded = tokenizer.encode(sentence)\n",
        "            encoded = encoded.ids\n",
        "            if len(encoded) > SEQUENCE_LEN:\n",
        "                encoded = encoded[:SEQUENCE_LEN]\n",
        "            elif len(encoded) < SEQUENCE_LEN:\n",
        "                padding = SEQUENCE_LEN - len(encoded)\n",
        "                for _ in range(padding):\n",
        "                    encoded.append(3)\n",
        "            encoded_sentences['input_ids'].append(encoded)\n",
        "        return encoded_sentences\n",
        "    elif TOKENIZER == 'gpt2':\n",
        "        print('using pretrained gpt-2 tokenizer')\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
        "        tokenizer.pad_token = '[PAD]'\n",
        "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
        "\n",
        "\n",
        "def prepare_dataframe(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    # some rows have no description, fill blank to avoid Null\n",
        "    data = data.fillna(' ')\n",
        "\n",
        "\n",
        "    if ADD_DESCRIPTION :\n",
        "      print(\"### text : title+description\")\n",
        "      d = {'text': (data['title'] + \" : \" + data[\"description\"]).tolist(), 'label': data['storypoint']}\n",
        "    else:\n",
        "      print(\"### text : title\")\n",
        "      d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
        "    print(\"Input data feed ::: \",d['text'][0])\n",
        "    return pd.DataFrame(data=d)\n",
        "\n",
        "\n",
        "def prepare_dataloader(seq, y, sampler_type):\n",
        "    global BATCH_SIZE\n",
        "    tensor_dataset = TensorDataset(seq, y)\n",
        "    if sampler_type == 'random':\n",
        "        sampler = RandomSampler(tensor_dataset)\n",
        "    elif sampler_type == 'sequential':\n",
        "        sampler = SequentialSampler(tensor_dataset)\n",
        "    print(\"BATCH_SIZE : \",BATCH_SIZE)\n",
        "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def within_project_split(data):\n",
        "    print('within project split!')\n",
        "    train_val_split_point = int(len(data) * 0.6)\n",
        "    val_test_split_point = int(len(data) * 0.8)\n",
        "    train_text = data['text'][:train_val_split_point]\n",
        "    train_labels = data['label'][:train_val_split_point]\n",
        "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
        "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
        "    test_text = data['text'][val_test_split_point:]\n",
        "    test_labels = data['label'][val_test_split_point:]\n",
        "    return train_text, train_labels, val_text, val_labels, test_text, test_labels\n",
        "\n",
        "\n",
        "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
        "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
        "\n",
        "    # Optimizerrr -->\n",
        "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)\n",
        "    # Total number of training steps is [number of batches] x [number of epochs]\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "    # Create the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    print(\"Start training for \", file_pair, \".....\")\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    # tensorboard writer\n",
        "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
        "    writer = SummaryWriter(writer_path)\n",
        "\n",
        "    # vars for model selection\n",
        "    min_eval_loss_epoch = [10000, 0]\n",
        "\n",
        "    time_records = []\n",
        "    MAE_RECORDS = []\n",
        "    MDAE_RECORDS = []\n",
        "    start_time = time.time()\n",
        "    loss_fct = nn.L1Loss()\n",
        "    for e in range(EPOCHS):\n",
        "        # ---TRAINING---\n",
        "        # clean GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\">>> epoch \", e)\n",
        "        # set model into train mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # pdb.set_trace()\n",
        "            b_input_ids = batch[0].to(DEVICE)\n",
        "            b_labels = batch[1].to(DEVICE)\n",
        "            model.zero_grad()\n",
        "            result = model(b_input_ids,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "            total_train_loss += loss.item()\n",
        "            # Calculates the gradients\n",
        "            loss.backward()\n",
        "            # The clip_grad_norm_ function clips (limits) the norm (magnitude) of the gradients to a maximum value specified by the user.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            #updates the weights and bias accrding to the calculated gradients\n",
        "            optimizer.step()\n",
        "            # update learning rates\n",
        "            scheduler.step()\n",
        "            # clean memory\n",
        "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
        "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
        "        # clean memory\n",
        "        del avg_train_loss, total_train_loss\n",
        "\n",
        "        time_records.append(time.time() - start_time)\n",
        "\n",
        "        # ---EVAL---\n",
        "        print(\"-\")\n",
        "        # set model into eval mode\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        for batch in val_dataloader:\n",
        "            b_input_ids = batch[0].to(DEVICE)\n",
        "            b_labels = batch[1].to(DEVICE)\n",
        "            model.zero_grad()\n",
        "            result = model(b_input_ids,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "            total_eval_loss += loss.item()\n",
        "            # clean memory\n",
        "            del b_input_ids, b_labels, batch, result, loss, logits\n",
        "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
        "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
        "\n",
        "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
        "            min_eval_loss_epoch[0] = avg_eval_loss\n",
        "            min_eval_loss_epoch[1] = e\n",
        "\n",
        "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
        "        # clean memory\n",
        "        del avg_eval_loss, total_eval_loss\n",
        "        # save model state to dict\n",
        "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
        "\n",
        "        print(\"===============================\")\n",
        "\n",
        "        # testing on holdout data\n",
        "        index = 0\n",
        "        for test_dataloader in all_test_dataloader:\n",
        "            test_file_name = test_file_names[index]\n",
        "            index += 1\n",
        "            testing_start_time = time.time()\n",
        "            predictions = []\n",
        "            true_labels = []\n",
        "            for batch in test_dataloader:\n",
        "                batch = tuple(t.to(DEVICE) for t in batch)\n",
        "                b_input_ids, b_labels = batch\n",
        "                with torch.no_grad():\n",
        "                    logits = model(b_input_ids)\n",
        "                logits = logits['logits'].detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "            # calculate errors\n",
        "            distance_records = []\n",
        "            for i in range(len(predictions)):\n",
        "                for j in range(len(predictions[i])):\n",
        "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
        "                    distance_records.append(distance)\n",
        "\n",
        "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
        "            MAE = np.mean(np.array(distance_records))\n",
        "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
        "            MdAE = np.median(np.array(distance_records))\n",
        "\n",
        "            MAE_RECORDS.append(MAE)\n",
        "            MDAE_RECORDS.append(MdAE)\n",
        "\n",
        "            global OUTPUT\n",
        "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
        "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
        "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
        "            print('MAE: ', MAE)\n",
        "            print('MdAE: ', MdAE)\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "\n",
        "    # select model\n",
        "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]),\n",
        "              'models/' + str(file_pair['train'][0]) + '_'\n",
        "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
        "\n",
        "    # del unwanted models\n",
        "    for i in range(20):\n",
        "        try:\n",
        "            os.remove(\"models/epo_\" + str(i))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
        "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
        "    global BATCH_SIZE\n",
        "    OUTPUT += 'batch size: ' + str(BATCH_SIZE) + '\\n'\n",
        "    global ADD_DESCRIPTION\n",
        "    OUTPUT += 'Description added : ' + str(ADD_DESCRIPTION) + '\\n'\n",
        "\n",
        "\n",
        "    print('all done for one project')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn3yXK4lhfbo"
      },
      "source": [
        "### Within Project Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "x-uMZ1Cfhfbo"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MwmBk2BPhfbo",
        "outputId": "ec951b15-7c68-4a3f-97e4-0c28c9120562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp/GPT2SP.py\u001b[0m(16)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     14 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     15 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 16 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     18 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### text : title\n",
            "Input data feed :::  Add CA against object literals in function invocations\n",
            "within project split!\n",
            "using pretrained gpt-2 tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-60d2e9e6066f>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pretrained gpt-2 tokenizer\n",
            "[[4550, 7257, 1028, 2134, 4187, 874, 287, 2163, 800, 20968, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [10260, 24150, 329, 2034, 7015, 1352, 13877, 284, 2034, 7015, 1352, 11112, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 649, 19449, 32815, 329, 26144, 1074, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [16447, 4935, 31458, 14161, 7873, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [3791, 27850, 4935, 16884, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n",
            "BATCH_SIZE :  525\n",
            "BATCH_SIZE :  525\n",
            "using pretrained gpt-2 tokenizer\n",
            "BATCH_SIZE :  525\n",
            "model params : <generator object Module.parameters at 0x79dac3eabf40>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
            ">>> epoch  0\n",
            "> \u001b[0;32m/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp/GPT2SP.py\u001b[0m(50)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     48 \u001b[0;31m        \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     49 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m        transformer_outputs = self.transformer(\n",
            "\u001b[0m\u001b[0;32m     51 \u001b[0;31m            \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     52 \u001b[0;31m            \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'view'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8500c81a1ce6>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-8500c81a1ce6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfile_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_test_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrain_eval_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_test_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-60d2e9e6066f>\u001b[0m in \u001b[0;36mtrain_eval_test\u001b[0;34m(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             result = model(b_input_ids, \n\u001b[0m\u001b[1;32m    191\u001b[0m                            \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                            return_dict=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp/GPT2SP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;31m#  We are doing regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
          ]
        }
      ],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = True\n",
        "\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
        "                        # {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
        "                        # {'train': ['bamboo'], 'test': ['bamboo']},\n",
        "                        # {'train': ['clover'], 'test': ['clover']},\n",
        "                        # # {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
        "                        # {'train': ['duracloud'], 'test': ['duracloud']},\n",
        "                        # {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
        "                        # {'train': ['mesos'], 'test': ['mesos']},\n",
        "                        # {'train': ['moodle'], 'test': ['moodle']},\n",
        "                        # {'train': ['mule'], 'test': ['mule']},\n",
        "                        # {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
        "                        # {'train': ['springxd'], 'test': ['springxd']},\n",
        "                        # {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
        "                        # {'train': ['talendesb'], 'test': ['talendesb']},\n",
        "                        # {'train': ['titanium'], 'test': ['titanium']},\n",
        "                        # {'train': ['usergrid'], 'test': ['usergrid']},\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSte0lR_hfbo"
      },
      "source": [
        "### Cross Project Training Script - Within Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9vp2J2hfbo"
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# within repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
        "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
        "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
        "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMcxbMB7hfbp"
      },
      "source": [
        "### Cross Project Training Script - Cross Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35iJqeeNhfbp"
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# cross repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['clover'], 'test': ['usergrid']},\n",
        "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['mule'], 'test': ['titanium']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "conda-root-py",
      "name": "workbench-notebooks.m117",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}