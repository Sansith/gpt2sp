{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sansith/gpt2sp/blob/ensemble-bert/model_training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyLoXxhlhfbk",
        "tags": []
      },
      "source": [
        "# Model Training Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ren7VyEyhfbl",
        "tags": []
      },
      "source": [
        "### Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMyN0hhThfbl",
        "outputId": "e89998ea-10be-4a86-88c1-203cbc59e84d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas===1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Collecting koila\n",
            "  Downloading koila-0.1.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas===1.5.3) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Collecting pynvml (from koila)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from koila) (13.7.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->koila) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->koila) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Installing collected packages: pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, koila\n",
            "Successfully installed koila-0.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynvml-11.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch pandas===1.5.3 transformers numpy tokenizers koila tensorboard Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIh96xVTiuIa",
        "outputId": "6361a3e0-94b3-4231-89d0-502fe112d37b",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3R16ZxuirGo",
        "outputId": "b2ad046e-b9f6-41d7-c628-58b540980799",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLuP4fBCSXRm",
        "outputId": "aef53bfe-d2e9-4d16-9627-bb0c5407949f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/gpt2sp'\n",
            "/content/drive/MyDrive/Year4/FYP/effort-estimation/gpt2sp\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/gpt2sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8TuUYj7h6St",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct8N4OzwyswX",
        "outputId": "1a378f47-90c7-4cc8-801c-14ea478e8255",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2SP.py                         dataset_analysis.ipynb\n",
            "GPT2SPEN.py                       \u001b[0m\u001b[01;34mlogo\u001b[0m/\n",
            "GPT2SP_inspection_notebook.ipynb  model_training_notebook.ipynb\n",
            "LICENSE                           \u001b[01;34mmodels\u001b[0m/\n",
            "README.md                         \u001b[01;34mresults\u001b[0m/\n",
            "\u001b[01;34m__pycache__\u001b[0m/                      \u001b[01;34msp_dataset\u001b[0m/\n",
            "\u001b[01;34mabe0\u001b[0m/                             \u001b[01;34mtb\u001b[0m/\n",
            "\u001b[01;34mall_tokenizers\u001b[0m/                   tokenizer_training_notebook.ipynb\n",
            "\u001b[01;34mcorpus_tokenization_comparison\u001b[0m/   untitled.txt\n",
            "\u001b[01;34mcustom_transformers_interpret\u001b[0m/    vocab_and_tokenization_comparison.ipynb\n",
            "\u001b[01;34mdata_model_analysis\u001b[0m/              \u001b[01;34mxai_tokens\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "oT-nxTVPHMBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2PreTrainedModel\n",
        "import torch\n",
        "\n",
        "\n",
        "class GPT2SPEN(GPT2PreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.transformer_description = GPT2Model(config)\n",
        "        print(\"n embd :: \",config.n_embd)\n",
        "        self.dense1 = nn.Linear(2* config.n_embd, 4 * config.n_embd, bias=False)\n",
        "        self.dense2 = nn.Linear(4 * config.n_embd,2* config.n_embd, bias=False)\n",
        "        self.score = nn.Linear(2* config.n_embd, self.num_labels, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids_title=None,\n",
        "        input_ids_description=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # Title model\n",
        "        transformer_outputs_title = self.transformer(\n",
        "            input_ids_title,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        title_hidden_states = transformer_outputs_title[0]\n",
        "\n",
        "\n",
        "        # Description model\n",
        "        transformer_outputs_description = self.transformer_description(\n",
        "            input_ids_description,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        description_hidden_states = transformer_outputs_description[0]\n",
        "\n",
        "        # Concatenate the output tensors from both models\n",
        "\n",
        "        concatenated_hidden_states = torch.cat((title_hidden_states, description_hidden_states), dim=2)\n",
        "        # pdb.set_trace()\n",
        "        # print(\"concatenated_hidden_states len :: \",len(concatenated_hidden_states))\n",
        "        # MLP Layer\n",
        "        # pdb.set_trace()\n",
        "        hidden_states = self.dense1(concatenated_hidden_states)\n",
        "        hidden_states = self.dense2(hidden_states)\n",
        "        # hidden_states = self.dense3(hidden_states)\n",
        "        # hidden_states = self.dense4(hidden_states)\n",
        "\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids_title is not None:\n",
        "            batch_size, sequence_length = input_ids_title.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        assert (\n",
        "            self.config.pad_token_id is not None or batch_size == 1\n",
        "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
        "\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids_title is not None:\n",
        "                sequence_lengths = torch.ne(input_ids_title, self.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "                logger.warning(\n",
        "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "                )\n",
        "\n",
        "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = nn.L1Loss()\n",
        "                # pdb.set_trace()\n",
        "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + transformer_outputs_title[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=transformer_outputs_title.past_key_values,\n",
        "            hidden_states=transformer_outputs_title.hidden_states,\n",
        "            attentions=transformer_outputs_title.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "p5b9whVSHKeT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mQFV95HsVKMt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel , BertPreTrainedModel\n",
        "import torch\n",
        "\n",
        "\n",
        "class BertEN(BertPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.transformer = BertModel(config)\n",
        "        self.transformer_description = BertModel(config)\n",
        "        print(\"n embd :: \",config.hidden_size)\n",
        "        self.dense1 = nn.Linear(2* config.hidden_size, 4 * config.hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(4 * config.hidden_size,2* config.hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.score = nn.Linear(2* config.hidden_size, self.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids_title=None,\n",
        "        input_ids_description=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask_title=None,\n",
        "        attention_mask_description=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # Title model\n",
        "        transformer_outputs_title = self.transformer(\n",
        "            input_ids_title,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask_title,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        title_hidden_states = transformer_outputs_title[0]\n",
        "\n",
        "\n",
        "        # Description model\n",
        "        transformer_outputs_description = self.transformer_description(\n",
        "            input_ids_description,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask_description,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        description_hidden_states = transformer_outputs_description[0]\n",
        "\n",
        "        # Concatenate the output tensors from both models\n",
        "\n",
        "        concatenated_hidden_states = torch.cat((title_hidden_states, description_hidden_states), dim=2)\n",
        "        # pdb.set_trace()\n",
        "        # print(\"concatenated_hidden_states len :: \",len(concatenated_hidden_states))\n",
        "        # MLP Layer\n",
        "        # pdb.set_trace()\n",
        "        hidden_states = self.dense1(concatenated_hidden_states)\n",
        "        hidden_states = self.relu1(hidden_states)\n",
        "        hidden_states = self.dense2(hidden_states)\n",
        "        hidden_states = self.relu2(hidden_states)\n",
        "        # hidden_states = self.dense3(hidden_states)\n",
        "        # hidden_states = self.dense4(hidden_states)\n",
        "\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids_title is not None:\n",
        "            batch_size, sequence_length = input_ids_title.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        assert (\n",
        "            self.config.pad_token_id is not None or batch_size == 1\n",
        "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
        "\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids_title is not None:\n",
        "                sequence_lengths = torch.ne(input_ids_title, self.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "                logger.warning(\n",
        "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "                )\n",
        "\n",
        "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = nn.L1Loss()\n",
        "                # pdb.set_trace()\n",
        "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + transformer_outputs_title[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=transformer_outputs_title.past_key_values,\n",
        "            hidden_states=transformer_outputs_title.hidden_states,\n",
        "            attentions=transformer_outputs_title.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n"
      ],
      "metadata": {
        "id": "gPXbt9o2HVGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bhcSAPbLhfbm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from GPT2SP import GPT2ForSequenceClassification as GPT2SP\n",
        "#from GPT2SPEN import GPT2ForSequenceClassification as GPT2SPEN\n",
        "from transformers import GPT2ForSequenceClassification as LinearGPT2\n",
        "from transformers import GPT2Config, BertTokenizer,BertConfig\n",
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbQNj41Ghfbn"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ch24eOM0hfbn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME , ADD_DESCRIPTION\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE_RATIO = 0.02# within proj: 0.3 / cross proj: 0.4\n",
        "SEQUENCE_LEN = 100\n",
        "LEARNING_RATE = 5e-4\n",
        "TOKENIZER = 'bert' # available: gpt2, wordlevel, sentencepiece, wordpiece\n",
        "MODEL_NAME = 'berten' # available:gpt2spen , gpt2sp, gpt2\n",
        "ADD_DESCRIPTION = True\n",
        "\n",
        "# define device\n",
        "global DEVICE\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# define files to be used\n",
        "global DATA_PATH\n",
        "DATA_PATH = './sp_dataset/marked_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH1j_lvmhfbn"
      },
      "source": [
        "### Static Methods and Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W1FZRuJ7hfbn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "OUTPUT = '  '\n",
        "MODEL = None\n",
        "DYNAMIC_BATCH = True\n",
        "BATCH_SIZE = None\n",
        "WITHIN_PROJECT = None\n",
        "MAE_RECORDS = []\n",
        "MDAE_RECORDS = []\n",
        "\n",
        "def data_processing(file_pair):\n",
        "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH\n",
        "\n",
        "    train_data = pd.DataFrame(columns=['title', 'description',\"label\"])\n",
        "    for train_file_name in file_pair['train']:\n",
        "        fname = DATA_PATH + train_file_name + '.csv'\n",
        "        df = prepare_dataframe(fname)\n",
        "        train_data = train_data.append(df)\n",
        "\n",
        "    # data split\n",
        "    if WITHIN_PROJECT:\n",
        "        train_title,train_description,train_labels,  val_title,val_description,val_labels,  test_title,test_description,test_labels = within_project_split(train_data)\n",
        "    else:\n",
        "        train_title,train_description , train_labels, val_title,val_description , val_labels = train_val_split(train_data, 0.6)\n",
        "    # define batch size dynamically based on training length\n",
        "    if DYNAMIC_BATCH:\n",
        "        BATCH_SIZE = int(len(train_title) * BATCH_SIZE_RATIO)\n",
        "    # tokenization\n",
        "    title_tokens_train = tokenization(train_title.tolist())\n",
        "    description_tokens_train = tokenization(train_description.tolist())\n",
        "\n",
        "    title_tokens_val = tokenization(val_title.tolist())\n",
        "    description_tokens_val = tokenization(val_description.tolist())\n",
        "\n",
        "\n",
        "    train_seq_titles = torch.tensor(title_tokens_train['input_ids'])\n",
        "    train_att_mask_title = torch.tensor(title_tokens_train['attention_mask'])\n",
        "    train_seq_descriptions = torch.tensor(description_tokens_train['input_ids'])\n",
        "    train_att_mask_description = torch.tensor(description_tokens_train['attention_mask'])\n",
        "\n",
        "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
        "\n",
        "    train_dataloader = prepare_dataloader(train_seq_titles,train_seq_descriptions ,  train_y, sampler_type='random',\n",
        "                                          attention_mask_title=train_att_mask_title , attention_mask_description=train_att_mask_description)\n",
        "\n",
        "\n",
        "    val_seq_titles = torch.tensor(title_tokens_val['input_ids'])\n",
        "    val_seq__descriptions = torch.tensor(description_tokens_val['input_ids'])\n",
        "    val_att_mask_title = torch.tensor(title_tokens_val['attention_mask'])\n",
        "    val_att_mask_description = torch.tensor(description_tokens_val['attention_mask'])\n",
        "\n",
        "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
        "\n",
        "    val_dataloader = prepare_dataloader(val_seq_titles,val_seq__descriptions ,val_y, sampler_type='sequential',\n",
        "                                        attention_mask_title=val_att_mask_title,attention_mask_description=val_att_mask_description)\n",
        "\n",
        "    # prepare testing datasets\n",
        "    all_test_dataloader = []\n",
        "    test_file_names = []\n",
        "    if WITHIN_PROJECT:\n",
        "        tokens_test_title = tokenization(test_title.tolist())\n",
        "        tokens_test_description = tokenization(test_description.tolist())\n",
        "\n",
        "        test_seq_title = torch.tensor(tokens_test_title['input_ids'])\n",
        "        test_seq_description = torch.tensor(tokens_test_description['input_ids'])\n",
        "        test_att_mask_title = torch.tensor(tokens_test_title['attention_mask'])\n",
        "        test_att_mask_description = torch.tensor(tokens_test_description['attention_mask'])\n",
        "\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "\n",
        "        test_dataloader = prepare_dataloader(test_seq_title,test_seq_description, test_y, sampler_type='sequential',\n",
        "                                             attention_mask_title=test_att_mask_title,attention_mask_description=test_att_mask_description)\n",
        "\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(file_pair['test'][0])\n",
        "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "    for test_file_name in file_pair['test']:\n",
        "        fname = DATA_PATH + test_file_name + '.csv'\n",
        "        test_data = prepare_dataframe(fname)\n",
        "\n",
        "        test_title = test_data['title']\n",
        "        test_description = test_data['description']\n",
        "        test_labels = test_data['label']\n",
        "\n",
        "        # tokenization\n",
        "        tokens_test_title = tokenization(test_title.tolist())\n",
        "        tokens_test_description = tokenization(test_description.tolist())\n",
        "        test_seq_title = torch.tensor(tokens_test_title['input_ids'])\n",
        "        test_seq_description = torch.tensor(tokens_test_description['input_ids'])\n",
        "        test_att_mask_title = torch.tensor(tokens_test_title['attention_mask'])\n",
        "        test_att_mask_description = torch.tensor(tokens_test_description['attention_mask'])\n",
        "\n",
        "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
        "        test_dataloader = prepare_dataloader(test_seq_title,test_seq_description, test_y, sampler_type='sequential',\n",
        "                                             attention_mask_title = test_att_mask_title,attention_mask_description = test_att_mask_description)\n",
        "\n",
        "        all_test_dataloader.append(test_dataloader)\n",
        "        test_file_names.append(test_file_name)\n",
        "    print('cross project data processing!')\n",
        "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
        "\n",
        "\n",
        "def train_val_split(data, split_ratio):\n",
        "    print('cross project split!')\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "\n",
        "    train_title = data['title'][:split_point]\n",
        "    train_description = data['description'][:split_point]\n",
        "    train_labels = data['label'][:split_point]\n",
        "\n",
        "    val_title = data['title'][split_point:]\n",
        "    val_description = data['description'][split_point:]\n",
        "    val_labels = data['label'][split_point:]\n",
        "    return train_title,train_description , train_labels, val_title,val_description , val_labels\n",
        "\n",
        "\n",
        "def tokenization(text_list):\n",
        "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
        "    # tokenization\n",
        "    if TOKENIZER == 'wordpiece':\n",
        "        print('using wordpiece tokenizer!')\n",
        "        tokenizer = BertTokenizer('all_tokenizers/word_piece/vocab.txt')\n",
        "    elif TOKENIZER == 'sentencepiece':\n",
        "        print('using sentencepiece tokenizer!')\n",
        "        tokenizer = XLNetTokenizer('all_tokenizers/sentence_piece/spm_tokenizer.model', padding_side='right')\n",
        "    elif TOKENIZER == 'wordlevel':\n",
        "        print('using wordlevel tokenizer!')\n",
        "        tokenizer = Tokenizer.from_file('all_tokenizers/word_level/wordlevel.json')\n",
        "        encoded_sentences = {'input_ids':[]}\n",
        "        for sentence in text_list:\n",
        "            encoded = tokenizer.encode(sentence)\n",
        "            encoded = encoded.ids\n",
        "            if len(encoded) > SEQUENCE_LEN:\n",
        "                encoded = encoded[:SEQUENCE_LEN]\n",
        "            elif len(encoded) < SEQUENCE_LEN:\n",
        "                padding = SEQUENCE_LEN - len(encoded)\n",
        "                for _ in range(padding):\n",
        "                    encoded.append(3)\n",
        "            encoded_sentences['input_ids'].append(encoded)\n",
        "        return encoded_sentences\n",
        "    elif TOKENIZER == 'gpt2':\n",
        "        print('using pretrained gpt-2 tokenizer')\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER)\n",
        "        tokenizer.pad_token = '[PAD]'\n",
        "    elif TOKENIZER == 'bert':\n",
        "        print('usingbert tokenizer')\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length',return_tensors='pt')\n",
        "\n",
        "\n",
        "def prepare_dataframe(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    # some rows have no description, fill blank to avoid Null\n",
        "    data = data.fillna(' ')\n",
        "\n",
        "\n",
        "    # if ADD_DESCRIPTION :\n",
        "    #   print(\"### text : title+description\")\n",
        "    #   d = {'text': (data['title'] + \" : \" + data[\"description\"]).tolist(), 'label': data['storypoint']}\n",
        "    # else:\n",
        "    #   print(\"### text : title\")\n",
        "    #   d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
        "\n",
        "    d = { 'title':(data['title']).tolist(), 'description': (data[\"description\"]).tolist() , 'label': data['storypoint'] }\n",
        "    print(\"Input data feed ::: \",d.keys())\n",
        "    return pd.DataFrame(data=d)\n",
        "\n",
        "\n",
        "def prepare_dataloader(seq_title,seq_description, y, sampler_type,attention_mask_title,attention_mask_description):\n",
        "    global BATCH_SIZE\n",
        "    tensor_dataset = TensorDataset(seq_title,seq_description, y,attention_mask_title,attention_mask_description)\n",
        "    if sampler_type == 'random':\n",
        "        sampler = RandomSampler(tensor_dataset)\n",
        "    elif sampler_type == 'sequential':\n",
        "        sampler = SequentialSampler(tensor_dataset)\n",
        "    print(\"BATCH_SIZE : \",BATCH_SIZE)\n",
        "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def within_project_split(data):\n",
        "    print('within project split!')\n",
        "\n",
        "    train_val_split_point = int(len(data) * 0.6)\n",
        "    val_test_split_point = int(len(data) * 0.8)\n",
        "\n",
        "    train_title = data['title'][:train_val_split_point]\n",
        "    train_description = data['description'][:train_val_split_point]\n",
        "    train_labels = data['label'][:train_val_split_point]\n",
        "\n",
        "    val_title = data['title'][train_val_split_point:val_test_split_point]\n",
        "    val_description = data['description'][train_val_split_point:val_test_split_point]\n",
        "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
        "\n",
        "    test_title = data['title'][val_test_split_point:]\n",
        "    test_description = data['description'][val_test_split_point:]\n",
        "    test_labels = data['label'][val_test_split_point:]\n",
        "\n",
        "    return train_title,train_description,train_labels,  val_title,val_description,val_labels,  test_title,test_description,test_labels\n",
        "\n",
        "\n",
        "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
        "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
        "\n",
        "    # Optimizerrr -->\n",
        "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)\n",
        "    # Total number of training steps is [number of batches] x [number of epochs]\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "    # Create the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    print(\"Start training for \", file_pair, \".....\")\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    # tensorboard writer\n",
        "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
        "    writer = SummaryWriter(writer_path)\n",
        "\n",
        "    # vars for model selection\n",
        "    min_eval_loss_epoch = [10000, 0]\n",
        "\n",
        "    time_records = []\n",
        "    MAE_RECORDS = []\n",
        "    MDAE_RECORDS = []\n",
        "    start_time = time.time()\n",
        "    loss_fct = nn.L1Loss()\n",
        "    for e in range(EPOCHS):\n",
        "        # ---TRAINING---\n",
        "        # clean GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\">>> epoch \", e)\n",
        "        # set model into train mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # pdb.set_trace()\n",
        "            b_input_ids_title = batch[0].to(DEVICE)\n",
        "            b_input_ids_description = batch[1].to(DEVICE)\n",
        "            b_labels = batch[2].to(DEVICE)\n",
        "\n",
        "            # attention masks\n",
        "            att_mask_title = batch[3].to(DEVICE)\n",
        "            att_mask_description = batch[4].to(DEVICE)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            result = model(input_ids_title=b_input_ids_title,\n",
        "                           input_ids_description=b_input_ids_description,\n",
        "                           attention_mask_title=att_mask_title,\n",
        "                           attention_mask_description=att_mask_description,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "            total_train_loss += loss.item()\n",
        "            # Calculates the gradients\n",
        "            loss.backward()\n",
        "            # The clip_grad_norm_ function clips (limits) the norm (magnitude) of the gradients to a maximum value specified by the user.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            #updates the weights and bias accrding to the calculated gradients\n",
        "            optimizer.step()\n",
        "            # update learning rates\n",
        "            scheduler.step()\n",
        "            # clean memory\n",
        "            del step, batch, b_input_ids_title,b_input_ids_description, b_labels, result, loss, logits, att_mask_title ,att_mask_description\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
        "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
        "        # clean memory\n",
        "        del avg_train_loss, total_train_loss\n",
        "\n",
        "        time_records.append(time.time() - start_time)\n",
        "\n",
        "        # ---EVAL---\n",
        "        print(\"-\")\n",
        "        # set model into eval mode\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                b_input_ids_title = batch[0].to(DEVICE)\n",
        "                b_input_ids_description = batch[1].to(DEVICE)\n",
        "                b_labels = batch[2].to(DEVICE)\n",
        "\n",
        "                # attention masks\n",
        "                att_mask_title = batch[3].to(DEVICE)\n",
        "                att_mask_description = batch[4].to(DEVICE)\n",
        "\n",
        "                model.zero_grad()\n",
        "                result = model(input_ids_title=b_input_ids_title,\n",
        "                               input_ids_description=b_input_ids_description,\n",
        "                               attention_mask_title=att_mask_title,\n",
        "                               attention_mask_description=att_mask_description,\n",
        "                               labels=b_labels,\n",
        "                               return_dict=True)\n",
        "                loss = result.loss\n",
        "                logits = result.logits\n",
        "                total_eval_loss += loss.item()\n",
        "                # clean memory\n",
        "                del b_input_ids_title,b_input_ids_description, b_labels, batch, result, loss, logits , att_mask_title ,att_mask_description\n",
        "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
        "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
        "\n",
        "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
        "            min_eval_loss_epoch[0] = avg_eval_loss\n",
        "            min_eval_loss_epoch[1] = e\n",
        "\n",
        "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
        "        # clean memory\n",
        "        del avg_eval_loss, total_eval_loss\n",
        "        # save model state to dict\n",
        "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
        "\n",
        "        print(\"===============================\")\n",
        "\n",
        "        # testing on holdout data\n",
        "        index = 0\n",
        "        for test_dataloader in all_test_dataloader:\n",
        "            test_file_name = test_file_names[index]\n",
        "            index += 1\n",
        "            testing_start_time = time.time()\n",
        "            predictions = []\n",
        "            true_labels = []\n",
        "            for batch in test_dataloader:\n",
        "                # batch = tuple(t.to(DEVICE) for t in batch)\n",
        "                b_input_ids_title = batch[0].to(DEVICE)\n",
        "                b_input_ids_description = batch[1].to(DEVICE)\n",
        "                b_labels = batch[2].to(DEVICE)\n",
        "\n",
        "                # attention masks\n",
        "                att_mask_title = batch[3].to(DEVICE)\n",
        "                att_mask_description = batch[4].to(DEVICE)\n",
        "\n",
        "                # b_input_ids, b_labels = batch\n",
        "                with torch.no_grad():\n",
        "                    logits = model(input_ids_title=b_input_ids_title,\n",
        "                                   input_ids_description=b_input_ids_description,\n",
        "                                   attention_mask_title=att_mask_title,\n",
        "                                   attention_mask_description=att_mask_description,\n",
        "                                   )\n",
        "                logits = logits['logits'].detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "\n",
        "                del b_input_ids_title, b_input_ids_description ,b_labels , att_mask_title ,att_mask_description\n",
        "            # calculate errors\n",
        "            distance_records = []\n",
        "            for i in range(len(predictions)):\n",
        "                for j in range(len(predictions[i])):\n",
        "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
        "                    distance_records.append(distance)\n",
        "\n",
        "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
        "            MAE = np.mean(np.array(distance_records))\n",
        "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
        "            MdAE = np.median(np.array(distance_records))\n",
        "\n",
        "            MAE_RECORDS.append(MAE)\n",
        "            MDAE_RECORDS.append(MdAE)\n",
        "\n",
        "            global OUTPUT\n",
        "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
        "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
        "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
        "            print('MAE: ', MAE)\n",
        "            print('MdAE: ', MdAE)\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "\n",
        "    # select model\n",
        "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]),\n",
        "              'models/' + str(file_pair['train'][0]) + '_'\n",
        "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
        "\n",
        "    # del unwanted models\n",
        "    for i in range(20):\n",
        "        try:\n",
        "            os.remove(\"models/epo_\" + str(i))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    OUTPUT += 'Epoch train summary---------- '\n",
        "    global BATCH_SIZE\n",
        "    OUTPUT += 'Batch size: ' + str(BATCH_SIZE) + '\\n'\n",
        "    OUTPUT += 'Training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'Eval minimum loss value : ' + str(min_eval_loss_epoch[0]) + '\\n'\n",
        "    OUTPUT += 'Eval minimum loss epoch : ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
        "    OUTPUT += 'Testing MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
        "    OUTPUT += 'Testing MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
        "\n",
        "    global ADD_DESCRIPTION\n",
        "    OUTPUT += 'Description added : ' + str(ADD_DESCRIPTION) + '\\n'\n",
        "\n",
        "\n",
        "    print('all done for one project')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn3yXK4lhfbo"
      },
      "source": [
        "### Within Project Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x-uMZ1Cfhfbo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwmBk2BPhfbo",
        "outputId": "cf8eb5d0-8702-4bb3-819c-34597bc76700",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n embd ::  768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertEN were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.dense1.bias', 'bert.dense1.weight', 'bert.dense2.bias', 'bert.dense2.weight', 'bert.score.bias', 'bert.score.weight', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.pooler.dense.bias', 'bert.transformer.pooler.dense.weight', 'bert.transformer_description.embeddings.LayerNorm.bias', 'bert.transformer_description.embeddings.LayerNorm.weight', 'bert.transformer_description.embeddings.position_embeddings.weight', 'bert.transformer_description.embeddings.token_type_embeddings.weight', 'bert.transformer_description.embeddings.word_embeddings.weight', 'bert.transformer_description.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.0.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.0.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.0.attention.self.key.bias', 'bert.transformer_description.encoder.layer.0.attention.self.key.weight', 'bert.transformer_description.encoder.layer.0.attention.self.query.bias', 'bert.transformer_description.encoder.layer.0.attention.self.query.weight', 'bert.transformer_description.encoder.layer.0.attention.self.value.bias', 'bert.transformer_description.encoder.layer.0.attention.self.value.weight', 'bert.transformer_description.encoder.layer.0.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.0.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.0.output.dense.bias', 'bert.transformer_description.encoder.layer.0.output.dense.weight', 'bert.transformer_description.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.1.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.1.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.1.attention.self.key.bias', 'bert.transformer_description.encoder.layer.1.attention.self.key.weight', 'bert.transformer_description.encoder.layer.1.attention.self.query.bias', 'bert.transformer_description.encoder.layer.1.attention.self.query.weight', 'bert.transformer_description.encoder.layer.1.attention.self.value.bias', 'bert.transformer_description.encoder.layer.1.attention.self.value.weight', 'bert.transformer_description.encoder.layer.1.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.1.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.1.output.dense.bias', 'bert.transformer_description.encoder.layer.1.output.dense.weight', 'bert.transformer_description.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.10.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.10.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.10.attention.self.key.bias', 'bert.transformer_description.encoder.layer.10.attention.self.key.weight', 'bert.transformer_description.encoder.layer.10.attention.self.query.bias', 'bert.transformer_description.encoder.layer.10.attention.self.query.weight', 'bert.transformer_description.encoder.layer.10.attention.self.value.bias', 'bert.transformer_description.encoder.layer.10.attention.self.value.weight', 'bert.transformer_description.encoder.layer.10.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.10.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.10.output.dense.bias', 'bert.transformer_description.encoder.layer.10.output.dense.weight', 'bert.transformer_description.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.11.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.11.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.11.attention.self.key.bias', 'bert.transformer_description.encoder.layer.11.attention.self.key.weight', 'bert.transformer_description.encoder.layer.11.attention.self.query.bias', 'bert.transformer_description.encoder.layer.11.attention.self.query.weight', 'bert.transformer_description.encoder.layer.11.attention.self.value.bias', 'bert.transformer_description.encoder.layer.11.attention.self.value.weight', 'bert.transformer_description.encoder.layer.11.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.11.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.11.output.dense.bias', 'bert.transformer_description.encoder.layer.11.output.dense.weight', 'bert.transformer_description.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.2.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.2.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.2.attention.self.key.bias', 'bert.transformer_description.encoder.layer.2.attention.self.key.weight', 'bert.transformer_description.encoder.layer.2.attention.self.query.bias', 'bert.transformer_description.encoder.layer.2.attention.self.query.weight', 'bert.transformer_description.encoder.layer.2.attention.self.value.bias', 'bert.transformer_description.encoder.layer.2.attention.self.value.weight', 'bert.transformer_description.encoder.layer.2.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.2.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.2.output.dense.bias', 'bert.transformer_description.encoder.layer.2.output.dense.weight', 'bert.transformer_description.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.3.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.3.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.3.attention.self.key.bias', 'bert.transformer_description.encoder.layer.3.attention.self.key.weight', 'bert.transformer_description.encoder.layer.3.attention.self.query.bias', 'bert.transformer_description.encoder.layer.3.attention.self.query.weight', 'bert.transformer_description.encoder.layer.3.attention.self.value.bias', 'bert.transformer_description.encoder.layer.3.attention.self.value.weight', 'bert.transformer_description.encoder.layer.3.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.3.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.3.output.dense.bias', 'bert.transformer_description.encoder.layer.3.output.dense.weight', 'bert.transformer_description.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.4.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.4.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.4.attention.self.key.bias', 'bert.transformer_description.encoder.layer.4.attention.self.key.weight', 'bert.transformer_description.encoder.layer.4.attention.self.query.bias', 'bert.transformer_description.encoder.layer.4.attention.self.query.weight', 'bert.transformer_description.encoder.layer.4.attention.self.value.bias', 'bert.transformer_description.encoder.layer.4.attention.self.value.weight', 'bert.transformer_description.encoder.layer.4.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.4.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.4.output.dense.bias', 'bert.transformer_description.encoder.layer.4.output.dense.weight', 'bert.transformer_description.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.5.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.5.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.5.attention.self.key.bias', 'bert.transformer_description.encoder.layer.5.attention.self.key.weight', 'bert.transformer_description.encoder.layer.5.attention.self.query.bias', 'bert.transformer_description.encoder.layer.5.attention.self.query.weight', 'bert.transformer_description.encoder.layer.5.attention.self.value.bias', 'bert.transformer_description.encoder.layer.5.attention.self.value.weight', 'bert.transformer_description.encoder.layer.5.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.5.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.5.output.dense.bias', 'bert.transformer_description.encoder.layer.5.output.dense.weight', 'bert.transformer_description.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.6.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.6.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.6.attention.self.key.bias', 'bert.transformer_description.encoder.layer.6.attention.self.key.weight', 'bert.transformer_description.encoder.layer.6.attention.self.query.bias', 'bert.transformer_description.encoder.layer.6.attention.self.query.weight', 'bert.transformer_description.encoder.layer.6.attention.self.value.bias', 'bert.transformer_description.encoder.layer.6.attention.self.value.weight', 'bert.transformer_description.encoder.layer.6.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.6.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.6.output.dense.bias', 'bert.transformer_description.encoder.layer.6.output.dense.weight', 'bert.transformer_description.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.7.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.7.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.7.attention.self.key.bias', 'bert.transformer_description.encoder.layer.7.attention.self.key.weight', 'bert.transformer_description.encoder.layer.7.attention.self.query.bias', 'bert.transformer_description.encoder.layer.7.attention.self.query.weight', 'bert.transformer_description.encoder.layer.7.attention.self.value.bias', 'bert.transformer_description.encoder.layer.7.attention.self.value.weight', 'bert.transformer_description.encoder.layer.7.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.7.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.7.output.dense.bias', 'bert.transformer_description.encoder.layer.7.output.dense.weight', 'bert.transformer_description.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.8.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.8.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.8.attention.self.key.bias', 'bert.transformer_description.encoder.layer.8.attention.self.key.weight', 'bert.transformer_description.encoder.layer.8.attention.self.query.bias', 'bert.transformer_description.encoder.layer.8.attention.self.query.weight', 'bert.transformer_description.encoder.layer.8.attention.self.value.bias', 'bert.transformer_description.encoder.layer.8.attention.self.value.weight', 'bert.transformer_description.encoder.layer.8.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.8.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.8.output.dense.bias', 'bert.transformer_description.encoder.layer.8.output.dense.weight', 'bert.transformer_description.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.9.attention.output.dense.bias', 'bert.transformer_description.encoder.layer.9.attention.output.dense.weight', 'bert.transformer_description.encoder.layer.9.attention.self.key.bias', 'bert.transformer_description.encoder.layer.9.attention.self.key.weight', 'bert.transformer_description.encoder.layer.9.attention.self.query.bias', 'bert.transformer_description.encoder.layer.9.attention.self.query.weight', 'bert.transformer_description.encoder.layer.9.attention.self.value.bias', 'bert.transformer_description.encoder.layer.9.attention.self.value.weight', 'bert.transformer_description.encoder.layer.9.intermediate.dense.bias', 'bert.transformer_description.encoder.layer.9.intermediate.dense.weight', 'bert.transformer_description.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer_description.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer_description.encoder.layer.9.output.dense.bias', 'bert.transformer_description.encoder.layer.9.output.dense.weight', 'bert.transformer_description.pooler.dense.bias', 'bert.transformer_description.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data feed :::  dict_keys(['title', 'description', 'label'])\n",
            "within project split!\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4159833027cb>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usingbert tokenizer\n",
            "usingbert tokenizer\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4159833027cb>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_seq_titles = torch.tensor(title_tokens_train['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_att_mask_title = torch.tensor(title_tokens_train['attention_mask'])\n",
            "<ipython-input-7-4159833027cb>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_seq_descriptions = torch.tensor(description_tokens_train['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_att_mask_description = torch.tensor(description_tokens_train['attention_mask'])\n",
            "<ipython-input-7-4159833027cb>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_seq_titles = torch.tensor(title_tokens_val['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_seq__descriptions = torch.tensor(description_tokens_val['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_att_mask_title = torch.tensor(title_tokens_val['attention_mask'])\n",
            "<ipython-input-7-4159833027cb>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_att_mask_description = torch.tensor(description_tokens_val['attention_mask'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE :  35\n",
            "BATCH_SIZE :  35\n",
            "usingbert tokenizer\n",
            "usingbert tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4159833027cb>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_seq_title = torch.tensor(tokens_test_title['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_seq_description = torch.tensor(tokens_test_description['input_ids'])\n",
            "<ipython-input-7-4159833027cb>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_att_mask_title = torch.tensor(tokens_test_title['attention_mask'])\n",
            "<ipython-input-7-4159833027cb>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_att_mask_description = torch.tensor(tokens_test_description['attention_mask'])\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE :  35\n",
            "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
            ">>> epoch  0\n",
            " Average training MAE loss: 3.14\n",
            "-\n",
            " Average eval MAE loss: 1.43\n",
            "===============================\n",
            "MAE:  1.3843683\n",
            "MdAE:  0.17981529\n",
            ">>> epoch  1\n",
            " Average training MAE loss: 2.68\n",
            "-\n",
            " Average eval MAE loss: 1.38\n",
            "===============================\n",
            "MAE:  1.304798\n",
            "MdAE:  0.0037961006\n",
            ">>> epoch  2\n",
            " Average training MAE loss: 2.83\n",
            "-\n",
            " Average eval MAE loss: 1.81\n",
            "===============================\n",
            "MAE:  1.6944574\n",
            "MdAE:  0.70111465\n",
            ">>> epoch  3\n",
            " Average training MAE loss: 2.67\n",
            "-\n",
            " Average eval MAE loss: 1.40\n",
            "===============================\n",
            "MAE:  1.3252746\n",
            "MdAE:  0.03975582\n",
            ">>> epoch  4\n",
            " Average training MAE loss: 2.78\n",
            "-\n",
            " Average eval MAE loss: 1.81\n",
            "===============================\n",
            "MAE:  1.6954187\n",
            "MdAE:  0.7028365\n",
            ">>> epoch  5\n",
            " Average training MAE loss: 2.62\n",
            "-\n",
            " Average eval MAE loss: 1.79\n",
            "===============================\n",
            "MAE:  1.6794996\n",
            "MdAE:  0.6743188\n",
            ">>> epoch  6\n",
            " Average training MAE loss: 2.65\n",
            "-\n",
            " Average eval MAE loss: 1.52\n",
            "===============================\n",
            "MAE:  1.4300699\n",
            "MdAE:  0.22748709\n",
            ">>> epoch  7\n",
            " Average training MAE loss: 2.71\n",
            "-\n",
            " Average eval MAE loss: 2.01\n",
            "===============================\n",
            "MAE:  1.8765589\n",
            "MdAE:  1.0273328\n",
            ">>> epoch  8\n",
            " Average training MAE loss: 2.69\n",
            "-\n",
            " Average eval MAE loss: 1.47\n",
            "===============================\n",
            "MAE:  1.389542\n",
            "MdAE:  0.15488577\n",
            ">>> epoch  9\n",
            " Average training MAE loss: 2.62\n",
            "-\n",
            " Average eval MAE loss: 1.74\n",
            "===============================\n",
            "MAE:  1.6319234\n",
            "MdAE:  0.58909035\n",
            ">>> epoch  10\n",
            " Average training MAE loss: 2.58\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  1.449547\n",
            "MdAE:  0.26237917\n",
            ">>> epoch  11\n",
            " Average training MAE loss: 2.64\n",
            "-\n",
            " Average eval MAE loss: 1.55\n",
            "===============================\n",
            "MAE:  1.4536899\n",
            "MdAE:  0.26980066\n",
            ">>> epoch  12\n",
            " Average training MAE loss: 2.63\n",
            "-\n",
            " Average eval MAE loss: 1.59\n",
            "===============================\n",
            "MAE:  1.4950172\n",
            "MdAE:  0.34383535\n",
            ">>> epoch  13\n",
            " Average training MAE loss: 2.64\n",
            "-\n",
            " Average eval MAE loss: 1.54\n",
            "===============================\n",
            "MAE:  1.4442096\n",
            "MdAE:  0.2528181\n",
            ">>> epoch  14\n",
            " Average training MAE loss: 2.57\n",
            "-\n",
            " Average eval MAE loss: 1.47\n",
            "===============================\n",
            "MAE:  1.3821075\n",
            "MdAE:  0.14156723\n",
            ">>> epoch  15\n",
            " Average training MAE loss: 2.65\n",
            "-\n",
            " Average eval MAE loss: 1.50\n",
            "===============================\n",
            "MAE:  1.4116722\n",
            "MdAE:  0.19453001\n",
            ">>> epoch  16\n",
            " Average training MAE loss: 2.57\n",
            "-\n",
            " Average eval MAE loss: 1.46\n",
            "===============================\n",
            "MAE:  1.3714976\n",
            "MdAE:  0.122560024\n",
            ">>> epoch  17\n",
            " Average training MAE loss: 2.63\n",
            "-\n",
            " Average eval MAE loss: 1.65\n",
            "===============================\n",
            "MAE:  1.5494887\n",
            "MdAE:  0.4414153\n",
            ">>> epoch  18\n",
            " Average training MAE loss: 2.62\n",
            "-\n",
            " Average eval MAE loss: 1.59\n",
            "===============================\n",
            "MAE:  1.4930172\n",
            "MdAE:  0.34025192\n",
            ">>> epoch  19\n",
            " Average training MAE loss: 2.60\n",
            "-\n",
            " Average eval MAE loss: 1.52\n",
            "===============================\n",
            "MAE:  1.4279414\n",
            "MdAE:  0.22367477\n",
            "all done for one project\n",
            "results have been written into a text file!\n"
          ]
        }
      ],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = True\n",
        "\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        # {'train': ['fusionx-set'], 'test': ['fusionx-set']},\n",
        "\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
        "                        # {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
        "                        # {'train': ['bamboo'], 'test': ['bamboo']},\n",
        "                        # {'train': ['clover'], 'test': ['clover']},\n",
        "                        # {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
        "                        # {'train': ['duracloud'], 'test': ['duracloud']},\n",
        "                        # {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
        "                        # {'train': ['mesos'], 'test': ['mesos']},\n",
        "                        # {'train': ['moodle'], 'test': ['moodle']},\n",
        "                        # {'train': ['mule'], 'test': ['mule']},\n",
        "                        # {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
        "                        # {'train': ['springxd'], 'test': ['springxd']},\n",
        "                       #  {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
        "                       #  {'train': ['talendesb'], 'test': ['talendesb']},\n",
        "                       # {'train': ['titanium'], 'test': ['titanium']},\n",
        "                       # {'train': ['usergrid'], 'test': ['usergrid']},\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        elif TOKENIZER == 'bert':\n",
        "            config = BertConfig(num_labels=1, pad_token_id=0)\n",
        "\n",
        "\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            if torch.cuda.is_available() :\n",
        "              MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            if torch.cuda.is_available():\n",
        "              MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2spen':\n",
        "            MODEL = GPT2SPEN.from_pretrained('gpt2', config=config)\n",
        "            if torch.cuda.is_available():\n",
        "              MODEL.cuda()\n",
        "        elif MODEL_NAME == 'berten':\n",
        "            MODEL = BertEN.from_pretrained('bert-base-uncased', config=config)\n",
        "            if torch.cuda.is_available():\n",
        "              MODEL.cuda()\n",
        "\n",
        "\n",
        "\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV0eL_3rAEbw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "pdb off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSte0lR_hfbo"
      },
      "source": [
        "### Cross Project Training Script - Within Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9vp2J2hfbo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# within repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
        "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
        "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
        "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'bbpe':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50257)\n",
        "        elif TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMcxbMB7hfbp"
      },
      "source": [
        "### Cross Project Training Script - Cross Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35iJqeeNhfbp"
      },
      "outputs": [],
      "source": [
        "global WITHIN_PROJECT\n",
        "WITHIN_PROJECT = False\n",
        "\n",
        "# cross repo\n",
        "TRAIN_TEST_FILE_PAIRS = [\n",
        "                        {'train': ['clover'], 'test': ['usergrid']},\n",
        "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
        "                        {'train': ['mule'], 'test': ['titanium']},\n",
        "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
        "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
        "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
        "                        ]\n",
        "\n",
        "\n",
        "def main():\n",
        "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME\n",
        "    for file in TRAIN_TEST_FILE_PAIRS:\n",
        "        if TOKENIZER == 'gpt2':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
        "        elif TOKENIZER == 'wordpiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'sentencepiece':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=0)\n",
        "        elif TOKENIZER == 'wordlevel':\n",
        "            config = GPT2Config(num_labels=1, pad_token_id=3)\n",
        "        if MODEL_NAME == 'gpt2':\n",
        "            MODEL = LinearGPT2.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        elif MODEL_NAME == 'gpt2sp':\n",
        "            MODEL = GPT2SP.from_pretrained('gpt2', config=config)\n",
        "            MODEL.cuda()\n",
        "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
        "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
        "        del MODEL\n",
        "        torch.cuda.empty_cache()\n",
        "        global OUTPUT\n",
        "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
        "            f.writelines(OUTPUT)\n",
        "            print('results have been written into a text file!')\n",
        "            OUTPUT = \"\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NBv9nKi45L8"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "conda-root-py",
      "name": "workbench-notebooks.m117",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "95502c86ed2e8b82df6e58f8450b4387aca3c902602792f25ea2aa6818e861bc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}