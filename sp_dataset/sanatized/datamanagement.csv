issuekey,title,description,storypoint,split_mark
DM-4,Transition git repositories to Stash,transition gitolite managed repositories to atlassian stash.,10,train
DM-8,Finalize DM mission statement,"the proposed mission statement for the lsst software stack development:  enabling lsst science by creating a well documented, stateoftheart, highperformance, scalable, multicamera, open source, o/ir survey data processing and analysis system  with the following rationale:   well documented  as otherwise it will be impossible to maintain or be usable for level 3  stateoftheart  because the quality of the instrument and needs of the science to be systematic limited require us to develop new algorithms an break new ground in a number of areas  highperformance  because of the massive amount of data we need to process for a reasonable budget  scalable  because we need it to run from between a single core (for developers, or level 3 users) to tens of thousands of cores (for data release production)  multicamera  because we need to process precursor data for development purposes, and because our asdelivered camera won't be ideal.  open source   because we want the community and future surveys to benefit from our efforts. ",2,train
DM-9,Open up LSST software mailing lists,"we all benefit from making lsst software development as open as possible and conducive to outside volunteer contributions (). one way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. for example, we could have:   softwaredevel@lsstcorp.org: the development mailing list, equivalent to current lsstdata  softwareusers@lsstcorp.org: the users mailing list, equivalent to current lsstdmstackusers mailing list (but it could possibly be replaced by stackoverflow/confluence questions)  lsstdm@lsstcorp.org: internal, dmstaff only mailing list, for the rare discussions/notices that should go out to staff only.  ( ) though we don't rely on them for meeting the project specs (legally required disclaimer :) ).",1,train
DM-10,Transition to Confluence Questions,"open the confluence questions site, for interaction with the community (and to enable community self help).  ",10,train
DM-17,Add derivatives-based optimizer to meas_multifit,"see https:/dev.lsstcorp.org/trac/ticket/3146  story points estimate is for remaining work only (just the code review, which is still substantial).",10,train
DM-20,Release EUPS 1.3.0,release eups 1.3.0 in rhl's github repository.,1,train
DM-21,Determine final URL/location for W'14 stack,need to know where w'14 stack files are going to be housed.,1,train
DM-22,Update newinstall.sh to check for existence of git and python on users' machines,nan,1,train
DM-23,Confirm stack builds on OS X 10.8,nan,1,train
DM-25,Build Winter'14 release,run lsst build scripts and 'eups distrib create' to build the winter'14 release.,1,train
DM-26,Update installation instructions,update confluence instructions on how to install the winter'14 stack.  the instructions are at: https:/confluence.lsstcorp.org/display/lswug/lsstsoftwareuser+guide,1,train
DM-28,Modify gitolite permissions to allow issue/DM-NNNN branches,"use issue/dm nnnn branches for issues tracked in jira, to differentiate them from tickets/nnnn branches that are still in trac.",1,train
DM-52,Qserv configuration - detailed design,detailed design covering how all qserv components will be configured for runtime. ,3,train
DM-53,Node configuration and bootstrapping - detailed design,"design covering how all qserv components will be configured for runtime.  design how new qserv nodes will be bootstrapped when we add them to the cluster, and how already added nodes will get updated after they were offline (crashed, turned off for maintenance etc) ",6,train
DM-55,Node bootstrapping - 1st prototype,nan,15,train
DM-56,Zookeeper-based CSS (v1),nan,40,train
DM-66,switch back to throwing exceptions in css/Facede.cc,nan,1,train
DM-70,Rewrite xrootd-facing code,"this task is related to dmtf1657009. it involves plugin in the new xrootd client (which has different interfaces, it is all async etc). we expect to also do a major cleanup on the code that is talking to xrootd while plugging in the new xrootd.",30,train
DM-71,Data Distribution Design v1,"need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from l2 ingest and user data (l3).",8,train
DM-75,Modify format of version numbers,"the versions autogenerated by the new eupsbuildbot look like this:   ========== $ eups list .... pipetasks            7.3.2.05g455c355d0f070b2c1b35  b61 pyfits                3.1.29ef17db9b7  b61 b60 python                0.0.1             b61 b60 scisql                0.32b5a2f1b52    b61 b60 scons                 2.1.02b5a2f1b52  b61 b60 sconsutils            6.2.0.011gf38997df3e759c3944a1         b61 sconsutils            6.2.0.019g755151c0a5759c3944a1         b60 shapelet              7.3.1.01g9331ee763c0c72f294dd  b61 shapelet              7.3.1.01g9331ee763ce56ee84a68  b60 skymap                7.3.1.01g64b750c066db36490146  b60 skymap                7.3.1.01ga6cd540cd3493a438aa2  b61 skypix                6.1.0.01g1157bf09ae65137c93cd  b61 skypix                6.1.0.01_gea335924636039c04989  b60 .... ==========   where the part before the plus sign is the output of git describe (slightly mangled), and the part after is the sha1 of the sorted namessha1s of the dependencies.  while this has the benefit that any two causally disconnected buildbots with the same inputs will build the same versions, many people have complained that they're plain ugly.  so here's an alternative proposal:   if a tag exist on a commit, use / as the version.  if there's no tag, use branchnamegsha1abbrev, where any illegal characters in branchname get turned into dots  if the package has dependencies, and a build of this package with different dependencies already exists, append a n to the end. keep the mapping of n > (dependency name, sha1s) in a special git repository. given the source code and this git repo, two causally disconnected buildbots will again generate the same set of versions.  example versions:  7.10.2.1  7.10.2.15  mastergdeadbeef  feature.dm1234gdeadbeef  feature.dm1234gdeadbeef3 ",1,train
DM-78,Save a git-branch when a forced push is detected,"create a gitolite hook that will save a branch when a forced push is detected.  e.g., if we have a ticket: 'tickets/dmaaaa' and someone rebases it and pushes  with 'force' before applying the update  then the hook will branch off the old state into (say):  backups/tickets/dm aaaa/nnnn where nnnn is a monotonically increasing number (per branch).",1,train
DM-83,from __future__ import division breaks division of Extent*,"if one does: from future import division the division operator on extent types raises an exception.  how to repeat: i've tried this with v73 and master:  from future_ import division import lsst.afw.geom as afwgeom npt = afwgeom.extent2i(10,10)/2  an exception is raised.  removing the first line succeeds as expected.",4,train
DM-85,Measurement - Aperture Corrections,"the current implementation of aperture corrections in measalgorithms' correctfluxes class is broken, and should not be replicated in measbase:   it only works when the psf model is correct   it doesn't work when the aperture to correct to is larger than the psf model image size    it doesn't propagate the uncertainty in the aperture correction  we probably need to do this by estimating the aperture correction and its errors on single frames, using the psf stars (not the psf models), then attaching that information to the psf object to be retrieved and coadded by coaddpsf.  jk: in pmcs this would be 10% bosch j 50% krughoff s and 40% owen r breakdown: jbosch 10%; krughoff 50%; rowen 40%",47,train
DM-90,Publish Winter 2014 binaries,nan,3,train
DM-92,tests/testPsfDetermination.py has a broken test,"in meas_algorithms tests/testpsfdetermination.py has a test testrejectblends which does not operate as expected. when it calls pcapsfdeterminer it results in no usable psf candidates before blends are rejected. formerly this resulted in a numpy array named ""sizes"" containing one uninitialized value, which might raise an unexpected exception or raise the desired exception, depending on whether that value was negative or positive.    on tickets/dm 3117 i pushed a fix for the bug that caused the invalid ""sizes"" array, but the unit test is now reliably broken because no viable psf candidates raises the wrong exception and does not test blend rejection in any case. so on this same ticket i have commented out the bad test for now.",2,train
DM-94,Configure transition screens for DM agile workflow,"whenever an issue is transitioned on jira agile board to 'ready for review', a screen should pop up to ask for a reviewer.  whenever it's moved out of that state, another screen should ask for a new assignee.",1,train
DM-95,Make lsst-build reuse buildIDs if nothing's changed,"all built packages are eups tagged with build ids (the bnnn eups tags). without this change, new eups tags are declared even when nothing changed since the previous build (and eups' tags code doesn't scale well at this time).  this will be implemented by comparing the newly built manifest against ones stored in versiondb, and reusing the build ids if a matching one is found.",1,train
DM-96,Write unit tests for lsst-build,"unit tests need to be written for lsst build; they should've been written together with the code, but due to winter'14 release fire drill they had to be postponed.",10,train
DM-98,clean up isr utility code,there is some commented code in isr.py.  this should be removed or updated so that it works.,2,train
DM-148,Improve naming of getters in AmpInfoTable,"the names of the methods to get values from a record on ampinfocatalog are potentially confusing.    this is because the convention is to call the getters get[attributename].  we could change the method names in the ampinfocatalog, or add methods in the swig wrapper.",1,train
DM-177,"Box2I(bbox.getMin(), bbox.getMax()) fails for an empty bbox","empty box2i cannot be round tripped:  from lsst.afw.geom import box2i b1 = box2i() b2 = box2i(b1.getmin(), b1.getmax()) assert b2.isempty()   it is confusing and surprising that this round tripping fails.  it is also a trap for the unwary because saving min and max is the logical way to store boxes in afw tables. records can contain points but not extents and so it saves casting back and forth and simplifies and clarifies the code to save max instead of extent. thus that is the path most users will take, and the problem can be a time bomb: it could be quite some time before somebody tries to store an empty box and finds that it does not get retrieved correctly.",1,train
DM-195,log4cxx-based logging prototype - v2 ,"this is continuation of dmtf1657016. initial work (v1) was done in branch u/bchick/protolog. v2 will include comments sent by kt and issues discussed at the qserv meeting march 13   free functions vs. a log object need to be discussed more.  in particular, when metadata key/value pairs need to be attached, an object might make more sense.  avoiding the getlogger() call when no logging is needed (due to threshold) can be significant.    it's a security breach to use vsprintf() with any userprovided arguments.  use vsnprintf() instead so that you can check for overflow. (or use stringstream or boost::format.)    in my prototype, i used a combination of a set of static log4cxx::levelptr variables with isenabledfor(level) and a set of cpp macros to avoid the switch.    the return value from getlogger() shouldn't need to be cast.      play with hierarchical names   don't execute code for formatting if debug level is off   use shorter threadid   experiment with defining special python handler, intercept and redirect     to our log4cxx based logging  ",15,train
DM-197,Replace PositionFunctor with some flavor of XYTransform,"afw has a special functor positionfunctor that acts like an xytransform. unless positionfunctor does not need to be invertible, it makes sense to merge these, likely by replacing positionfunctor with the transform from afw::image::xytransformfromwcspair (as suggested by jim bosch on trac ticket #2214).",1,train
DM-198,"Rework JOIN support, including Ref*Match tables",add support to the ref match tables. the relevant code in qserv core (supporting joins) has already been written. this task is related to dmtf164020,16,train
DM-199,Develop new master-worker result system ,"reimplement how results are returned from worker to the czar. currently it relies on mysqldump, which is fairly inefficient. this is related to dmtf1650045",20,train
DM-201,Qserv: unit testing (controller module) ,"design and build toy prototype of a test framework for testing controller module. this might require a mock framework, as we want to be able to test things in isolation, without testing everything around the controller module at the same time. this is related to dmtf1657020.",10,train
DM-202,Qserv: unit testing (query execution) ,"design and build toy prototype of a test framework for testing query execution module. this might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. this is related to dmtf1657021.",8,train
DM-203,Prepare for setting up new cluster at IN2P3 for continuous integration/testing,"once the hardware is available, setup the environment where we could easily run integration testing of different qserv releases, including testing/comparing performance.  integrate changes implemented in dm1078  add install script that exposes individual steps and allows modifications to the config file: newinstall, qservconfigure prepare, then edit config file (or copy from somewhere), qserv configure",4,train
DM-207,Migrate Qserv czar code to the new logging system,this includes switching qserv to the new logging. finetuning (what messages are printed deciding on error level) is covered in dm685.,10,train
DM-208,catch exceptions from CSS,nan,1,train
DM-210,S15 Data ingest,"rework existing scripts used to load data into plain mysql. this involves mostly simplifying it, and pushing some functionality like converting types outside of the loading scripts.   jk: refer to loading spreadsheet for pmcs assignments",10,train
DM-211,Revise design for Qserv front-end rearchitect ,"revisit the architecture. this includes proxy, down to xrootd client (mysqld, python, zookeeper). capture all findings in new stories, add these stories to dm 1707",10,train
DM-212,Migrate away from using env variables in Qserv,qserv is currently relying on many env variables. we should migrated away from that to the extend possible.,10,train
DM-213,Setup multi-node testbed,"it'd be useful to test qserv using winter2014 or summer2014 data set on a multi node cluster, just to exercise all pieces of the software and double check we are not missing anything.",5,train
DM-214,W15 C++ geometry,"port the geometry related code used by qserv (it is currently written in python) to c, and switch qserv to the c based version.  jk: refer to loading spreadsheet for pmcs assignments",47,train
DM-215,Implement C++ geometry primitives for Qserv,nan,10,train
DM-216,Switch Qserv to C++ geometry primitives,nan,10,train
DM-218,Experiment with no-subchunking based approach,nan,10,train
DM-219,Qserv worker scheduler – code cleanup,"the qserv worker scheduler code is a bit ugly.  the actual composable scheduler classes (fifoscheduler, scanscheduler, groupscheduler, blendscheduler) might be pretty clean, but the interactions with the rest (wdb, wcontrol) may be harder to understand.  there should be a small amount of low hanging fruit of code to clean up, but to make things more sensible and understandable may require some new abstractions and shuffling of logic to new/different classes.  since this issue was opened, some refactoring work has been done as part of the new xrdssi port, so the organization may be somewhat cleaner now. still, it's worth it to take a fresh look to evaluate the design/interactions to see how much can/should be reorganized.",6,train
DM-221,Implement new (async) XrootD client,"(oops, earlier description was meant for dm878. sorry for the confusion. danielw) (i also made a mess with the assignees. i'm sorry.  danielw)",30,train
DM-222,Reference Test Server using new XRootD,nan,5,train
DM-224,Switch to MariaDB,we should switch qserv to the mariadb foundation based mysql.,3,train
DM-228,Setup dev test environment,"setup whole qserv environment, including installing data set, and validate it by running some simple queries. suggest changes/improvements as appropriate.",8,train
DM-231,Refurbish existing configuration ,refurbish existing configuration scripts to make them work with the new packaging/build system.,10,train
DM-240,meas_base plugins for CModel magnitudes,"create measbase plugins for singleframe and forced measurement that uses the modelfitting primitives in measmultifit to implement sdssstyle cmodel magnitudes, in which we fit an exp and dev model separately and then fit the linear combination with ellipse parameters held fixed.  an oldstyle plugin has already been implemented on the hsc fork, and should be used as a guide; this issue involves adapting that implementation to measbase and potentially cleaning it up a bit.  note that the hsc implementation cannot be transferred directly to the lsst side because the measalgorithms apis are slightly different on the two forks.",6,train
DM-241,refactor forced tasks into two tasks,"after looking at it a bit more, i think we should refactor the current meas_base forced photometry task to separate the cmdlinetask from the measurement task.  this will allow the forced measurement task to share a common base class with singleframemeasurementtask (allowing us to move the callplugin free functions into that base class), and give us better parallels with existing tasks:   procesimageforcedtask (my proposed name for the base commandline task) will be more similar to processimagetask.  we'll also have processforcedccdtask and processforcedcoaddtask.    forcedmeasurementtask will be more similar to singleframemeasurementtask.  in short, i think this will both clean up the ugliness in callplugin and make the whole hierarchy easier for newcomers to understand.",6,train
DM-242,switch from '.' to '_' in afw::table fields,"we've been mapping '.' to '' in afw::table i/o, which unnecessarily complicates lots of things.  we'd like to switch to using '' in the field names themselves, which requires ending this mapping in i/o, but we need to be backwards compatible.  so we'll add a version to the fits headers, and continue the mapping if the version is not present or is less than some value.  until we do this, the new field names being used in meas_base won't round trip.",3,train
DM-245,Implement HSC camera in new camera framework,"the hsc camera needs to be put in the new camera geometry.  i will implement is like we did lsstsim and sdss.  that is i will generate a repository with the camera config and ampinfo fits files that can be unpersisted by the butler.  these changes should only require modifying obs_subaru.  my plan is to just use the policy file to populate the new geometry, but if there is more up to date information, i'll happily use that.",10,train
DM-246,Investigate compensation for Dcr,"this is a continuation of the w14 image differencing work.  we have characterized the negative effects of dcr on difference images, and now need to start working on compensation for these effects.  this work will also touch on the wcs and psf classes, which are probable consumers of this information.  scope includes designing and implementing a class to describe the astrometric effects of dcr, with consideration as to the other classes (psf, wcs) and tasks (imagedifferencingtask) that may use it",60,train
DM-247,Design of Dcr Class,"describes the design process for implementation of a class to model the effects of dcr.  includes design itself, and the design review process.",20,train
DM-248,Implementation of Dcr Class,"core implementation of the class that represents the effects of dcr.  this only includes the initial implementation.  we should realistically expect that this class will evolve as it encounters more use cases, no matter how thorough the design process is.  ",20,train
DM-259,Test and migrate to swig 3.0," original message  subject: [lsstdata] swig 3.0 is out (with c11 support) date: mon, 17 mar 2014 08:26:05 0400 from: robert lupton the good / to: lsst data /  i tried a prerelease on os/x 10.7.5 and it failed some tests, but i haven't tried this version.  i had some discussion about this with william, but haven't had time to follow through.         r   > date: sun, 16 mar 2014 22:44:42 0000 > from: william s fulton / >  >  announce: swig 3.0.0 (16 mar 2014)  >  > http:/ >  > we're pleased to announce swig3.0.0, the latest swig release. >  > what is swig? > ============= >  > swig is a software development tool that reads c/c header files and > generates the wrapper code needed to make c and c code accessible > from other programming languages including perl, python, tcl, ruby, > php, c#, go, java, lua, scheme (guile, mzscheme, chicken), d, ocaml, > pike, modula3, octave, r, common lisp (clisp, allegro cl, cffi, uffi). > swig can also export its parse tree in the form of xml and lisp > sexpressions.  major applications of swig include generation of > scripting language extension modules, rapid prototyping, testing, > and user interface development for large c/c systems. >  > availability > ============ > the release is available for download on sourceforge at >  >      http:/prdownloads.sourceforge.net/swig/swig3.0.0.tar.gz >  > a windows version is also available at >  >      http:/prdownloads.sourceforge.net/swig/swigwin3.0.0.zip >  > please report problems with this release to the swigdevel mailing list, > details at http:/ >  > release notes > ============= > swig3.0.0 summary: >  this is a major new release focusing primarily on c improvements. >  c+11 support added. please see documentation for details of supported >   features: http:/ >  nested class support added. this has been taken full advantage of in >   java and c#. other languages can use the nested classes, but require >   further work for a more natural integration into the target language. >   we urge folk knowledgeable in the other target languages to step >   forward and help with this effort. >  lua: improved metatables and support for %nspace. >  go 1.3 support added. >  python import improvements including relative imports. >  python 3.3 support completed. >  perl director support added. >  c# .net 2 support is now the minimum. generated using statements are >   replaced by fully qualified names. >  bug fixes and improvements to the following languages: >   c#, go, guile, java, lua, perl, php, python, octave, r, ruby, tcl >  various other bug fixes and improvements affecting all languages. >  note that this release contains some backwards incompatible changes >   in some languages. >  full detailed release notes are in the changes file. ",2,train
DM-260,Board workflow modifications,"on dm software development board, i propose we:   change ""ready to merge"" to ""review complete"". per kt:   that's what i thought ""review complete"" would be  most of the work is done, but some fixups are needed before merging, and a rereview is not necessary.     remove ""ready for review"". instead, the developer should just drag the issue to ""in review"" and assign it to a reviewer. if/when we reinstate the ""review master"", we may reintroduce ""ready for review""",1,train
DM-271,Setup the new Buildbot CI system,"setup the buildbot 0.8.8 testbed for the dm environment.  this includes: (1) setting up slaves on the set of common os on which the dm stack runs; (2) creating a new continuous integration slave using the new eupspkgbased build and distribution support,  definition of done:  every git change of master should trigger a build of master  if a build failed, an email will be sent to lsstdata (if the build succeeds, nothing happens)  failures due to internal buildbot issues (e.g., config problems, transient system availability issues, timeouts, etc.) should go to the buildbot owner.  allow usertriggered builds via web page (with specified refs to be built), with a common user (until we get lsstc ldap directory hooked up). it's understood that locking may not be fully implemented/fleshed out in this story.   it should be possible for a user on lsst dev to easily setup the stack for either a failed or a successful build. ",100,train
DM-272,Move TCT-relevant  twiki documentation to Confluence,"congregate all the trac tct relevant documents (standards, policies, guidelines, meeting history) onto confluence.",2,train
DM-273,Develop and then create the organizational structure for DM Confluence space,"before we start populating the dm confluence space with active pages, we should define an overall organizational structure/taxonomy.",1,train
DM-274,Adding support for numpy scalar array types as arguments to SWIG wrapped methods,"building against the anaconda on lsstdev causes construction of point2i (and other point types) with numpy dtypes to fail with the standard exception:  notimplementederror: wrong number or type of arguments for overloaded function  i did not know that was not allowed.  if i am doing it others are as well.  i think this should be addressed in some way before w14 release.  how to repeat on lsstdev:  setenv eupsdir lsstsw/stack/ source lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.point2i(5,5)\nval = numpy.int64(5)\nprint ag.point2i(val,val)"" > test.py python test.py   using the old 73 stack you can do a similar test:  source /lsst/dc3/stacks/default/loadlsst.csh setup  t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.point2i(5,5)\nval = numpy.int64(5)\nprint ag.point2i(val,val)"" > test.py python test.py ",3,train
DM-278,Improve handling errors occuring in AsyncQueryManager,"asyncquerymanager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readconfig()) and gracefully handled upstream.",1,train
DM-280,clean up multiple aperture photometry code,"i've been doing some minor work on the hscside apertureflux algorithm, and i wanted to record some concerns here (from both me and rhl) that should be addressed in the new meas_base version:   we should consider merging apertureflux and ellipticalapertureflux into the same algorithm (with a config field to choose whether to use elliptical apertures).  we could still register it twice, with a different default config value, and this should eliminate a lot of code duplication.  we could also consider having them inherit from a common base class (instead of having ellipticalapertureflux inherit from apertureflux, as is done now).   we should test that the threshold at which we switch from sinc to naive apertures is obeyed exactly.   we should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point.  if possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out.   (somewhat offtopic) we should consider having utility functions on sourceslotconfig to set all slots to none for use in unit tests.",5,train
DM-289,More helpful location information for errors in duplicator/partitioner input,"currently, if the qserv duplicator or partitioner encounters erroneous input (such as a formatting error, or missing columns), the error message it outputs does not include a mapping back to the location of the error, making it very hard/annoying indeed to fix that erroneous input.  fabrice would (quite reasonably) like to see a filename and line number in the error message.  unfortunately, producing a line number is complicated by the following facts:   multiple threads may be reading subsections of the same input file in parallel   subsections are not guaranteed to be read in order    lines can have varying length  in other words, the code reading/parsing the input has no idea which line it is working on, and making that information available would involve either deferring error reports or extra synchronization (performance loss).  what we can easily (and should) do is to arrange for processing code to know the file and byte offset of the input text being processed; error messages should include both pieces of information.",2,train
DM-290,Eliminate dependence of query analysis on parser and antlr,"i would like to write and compile query analyzer code completely independently of the parser and antlr (transitively). this doesn't seem to work right now. this is not currently possible.  this might take any where from a day to a week. (i'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster).  updates to follow after the scope is estimated.  dependencies to be broken: query > parser, antlr (due to predicate depending on antlr nodes) qana > parser, antlr ",8,train
DM-291,Investigate new XrdSsi interface,nan,10,train
DM-295,xrootd initialization should abort if mysql connection fails,"currently xrootd will happily start even if it can't connect to mysql, it will only print a message:   configration invalid: unable to connect to mysql with config:   this can be easily overlooked, plus, it is a fatal error and xrootd initialization should be aborted.  while working on this, i propose to also improve validatemysql(). at the moment it connects to mysql and database in one call. if connection is fine, but the database does not exist, it will fail without telling user why it failed. this can be confusing. it'd better to connect to mysql without connecting to database, then do ""select_db"", and if that fails, inform the user that the database does not exist.  (transferred from trac ticket 3165)",1,train
DM-296,fix namespaces in all Qserv core modules,"this was suggested by the code review for ticket 1945 (https:/dev.lsstcorp.org/trac/wiki/sat/codereviews/1945), pasted below:  common/src/ :  while it's not required by the coding standards, i'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::.  at some point i'd recommend changing the header file extension from .hh to .h to match the rest of the lsst dm code, unless it's a big backwards compatibility issue.  (transferred from trac ticket 2528)",5,train
DM-297,Qserv should check for loaded spatial UDFs,"qserv should have a way of checking for the existence of spatial udfs loaded in the worker mysql instances.  obviously, the worker must perform the physical check. however, the worker has no knowledge that spatial udfs even exist, since the master is responsible for translating spatial spec into udf call.  at the moment, the preferred way of checking would be some sort of administrative command that runs on all nodes. an alternative would be some sanity check that is run on a worker before an admin starts a worker. or the master could devise a mysql query to check for things and dispatch to all chunks (although this would not get full coverage when replica exist, while being redundant while workers host more than 1 chunk).  (transferred from trac ticket 1959)",2,train
DM-298,restarting mysqld breaks qserv,"restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). the error message is:  error 2013 (hy000): lost connection to mysql server during query this happens most likely because qserv caches the connection, which becomes invalid when server is restarted. i am guessing the same will happen when there is a long period of inactivity (the connection times out).  (transferred from trac 2853)",1,train
DM-300,Centralize hardcoded constants,"some values in qserv need to become constant(e.g. chunkid column names, dirs, filenames). some of these are configurable, others are hardcoded in nonobvious places in the code. when multiple places need this value, they really need to agree, and unfortunately, qserv doesn't have a wellknown place for these constants yet.  any (constant) value that is needed by different parts of the code needs to be managed in a way that is reasonably obvious to unfamiliar programmers.  list of values:  chunkid, subchunkid column names (master...indexing.py, app.py)  environment variable names   + others. this should actually be fairly simple to implement, once the right (?) design is conceived and worked out.  (transferred from trac ticket #2405)",6,train
DM-309,Jira for Qserv,"jira setup for qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more...",8,train
DM-312,Come up with a standard to handle C++ Exceptions in Qserv (and the rest of DM?),"currently css is using one class and relies on different error codes to differentiate between different type of exceptions, while other parts of qserv core define an exception for each different error. it'd be good to standardize and use the same approach. ",4,train
DM-313,cleanup includes in Qserv core modules,"includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. also, unify forward declarations.",2,train
DM-318,CSS - surviving mysql and zookeeper glitches,css should gracefully recover from failures such as lost connection to mysqld or zookeeper. it is not clear if it would survive such glitches right now  this needs to be tested and the code improved as necessary.,4,train
DM-319,Create a board (virtual or otherwise) with pictures and names of everyone in DM,i tried compiling a list of everyone who works or has worked on the dm code  it was nearly impossible. we should have a (public) list both of current staff and our alumni (and where they are now).,1,train
DM-321,Re-think thread.cc and dispatcher.cc python interface,the mess of thread.cc and dispatcher.cc need to be rethought and redesigned so that the interface is smaller and more obvious.  ,2,train
DM-322,Trim python importing by czar in app.py,"clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master./   (migrated from trac #2369)",2,train
DM-326,"Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib","a report from darko jevremovic /:  hi mario,  i managed to build stack v8 on opensuse13.1  there were standard problems with lib/lib64  namely system builds libraries in $prefix/lib64 and some programs are hard wired for $prefix/lib  if you could  change the last line of  mysqlclient5.1.65+3/ups/eupspkg.cfg.sh  from  (cd $prefix/lib && ln s mysql/ . )  to  ( cd $prefix && if [ ! f ""lib"" ] ; then  ln sf lib64 lib; fi &&cd $prefix/lib && ln s mysql/ . )  or something along that line (am not sure whether the syntax would  work).  also if you could add  in the same manner to ups/eupspkg.cfg.sh  ( cd $prefix && if [ ! f ""lib"" ] ; then  ln sf lib64 lib; fi)  for the following packages:  minuit2 gsl cfitsio wcslib  ",1,train
DM-327,Take RAM into account when computing NCORES to use in installs,"darko jevremovic reported he's had to switch off hyperthreading and manually override ncores, makeflags and sconflags because his 8core machine had too little ram to build afw with j 8.  to fix this, eupspkg default build routines should take ram into account when computing the level of build parallelism.  in the meantime, we should document the workaround (contact darko@aob.rs).",1,train
DM-330,Local lsst-build invocations should use a different build number prefix,"buildbotinvoked lsstbuild installs packages in the stack with ""b#"" tags.  these are propagated to the distribution server by eups distrib create.  local stacks maintained with lsst build should use different tags so that they don't conflict with these distributed tags.",1,train
DM-331,S14 Enhancements in Qserv installation procedure,nan,15,train
DM-334,Cut Qserv release,"it'd be very useful to have fully functioning qserv release with the latest set of changes (build, packaging, css, daniel's fixes etc) during the hackathon week.",2,train
DM-335,Migrate std::lists to std::vectors,"suggested by andy when reviewing dm296, discussed at qserv mtg 3/27.  std::list  > std::vector   why? default now is vector, iterating over vector is much more efficient than over list   revisit on case by case bases, do not blindly replace    preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector",8,train
DM-336,improve code that initializes shared_ptrs ,"reported by andy when reviewing dm296. discussed at qserv mtg 3/27.   boost::sharedptr =(new t())""  > boost::makeshared()",4,train
DM-337,removed dead code in stringUtil.h,remove obsolete strtodoublefunc (and more) in util/stringutil.h.,1,train
DM-339,Make it easy to build and release point releases,nan,4,train
DM-342,Establish github mirror of LSST repositories,nan,4,train
DM-343,Transition to Stash,nan,10,train
DM-345,Enable gravatars,could you enable gravatars for all our atlassian products (at least jira  agile; confluence)  https:/confluence.atlassian.com/display/aod/configuringgravatar+support,1,train
DM-354,Add cameraGeom overview to Doxygen documentation,the camerageom package needs an overview page (part of afw's main.dox) as part of the doxygen documentation. i think it's up to simon or me to add this.,2,train
DM-355,Install and tag multiple Qserv versions on the same distserver,done in dm 366,1,train
DM-359,Simplify Co-add example in Software User Guide,"the current example in the lsst software user guide for coaddition reflects the processes necessary to perform a dr production. while thorough, it only really works on the lsst cluster. the example should be simplified to work on a smaller subset of data, and on singleuser machines.  definition of done:  following the documentation, it will be possible for users to identify the sdss data they need (the subset of files)  there will be instructions on how to download the necessary files to their local machine  there will be instructions on how to build the necessary repositories  there will be instructions on how to run the co add+forced photometry tasks.    any issues requiring access rights to lsst machines or databases will be identified and issues created for later.",8,train
DM-365,Integration tests dataset should be packaged in eupspkg,"a qservintegrationtests package should be created :  it would allow to manage easily, in ups/qserv.table, tests version for a given qserv version.  it would allow to install qserv dependencies related to testing, like partition (and other data ingest code which may arrive.",3,train
DM-366,Refactor install/distribution procedures using lsst-sw," here's andy salnikov remark, during #3100 review : https:/dev.lsstcorp.org/trac/ticket/3100#comment:18",5,train
DM-367,CSS performance optimizations (avoiding redundant checks),"facade.cc: it seems worthwhile to think about how to tweak the implementation to reduce redundant calls. e.g., getchunkedtables() calls _cssi>exists() for the database for each contained table. it also calls exists() for each table, which it just retrieved. in reality, it only needs to call exists() once, for the db. so we are doing 13t reads rather than 1t.  (this came up in the review of dm56, the review comments are captured in dm 225) ",8,train
DM-368,integrate qserv_admin backend into czar or separate admind,"client/qservadminimpl.py: i know the czar's python code is ugly. but the concept of having more than one entry point for users is also ugly. i feel like the client should just wrap up stuff into json and make a rest call into the czar. ideally the proxy would use rest, perhaps via: https:/github.com/fperrad/luaspore/, which at least seems more thoughtout than luaxmlrpc.  (this came up in the review of dm56, the review comments are captured in dm 225)",15,train
DM-369,Improve how CSS exceptions are handled,"css has one class for all exceptions. the model adopted by daniel (each type as a separate exception) is better, as we can catch individual exceptions  instead of (a) having to catch all css exceptions, (b) checking type and (c) rethrowing if it is not the type we want. to be able to catch all css exceptions, we can just introduce a new base class (that is new comparing to daniel's version).",6,train
DM-370,improved how default values for CSS are handled,"need to improve how defaults are handled in qserv_admin. there seems to be some desire to warn when values are not sethow about setting defaults and just printing what configuration is being used? if this is something humancreated, we should have reasonable defaults and not bother the user, unless no default is viable. i think we should only be strict on machinegenerated input, where we would like to catch bugs as soon as possible.   (this came up in the review of dm56, the review comments are captured in dm225)",5,train
DM-372,fix testQueryAnalysis,5 tests fail in the testcppparser.,2,train
DM-373,Catch AttributeError problems in czar,"i wonder if we could catch more exceptions in czar to simplify debugging. for example, if i change:   a/core/modules/czar/lsst/qserv/master/app.py  b/core/modules/czar/lsst/qserv/master/app.py @@ 418,7 418,7 @@ class inbandqueryaction:                     self.constraints.size())          dominantdb = getdominantdb(self.sessionid)          dbstriping = getdbstriping(self.sessionid)          if (dbstriping.stripes /, attributeerror('x',), /)   with no other clues, traceback or information in the log. ",2,train
DM-375,"S14 Improve error handling in all parts of Qserv, report sensible errors to users","qserv needs to handle error gracefully. no matter what error occurs, it should try to automatically recover, and if it can't it should report a reasonable error to user. we should try to poke around and trigger various errors in random places in qserv and watch what happens, how system fails and/or how it reports the error to user (then we should fix it...). i expect we will need to design a better error handling framework to achieve graceful error handling.",60,train
DM-380,"loadLSST bug(s) for csh, ksh","a flaw in the v8.0 loadlsst scripts (and/or in eups/bin/setups) causes the following errors:     1) when using ksh:        $installdir/loadlsst.ksh     ksh: /path/to/installdir/eups/bin/setups.ksh: cannot open [no such file or directory]    and indeed, there is no eups/bin/setups.ksh file.     2) when attempting to run the installation demo (v7.2.0.0):       $> printenv shell     /bin/tcsh    [the same issue appears with csh, unsurprisingly.]       $> source /path/to/installdir/loadlsst.csh     $> cd /path/to/demo     $> setup obssdss     $> ./bin/demo.sh     ./bin/demo.sh: line 7:  /volumes/d0/lsst/stack80/eups/default/bin/setups.sh: no such file or directory     ./bin/demo.sh: line 12: setup: command not found    after hand editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion.     note that everything works fine for bash with v8.0, which is what i tested awhile back. ",1,train
DM-384,Add Versioning to SourceTable in lsst::afw::table,"add version to afw::table::sourcetable.  persist that version number to fits file when the table is saved, and restore when the table is restored.  tables created and saved to disk prior to this modification will have the version number 0, by default.  tables created with the s14 version will have the version number 1.    this change is to enable a new version of slots and field naming conventions as needed by the measurement framework overhaul, at the same time allowing current clients of sourcetable to continue to function.  the work to define and persist the slots depending on the version will be on a separate issue.  should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded.  getversion and setversion methods will be used to allow clients to alter this number.",1,train
DM-386,Create Command/Event Sender,"simulate the ocs and ccs (via the ocs) sending message to the base dmcs.  write a library to send commands via method calls, which will be used by commandable  entities and by the base dmcs.  the method calls in this library will be used to simulate sending commands from the ocs.  this will initially be developed using dm messages, and later switched to use dds.  write a command line tool to send these messages.  commands with no arguments: init, enable, disable, release, stop, abort, reset.  commands with arguments: configure   arguments are: set of computers, software and versions to be executed, parameters used to control that software.  events with arguments: startintegration, nextvisit.  definition of done:   a library that has method calls to each of these commands/events.  each method call sends one message to the given topic.  command line tool that can send any of these commands/events to a commandable entity subscribed to a topic.   unit tests for each of the commands/events",6,train
DM-387,Build Base DMCS communications library,"write a library to be used by each commandable entity and the base dmcs.   methods in this library receiving commands from the simulated ocs. specific command actions will be handled by each entity, and events are handled by the base dmcs.  this will initially be developed using dm messages, and later switched to use the dds.  the library will include:   an object with methods for blocking receives, blocking receives with timeouts, and non blocking receives.  any commands received by these methods are given to another class to call appropriate action methods.  an abstract class implementing each of the following methods for commands: init, configure, enable, disable, release, stop, abort, reset.   an abstract class with methods for implementing the following methods for events:  startintegration and nextvisit",6,train
DM-388,Build replicator,"replicator  on boot:   establishes connection with single, preconfigured distributor.  checks connection with the network outage buffer, the base raw image cache, and the tape archive.  on success, the replicator registers itself with the orchestration manager in the fullyoperational pool.  from kt: note that what to check should eventually be dependent on what was specified in the configuration.   that way, if the tape library is down, we could still run using the network outage buffer or raw image cache as the redundancy source.  in operation:   receives a job with visit id, exposure sequence number within the visit and raft id.  queries the base efd replica for information needed to process the image.  subscribes to the camera data system (cds) “startreadout” event and to the ocs telemetry topics.  waits for a startreadout event  request the crosstalk corrected exposure for the raft using the cds client interface and block until it is available.  on receipt of crosstalkcorrected image:   verify integrity of image (hash or checksum)  send image with telemetry to the distributor, compressing it if the configuration says to do so.  at the same time, the image is written to the network outage buffer and the raw image cache using the data access client framework, retrying until successful for a configured number of tries. all images that are written are tagged with the archiver mode (by the replicator job).   request the raw image using the cds client interface and block until it is available.  on receipt of the raw image:   verify integrity of image (hash or checksum)  send image to the distributor and simultaneously write to the network outage buffer and the tape archive.  all image transmission statuses (successful and unsuccessful) are recorded to the alert production database.  note:  image some calibration or engineering modes, there may only be rawimage data, not crosstalkcorrected image data.  the replicator job configuration will provide for this).  note: replicator jobs will need to detect they are not completing in the expected amount of time.  note: something (the base dmcs?  the event/message monitor?) needs to pay attention to the number of replicator reregistering themselves with the localonly pool and notify netops to investigate and resolve the issue.  on errors sending or keeping connected to the distributor, the replicator:  unregisters from the “fullyoperational” pool  registers with the “localonly” pool  the replicator will continue to try and connect to the distributor, and if it is able to reestablish the connection:  unregisters from the “localonly” pool  registers with the “fullyoperational” pool  heartbeat messages between replicators and distributors will indicate when the replicators will be able to reregister with the fully operational pool.",10,train
DM-389,Build distributor,"distributor  on boot:   checks the network, archive raw image cache and the tape archive.  after all have tested successfully, the distributor waits for a connection from its associated replicator.  in operation:   on receipt of a visit id, exposure sequence number and raft id from its associated replicator, the distributor publishes them along with its network address to the archive dmcs.  on receipt of crosstalkcorrected image and associated telemetry from its associated replicator:   verifies integrity using a hash or checksum  writes to the raw image cache using the data access client framework  decompresses (if necessary)  separates into individual ccdsized portions  sends portions to the appropriate connected workers  note: workers can connect to request a ccdsized crosstalkcorrected image.  on receipt of raw image:   writes it to the tape system.   all images are tagged with the archive mode.   should include heartbeat monitoring of the replicator/distributor connection.",10,train
DM-405,Write Linux Standard Base - compliant init.d scripts,"qserv services init.d scripts have to rely on lsb, in order to work on multiple systems.  remark : xrootd has to be launched as a background process (i.e. with a & at the end). but this always send of return code equal to 0, even if xrootd fails to start, a shell function waitforpid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script).",5,train
DM-413,Simulate computation and production of VOEvents for worker batch job,simulate jobs which do alert processing.  produce voevents based on those results.,6,train
DM-417,finish adding aliases to afw::table::Schema,"this issue picks up the partiallycompleted trac ticket #2351, which adds a stringsubstitutionbased alias mechanism to afw::table::schema. there are still some issues to sort out w.r.t. constness  in other respects, a table or catalog's schema cannot be changed once the table has been constructed, but we do need to be able to change aliases after table construction.",6,train
DM-419,Use aliases in slots,nan,2,train
DM-420,Remove measurement code from meas_algorithms,nan,4,train
DM-421,add basic FunctorKeys,"add the basic functorkeys mechanism, and enough implementations to support slots.",4,train
DM-422,Add FunctorKeys to replace compound field functionality,add more functorkeys to replace the functionality in all current compound field types and measbase result objects.  may involve moving some measbase definitions to afw.,4,train
DM-423,Add FunctorKeys for common analysis tasks,"add functorkeys for simple, common, calculated fields, including:   magnitudes from fluxes   coords from points, points from coords    ellipse conversions and radius/ellipticity extraction",4,train
DM-424,Integrate FunctorKeys with SchemaMapper,nan,10,train
DM-425,Remove support for compound field types,nan,2,train
DM-428,add live DS9-based debugging to measurement framework,"the old measurement framework had a lot of live ds9 based display options.  we should ensure the new one has at least as many, and that it still works.    at some point, we should consider a different mechanism for enabling those displays and possibly other tools for displaying them, but that's out of scope for this issue.",6,train
DM-429,Make NoiseReplacer outputs reproduceable,"we need a way to get back the noisereplaced exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noisereplacement on all the previous objects again.  there is already code in afw::math::random to output its state as a string; i think we should probably just save this string in the output catalog.  this will require some api changes to allow the noisereplacer to modify the schema and set a field in the output records.",3,train
DM-430,Control log levels on a per-plugin basis,we should be able to control log levels so that certain plugins are run at one level while the rest are run at another (to allow a particular plugin being debugged to be more verbose).,4,train
DM-433,Add slot support for meas_base-style outputs,"the slot mechanism in afwtable currently uses compound fields to save the 3 slot types:  flux, centroid, and shape.  since the new measurement framework uses a flattened representation in the sourcetable where these types are saved as multiple scalar fields, the slot mechanism need to be altered to handle this new table type.  1.  an alternative to keytuple for storing the keys required by the slot 2.  fixup get(centroid, flux, shape) in sourcerecord to use correct keys. 3.  fixup the single value getters (getx, gety, etc) to use the correct keys. 4.  persist slot info to fits correctly, based on table version.",4,train
DM-435,add aperture-correction measurement code to the end of calibrate,"at the end of calibratetask, we'll want to compute the psf and aperture fluxes of the psf stars, and send those to the psf model to be stored and interpolated (using the featured added via dm434).  we'll also need to run any other flux measurement algorithms that need to be tied to the psf fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the psf stars, rather than requiring all these algorithms to be run as part of calibrate.measurement.  the relationships between these fluxes and the psf fluxes will be additional fields to be added to and interpolated by the psf.  the hsc implementation of this work (as well as that of dm436) was done on issue hsc191: https:/hscjira.astro.princeton.edu/jira/browse/hsc191 there were changes to many packages, but the relevant ones for lsst are: https:/github.com/hypersuprimecam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https:/github.com/hypersuprimecam/measalgorithms/compare/hsc3.0.0...u/jbosch/dm191 https:/github.com/hypersuprimecam/pipetasks/compare/4c3a53e7238cbe9...u/jbosch/dm 191",8,train
DM-436,apply aperture corrections in measurement tasks,"we need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes.  the hsc implementation of this work (as well as that of dm435) was done on issue hsc191: https:/hscjira.astro.princeton.edu/jira/browse/hsc191 there were changes to many packages, but the relevant ones for lsst are: https:/github.com/hypersuprimecam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https:/github.com/hypersuprimecam/measalgorithms/compare/hsc3.0.0...u/jbosch/dm191 https:/github.com/hypersuprimecam/pipetasks/compare/4c3a53e7238cbe9...u/jbosch/dm191  note that on the lsst side, we'll want to apply the aperture corrections either within a new plugin in measbase or as a new part of basemeasurementtask, not as a change to the correctfluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in measalgorithms).",7,train
DM-441,Setup of four new measurement algorithms for processCcd testing,"the goal of this story is to take the following algorithms and make them fully operational with processccd.  psfflux,sdssshape,sdsscentroid,sincflux.  this code was moved to measbase in w14, but has yet to be used in full operation.  this is the first step in that process.  each algorithm will at least have one test to confirm that it works, but the unit tests will be very simple.  the actual confirmation of full operability will the to run tests against real data, and compare against the existing measalgorithms.  the algorithm code will still require cleanup even after this story is completed.  that is because it has intentionally been left the same as what existed in measalgorithms.    the goal of this ticket is to confirm that the each algorithm can (1) complete its measurement with some reasonable result, (2) responds to its major configuration options, and (3) handles at least some of its defined exceptions (that is, flags) correctly.  confirmation that the results are identical with measalgorithms is a subsequent ticket  cleanup and documentation of this code will be done in a subsequent ticket.",4,train
DM-443,Approve/Acquire HipChat licenses," original message  subject: getting hipchat licenses date: wed, 09 apr 2014 08:51:46 0700 from: mario juric / organization: large synoptic survey telescope inc. to: jeffrey kantor /, iain goodenow /,  stefan dimmick /  jeff et al.,  i'd like to start the process to acquire hipchat licenses (we have 10 days left on our evaluation one). right now we have 26 users, so our first total would be $52/month. i'm expecting it will eventually grow to ~50 users or so.   in terms of an account to charge, this should really be considered a projectwide tool, but if that's going to cause unnecessary delays i'd propose we charge it to dm as a sign of our infinite kindness and good will :).   hipchat wants to simply bill a credit card  once we get the necessary approvals, iain, i can add you as an admin, and you can use our corporate one if that's ok. ",1,train
DM-444,Write job ads for Tucson DM positions,nan,4,train
DM-445,Test four algorithms for compatibility with original meas_algorithms,"do a trial run of a small area of sky (using a single exposure from measmosaicdata).  first create a source catalog using the old measurement.py, then run the same test with the measurement task in sfm.py.  compare the results.  if the code has been ported correctly, they should match.",4,train
DM-446,Setup PeakLikelihoodFlux with new Algorithm Framework,move peaklikehoodflux to meas_base framework ,1,train
DM-447,Setup Flux algorithms for testing with processCcd,"similar to dm 441 for flux algorithms gaussianflux and naiveflux  unit tests for major config options, just to be sure that they do something reasonable.  test of at least one exception (flag)",2,train
DM-448,Test Centroid algorithms against meas_algorithms,nan,2,train
DM-449,Test Flux algorithms agains meas_algorithms,"test for compatibility of naiveflux, gaussianflux, and psfflux against meas_algorithms  ",2,train
DM-454,reimplement shapelet PSF approximations,"the cmodel code we want to transfer from hsc in dm240 currently relies on the old ""multishapelet.psf"" algorithm in measextensionsmultishapelet.  that means we either need to convert that psftoshapelets code in measextensionsmultishapelet to use the measbase interface, or we need add new psftoshapelets code in measmultifit.  i think the latter is the better choice, even if we delay dm240 as a result; the heavy algorithmic code is already available as primitives in meas_multifit, so it should just be a matter of packing those into a simple driver, creating a config class for it, and testing it on a few real and simulated psfs to learn reasonable defaults for the configs.",6,train
DM-460,Implement backup/restore for CSS,"it'd be very useful to have a way to ""dump"" and ""restore"" entire contents of the key/value store in zookeeper. the dump part is pretty much there (can dump to an ascii file) ",4,train
DM-461,Add Classes of MeasurementError,"some measurement failures are global for a whole exposure, such as a missing psf or wcs.  the framework currently does not distinguish this from a failure in a single measurement.  should add a new subclass of measurementerror which can be thrown in these cases.  should also add configuration option to the measurement framework to determine what should be done with this type of error.  we should also add an exception class and associated howtohandle config for problems that indicate that something has gone wrong in pre measurement processing, such as nans in the image.",4,train
DM-463,"PixelsFlags, SkyCoordAlgorithm, and Classification","skycoord was moved to dm441  done in python  classification is also simple if done in python  pixel keys will be done in c  some work left from sdssshape will be done with this ticket  also, since these are out only two python algorithms in the base set, i will add the exception handling and base fail() methods at this time. ",2,train
DM-464,"add and use ""suspect"" flag in slots and slot-like measurements","we need to have both ""fatal"" and ""suspect"" flags to handle different levels of warnings in measurement.  (other tasks previously included on this ticket have been split into other tickets, including dm461 and dm984)",4,train
DM-466,lsst-build updates based on feedback from 1st month of use,"change lsstbuild interface to:    rename 'prepare' to 'clone'    use current directory to clone to    remove the default repository_pattern, as it isn't complete    use the current username as default build tag prefix    don't write the manifest by default; use manifest=/ option instead    refuse to change/overwrite repositories if they're dirty, use  force to override    create a separate 'version' verb    productfetcher.fetch doesn't need to return ref, sha1    read config file within build directory, .btconfig    rename the binary to bt     have the versioner use git db by default, but fall back to generating versions from hash by default  see the linked web page for a mock of the command line interface.",6,train
DM-467,Understand galfast bugs,nan,2,train
DM-468,Alias measurement.plugins to measurement.algorithms,"the config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  the creates a backward compatibility issue for code which refers to this class member.  jim's suggested fix is to alias plugins with algorithms in the new measurement task.",1,train
DM-470,Rework exceptions in css (python side),"rework exceptions in css/kwinterface.py: split into keyvalue related exceptions, possibly moving the rest that deals with db/tables into client.  this came up in the css review, see dm225: ""cssexception feels a bit out of place....""",1,train
DM-471,Create LSSTsim processing example for SW User Guide,"create a worked example in the software user guide of processing raw data with obslsstsim. the example should include:    pointers to documents for creating multiband, raw datasets with phosim  creating a data repository for the data  creating an astrometrynet_data repository  processing raw files through processcdc (including isr, measurement, etc.)  creating a catalog of measurements   visualizing the output using existing tools  it is possible to work initially with a limited set of data (a raft, or ccd), and to start with eimages rather than raw; the example can be elaborated in due course.",10,train
DM-472,Create an iPython Notebook visualization of the LSST Demo Data,"develop an ipython notebook to illustrate the processing and results of the lsst demo. this can be used as both a basic tutorial for the stack, and a how to for creating additional visualizations for dm. the visualization should include the source catalog, and possibly also one or more of the processed images. ",4,train
DM-473,Prioritize and define the backlog for Summer 2014,"at the dmlt meeting in seattle, finish the backlog prioritization for summer 2014 cycle.  definition of done:  all epics and major stories defined for summer 2014.  rank ordered in terms of priority.  leadership teem agreement on their prioritization and backlog.  teams identified to execute the stories, and stories labeled accordingly. ",2,train
DM-474,Document how to create an astrometry_net_data repository,"document how to create astrometrynetdata index files for an arbitrary astronomical dataset, as well as how to create a repository of such data to support processing with the lsst stack. it must be possible to limit the area of sky coverage in the index files to that appropriate for the images that the user wishes to process. ",6,train
DM-488,Make JIRA notification e-mail more useful,"from hipchat/data management:  [12:09] mario juric: @jbosch @ktl @ksk could you doublecheck if any of you got an email from jira on saturday (apr 12th) re issue dm78 (i made you reviewers, but it looks like you weren't notified)? [12:10] kt lim: i don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] simon krughoff: i did get an email. [12:10] jim bosch: @mjuric, ah, it appears that i actually did.  the fact that i was a reviewer was just buried, and i didn't notice it. [12:10] simon krughoff: i must have missed that i was a reviewer. [12:10] mario juric: ok, thanks!   that gives me not one, but two useful data points (#1  emails work, #2  they're useless :) ). [12:12] simon krughoff: i'm not sure why they are useless.  the emails from trac were a very important part of my workflow as far as being notified of review responsibility goes.   maybe it's just the volume from jira. [12:14] jim bosch: yeah, same here.  though the volume from jira hasn't been so bad, so i don't think that's it.  maybe my brain just has to get used to the new email format. [12:14] k t lim: (in my case, i'm mostly paying attention to the rss feed although the mailbox serves as a backup.) [12:22] robert lupton: one of the things that made gnats a good bug tracker was that the emails contained the right amount of information (i did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  i bet we can fiddle with jira to make its mail more useful;  i don't just mean filtering what it sends, but making sure that each email is self contained, but not too long",1,train
DM-495,Build Base DMCS Archiver Command Receiver,"base dmcs archiver  the archiver receives commands from the ocs, and performs the following actions:   init  move from offline mode to idle state.   configure  (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the ocs.  on failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the ocs.  prerequisite for configure command  sufficient replicator/distributor pairs are available.  note that after configure, the archiver remains disabled.   enable  subscribe to the startintegration event.   disable  unsubscribe to the startintegration event.  note that this does not terminate any replicator jobs which are already executing.   release  unsubscribe to the startintegration event, and go into offline state.   abort  if received during a configure command, causes this component to go into the error state with no configuration.  if received at any other time, the system transitions into an error state, but nothing stops that was already in progress.    reset  unsubscribe to the startintegration event, go into idle state, with no configuration",6,train
DM-502,Build Base DMCS Catch-Up Archiver Command Receiver,"base dmcs catchup archiver  the catchup archiver receives commands from the ocs, and performs the following actions:   init  move from offline mode to idle state.   configure  (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the ocs.  on failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the ocs.  prerequisite for configure command  sufficient catchupdedicated replicator/distributor pairs are available.  note that after configure, the archiver remains disabled.   enable  1) allows catchup archiver to scan for unarchived images to be handled; 2) enables the orchestration manager to schedule image archive jobs.   disable  1) stop scanning for unarchived images; 2) tell the orchestration manager to stop scheduling any new image archive jobs.   release  unsubscribe to the startintegration event, and go into offline state.   abort  if received during a configure command, causes this component to go into the error state with no configuration.  if received at any other time, the system transitions into an error state, but nothing stops that was already in progress.    reset   unsubscribe to the startintegration event, go into idle state, with no configuration",6,train
DM-503,Build Base DMCS EFD Replicator Command Receiver,"base dmcs efd replicator  the efd replicator receives commands from the ocs, and performs the following actions:   init  move from offline mode to idle state.   configure  (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the ocs.  on failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the ocs.  prerequisite for configure command  communication with the us dac efd replica is possible.  note that after configure, the efd replicator remains disabled.   enable  causes the base dmcs to enable the us dac efd replica to be a slave to the chilean dac efd replica.   disable  causes the base dmcs to disable the slave operation of the us dac efd replica.   release  unsubscribe to the startintegration event, and go into offline state.   abort  if received during a configure command, causes this component to go into the error state with no configuration.  if received at any other time, the system transitions into an error state, but nothing stops that was already in progress.    reset  unsubscribe to the startintegration event, go into idle state, with no configuration",6,train
DM-504,Build Base DMCS Alert Production Cluster Command Receiver,"base dmcs alert production cluster  the alert production cluster receives commands from the ocs, and performs the following actions:   init  move from offline mode to idle state.   configure  (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the ocs.  on failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the ocs.  prerequisite for configure command  sufficient workers are available.  note that after configure, the alter production cluster remains disabled.   enable  causes the base dmcs to subscribe to the “nextvisit” topic in normal science mode; another event may be subscribed to in calibration or engineering mode.   disable  causes the base dmcs to unsubscribe from the “nextvisit” topic.  it does not terminate any worker jobs already executing.  in particular, the processing for the current visit (not just exposure) will normally complete.   release  unsubscribe to the startintegration event, and go into offline state.   abort  if received during a configure command, causes this component to go into the error state with no configuration.  if received at any other time, the system transitions into an error state, but nothing stops that was already in progress.    reset  unsubscribe to the startintegration event, go into idle state, with no configuration",6,train
DM-505,improve initialization of kvMap in testQueryAnalysis,build the kvmap at buildtime and embed it into the executable. (this was brought up in dm225),1,train
DM-506,improve generating kvMap in testFacade.cc,"generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in dm 225)",1,train
DM-508,shorten internal names in zookeeper,rename database_partitioning to partitioning  rename databases to dbs,2,train
DM-509,"rename ""dbGroup"" to ""storageClass"" in CSS metadata","it is meant to be used to indicate l1, l2, l3... at qserv design week we decided to rename it (original plan was to remove it all together) ",1,train
DM-510,Tweak metadata structure for driving table and secondary index,"there seem to be confusion about driving table and secondary index. at the moment in zookeeper structure we have  /databases/objidindex /databases/tables/partitioning/secindexcolname /databases/tables/partitioning/drivingtable /databases/tables/partitioning/latcolname /databases/tables/partitioning/loncolname /databases/tables/partitioning/keycolname   issues to think about:   we can't call it objidindex, it is too lsstspecific.   drivingtable and keycolname  perhaps these should be at database level, which means we would only allow one drivingtable and one secondary index per database?   or, maybe instead of database level, it is a partitioning parameter? note that two databases might use different name for secondary index or driving table, yet they might be joinable. that argues for introducing a new group, something like /database/partitioning in addition to /database_partitioning.   consider renaming drivingtable to keytable    do we really need secindexcolname and keycolname? can't we get rid of one, and rename to keycolname? ",2,train
DM-511,rework ubuntu.patch,nan,1,train
DM-512,Generalizing data chunking (n-level chunking rather than stripes/subStripes),"it'd be cleaner to use numbering (e.g. 0 for no chunking, 1 for chunks, 2 for subchunks etc) instead of ""chunks"", ""subchunks"" throughout qserv code. this might also be true in partitioner, where flags like s and s etc are not entirely obvious.  this came up in the review of css, see dm 225.",3,train
DM-513,fix threading issues in CSS watcher,"fix problems with threads in watcher.py brought up in dm 225 by serge:   a thread per database doesn't scale   there is a thread leak when a database is deleted    there is another design problem, in that each database thread looks like it is holding on to the same lsst.db.db instance under the hood. i don't remember any consideration for thread safety from the lsst.db code when i reviewed it. note for one that it is not safe to use a mysql connection simultaneously from multiple threads (and i seem to recall that you are caching a connection inside db instances). in practice, even the python gil may not save you, since calls into c code (i.e. the mysql client library) may very well release it.",8,train
DM-514,"Switch to the ""czar"" name consistently",1) change lsst.qserv.master to  lsst.qserv.czar in the czar module.  2) rename masterlib to czarlib  3) rename startqserv to startczar ,4,train
DM-516,Fix race condition when creating db (and elsewhere?) in client/qserv_admin_impl.py,"this came up in the review of css, see dm225:  qservadminimpl: so, if there's a problem while inserting the various keys for a database, and another exception during cleanup, you can end up with a partially constructed database. how do you plan on handling this? this is perhaps another argument for trying to batch metadata more  if it's all in a single key, this worry goes away (at least in this particular case). the createdb implementations seem racy. consider what happens if 2 admins try to create the same database d. suppose admin a does the _dbexists(d) check first; it returns false. then admin b comes along, and does the same check, which returns false again. let's further suppose that admin b successfully issues all the key creates for d without being interrupted by a. back to a: when a tries to create all the keys required for d, an exception will be raised immediately since d exists. at this point a will dutifully try to clean up (to avoid leaving around a partially constructed d), deleting d as created by b. end result: d was successfully created, no explicit drop was issued, but d does not exist. database drops can also race with table creates: zookeeper doesn't have a native recursive delete. in other words, it is perfectly possible for an admin a to issue a database drop while someone else is issuing a table create. the recursive delete might descend into the table ""directory"" while the create is populating it with child znodes. the recursive delete will get an incomplete list of children, remove them, and finally fail to remove the table directory because the table create has since added more children, and it is illegal to remove a non empty directory. at this stage, the drop will report success even though it has failed (that's the recursive delete behavior that kazoo gives you), and the table create will report success. but the database will still exist, and table keys may be partially present in zookeeper.",10,train
DM-517,qserv_admin needs to deal with uncommon names/characters,"qserv/admin/bin/qserv_admin.py needs to deal with strange names (e.g. with embedded spaces or semi colons), or at minimum, catch and forbid them.",3,train
DM-518,Rework exceptions in qserv client,"there is a bunch of (i think) unnecessary translation from kvexception to qservadmexception. can't you just handle printing kvexception in commandparser.receivecommands(), and get rid of the csserr error code? (this is in /admin/bin/qservadmin.py)  (this came up in dm225)",1,train
DM-519,rethink configuration for client,"fetchoptionsfromconfigfile in client/qservadmin.py:: if you are going to use the configparser library, please use safeconfigparser (or rawconfigparser if you don't need string interpolation). but anyway, i'm not sure the ini file format is the right one to use here, as it forces the use of sections in parameter files, which doesn't make much sense (i notice your code just throws away the section names when reading parameter files). i found myself wishing that the admin client just had syntax for the various options, rather than relying on more or less undocumented parameter files.  (this came up in dm 225)",3,train
DM-520,Remove old partitioner/ loader and duplicator,"once fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in client/examples).",1,train
DM-521,Confusing error message (non-existing column referenced),"a query that references non existing column for nonpartitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"".  to repeat, run something like  select whatever from /;   similar error occurs when we try to reference nonexisting table, try something like:   select sce.filtername  from strangetable as s,       scienceccdexposure as sce  where  (s.scienceccdexposureid = sce.scienceccdexposureid);  ",5,train
DM-522,gracefully handle misconfigured scons,"when i try to run scons using the latest master (89aaa6), it fails with   scons: reading sconscript files ... attributeerror: 'nonetype' object has no attribute 'rfind':   file ""/home/becla/cssproto/qservcss6/sconstruct"", line 17:     state.init(srcdir)   file ""/home/becla/cssproto/qservcss6/sitescons/state.py"", line 161:     initenvironment(srcdir)   file ""/home/becla/cssproto/qservcss6/sitescons/state.py"", line 128:     initvariables(srcdir)   file ""/home/becla/cssproto/qservcss6/sitescons/state.py"", line 89:     (pathvariable('xrootddir', 'xrootd install dir', findprefix(""xrootd"", ""xrootd""), pathvariable.pathisdir)),   file ""/home/becla/cssproto/qservcss6/sitescons/state.py"", line 53:     (binpath, binname) = os.path.split(binfullpath)   file ""/usr/lib/python2.7/posixpath.py"", line 83:     i = p.rfind('/') + 1   the scripts should check that requires variables are not set, and print appropriate error (and ideally, suggest how to fix it) ",1,train
DM-527,make Image construction robust against integer overflow,i just fixed a bug on the hsc side (dm 523) in which integer overflow in the multiplication of width and height in image construction caused problems.  we should backport this fix to lsst.,1,train
DM-530,Table column names in new parser,"running tests (qservtestdata.sh) on preloaded data i have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. here is an example of query reqult returned from mysql:  mysql> select sce.filtername, sce.field, sce.camcol, sce.run from scienceccdexposure as sce where sce.filtername = 'g' and sce.field = 670 and sce.camcol = 2 and sce.run = 7202;   field  run   g                2    and this is the same query processed by qserv:  mysql> select sce.filtername, sce.field, sce.camcol, sce.run from scienceccdexposure as sce where sce.filtername = 'g' and sce.field = 670 and sce.camcol = 2 and sce.run = 7202;   qs2pass  qs4pass  g                2     we discussed this already with daniel yesterday and at qserv meeting today, here i just want to collect what we know so far so that we can return to this again later.   as daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. this helps with tracking query proceeding through the processing pipeline. daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. additionally there are issues with conflicting column names and names which are complex expressions.  difference in column names breaks our tests which dump complete results including table header. the tests could be fixed easily, we could just ignore table headers when dumping the data. more interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. in particular standard python mysql interface allows one to use column names to retrieve values from queiry result. if qserv assigns arbitrary aliases to the columns it may confuse this kind of clients.  this issue depends very much on what kind of api qserv is going to provide to clients. if mysql (wirelevel) protocol is going to be the main api (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. otoh if we decide to provide our own api then this may not be an issue at all (but we still need to fix current test setup which is based on mysql).  we probably should discuss api question at our dev meeting.",5,train
DM-531,Qserv returns error table instead of error code,"running the tests on preloaded data i noticed that for some queries qserv returns result which does not look like it is related in any way to the query  result column names are different from the columns in the query (different in a different way from dm530). here is an example of query and result produced:  mysql> select sce.filtername, sce.field, sce.camcol, sce.run from   scienceccdexposure as sce where  sce.filtername like '%'    and sce.field = 535    and sce.camcol like '%'    and sce.run = 94;   code  timestamp         1  null                                                                                                                                                                             100  1.39779e09    32767  query added: url=xroot:/qsmaster@127.0.0.1:1094/q/lsst/1234567890, savepath=/dev/shm/qservsalnikov80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/712345678900   1300  1.39779e09    32767  results read.                                                                                                                                                                   1500  1.39779e09    32767  query resources erased.                                                                                                                                                         2000  1.39779e09 | +   daniel and bill explained to me what is happening there  when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue.   this feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. for regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. it may also be ambiguous in a sense that the legitimate query could produce result with the same column names.  like in dm530 this issue is tied to a question what kind of api we want to provide to qserv clients. if mysql wirelevel protocol is going to be our main api then we should probably try to be more mysqlcompatible. otoh if we are going to hide everything behind our own api then we may have more freedom in re defining what kind of result error condition produces.",4,train
DM-533,transfer multiband processing changes from HSC,"we have a new multiband processing scheme for coadds on the hsc side, encompassing changes in afw, measdeblender, and pipetasks.  these should be transferred to the lsst side, with rfcs for any backwards incompatible changes.  changes to the footprint classes may only be temporary, as we plan to refactor those classes soon anyway, but they're still worth doing now to support the higherlevel changes.",8,train
DM-536,Move HSC issues to hsc-jira.astro.princeton.edu,nan,10,train
DM-545,"ensure pipe_tasks, obs*, and other packages are compatible with meas_base","the switch to measbase will involve changing the names of most measurement algorithms, as we're using a new naming convention that provides more traceability.  this will break downstream code that:   uses field names instead of slots to access measurements   reads or modifies the list of configuredtorun algorithms.  whenever possible, we should fix the former by converting them to use slots, as this will automatically provide backwards compatibility.  i'm not yet sure how to handle the latter; the easiest solution would be to give up on full backwards compatibility with measalgorithms, but we might be able to find some way to make the old names aliases to the new ones during the deprecation period. ",12,train
DM-546,scons rebuilds targets without changes,"i'm seeing something strange when i run scons from current master  running 'scons install' after 'scons build' recompiles several c files even though nothing has changed between these two runs:  $ scons build scons: reading sconscript files ... ... scons: building targets ... scons: `build' is up to date. scons: done building targets.  $ scons install scons: reading sconscript files ... ... scons: building targets ... swig o build/czar/masterlibwrap.cc ibuild i/usr/include/python2.6 python c iinclude build/czar/masterlib.i g o build/czar/masterlibwrap.os c g fpic dfileoffsetbits=64 fpic i/u2/salnikov/stack/stack/linux64/protobuf/2.4.1/include i/u2/salnikov/stack/stack/linux64/xrootd/qs5/include/xrootd i/u2/salnikov/stack/stack/linux64/mysql/5.1.65/include ibuild i/usr/include/python2.6 build/czar/masterlibwrap.cc g o build/control/asyncquerymanager.os c g fpic dfileoffsetbits=64 fpic i/u2/salnikov/stack/stack/linux64/protobuf/2.4.1/include i/u2/salnikov/stack/stack/linux64/xrootd/qs5/include/xrootd i/u2/salnikov/stack/stack/linux64/mysql/5.1.65/include ibuild i/usr/include/python2.6 build/control/asyncquerymanager.cc g o build/control/dispatcher.os c g fpic dfileoffsetbits=64 fpic i/u2/salnikov/stack/stack/linux64/protobuf/2.4.1/include i/u2/salnikov/stack/stack/linux64/xrootd/qs5/include/xrootd i/u2/salnikov/stack/stack/linux64/mysql/5.1.65/include ibuild i/usr/include/python2.6 build/control/dispatcher.cc g o build/control/thread.os c g fpic dfileoffsetbits=64 fpic i/u2/salnikov/stack/stack/linux64/protobuf/2.4.1/include i/u2/salnikov/stack/stack/linux64/xrootd/qs5/include/xrootd i/u2/salnikov/stack/stack/linux64/mysql/5.1.65/include ibuild i/usr/include/python2.6 build/control/thread.cc g o build/merger/tablemerger.os c g fpic dfileoffsetbits=64 fpic i/u2/salnikov/stack/stack/linux64/protobuf/2.4.1/include i/u2/salnikov/stack/stack/linux64/xrootd/qs5/include/xrootd i/u2/salnikov/stack/stack/linux64/mysql/5.1.65/include ibuild i/usr/include/python2.6 build/merger/tablemerger.cc scons: `install' is up to date. scons: done building targets.   this is kind of unexpected, or at least i can't understand now why it happens. trying to run with  debug=explain shows that some dependencies have disappeared and in some dependencies order is different. no clue yet what that means and how it could happen. need to study our scons scripts to understand what is going on.",3,train
DM-548,rearchitect Qserv to fix dependencies between modules in qserv/core,"i looked at includes in ""core/modules"", here is a summary.   "":"" indicates a dependency  global, log, util, wbase, wlog and xrootd are low level, it is an easy, i didn't bother showing them below   """" indicates circular dependency   lowerlevel modules first   css:      / global:   / mysql:    / obsolete: / proto:    / sql:      mysql wconfig:  sql, mysql wpublish: sql, wconfig   wsched / wcontrol  wcontrol / wdb wsched:   proto, wcontrol wcontrol: mysql, sql, wsched, wdb, wcontrol, obsolete, proto wdb:      sql, mysql, proto, wconfig, wcontrol  xrdfs:    wpublish, wdb, wconfig, sql, obsolete, wcontrol merger:   sql, xrdfs xrdoss:   obsolete, wpublish, xrdfs   control / qdisp  qana / qproc  qana / query  parser > query >qana > parser  query > qana > qproc  > query control:  css, merger, obsolete, qdisp, qproc, query qdisp:    control qana:     css, parser, qproc, query qproc:    css, merger, parser, proto, qana, qdisp, query query:    css, qana, qdisp parser:   query  ",10,train
DM-554,Base DMCS and Replicator Interaction for Simulator - v1,"create the base dmcs and replicator processes that demonstrate  send startintegration send startreadout  assume:    archiver and alert production cluster already enabled, replicators already registered in fully operational pool.  demonstrate:  sending startintegration event to the base dmcs. replicator jobs submission replicator jobs subscribe to startreadout sending the startreadout event to the replicator jobs. replicator jobs receive startreadout replicator jobs pull data ",10,train
DM-559,"clean up include <> --> """" for third party includes","according to our coding standard 4.15: https:/dev.lsstcorp.org/trac/wiki/c%2b%2bstandard/files#a4 15.onlysystemincludefilepathsshallbedelimitedwith  we should be using """" for boost, but in quite a few places we do not:   grep 'include <boost' / |wc      146     314    7916  ",1,train
DM-560,cleanup includes - add module name,"change places like #include ""cssexception.h"" to #include ""css/cssexception.h""   ",1,train
DM-561,Add distributor to simulator - v2,add distributor to the simulator  demonstrate:  replicator node associated with to distributor node replicator jobs send job info to distributor node replicator jobs copy data to distributor node.,6,train
DM-564,Add Archive DMCS to simulator - v3,"add archive dmcs  demonstrate:  receiving information from duplicator (visit id, exposure sequence number, raft id, and network address).",4,train
DM-566,Fix Doxygen Doc for new meas_base classes,the final stage of moving the old algorithms to measbase will be to generate the doxygen documentation and be sure that it is useful.  we will do this after the  cleanup of the old measalgorithms code.,3,train
DM-583,Investigate Approaches to Dcr,"this story captures the work of investigating the implications of the different options to compensate for dcr: ignore objects with extreme colors, compensate for dcr in measurement, and compensate perimage at the perobject/pixel level.  we anticipate the latter option will be required going forward, but we need to do the background work to justify this.  we need to understand exactly which classes will need to make use of dcr information, and what the limitations are of operating at the per pixel level (e.g. blends).",20,train
DM-584,Validation of Dcr Approach at the Pixel Level,"before we start the process of extending the dcr functionality into the stack (e.g. psf, wcs, etc), we need to validate that the implementation works at the pixel level.  in this task we will work with simulated images to debug and optimize the implementation, to the degree possible.",30,train
DM-586,Cleanup Source.h.m4,this include file was damaged somewhat by the addition of slot routines to work with the flattened field definitions.  it would be nice to put the measurement abstraction back in place  or get rid of it.  we need to decide whether the old slot and compound key mechanisms from sourcetable version 0 are going to be continued for doing this.   ,2,train
DM-587,The way the observation date is translated by obs_cfht breaks the defect registry.,the fields of the observation date and time as stored in the cfht ls headers are not padded with zeros.  this makes the sqlite datetime constructor return a null string when the observation time is < 10hrs.  the fix is to force the fields to be padded when the metadata from the input files are ingested.,1,train
DM-588,Get one HTCondor ClassAds scenario running,"we have a worker script that takes three arguments: a ccd number, a cache limit, and a length of time.  the script starts out looking at a slot specific filesystem for a calibration file /local/slot#/calib/ccd#.  if that file doesn't exist, it checks to see if there are more than the cache limit of files in /local/slot#/calib.  if there are, it removes one at random (later we could try other algorithms).  it then immediately transfers /gpfs/calib/ccd# (maybe a 1 gb file) to /local/slot#/calib/ccd#.  the script then waits for the given length of time and exits.  we make sure we can measure how many file copies from /gpfs are performed and when so that we can determine peak demand rates and average rates.  meanwhile, the htcondor hook checks for the same /local/slot#/calib/ccd# files and sets the classads based on which ccd#s are present.  scenario 1) 4 slots, 4 ccds, cache limit 1, jobs take 1 min, total of 5 jobs per ccd, 1 job per ccd submitted every 2 min (1234 pause 1234 pause 1234 pause 1234 pause 1234)",14,train
DM-590,load non-LSST FITS tables as version 1,"in dm384 and dm242, we disabled the periodstounderscores translations for new tables (""version 1"") but left it in place for old tables (""version 0"").  in addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility.  i think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the afw_type key.  otherwise we should assume it is version 1.  this will allow us to load externally produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw.",1,train
DM-593,Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST,"the lsstcorp.org/legalnotices/ need to be updated to reference aura/lsst. the referenced list of lsst partner institutions needs to be either resurrected or the reference deleted.  the git repository for devenv/templates needs the copyright templates to be  updated.  lsstlicensestatement.txt needs to be updated to include recent additions of 3rd party tools' licenses (10 tools)  to the dm stack  and all the qserv 3rd party tools' licenses (25 tools).  the copyright banner in all software needs to be updated to reflect the new reality of aura/lsst in place of lsst corporation.  files with no copyright banner, need to add it.  update may occur 'the next time' the code file is updated. this needs to be broadcast to the developers once the copyright templates and the website versions are updated.",2,train
DM-594,running multiple Qserv installations on the same machine,"it would be very useful to be able to run multiple installations of qserv on the same machine (say, 2 developers playing with qserv on the same machine). i guess we are almost there, we just need to know how to configure all ports so that we are not colliding. can you test it, tweak whatever is necessary and document what is involved in changing defaults to a unique set of port numbers? ",6,train
DM-595,Setup multi-node Qserv ,"we are currently focusing on singlenode qserv. it'd be nice to try setting up multinode qserv (say 4 workers and a czar on lsst dbdev ), and improve installation scripts to simplify the process.",6,train
DM-596,Fix automated tests after css migration,"after yesterday's merge of dm 58 into master automated tests do not work any more. the part which is broken now is loading of metadata into qserv. we need to replace old script which created metadata with something different that creates metadata using new css.   the code which loads metadata in tests is in qservdataloader class, createqmsdatabase() method (in tests/python/lsst/qserv/test/ directory).",2,train
DM-597,reorganize client module,"move everything in the client package (qserv_admin , associated tests and examples) to admin/  move css/bin/watcher.py to admin/  ",1,train
DM-599,Investigate off-the-shelf data distribution tools,"research available off the shelf tools, try to find one that would best fit our needs. produce a document (trac page).",12,train
DM-600,fix 12 issues in testCppParser (related to switching to CSS),nan,3,train
DM-602,Add support for installing qserv on machines without internet,it is common we install qserv on machines that are on internal network without external network connectivity. how do we do that?,9,train
DM-603,Look into git-fat,"look into the use of git fat with the lsst dm workflow.  specifically, how does this work with anonymous access.",4,train
DM-604,Update parse/analysis tests to detect missing css-kvmap early,"due to the css code merge, the testcppparser test depends on an external kvmap file. if this file doesn't exist, nearly every test will fail. there isn't a way to check whether the cssfacade (constructed from kvmap file) is valid.  this ticket includes, at minimum:  changes to css/ and qproc/ to make the unit tests fail early if the facade could not be constructed.  optionally, this ticket could include:  renaming testcppparser to testqueryanalyzer   compile time linking the kvmap into testcppparser to eliminate the need for an external kvmap file. ",4,train
DM-607,CSS throws exception if tableIsSubChunked is called for non partitioned table,nan,3,train
DM-608,referring to table without database context crashes czar,"running a query like ""select  from object"" if we do not have database context results in   terminate called after throwing an instance of 'lsst::qserv::css::cssexception_internalruntimeerror'   what():  internal runtime error. ( css::kvinterfaceimplzoo::exists(). zookeeper error #8. (/databases/))   need to gracefully catch the zookeeper  8. need to detect that empty database name is passed in facade. need to avoid it higher up. ",3,train
DM-609,afw unit tests not built unless afwdata available,"if afwdata is not available then the afw unit tests are not built. i think it would make more sense to build all of them and run those we can (not all depend on afwdata). one advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. this is presently not practical because the sconstruct file is not installed (i intend to open a ticket about that, as well).",1,train
DM-611,Switch kazoo version to 2.0b1 or later,"while we aren't using any of the new features of kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature.  the 2.0b1 release seems at least as stable as our own code, so i don't think we'll see any negative effects.  this ticket covers:  upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later).  (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. this can be done in a later ticket, though.  i note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the pythonpath.",2,train
DM-612,remove obsolete QMS-related code,try running   find core css admin client tests site_scons | xargs grep  i qms   there is a lot of old unused qms related code.,2,train
DM-613,Automated test differences after CSS migration,i'm running automated tests with recent master after merging dm 605 and observe some differences between mysql and qserv. here i'm going to document all differences (and everything related).,6,train
DM-614,rename qserv_admin.py to qserv-admin.py,"we have 6 scripts in qserv/admin/bin that start with ""qserv"", and one that starts with ""qserv"", we should rename qservadmin to qservadmin.",1,train
DM-616,commons.config should always be managed as a global variable,"qserv configuration object (i.e. commons.config) used in admin and tests should be managed the same way a logger object is (cf. http:/hg.python.org/cpython/file/2.7/lib/logging/init.py).  i.e. a function called config.getconfig(""config type"") (returning an config occurence of a global dictionnary), should be used in order to ease retrieval of all configuration objects related to client, data, (and server?) anywhere in the admin/test python code.",2,train
DM-621,User friendly single node loading script,"this entails the creation of an endtoend loading script that, given database and table param files, a sql table schema and one or more csv data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables.",8,train
DM-622,Qserv configuration tool refactoring,"  instance configuration should not be in the same directory where qserv software is installed qserv configuration, including things like mysql or zookeeper data directory should be separate from where qserv software is installed. at the moment, when i want to work with branch ""a"" and ""b"", in order to switch from one to the other i have to run ""cd $qserv_dir/admin; scons"". this is ok the very first time after i created a branch, but it is way too heavy afterwards, because it wipes out all data from zookeeper and mysql.",10,train
DM-623,Package antlr 2.7 in eups,nan,5,train
DM-624,Make sure we have queryId in all places where needed in logging,nan,4,train
DM-625,Too many connections from czar to zookeeper,"i have just managed to crash qserv czar by running repeated queries against it. what i see in the logs:  qservczar.log:  20140502 13:21:10,861:22525(0x7f76e37ab700):zooerror@handlesocketerrormsg@1723: socket [127.0.0.1:12181] zk retcode=4, errno=104(connection reset by peer): failed while receiving a server response 20140502 13:21:10,861:22525(0x7f76e37ab700):zooerror@handlesocketerrormsg@1723: socket [127.0.0.1:12181] zk retcode=4, errno=104(connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::cssexception_connfailure'   what():  failed to connect to persistent store.   and zookeeper.log:  20140502 13:21:10,861 [myid:]  warn  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxnfactory@193]  too many connections from /127.0.0.1  max is 60 20140502 13:21:10,861 [myid:]  warn  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxnfactory@193]  too many connections from /127.0.0.1  max is 60 20140502 13:21:14,585 [myid:]  warn  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxn@357]  caught end of stream exception endofstreamexception: unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket         at org.apache.zookeeper.server.nioservercnxn.doio(nioservercnxn.java:228)         at org.apache.zookeeper.server.nioservercnxnfactory.run(nioservercnxnfactory.java:208)         at java.lang.thread.run(thread.java:744) 20140502 13:21:14,585 [myid:]  info  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxn@1007]  closed socket connection for client /127.0.0.1:49913 which had 20140502 13:21:14,585 [myid:]  warn  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxn@357]  caught end of stream exception endofstreamexception: unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket         at org.apache.zookeeper.server.nioservercnxn.doio(nioservercnxn.java:228)         at org.apache.zookeeper.server.nioservercnxnfactory.run(nioservercnxnfactory.java:208)         at java.lang.thread.run(thread.java:744) 20140502 13:21:14,586 [myid:]  info  [nioservercxn.factory:0.0.0.0/0.0.0.0:12181:nioservercnxn@1007]   closed socket connection for client /127.0.0.1:49939 which had ",1,train
DM-626,ORDER BY and DISTINCT do not work reliably in qserv,"queries with order by and distinct are buggy. for example, results do not always come ordered and order changes from one run to another:  mysql> select objectid from object   where qservareaspecbox(0, 0, 3, 10)  order by objectid;              9 rows in set (1.27 sec)  mysql> select objectid from object   where qservareaspecbox(0, 0, 3, 10)  order by objectid;              9 rows in set (1.24 sec)   this was done with testdata/case01 data, let me know if you need to load that data.  also, using case03 data, e.g.  select distinct run, field  from   scienceccdexposure where  run = 94 and field = 535;  returns 6 rows in qserv (vs 1 in mysql).  the full list of distinct failures is (all with testdata/case03):  0002fetchrunandfieldbyid.txt  0021selectscienceccdexposure.txt   0030_selectscienceccdexposurebyrunfield.txt ",1,train
DM-627,"Switch to using new partitioner, loader",integrated tests procedure has to rely on new loader,4,train
DM-630,Non-partitioned table query returns duplicated rows,"running automated test i noticed that a query on nonpartitioned table returns multiple copies of the same row, one copy per chunk. here is example:  mysql> select offset, mjdref, drift from leapseconds where offset = 10;   mjdref     41317    41317    41317    41317    41317    41317    41317    41317    41317    41317    41317    41317    41317   13 rows in set (5.62 sec)   czar log file shows that it correctly finds that table is nonchunked but sends query to each chunk anyway:  20140502 16:22:08.745081 0x3172430 inf  kvinterfaceimplzoo::exist(), key: /databases/lsst/tables/leapseconds/partitioning 20140502 16:22:08.745735 0x3172430 inf  lsst.leapseconds is not chunked. 20140502 16:22:08.745762 0x3172430 inf  kvinterfaceimplzoo::get2(), key: /databases/lsst/tables/leapseconds/partitioning/subchunks 20140502 16:22:08.746393 0x3172430 inf  lsst.leapseconds is not subchunked. 20140502 16:22:08.746409 0x3172430 inf getchunklevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 inf / using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 inf / using /usr/local/home/salnikov/dm613/build/dist/etc/emptychunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 inf / emptylsst.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 err / couldn't find emptylsst.txt, using /usr/local/home/salnikov/dm613/build/dist/etc/emptychunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 inf querysession::buildchunkqueries() : nonsubchunked 20140502 16:22:08.891498 0x7fbb28003660 inf msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 inf added query id=6630 url=xroot:/qsmaster@127.0.0.1:1094/q/lsst/6630 with save /dev/shm/qservsalnikovb93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/166300 20140502 16:22:08.891694 0x7fbb28003660 inf opening xroot:/qsmaster@127.0.0.1:1094/q/lsst/6630 20140502 16:22:08.891705 0x7fbb28003660 inf querysession::buildchunkqueries() : nonsubchunked 20140502 16:22:08.891882 0x7fbb28003660 inf msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 inf added query id=6631 url=xroot:/qsmaster@127.0.0.1:1094/q/lsst/6631 with save /dev/shm/qservsalnikovb93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/166310 20140502 16:22:08.892087 0x7fbb28003660 inf opening xroot:/qsmaster@127.0.0.1:1094/q/lsst/6631 20140502 16:22:08.892097 0x7fbb28003660 inf querysession::buildchunkqueries() : nonsubchunked 20140502 16:22:08.892275 0x7fbb28003660 inf msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 inf added query id=6800 url=xroot:/qsmaster@127.0.0.1:1094/q/lsst/6800 with save /dev/shm/qservsalnikovb93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/168000 ...   looking at the code together with daniel we found that at the python level (czar/app.py) the code that dispatches query does not check for chunklevel, this is likely why this happens. the code to look at is in inbandqueryaction.applyconstraints() method.",5,train
DM-631,Define C++ API for C++ Geometry,nan,10,train
DM-633,Query sessions are never destroyed,"please see dm 625, when i run say 10 ""select count( ) from lsst.object"" queries, for each query a new asyncquerymanager is created in dispatcher, but the sessions are never destroyed.",3,train
DM-635,admin/tests/test_qservAdminImpl.py has hardcoded connection info,nan,1,train
DM-637,complexity of eups dependencies relationships  for db package,"hello,  i'm currently trying to use the very last version of db package (the one which relies on sconsutils), but, in order to make it works with qserv, i had to introduce next update :  fjammes@clrlsstwn02vm:/src/qservpackager/dist/dependencies/db (master) $ git diff head1 diff git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644  a/ups/db.cfg  b/ups/db.cfg @@ 3,7 3,7 @@  import lsst.sconsutils    dependencies =     config = lsst.sconsutils.configuration( diff git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644  a/ups/db.table  b/ups/db.table @@ 1,5 1,5 @@  setuprequired(python) setuprequired(mysqlclient) setuprequired(mysql)  setuprequired(mysqlpython)  setuprequired(sconsutils)   is there a solution to describe  in eups that mysqlclient is included in mysql ?  thanks,  fabrice",1,train
DM-644,update overview docs to clarify roles of meas_multifit and shapelet packages,"from the review of dm 17:  it was not obvious how responsibility is split between measextensionsmultishapelet, shapelet, and measmultifit.  shapelet could use an overview.dox file.   measextensionsmultishapelet is on its way out, so we'll wait until that's done and then document the relationship between measmultifit and shapelet.",2,train
DM-645,meas_base plugin for sampling-based galaxy fitter,"in addition to a cmodel plugin for galaxy photometry (dm240), we should create a plugin to do samplingbased galaxy fitting, using the optimizer as a starting point and the existing adaptiveimportancesampler class to do most of the work.  a major blocker for this is the fact that the sourcetable/sourcerecord don't currently provide any way to save multiple samples per object.  this issue may get worked on before it falls into an official sprint, as it's something i want to on my 20% time.",8,train
DM-646,Implement DISTINCT aggregate in qserv,it looks like distinct aggregate is not supported yet in qserv. daniel told me that this should be relatively straightforward to add. adding this ticket so that we do not forget it.,2,train
DM-647,support samples and other many-to-one outputs in SourceRecord,"for dm645, i need to be able to save multiple samples (each a parameter vector and possibly a weight) with each sourcerecord, with the number of samples possibly differing from sourcerecord to sourcerecord.  i'll also want to save mixture distributions, which can also be easily represented as a sequenceoftuples of values to be associated with a record.  since we don't want to hardcode the outputs of a particular plugin into sourcerecord (which is what i've done with the modelfitrecord class in meas_multifit, which i'd like to remove), i think this means we want a more general way of allowing algorithms to define manytoone tables to be associated with each source.  while few algorithms will likely want to save samples, this plugin may not be the only one, and it's likely a general manytoone feature could be used by other algorithms to save diagnostics when configured to do so.  i'm worried this is another case where afw::table may be intruding on a feature best left to a true rdbms, but i don't like the idea of tying an rdbms directly to the source measurement framework either.  i'm open to other ideas, if anyone has one.  it's also worth pointing out that because this is blocking dm645, which is important for my 20%time science work, i'd like to get moving on this even before it becomes a real priority for lsst.  but i'd still like to get some design feedback on this.",20,train
DM-648,Add support for running unit tests in scons,add code in scons that runs unit tests for qserv.,5,train
DM-649,"framework for documenting ""how to run qserv""","we need to have permanent location for howtorun qserv, currently we keep it in https:/dev.lsstcorp.org/trac/wiki/db/qserv/redesignfy2014/hackathon2/howtorunqserv. it should be in the code repo. need to decide on how we format it, and need to expose it on our qserv trac/confluence pages ",3,train
DM-650,Migrate database trac pages to Confluence,need to migrate qserv trac pages to confluence.,10,train
DM-653,Run baseline HTCondor ClassAds Scenarios,"run initial htcondor classads scenarios to verify that the implementation for utilizing rank to place jobs near data is operating as anticipated.  the main test of the baseline scenarios is to verify that, e.g., a job for a ccd that has calibrations advertised for a particular slot/node  will consistency be executed within that location for appropriate rank expression. this is to occur even when other open slots are always available (e.g., 4 ccds, 5 slots). ",4,train
DM-654,"Run ""single slow worker"" HTCondor ClassAds Scenario","we run and study a ""single slow worker"" htcondor classads scenario. the scenario is a perturbation of the baseline htcondor classads scenario. in the baseline, jobs for a given ccd  are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that ccd.  a baseline run may proceed, for example,  with jobs for 4 ccds repeating executing within same htcondor slot on a node (even when spare processing slots are readily available.). the baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. in the ""single slow worker"" scenario, we cause one of the jobs for a chosen ccd to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs.  we seek to observe how rank places jobs in this scenario, and work to assign rank (especially for spare slots) in an optimal way so as to minimize file transfers. ",4,train
DM-655,unknown column derails Qserv,"running a query that references invalid column hangs qserv, it looks like the query is never squashed.  for example, i run a query:  select distinct run, field  from   scienceccdexposure where  run = 94 and field = 535;  on the pt1.1 data set  corresponding log from xrootd:   foreman:>>unknown column 'run' in 'field list' unable to execute query: create table r7d8e5a45a6aba94de141cf1d8ab510c8935980 select run,field from lsst.scienceccdexposure \ as qst1 where run=94 and field=535; />> scansched:chunkdisk busyness: no scansched:getnexttasks /> > groupsched:getnexttasks <<<<< 140509 15:31:27 13960 cms_finder: waiting for cms path /usr/local/home/becla/qserv/1/qserv/build/dist/tmp/worker/.olb/olbd.admin   and the log in czar has  20140509 15:35:40.070654 0x7f4694003660 inf still 1 in flight.    ",4,train
DM-658,"Database name used by  integration tests should use their own dedicated database, not ""LSST""","currently automated tests use the database ""lsst"", and they will go ahead and delete that database without any warning. this is far from ideal, we should have a more unique database name. how about something like qservautotest_/.",9,train
DM-661,"Parser has inverted order for ""limit"" and ""order by"""," select run from lsst.scienceccdexposure order by field limit 2   works in mysql, fails in qserv (error 4120 (proxy): error executing query using qserv.)   select run from lsst.scienceccdexposure limit 2 order by field   works in qserv, fails in mysql (limit should be after order by) ",1,train
DM-664,"""out of range value"" message when running qserv-testdata (loader.py)","fabrice  i am getting ""out of range value"" when i run the qservtestdata:  are you seeing that too?   201405 09 18:11:55,975  info     stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibdetected' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibpsfcandidate' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibpsfused' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagsnegative' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagsbadcentroid' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'centroidsdssflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixeledge' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelinterpolatedany' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelinterpolatedcenter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelsaturatedany' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelsaturatedcenter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelcrany' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelcrcenter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'centroidgaussianflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'centroidnaiveflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdssflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdsscentroidflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdssflagsunweightedbad' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdssflagsunweighted' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdssflagsshift' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'shapesdssflagsmaxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxpsfflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxpsfflagspsffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxpsfflagsbadcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxnaiveflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxgaussianflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxgaussianflagspsffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxgaussianflagsbadcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'fluxsincflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletpsfflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletpsfflagsmaxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletpsfflagstinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletpsfflagsconstraintr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletpsfflagsconstraintq' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevfluxflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagspsffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagsbadcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagsmaxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagstinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagsconstraintr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagsconstraintq' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletdevflagslargearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpfluxflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagspsffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagsbadcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagsmaxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagstinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagsconstraintr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagsconstraintq' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletexpflagslargearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletcombofluxflags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletcomboflagspsffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'multishapeletcomboflagsbadcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibdetected' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibpsfcandidate' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'calibpsfused' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagsnegative' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagsbadcentroid' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'centroidsdssflags' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixeledge' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelinterpolatedany' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelinterpolatedcenter' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelsaturatedany' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: warning: out of range value for column 'flagspixelsaturatedcenter' at row 2   self.cursor.execute(stmt) ",2,train
DM-666,partition package has to detect eups-related boost,"partition package doesn't detect eupsrelated boost. this has to be fixed by using sconsutils, or handmade procedure.   [fjammes@lsstdev lsstsw]$ export ldlibrarypath=""$lsstsw/anaconda/lib:$ldlibrarypath"" [fjammes@lsstdev lsstsw]$ setup boost 1.55.0.11 [fjammes@lsstdev lsstsw]$ rebuild partition            partition:  ok (0.5 sec).                boost:  ok (0.3 sec).               python:  ok (0.3 sec).                scons:  ok (0.4 sec). # build id: b49               python: mastergcbf93ab65b (already installed).                scons: 2.1.08 (already installed).                boost: 1.55.0.11 (already installed).            partition: mastergf2ef2cf2dc error (1 sec).  error building product partition.  exit code = 1  log is in /lsst/home/fjammes/src/lsstsw/build/partition/build.log  last few lines: :::::  scons: reading sconscript files ... :::::  checking for c library boostsystemmt... no :::::  checking for c library boostsystem... no :::::  checking for c library boostthreadmt... no :::::  checking for c library boostthread... no :::::  checking for c library boostfilesystemmt... no :::::  checking for c library boostfilesystem... no :::::  checking for c library boostprogramoptions mt... no :::::  checking for c+ library boostprogram_options... no :::::  missing required boost library! # build b49 completed. ",3,train
DM-674,fix handling of nested control objects,"work on the hsc side has revealed some problems with nested control objects being wrapped into config objects.  this is a pull request for those changes (along with writing a unit test for some of them).  some (but not all of these changes) are part of trac ticket #3163 (https:/dev.lsstcorp.org/trac/ticket/3163), which i'll now close as a duplicate.",2,train
DM-675,Citizen methods should be private and accessible only through a friend interface,"the citizen interface is useful, but it pollutes its derived classes with methods and attributes that can cause confusion later on (i've got a concrete example of that confusion that perry and i just spent a few days tracking down   citizen's getid() was being mistaken for sourcerecord.getid()).  i think everything citizen provides should be hidden and only accessible through a friend interface, e.g.:  afw::image::image/ image(4, 5); daf::base::citizenaccess::getid(image);   we should also make an effort to ensure that other aspects of citizen's design don't affect derived classes, perhaps by prefixing an name that could be seen by derived classes with a ""citizen"" prefix; see https:/dev.lsstcorp.org/trac/ticket/2461.",4,train
DM-676,Implement HTCondor dynamic classad solution for Slot based values,"the htcondor team will be updating their howto for managing slot based classads/dynamic classads set by a cron startd process.  we currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the htcondor team plans to provide.",2,train
DM-677,Develop monitoring for identifying Data processed on a Node/in a Slot,"to understand the effectiveness with which we are mapping jobs to data, it is vital to monitor/record what data has been processed on a given node, or within a given slot on a node.   under this issue we examine htcondor monitoring standards like startd_history,  as well as more custom implementation of blackboard type records via  job update hooks to be  executed on the execute node (along the lines of owl.) ",4,train
DM-678,Run HTCondor ClassAds Scenarios with heterogeneous data cache,"the initial series of tests with htcondor classads work with jobs for individual ccds with a single file representing the data dependency  for the job. in this issue we consider the management of multiple types of data dependencies that may have to be cached for jobs (calibrations, templates, catalogs of sources/objects, etc). ",4,train
DM-680,Study ORDER BY support,"we don't have a proper implementation of order by. actually, to support order by properly, we really have to manage all the column names, so we would need to have an in memory list of columns for the table in question. this is because the general case requires us to order by a column that may not exist in the select list. in this case, we must add it to the select list if it is not there (or apply  ). the easiest solution is to only allow order by if the sort key column exists explicitly in the select list.",4,train
DM-681,Parser ignores syntax after LIMIT,"parser stops after the limit condition, believing that it has a complete select statement. it ignores whatever is afterwards.to fix this, we would need to alter the parser to make sure that there isn't garbage afterwards. so we have to make sure that the only thing acceptable afterwards is a semicolon or a comment. the right thing to do is probably to add a grammar rule for this. the easiest thing is to check to see if the entire string was consumed (giveortake a semicolon), but this would disallow comments.",4,train
DM-684,Estimate expected counts of unassociated sources ,"we need to have an idea how many sources/forcedsource/diasources that are not associated with any object we will have to deal with in the database. can we have a rough estimate? (e.g., per dr).",2,train
DM-685,Fine-tune logging messages,"fine tune log messages in qserv (what messages are printed, what is the error level, etc)",5,train
DM-689,During scons configure : check if mysql isn't runing,mysqld can't be configured is its running before configuration step.,1,train
DM-690,Minor possible enhancements in install procedure,"usefull enhancements :   add swigged target to ""build"" alias (run scons install to see that swigged target are rebuilded at install time)  other possibles enhancements :  manage default (i.e. const.py) for server configuration file ?  state.py : where to save state ? print it to sdtout ? ",7,train
DM-693,Create tutorial on the use of eups,"create a beginner's guide to the use of eups for the dm developer guide. describe the capabilities and basic usage for ""everyday"" user and developer activities. provide pointers to the eups reference manual for advanced usage. ",10,train
DM-699,rename git repository qservdata to qserv_testdata,"eups package have the same name as their related git repos. renaming git repos would lead to a more understandable name.  please note that the qservtestdata may also be cloned from qservdata, and and qservdata be removed.  new repos will also have to be distributed with lsstsw tool.",1,train
DM-702,Buildbot CI needs to save manifest file of failed build for later user debug,"the manifest file created during a build instance is transient and removed as soon as the next build commences.  due to that volatility, it's important to save the manifest to some well known location so that the developer responsible for debug and repair can easily setup the failing environment. the location of the manifest file will be provided to the developer(s) in the failure notification.",4,train
DM-703,Use of HipChat for Buildbot CI failure notifications should be explored,k t recommended the use of hipchat rather than email when notifying users of a buildbot build failure.  the purpose was twofold: get immediate attention from the developers and help change the culture towards using hipchat more.  this issue is to explore the feasibility of using hipchat for the notifications.,1,train
DM-704,Better review notification e-mails,"russell writes:   i think our system for getting code reviewed using jira needs some improvements. it seems that people don't always know that they have been assigned to review a ticket. also, even if i know i have been assigned to review a ticket, i find it hard to find on jira.  more concretely, i would like to see these improvements:  much clearer notification that one has been assigned as a reviewer. presently the email is quite generic and easy to miss. in fact i find that most jira notifications are rather hard to read  it's not always easy to see what has changed and thus why i should care. the signal to noise ratio is poor.   by default a user should see which issues they have been assigned as reviewer when they log into jira. (if there is a way to reconfigure the dashboard for this, i'd like to know about it, but it really should be the default). one way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.   russell   and i added:   in fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  if the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  with trac, the important information was at the top of the e mail. ",2,train
DM-705,"CSS keys are too fine-grain, consider merging them together (design)","i've seen a lot of sentiment (from serge and daniel) to try and combine keys in css. current design has one key for each tablechunk (leading to tens of thousands of keys in production), and smallgrain keys like ""nstripes"", ""nsubstripes"", and ""overlap"" in partitioning are each stored in separate key. this issue will capture discussion, decisions, and implementation of a more compact version. ",10,train
DM-706,cleanup extra file names in docstring,"reported by serge in email:  when using doxygen to document c source, you can mark a comment block with just:   / @file    blah blah   /   in which case doxygen assumes you want the comment block tied to the file it appears in. we seem to have lots of ""@file /” statements all over the place, which is an extra thing we have to remember to change when renaming files. is there some reason to do it that way that i’m missing?",1,train
DM-707,cleanup exception code in CSS,"reported by serge:  in cssexception.h you’ve got:   class cssruntimeexception: public std::runtimeerror ; class cssexceptionxxxx : public cssruntimeexception ;   this is inconsistent (shouldn’t it be cssruntimeexceptionxxx, or maybe even cssruntimeerror?), lengthy, violates the lsst c naming conventions, and doesn’t match the kvinterface docs, which all still talk about a cssexception class that does not exist. can we consider changing this to something more like:   class csserror : public std::runtimeerror class keyerror : public csserror class nosuchtable : public keyerror class nosuchdb : public keyerror class autherror : public csserror class connerror : public csserror   ? then we can succinctly throw and catch css::nosuchtable, css::autherror etc…",2,train
DM-708,IN2P3: Cloud-computing Platform / OpenStack,"cc in2p3 offer computing resources via openstack :  https:/indico.in2p3.fr/getfile.py/access?sessionid=2&resid=0&materialid=0&confid=8236  this platform allow to boot virtual machines on multiples linux distribution. it could be a powerfull tool for continuous integration and testing.  virtual machines are easilly available to all in2p3 users, only a ssh account on ccage.in2p3.fr is required. ",15,train
DM-709,Prepare a fedora64 openstack image which allow to easily build and test Qserv,qserv packaging procedure requires to often rebuild qserv and relaunch integration tests.  in2p3 openstack platform offer next virtual machines :   [fjammes@ccage030 ~]$ nova flavor list   name               disk  swap  rxtxfactor   m1.tiny            0           1.0           cc.windows.small   20          1.0           cc.windows.xlarge  50          1.0           m1.small           10          1.0           m1.medium          10          1.0           m1.large           10          1.0           m1.xlarge          10          1.0           cc.lsst.medium     20          1.0           cc.lsst.large      20          1.0           cc.lsst.xlarge     20          1.0           cc.lsst.xlarge would allow a quick build/test of new qserv release.,5,train
DM-710,Reduce and comment client configuration file,"client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file""",1,train
DM-720,Upgrade various external packages,i note that the lsst and hsc versions of external packages are slightly out of sync.  i propose uprevving the lsst packages to match as hsc has tested these versions.   cfitsio               3.360             hsc cfitsio               33102            sims winter2014 current b4 b5 b6 b3 doxygen               1.8.22           sims b4 winter2014 current b5 b6 b3 doxygen               1.8.5             hsc eigen                 3.1.12           winter2014 current b5 b6 b3 b4 eigen                 3.2               hsc fftw                  3.3.22           winter2014 current b5 b6 b3 b4 fftw                  3.3.3             hsc gsl                   1.152            winter2014 current b5 b6 b3 b4 gsl                   1.16              hsc minuit2               5.22.002         winter2014 current b5 b6 b3 b4 minuit2               5.28.00           hsc mysqlclient           5.1.653          winter2014 current b5 b6 b3 b4 mysqlclient           5.1.73            hsc pyfits                3.1.22           sims b4 winter2014 current b5 b6 b3 pyfits                3.2               hsc scons                 2.1.07           sims b4 winter2014 current b5 b6 b3 scons                 2.3.0             hsc sqlite                3.7.142          winter2014 current b5 b6 b3 b4 sqlite                3.8.2             hsc wcslib                4.14        wcslib                4.143            b4 winter2014 current b5 b6 b3 xpa                   2.1.142          winter2014 current b5 b6 b3 b4 xpa                   2.1.15            hsc  ,20,train
DM-736,Rewrite secondary index system and merge empty chunks functionality,"we'd like to rewrite the indexing system so that it doesn't depend on a narrow innodb table. we'd also like to fold in the emptychunks functionality.  secondary index: dircolumn>(chunkid, subchunkid) emptychunks: haschunk = chunkid > bool  danielw has a dirt simple implementation of the ""create"" portion of the secondary index.",30,train
DM-737,Rendering an IR node tree should produce properly parenthesized output,"it appears that rendering a tree of ir nodes doesn't always result in correct generation of parentheses. consider the following tree:   orterm  boolfactor  nullpredicate  valueexpr  valuefactor: columnref(""refobjectid"")  boolfactor  comppredicate  valueexpr  valuefactor: columnref(""flags"")  token(""/"")  valueexpr  valuefactor: const(""2"")   which corresponds to the sql for: ""refobjectid is null or flags/2"". if one prepends this (via whereclause.prependandterm()) to the whereclause obtained by parsing ""... where foo!=bar and baz/2 and foo!=bar and baz/2 and foo!=bar and baz/2) and foo!=bar and baz<3.14159  this issue involves surveying all ir node classes and making sure that they render parentheses properly. (one way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. this would give us some ir that we can render to a string and reparse back into ir. if the rendering logic is correct, one should obtain identical ir trees). other possibilities that might explain the behavior above is that the input tree is somehow invalid or that whereclause.prependandterm creates invalid ir.",8,train
DM-740,Implement abstract base class for approximated or interpolated fields,"the user of an approximate or interpolate object doesn't care which of these they have, once the object has been constructed, and we should make these inherit from a common base class that only contains an interface for accessing the interpolated/approximated function while making no assumptions about its functional form.  the new class will represent a scalar field defined over an integer bounding box, and will have methods for evaluating the field at a point and creating an image of the field.  we could also consider giving it arithmetic interoperability with image.  i don't have a strong candidate for the name; i've called it ""boundedfield"" on the hsc side but would like that to be revisited.  adding derived classes to replace the functionality currently in the approximate and interpolate classes will be on a separate issue.  the main work on this issue is tweaking the design on the hsc side and getting signoff on the final design from lsst developers; the actual coding should be minimal, as it's just an interface and we already have a good starting point for it.  the hscside issue (which also includes work that is part of dm1124) is here: https:/hscjira.astro.princeton.edu/jira/browse/hsc796 and the associated git commits are here: https:/github.com/hypersuprimecam/afw/compare/releases/s14a_0...tickets/dm796",10,train
DM-742,Use geom eups package for installing geometry,use geom eups package instead of downloading geometry.py during qserv configuration step.,3,train
DM-744,"Qserv release (12.04) - final build, testing and cutting release",nan,4,train
DM-746,Simplify (script) install procedure,"install procedure described in readmes.txt is complex and error prone.  it could be encapsulated in two scripts :  1. the first for installing qserv current version in the eups stack, following the lsst official install procedure, 2. the second, developer oriented, for installing qserv from a git repository to the eups stack installed during 1.  this two scripts logs could be colorized for better ergonomy.  ",3,train
DM-750,Integrate sciSQL in eups,"in order to become compliant with eups, scisql install process may have to be refactored  :   provide a scons build and install target which build and install scisql binary (.i.e. reduce waf tool features)  provide a configuration procedure (scisql deploy.py) which install udf in mysql datadir, and deploy .so file in mysql plugin directory.  then it has to be packaged in eups format.",9,train
DM-751,Replacing boost system lib with eups libs breaks scons build,"while detecting boost, qserv build system checks for both system lib and then eups lib. this procedure use next code :   class boostchecker:     def init(self, env):         self.env = env         self.suffix = none         self.suffixes = [""gcc41mt"", ""gcc34mt"", ""mt"", """"]         self.cache = {}         pass      def getlibname(self, libname):         if libname in self.cache:             return self.cache[libname]          r = self.getlibname(libname)         self.cache[libname] = r         return r      def getlibname(self, libname):         state.log.debug(""boostchecker.getlibname() libpath : %s, cpppath : %s"" % (self.env[""libpath""], self.env[""cpppath""]))         if self.suffix == none:             conf = self.env.configure()              def checksuffix(sfx):                 return conf.checklib(libname  sfx, language=""c"", autoadd=0)   and this last line run next gcc command :   g o .sconftemp/conftest10.o c g pedantic wall wnolonglong dfileoffsetbits=64 fpic i/data/fjammes/stack/linux64/protobuf/masterg832d498170/include i/data/fjammes/stack/linux64/boost/1.55.0.1/include i/data/fjammes/stack/linux64/zookeeper/mastergc48457902f/cbinding/include i/data/fjammes/stack/linux64/mysql/masterg5d79af2a50/include i/data/fjammes/stack/linux64/antlr/mastergc05368a54f/include i/data/fjammes/stack/linux64/xrootd/mastergfc9bfb2059/include/xrootd ibuild i/data/fjammes/stack/linux64/anaconda/1.8.0/include/python2.7 .sconftemp/conftest10.cpp g o .sconftemp/conftest10 .sconftemp/conftest10.o l/data/fjammes/stack/linux64/xrootd/mastergfc9bfb2059/lib l/data/fjammes/stack/linux64/protobuf/masterg832d498170/lib l/data/fjammes/stack/linux64/antlr/mastergc05368a54f/lib l/data/fjammes/stack/linux64/zookeeper/mastergc48457902f/cbinding/lib l/data/fjammes/stack/linux64/mysql/masterg5d79af2a50/lib l/data/fjammes/stack/linux64/boost/1.55.0.1/lib lboostregexmt scons: configure: yes   as the ""mt"" suffix is searched before the empty suffix, previous command succeed.in my example boostregexmt is a system lib. when launching ""scons build"", then checklib only looks for boost in /data/fjammes/stack/linux64/boost/1.55.0.1/lib, not in /usr/lib/. this behaviour is eupscorrect, but prevents to find boost_regexmt.  in this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g+ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. is it possible to to it with scons ?  mario, did you meet the same problem with sconsutils ?    thanks  fabrice",3,train
DM-754,Update obs_decam for new CameraGeom,the obs_decam package worked on by paul and andy b. needs to be updated to reflect changes in the camera geometry.,6,train
DM-758,Stretch: Data and test script specification for daily/automated QA/integration tests,"[i have renamed this epic to better capture the current planned scope]          per discussions i am pulling this epic into 02c.01.02 [""my"" wbs]    the intent for it is to cover the science and scientific programming portion activities associated with setting up a daily automated qa/integration run.     the infrastructure activities to enable this  are covered by other epics. it is a stretch because it will be primarily worked on by the tucson scientist, still to be identified icw tucson scientific programmer, still to be identified.     jk: in pmcs this would be new hire ls6",70,train
DM-760,Identify Data Set for MiniProduction,nan,3,train
DM-761,Generate data for MiniProduction,"assuming that the data needed for mini production are to be simulated, the input files need to be created and simulated.  the input data then need to be put in a repo with appropriate calibrations.",5,train
DM-762,Create scripts to run MiniProduction,write command line script to run mini production.  the scripts should be able to be handed directly to the orca layer.,3,train
DM-763,Eliminate local pixel indexing; always use parent instead,"as jim bosch notes in /:  our image classes currently handle two different pixel coordinate systems which differ only by the offset commonly referred to as ""xy0"". the parent coordinate system puts the first pixel in the image at xy0; the local coordinate system always puts the first pixel at (0,0).  our general rule has long been ""always pay attention to xy0"", which implies parent, but the image class itself doesn't: in many operations, including subimage accessors and bbox getters, local is the default, whereas in others  particularly the pixel and iterator accessors  there isn't even an option to use parent. imo this is the main source of imageoffset bugs in the code.  we plan to address this issue as follows:  modify the methods on imagelike objects that return pixel iterators or locators based x and/or y index, so that they use parent coordinates.  eliminate the imageorigin enum argument, which appears in imagelike constructors, imagelike getbbox methods.  eliminate the image origin string argument in the butler's get and put methods for imagelike objects.  at the end of this transition all indices will be parent indices; the concept of local will be gone. this will reduce ambiguity and eliminate an argument that is a source of confusion.  furthermore, at jim's suggestion and kt's agreement, we plan to do this in two basic stages:  rename methods or otherwise make changes that should break any code using local indices. fix the code accordingly.   rename methods back and eliminate the imageorigin argument. fix the code accordingly.",53,train
DM-764,Exception naming convention,the naming convention for exceptions in pexexceptions is quite redundant.  this issue will make the convention more compact and update all packages that make use of pexexceptions.,5,train
DM-765,Evaluate moving to C++11 for .cc files,check that c11 works on .cc files.  make c11 the default in sconsutils.,5,train
DM-766,Improve afw::CameraGeom::utils code,some of the utility code in camerageom was not completely ported in w13 and documentation is in need of updating.,3,train
DM-767,Determine scope of XY0 convention update,it's unclear exactly how much effort will be involved in making a change to how the xy0 is used.  if the parent/child argument is removed completely this change could be quite invasive and wide reaching.,2,train
DM-768,Does SWIG 3.x work with DM stack?,"this task is to evaluate whether we can use swig 3.x (and is it stable enough).  assuming swig 3.x can be used with the stack, evaluate how much work will be involved in moving over to use it by default.",10,train
DM-769,Create scripts to assess and report MiniProduction quality.,write a command line script to assess the quality of the mini production run.  this will involve comparing output data to data produced using a standard stack.  the script will provide a report.,7,train
DM-770,Create script to clean up after a MiniProduction run,the run of a mini production will produce an output repository.  it's likely that we will not want to save all output data.  a script to clean up and potentially save parts of the repo is needed.,3,train
DM-772,Package log4cxx,"fabrice, can you package log4cxx? i should have asked you earlier, sorry i waited so long, not it becoming urgent! bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. many thanks.  log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at apache. ",2,train
DM-774,XLDB in Rio,prepare for and attend xldb south america in rio.,15,train
DM-775,XLDB-2015 report,"writing the report, most work done by daniel, with input from jacek and k t.",8,train
DM-776,Researching partnership opportunities,nan,8,train
DM-778,Restructure and package logging prototype,"restructure and package log4cxx based prototype (currently in branch u/bchick/protolog). it should go into package called ""log""",8,train
DM-780,Access patterns for data store that supports data distribution ,"data distribution related data store includes things like. chunk > node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). need to understand access patterns   who needs to access, how frequently etc. ",5,train
DM-781,research mysql cluster ndb,checkout mysql cluster ndb from the perspective of data distribution   could it be potentially useful to store data related to data distribution?,2,train
DM-782,Automated test should optionally ignore column headers,some types of queries (like count( )) may return different column headers in qserv and mysql. this differences break our automated tests which dump and compare complete result including headers. it looks like we will not be able to guarantee that qserv can be made to return the same column headers as mysql except for providing aliases in the query itself. running those queries without aliases is a legitimate use case so it would be nice to have an option in the test runner which ignores headers for some queries.,4,train
DM-783,Disable failing test cases in automated tests,there are currently 4 test cases failing in out automated tests. until we have a fix we want to disable them.,1,train
DM-786,JOIN queries are broken,"running a simple query that does a join:   select s.ra, s.decl, o.rarange, o.declrange from   object o join   source s using (objectid) where  o.objectid = 390034570102582 and    o.latestobstime = s.taimidpoint;   results in czar crashing with:  2terminate called after throwing an instance of 'std::logicerror'   what():  attempted subchunk spec list without subchunks.   this query has been taken from integration tests (case01, 0003selectmetadataforonegalaxy.sql) ",3,train
DM-787,Understand DCR amplitudes using realistic distribution of stars,nan,4,train
DM-788,Determine refraction amplitudes as a function of SED,nan,2,train
DM-789,Model refraction amplitudes as a function of SED,nan,2,train
DM-790,Determine DCR ampltiudes as a function of SED,nan,2,train
DM-791,Model DCR amplitudes as a function of SED,nan,2,train
DM-792,Determine requirements on atmospheric measurements to predict refraction/DCR,nan,2,train
DM-793,Put together requirements to model refraction/DCR for a given source,nan,8,train
DM-794,SQL injection in czar/proxy.py,"running automated tests for some queries i observe python exceptions in czar log which look like this:  20140529 19:47:19.364371 0x7faacc003550 inf / query dispatch (7) tounhandled exception in thread started by / traceback (most recent call last):   file ""/usr/local/home/salnikov/qservmaster/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitandunlock     lock.unlock()   file ""/usr/local/home/salnikov/qservmaster/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock     self.savequerymessages()   file ""/usr/local/home/salnikov/qservmaster/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in savequerymessages     self.db.applysql(lock.writetmpl % (self.tablename, chunkid, code, msg, timestamp))   file ""/usr/local/home/salnikov/qservmaster/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applysql     c.execute(sql)   file ""/u2/salnikov/stack/linux64/mysqlpython/1.2.38/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/cursors.py"", line 174, in execute     self.errorhandler(self, exc, value)   file ""/u2/salnikov/stack/linux64/mysqlpython/1.2.38/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 36, in defaulterrorhandler     raise errorclass, errorvalue mysqlexceptions.programmingerror: (1064, ""you have an error in your sql syntax; check the manual that corresponds to your mysql server version for the right syntax to use near 'r' and sce.tract=0 and sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds   i believe this is due to how query string is being constructed in czar/proxy.py:  class lock:      writetmpl = ""insert into %s values (%d, %d, '%s', %f);""  # ...................             self.db.applysql(lock.writetmpl % (self.tablename, chunkid, code, msg, timestamp))   if msg happens to contain quotes then resulting query is broken. one should not use python formatting to construct query strings, instead the parameters should be passed directly to cursor.execute() method. ",2,train
DM-800,Zookeeper times out,"i noticed running some queries, leaving system up and them returning few hours later and running more queries can result in:   zooerror@handlesocketerrormsg@1723:  socket [127.0.0.1:12181] zk retcode= 4, errno=112(host is down):  failed while receiving a server response   it needs to be investigated (if we can reproduce) ",3,train
DM-803,"Provide a ""stackfitCalib"" for coadds",coadds have a psf class that returns the proper sum of input psfs at a point.  we need the same functionality for the calib object associated with the coadd exposure.  this will require making a calib a baseclass (it currently doesn't have a virtual dtor (although it does have virtual protected members)) ,4,train
DM-813,PhotoCalTask doesn't return information about which stars were used in calibration,"the photocaltask returns numpy arrays of the source and reference fluxes (and errors) of matched ""good"" photometric objects, typically stars.  however, while estimating the zero point, it clips outliers so the actual list of objects used is shorter.  please add another output to the returned struct, a numpy bool array ""good"", to indicate which objects are kept. ",1,train
DM-814,Cleanup in core/examples and core/doc,  core/examples and core/doc seems to be out of data.  some cleanup here would be welcome.,1,train
DM-815,Define filter for Epics and mapping of Epics to WBS,nan,2,train
DM-817,qserv have to use boost from stack,"to quote jacek and kt:  andy, re dm751, kt says never use the system version.  j.   so we need to switch qserv to eupsboost. this should be easy once dm751 is done, just add boost to qserv.table. then one can remove conditional part of boostchecker which works with systeminstalled boost. ",1,train
DM-820,Simplify copying tables while adding columns,"currently, if i want to copy a table while adding a few columns (as specified by schema in the example) i need to do something like:          cat = afwtable.sourcecatalog(schema)         cat.table.definecentroid(srccat.table.getcentroiddefinition())         cat.table.definepsfflux(srccat.table.getpsffluxdefinition())         # etc.          scm = afwtable.schemamapper(srccat.getschema(), schema)         for schel in srccat.getschema():             scm.addmapping(schel.getkey(), true)          cat.extend(srccat, true, scm)   please make this easier!  for example   by adding a flag to the schemamapper constructor that automatically does the addmapping (should this be the default?)   by making it possible to copy all the slots (maybe this'll be the case when the new alias scheme is implemented?).  maybe we just need a new method:  cat = srccat.extend(schema)  that does all the above steps.",4,train
DM-827,Reimplement C++/Python Exception Translation,"i'd like to reimplement our swig bindings for c exceptions to replace the ""lsstcppexception"" class with a more userfriendly mechanism.  we'd have a python exception hierarchy that mirrors the c hierarchy (generated automatically with the help of a few swig macros).  these wrapped exceptions could be thrown in python as if they were purepython exceptions, and could be caught in python in the same language regardless of where they were thrown.  we're doing this as part of a ""measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language.",8,train
DM-828,Design Prototypes for C++ Algorithm API,"we've never really been happy with the new design for the c algorithm api, and perry and jim have a few ideas to fix this that need to be fleshed out.  each of the subtasks of this issue will correspond to a different design idea.  ideally, for each one, we'll try to do a nearly complete conversion of the sdssshape algorithm (as a good example of a complicated algorithm) to see how these ideas work in practice.",6,train
DM-829,Algorithm API without (or with optional) Result objects,"in this design prototype, i'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate result object.  ideally the result objects would still be an option, but they may not be standardized or reusable.",3,train
DM-832,add persistable class for aperture corrections,"we need to create a persistable, maplike container class to hold aperture corrections, with each element of the container being an instance of the class to be added in dm740.  a prototype has been developed on dm797 on the hsc side: https:/hscjira.astro.princeton.edu/jira/browse/hsc797 and the corresponding code can be found on these changesets: https:/github.com/hypersuprimecam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/dm 797",2,train
DM-833,implement coaddition for aperture corrections,we need to be able to coadd aperture corrections in much the same way we coadd psfs.  see the hscside hsc798 and hsc897 implementation for a prototype: https:/hscjira.astro.princeton.edu/jira/browse/hsc798 https:/hscjira.astro.princeton.edu/jira/browse/hsc897 with code here: https:/github.com/hypersuprimecam/measalgorithms/compare/d2782da175c...u/jbosch/dm798 https:/github.com/hypersuprimecam/measalgorithms/compare/c4fcab3251...u/price/hsc897a https:/github.com/hypersuprimecam/pipe_tasks/compare/6eb48e90be12d...u/price/hsc 897a,3,train
DM-834,reduce code- and object-duplication in aperture correction and PSF coaddition,"the aperture correction code to be added on dm 833 will likely not be as closely integrated with coaddpsf as it could be, because the original design of coaddpsf didn't anticipate the addition of other, similar classes.  we should work on allowing these classes to share code.",8,train
DM-836,design Array fields for table version 1,"while we're trying to eliminate the need for compound fields in afw::table, arrays present a few problems.  we could use functorkeys, the way we plan to use other compound fields, but here we need to guarantee that the per element keys are contiguous, and we also be able to support views.  we also need to determine the naming scheme.  finally, we need to make sure these work with aliases and slots.",6,train
DM-837,Rewrite multiple-aperture photometry class,"we've never figured out how to handle wrapping multipleaperture photometry algorithms.  they can't use the existing result objects  at least not out of the box.  we should try to write a new multipleaperture photometry algorithm from the ground up, using the old ones on the hsc branch as a guide, but not trying to transfer the old code over.  the new one should:   have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.    have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons).",2,train
DM-839,"Rename methods that return pixel iterators and locators in image-like classes, and change to use parent indexing","image like classes have a large number of methods that return pixel iterators and locators based on row and/or column. we wish to change these to use parent indexing (making row and column relative to xy0). as the first step in doing this, rename all these methods and modify them to parent indexing. renaming will help identify and fix all code that uses these methods.",6,train
DM-840,Change code so ImageOrigin must be specified (temporary),"image like classes have a getbbox method and various constructors that use an imageorigin argument which in most or all cases defaults to local. as the first stage in cleaning this up, try to break code that uses the default as follows:  remove the default from getbbox(imageorigin) so an origin must be specified.  change the default origin of constructors to a temporary new value undefined    modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is undefined.  note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. however, that is messy because then the bounding box will also have to be always specified, and possibly an hdu, so it would be a much more intrusive change.",2,train
DM-841,Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary),"as part of making parent the default for image origin, change the data butler to require that imageorigin be specified if bbox is specified when reading or writing imagelike objects.  note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of dm840.",2,train
DM-842,Update code that uses pixel iterators and accessors,fix all code that uses pixel iterators and locators as per the changes in dm 839. this affects roughly 60 files distributed over many packages.,16,train
DM-843,Restore names of methods that return pixel iterators and locators,restore the names of methods that return pixel iterators and pixel locators on image like classes. (this is part of the final stage of eliminating local pixel indexing).,2,train
DM-844,Eliminate ImageOrigin argument,eliminate the imageorigin enum and argument from image like classes.,2,train
DM-845,Eliminate image origin argument from butler for (un)persisting image-like objects,eliminate the image origin argument for butler get and put when dealing with image like objects.,2,train
DM-846,Change code to always specify image origin (temporary),"the next step after dm840 is to change all code that uses imagelike getbbox and constructors that take an image origin argument to always specify the origin (dm840 will break any code that tries to use a default image origin).  this results in a working stack after dm840.  neither this nor dm840 will be merged to master, though it can be used by hsc and other users as they transition to a default image origin of parent.  see dm1176 for the final step.",10,train
DM-847,Update code to use restored names for methods that return pixel iterators and locators,"for all code changed in dm842, change it again to use the restored names for methods that return pixel iterators and locators.  this task is much simpler than dm842 because it is merely renaming methods. nonetheless, it touches many files in many packages.",4,train
DM-848,Specify ImageOrigin = PARENT in all cases,"for all code that uses the imageorigin argument (either explicitly or using a bbox and the default value) change the code to use an explicit imageorigin of parent.  in other words, update all code to match the changes in dm840 and dm841.  this affects 40 ish modules in many packages. i am concerned about testing the modified code because we are missing unit tests for so many tasks. but we can do some data processing and check those results. i will want some help with that.",12,train
DM-849,Eliminate use of ImageOrigin argument,"for all code that uses the imageorigin argument, eliminate the use of that argument. in other words, update all code to match the changes in dm844 and dm845,  this affects all the code affected by dm 848, plus any code that was already using imageorigin=parent, but is a simpler change.",5,train
DM-854,duplicate column name when running near neighbor query,"running a simplified version of near neighbor query on test data from case01:   select distinct o1.objectid, o2.objectid from   object o1,         object o2 where  scisqlangsep(o1.raps, o1.declps, o2.raps, o2.declps) / o2.objectid   result in an error on the worker:   foreman:broken! ,q38f9queryexec duplicate column name 'objectid' unable to execute query: create table r13237cd4cfc9e0fa01497bcf\ 67a91add266300 select o1.objectid,o2.objectid from subchunkslsst6630.object66300 as o1,subchunkslsst6630.object66300 as o2\  where scisqlangsep(o1.raps,o1.declps,o2.raps,o2.declps)/o2.objectid;   it is fairly obvious what is going on. ""select t1.x, t2.x"" is perfectly valid, but if we add ""insert into select t1.x, t2.x"", we need to add names, eg. something like ""insert into select t1.x as x1, t2.x as x2""",8,train
DM-860,Passing error messages from czar to end user - design,nan,3,train
DM-861,Provide a detailed integration tests report,"test output is very low level. indeed only a verbose logfile and slq queries output are currently available. furthermore failing queries (i.e. .sql.fixme) aren't launched.  this ouput could be leveraged to a detailed html web report (using sbadmin for example : http:/startbootstrap.com/templates/sbadmin/) which could :   launch all queries, even failing ones,  print qserv services status after each query,  execution time for each query,  information about mysql and qserv results differences,  href to qserv services log files   all other interesting information about queries execution    ",10,train
DM-862,apr and apt_util packages do not install shared library,"when we installed apr and apr_utils packages (as a dependency of new log4cxx package, see dm 772) we discovered that both these packages only build static libraries but no shared libs are installed. this is problematic if mixed with shared libs and we use shared libs everywhere else. we certainly need to build shared libs for these packages, this ticket is to follow up on this problem.",2,train
DM-863,near neighbor does not return results,"a query from qservtestdata (case01/queries/1051nn.sql) runs through qserv, but it returns no results, while the same query run on myql does return results.  the exact query for qserv is:    select o1.objectid as objid  from object o1, object o2  where qservareaspecbox(0, 0, 0.2, 1)  and scisqlangsep(o1.raps, o1.declps, o2.raps, o2.decl_ps) / o2.objectid; ",1,train
DM-869,disable extraneous warnings from boost (gcc 4.8),"compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. we should use the flag ""wnounusedlocaltypedefs"".",1,train
DM-870,XLDB Rio - workshop report,nan,3,train
DM-873,XLDB - strategic positioning,"discussions with strategic partners. improving website and adding new context (community, speakers). 1 pager document",3,train
DM-874,W'14 newinstall.sh picks up wrong python?,"newinstall.sh fails with:  installing the basic environment ...  traceback (most recent call last):   file ""/tmp/testlsst/eups/bin/eupsimpl.py"", line 11, in ?     import eups.cmd   file ""/tmp/testlsst/eups/python/eups/init.py"", line 5, in ?     from cmd        import commandcallbacks   file ""/tmp/testlsst/eups/python/eups/cmd.py"", line 38, in ?     import distrib   file ""/tmp/testlsst/eups/python/eups/distrib/init.py"", line 30, in ?     from repositories import repositories   file ""/tmp/testlsst/eups/python/eups/distrib/repositories.py"", line 8, in ?     import server   file ""/tmp/testlsst/eups/python/eups/distrib/server.py"", line 1498     mapping = self.noreinstall if outversion and outversion.lower() == ""noreinstall"" else self._mapping                                  ^ syntaxerror: invalid syntax  perhaps from running the wrong version of python.  full script/log is attached. ",1,train
DM-875,lsst_dm_stack_demo,"lsst dmstackdemo has obsolete benchmark files (circa release 7.0)  which fail to serve the purpose of validating, for the user, the correct functioning of a freshly built release v8.0 stack.   at the very least,  the benchmark files should be regenerated for each official release. tasks:   (1) build the benchmark files for release v8.0  (2) debate (a) recommending the  use of 'numdiff'  to check if the output is within realistic bounds.   or, (b) develop another procedure to better show how the current algorithms compare to the algorithms used at the benchmarked release. (3) depending on result of the debate on #2: for: (a) provide appropriate 'numdiff' command invocation in manual.; for (b) implement the new procedure.",40,train
DM-882,Segmentation fault from writing dotted FITS header keywords,"observed on hsc, but i'm not aware of any fix for this on the lsst side either.   pprice@tiger3:~ $ gdb python gnu gdb (gdb) red hat enterprise linux (7.260.el64.1) (gdb) r starting program: /tigress/hsc/products20130212/linux64/python/2.7.6/bin/python  >>> from lsst.afw.image import exposuref >>> exp = exposuref(1,1) >>> exp.getmetadata().add(""a.b.c.d"", 12345) >>> exp.writefits(""test.fits"")  program received signal sigsegv, segmentation fault. lsst::daf::base::propertyset::combine (this=0x1eda370, source=...)     at src/propertyset.cc:746 746         if (dj == map.end())   this appears to only affect dotted keywords (which should be supported through use of hierarch, and even if they're not supported, this should never fail with a segfault).",4,train
DM-887,Optimize template engine used in configuration tool,"string.template and string interpolation are quite weak, some additional feature would be welcomed :   interpolation interprets wrongly ""%d"" in template files as template  string.template doesn't manage correctly non referenced template parameters present in template files.",4,train
DM-895,Use an existing qserv_run_dir with a new Qserv instance/binary,"here's what should be added to qservconfigure.py :   edit $qservrundir/admin/qserv.conf and change qserv instance dir to current one (which qservconfigure.sh),  check compliance of qservrundir with new qserv instance (version check ?) and/or update configuration files,  reinitialize services, if needed, without breaking already loaded data. ",4,train
DM-898,unset BASH_ENV in newinstall.sh or surrounding instructions,"nutbar users such as myself may have bashenv set, which can mess with your carefully constructed shell environments.  unsetting bashenv should help.  this could perhaps go in newinstall.sh, or the documentation alongside where we suggest they unset other variables: https:/confluence.lsstcorp.org/display/lswug/buildingthev8.0lsststackfromsource",1,train
DM-900,Update PhoSim tutorial to use CatSim for creating instance catalogs,"update the process phosim images tutorial (https:/confluence.lsstcorp.org/display/lswug/processphosimimages) to use catsim to generate instance catalogs, once catsim documentation is available. this will obviate the need for the helper script refcalcat.py. ",2,train
DM-903,"SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both""","the sourcedetectiontask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdparity == ""both"".  as adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task.  please change the code to only modify the schema if it's going to set it. ",1,train
DM-911,Provide Task documentation for DipoleMeasurementTask,see summary. ,2,train
DM-912,Provide Task documentation for PsfMatchTask,see description (it's currently called psfmatch) ,4,train
DM-913,Provide Task documentation for ImagePsfMatchTask,see summary,2,train
DM-914,Provide Task documentation for SnapPsfMatchTask,see summary,2,train
DM-915,Provide Task documentation for AssembleCcdTask,see summary,4,train
DM-916,Provide Task documentation for IsrTask,see summary,4,train
DM-926,Provide Task documentation for SourceDeblendTask,see summary,2,train
DM-927,Provide Task documentation for CmdLineTask,see summary,4,train
DM-928,Provide Task documentation for RepairTask,see summary,4,train
DM-929,"How to write your own command line task, including how-to-retarget sub-tasks","please provide documentation on how to write a command line task, and how to retarget tasks (e.g. reusing bits of isrtask for a new camera)  the documentation should include a complete annotated example.  ",4,train
DM-930,Improve install/configuration/tests documentation and migrate it to reST format,"this ticket propose to migrate readme and readmedevel to rest format (see http:/sphinxdoc.org/rest.html). the output is located here : http:/lsstweb.ncsa.illinois.edu/~fjammes/qservdoc/  furthermore this ticket wil integrate andy s. dm622 valueadded remarks about qserv embedded documentation.  readme.txt needs a bit of formatting, whole ""note for developers"" is one long line which may need scrolling depending on what do you use to read the file, same applies to readmedevel.txt the install procedure in readme.txt implies that the whole stack has to be installed including eups. if people have some part of it installed already the it would probably be better to reuse existing stack. shall we spit install instructions into ""install eups (if not installed already)"" and ""install qserv""? readmedevel.txt says ""once qserv is installed..."", i don't think that we need or want to install whole qserv before we start development (what if qserv is not available yet for the platform i'm trying to test). what probably needed is installed dependencies, and this should be covered by the comments before 'setup  r .'  ",1,train
DM-933,"Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active","the photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit.  if a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername.    please change the code so that ""flux"" is ignored, and the flux associated with filtername is used.",1,train
DM-936,Replace getXXXKey for slots with returning functorKeys similar to existing compound Keys,"make any changes in the functor key capabilities necessary to support getxxxkey, getxxxerrkey, and getxxxflagkey for the 3 different types of slots.   change these routines in source.h.m4 to return functorkeys for both version 0 and version 1 tables.  then fixup any compilation breaks on the c size which this causes.  remove the version 1 specific accessors.   i am assigning this to jim to confirm that the first part is done, then he can assign the remainder to perry either by reassigning, or as a subtask.",4,train
DM-944,Package xrootd-4.0.0rc3-qsClient2 with eups,nan,1,train
DM-945,Prevent conflict related to non-unique temporary files created during SciSQL install,"scisql install sometime fails with next message :   [1/2] mysqlscript: scripts/install.mysql [2/2] mysqlscript: scripts/demo.mysql running testhtm                          : ok running testselect                       : ok running testangsep.py                    : ok running testmedian.py                    : ok running testpercentile.py                : ok running tests2cpoly.py                   : ok running tests2ptinbox.py                 : ok running tests2ptincircle.py              : ok running tests2ptinellipse.py             : ok running docs.py                          : fail [see /usr/local/home/salnikov/qservrun/u.fjammes.dm622g86a30ec72a/tmp/scisql0.3.2/build/tools/docs.log]   9 tests passed, 1 failed, and 0 failed to run    one or more scisql unit tests failed make:   [install] error 1  critical: error code returned by command : / u s r / l o c a l / h o m e / s a l n i k o v / q s e r v  r u n / u . f j a m m e s . d m  6 2 2  g 8 6 a 3 0 e c 7 2 a / t m p / c o n f i g u r e / s c i s q l . s h   $ cat /usr/local/home/salnikov/qservrun/u.fjammes.dm622g86a30ec72a/tmp/scisql0.3.2/build/tools/docs.log rm: cannot remove `/tmp/scisqldemoccds.tsv': operation not permitted failed to run documentation example:     rm f /tmp/scisqldemoccds.tsv     error 1086 (hy000) at line 4: file '/tmp/scisqldemoccds.tsv' already exists   it looks like test uses non unique temporary file name and someone already tried to run installation on the same host. ",2,train
DM-946,Improve management of qserv-run-dir in start/stop scripts.,qservstart.sh by default tries again to use /usr/local/home/salnikov/qservrun/2014_05.0 directory. there may be a confusion if we have multiple run directories. would it be better to install qserv start/stop scripts into run directory itself so that they don't need to guess anything?,2,train
DM-951,Add Doxygen documentation on rebuilds,master branch doxygen documentation should be rebuild on every full master build.,20,train
DM-953,Move qserv-testdata.py to qserv_testdata package,"andy s. suggests this during dm 622 review, but this needs to be studied deeply, as qserv_testdata goal is to store large datafile, difficult to version in git.",3,train
DM-954,Clearer and shorter output for qserv-configure.py,qserv configure.py produces a lot of output which could be confusing if people try to look at it and understand it. it may be better to reduce it to something that just indicates that each step is completed successfully.,1,train
DM-956,Buildbot email should state if the build used master only or included other branches,buildbot build status report currently doesn't state if the build was only for master or included other branches requested by the user.,4,train
DM-957,Use aliases to clean up table version transition,"the addition of schema aliases on dm417 should allow us to clean up some of the transitional code added on dm545, as we can now alias new versions of fields to the old ones and vice versa.",2,train
DM-958,Move table versions to Schema,"we've added a version number to afw::table::basetable to help with the transition to a new approach with different naming conventions and functorkeys instead of compound keys.  however, it looks like schema objects need to know about this version number as well, in order to change subschema objects to split/join using underscores instead of periods.  that means we'll have to move the version down into the schema object, which may affect a lot of downstream code.  other than touching a lot of code, the changes should be trivial.",4,train
DM-963,measAlg.interpolateOverDefects doesn't accept a python list of Defects,the python binding of interpolateoverdefects should accept a python list of defects as well as a swig wrapped std::vector/; it doesn't (try using a python list in test818 in meas_algorithms/tests/interp.py)  ,1,train
DM-964,Include aliases in Schema introspection,schema stringification and iteration should include aliases somehow.  likewise the extract() python methods.,1,train
DM-966,fix int/long conversion on 32-bit systems and selected 64-bit systems,"tests/wrap.py fails in pexconfig on 32bit systems and some 64bit systems (including ubuntu 14.04) with the following:  tests/wrap.py  ...ee.e. ====================================================================== error: testdefaults (main.nestedwraptest) test that c control object defaults are correctly used as defaults for config objects.  traceback (most recent call last):   file ""tests/wrap.py"", line 89, in testdefaults     self.assert(testlib.checknestedcontrol(control, config.a.p, config.a.q, config.b))   file ""/home/boutigny/cfht/stack5/build/pexconfig/tests/testlib.py"", line 987, in checknestedcontrol     return testlib.checknestedcontrol(args) typeerror: in method 'checknestedcontrol', argument 2 of type 'double'  ====================================================================== error: testint64 (main.nestedwraptest) test that we can wrap c control objects with int64 members.  traceback (most recent call last):   file ""tests/wrap.py"", line 95, in testint64     self.assert(testlib.checknestedcontrol(control, config.a.p, config.a.q, config.b))   file ""/home/boutigny/cfht/stack5/build/pexconfig/tests/testlib.py"", line 987, in checknestedcontrol     return testlib.checknestedcontrol(args) typeerror: in method 'checknestedcontrol', argument 2 of type 'double'  ====================================================================== error: testreadcontrol (main.nestedwraptest) test reading the values from a c control object into a config object.  traceback (most recent call last):   file ""tests/wrap.py"", line 82, in testreadcontrol     config.readcontrol(control)   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/wrap.py"", line 212, in readcontrol     at=at, label=label, reset=reset)   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/wrap.py"", line 217, in readcontrol     self.update(at=at, label=label, values)   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/config.py"", line 515, in update     field.set(self, value, at=at, label=label)   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/config.py"", line 310, in set     raise fieldvalidationerror(self, instance, e.message) fieldvalidationerror: field 'a.q' failed validation: value 4 is of incorrect type long. expected type int for more information read the field definition at:   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/wrap.py"", line 184, in makeconfigclass     fields[k] = fieldcls(doc=doc, dtype=dtype, optional=true) and the config definition at:   file ""/home/boutigny/cfht/stack5/build/pexconfig/python/lsst/pex/config/wrap.py"", line 131, in makeconfigclass     cls = type(name, (base,), )    ran 8 tests in 0.017s  failed (errors=3)   there is a partial fix on u/jbosch/intwrappers; this seems to work for ubuntu 14.04, but not on 32bit systems.",2,train
DM-967,qserv-configure.py is broken in master,"it looks like there was a bug introduced either during the merge of dm622 with master or right before that. running qservconfigure.py from master fails now:  $ qservconfigure.py    file ""/usr/local/home/salnikov/qservmaster/build/dist/bin/qserv configure.py"", line 229     (""do you want to update user configuration file (currently pointing                                                                       ^ syntaxerror: eol while scanning string literal  i assign this to myself, fabrice is on vacation now and we need to fix this quickly.",1,train
DM-973,"Create a ""stub"" package that checks for system dependencies for Qserv","create a special ""stab"" package that checks and reports missing dependencies.",2,train
DM-975,setup standard aliases for frequently-used measurements,"frequentlyused measurement fields such as classification and pixel flags should have shortened aliases that can be used instead of their full, packagequalified versions.  in some cases, these may serve as a sort of slot (e.g. we may have multiple classifications algorithms someday).  in this issue, we should audit all current measurement algorithm fields that don't already have a slot that works for them and consider whether there should be a standard alias.  we also need to work out a system for defining these aliases, probably in the config for singleframemeasurementtask.",4,train
DM-976,Detailed documentation for meas_base tasks,we should follow rhl's example for detailed task documentation and document all meas_base tasks.,2,train
DM-977,Documentation audit and cleanup for meas_base plugins,"many meas_base plugins and algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm.  these need to be fixed.",2,train
DM-978,add base class for measurement tasks,"we should consider adding a base class for measurement tasks (singleframemeasurementtask, forcedmeasuremedtask) that includes the callmeasure methods.  i'm hoping this will help cleanup callmeasure and improve code reuse.",1,train
DM-979,Add FunctorKey to replace Coord compound keys,"unlike previous functorkey replacements, coord compound keys are used in the minimal schema for simpletable and sourcetable, which makes removing them problematic.",1,train
DM-980,convert measurement algorithms in ip_diffim,ip_diffim includes a few measurement algorithms which need to be converted to the new framework.,5,train
DM-981,convert measurement algorithms in meas_extensions_shapeHSM,"this is a lowpriority ticket to replace the oldstyle plugins in measextensionsshapehsm with new ones compatible with measbase.  as this isn't a part of the main line stack, we should delay it until other the measbase conversion is nearly (or perhaps fully) complete.",3,train
DM-982,convert meas_extensions_photometryKron to new measurement framework,"this is a lowpriority ticket to replace the oldstyle plugins in measextensionsphotometrykron with new ones compatible with measbase.  as this isn't a part of the main line stack, we should delay it until other the measbase conversion is nearly (or perhaps fully) complete.",3,train
DM-983,Finish the Log packaging,finish the work started by bill (dm 778),6,train
DM-984,allow partial measurement results to be set when error flag is set,we need to be able to return values at the same time that an error flag is set.  the easiest way to do this is to have algorithms take a result object as an output argument rather than return it.  we'll revisit this design later. ,2,train
DM-989,.my.cnf in user HOME directory breaks setup script,"presence of .my.cnf file in the user home directory crashes qservconfigure.py script if parameters in .my.cnf conflict with parameters in qserv.conf.  how to reproduce:  create .my.cnf file in the home directory:  [client] user     = anything # host/port and/or socket host     = 127.0.0.1 port     = 3306 socket   = /tmp/mysql.sock   try to run qservconfigure, it fails with error:  /usr/local/home/salnikov/qservrun/u.salnikov.dm595/tmp/configure/mysql.sh: connect: connection refused /usr/local/home/salnikov/qservrun/u.salnikov.dm595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: connection refused error 2003 (hy000): can't connect to mysql server on '127.0.0.1' (111)   it looks like ~/.my.cnf may be a leftover from some earlier qserv installation. if i remove it and rerun qservconfigure.py now it's not created anymore. maybe worth adding some kind of protection to qservconfigure.py in case other users have this file in their home directory.",2,train
DM-991,add query involving a blob to the integration tests,we need to add a query (or more?) to the qserv_testdata that involve blobs. blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc. ,2,train
DM-993,improve message from qserv_testdata,"currently, when i try to run qservbenchmark but qservtestdata was not setup, i am getting   critical unable to find tests datasets.  for eups users : please run :    eups distrib install qservtestdata    setup qservtestdata for noneups users : please fill 'testdatadir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use testdata dir option.   it is important to note in the section for eups users that this has to be called before qserv is setup, otherwise it has no effect. ",1,train
DM-994,make slot config validation more intelligent,"slot config validation currently assumes that field names match plugin names, which is not always a safe assumption.  this can prevent certain algorithms from being used in slots.  we probably can't do this validation in config.validate(); we need to check in the measurement task constructor after the schema has already been constructed.",1,train
DM-995,Make NoiseReplacer a context manager,"i think the api for noisereplacer could be made more idiomatic (and possibly safer) by turning it into a ""context manager"" (i.e. so it can be used with the ""with"" statement).",1,train
DM-998,Adapt integration test to multi-node setup v1,following dm595 we can start qserv in multinode configuration. next step is to be able to run integration tests in that setup. this needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.,10,train
DM-999,rename config file(s) in Qserv,"rename local.qserv.cnf to qservczar.cnf. it is quite likely there are some other config files that would make sense to rename. if you see some candidates, let's discussion on qservl and do the renames.",1,train
DM-1001,Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test,"in unit test, the comparison     self.assertalmostequal(skp1[nk][np], skp2[nk][np], 4)   fails.  however if changed to    self.asserttrue(abs(skp1[nk][np]skp2[nk][np]) < 104)  which is the desired test, this succeeds.   this ticket will remove all assertalmostequals from subtractexposure.py and replace with the fundamental comparison operator of the absolute value of the differences.",1,train
DM-1002,Research Apache Mesos and Google Kubernetes,it'd be good to check out apache mesos and google kubernetes and see how relevant they are for qserv / lsst database ,4,train
DM-1004,Provide Task documentation for ModelPsfMatchTask,see description (it's currently called psfmatch) ,2,train
DM-1005,Migrate Qserv worker code to the new logging system,nan,8,train
DM-1010,fix names of meas_base plugins to match new naming standards,some meas_base plugins still have old style algorithm names.,1,train
DM-1011,Remove use of compound fields in minimal schema,"if we want to ultimately remove compound fields, we need to remove the ""coord"" field from the simpletable/sourcetable minimal schema, and provide a different way to get ra,dec from a source.",4,train
DM-1012,remove temporary workaround in new SkyCoord algorithm,"singleframeskycoordplugin is using the footprint peak, not the centroid slot.  according to comments in the code, this is a workaround for some problem with centroids.  this needs to be fixed.",1,train
DM-1013,Classification should set flags upon failure,"the classification algorithm claims it can never fail.  it can, and should report this.",2,train
DM-1014,Detailed documentation for GaussianCentroid,"we need more detailed documentation for the gaussiancentroid algorithm, in terms of how it actually computes the centroid.  we (jim and perry) have done what we can, but we need help from whoever actually wrote it (rhl, we think) to provide the rest.  in particular:   additional detail should be filled in in the class doxygen for gaussiancentroidalgorithm, in gaussiancentroid.h   the ""nopeak"" flag field description and name should be compared to what the algorithm actually does with it.  it looks to me like it's a bit misnamed (and maybe shouldn't be considered an error condition at all, if we want to run this on difference images), but i'm not sure.",1,train
DM-1015,"convert GaussianFlux to use shape, centroid slots",we should cleanup and simplify the gaussianflux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up.,1,train
DM-1016,Detailed documentation for SdssCentroid,"we needed detailed documentation for how sdsscentroid actually works.  i think it involves fitting something like a symmetrized psf model, but i'm sufficiently unsure of the details that rhl should write this, not me.  it should go in the doxygen class docs for sdsscentroidalgorithm in sdsscentroid.h in meas_base.",1,train
DM-1017,fix testForced.py,"testforced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a footprint with peaks (i suspect because transforming a footprint might remove the peaks).  prior to dm976, that would have caused a segfault; on dm976, i've turned it into an exception, which is then turned into a warning by the measurement framework.",2,train
DM-1018,Fix incorrect eupspkg config for astrometry_net,"the clang patch from 8.0.0. version was (correctly) deleted. however, the patch identity was still left in the eupspkg config's protocol.  this will delete the last vestige of the formerly necessary clang patch.",2,train
DM-1022,fix warnings related to libraries pulled through dependent package,"this came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see dm983, essentially:   eups distrib install c log4cxx 0.10.0.lsst1 r http:/lsstweb.ncsa.illinois.edu/~becla/distrib r http:/sw.lsstcorp.org/eupspkg   cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/dm207 of qserv and building it.  the warnings looks like:  /usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservdev/linux64/log/1.0.0/lib/liblog.so, not found (try using rpath or rpathlink) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservdev/linux64/log/1.0.0/lib/liblog.so, not found (try using rpath or rpathlink) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservdev/linux64/log/1.0.0/lib/liblog.so, not found (try using rpath or rpath link)   and they show up when i build qserv package, and are triggered by the liblog. i suspect sconsutils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. fabrice, can you try to find a reasonable solution for that? thanks!",1,train
DM-1026,make use of MeasurementDataFlags,"we started adding a system to allow algorithms to declare what kind of data they can run on, but never really put it in place.  to do this, we should:   add more flags (at least nocalib).   pass these flags in pipetasks and other places with tasks that use measurement tasks as subtasks (for instance, we should set nowcs and nocalib during calibrate.initialmeasurement, and coadd in processcoadd).    add checks for these flags in appropriate algorithms.  for instance, psfflux should fail in construction if no_psf is set, and peaklikelihoodflux should fail if preconvolved is not set.",6,train
DM-1028,qserv-version.sh produces incorrect version number,i have just installed qserv on a clean machine (this is in a new virtual machine running ubuntu12.04) which got me version 201407.0 installed:  $ eups list qserv    201407.0    current b76 $ setup qserv $ eups list qserv    201407.0    current b76 setup $ echo $qservdir /opt/salnikov/stack/linux64/qserv/201407.0   but the qservversion.sh script still thinks that i'm running older version:  $ qservversion.sh 201405.0 ,2,train
DM-1029,"""source"" command is not in standard shell","qservstart.sh script fails when installed on ubuntu12.04:  $ ~/qservrun/201405.0/bin/qservstart.sh /home/salnikov/qservrun/201405.0/bin/qservstart.sh: 4: /home/salnikov/qservrun/201405.0/bin/qservstart.sh: source: not found /home/salnikov/qservrun/201405.0/bin/qservstart.sh: 6: /home/salnikov/qservrun/201405.0/bin/qservstart.sh: checkqservrundir: not found   it complains about source command. source is not standard posix shell command, it is an extension which exists in many shells. apparently in older ubuntu version /bin/sh is stricter about nonstandard features.   to fix the script one either has to use standard . (dot) command or change shebang to #!/bin/bash. this of course applies to all our executable scripts.",2,train
DM-1030,W15 Refactoring of Qserv,"refactoring qserv code to make it cleaner, better, and most importantly more robust and resilient to failures.  jk: refer to loading spreadsheet for pmcs assignments",84,train
DM-1031,W15 Integration Testing of Qserv,"improvements to the integration tests suite for qserv, including making it more generic (it currently uses a hardcoded database name), integrating new partitioner, and adding a data set from one of the last data releases.  jk: refer to loading spreadsheet for pmcs assignments",19,train
DM-1033,W15 Design Metadata Store for production tracking (v1),"come up with a design of metadata store that will track all information in the science data archive (across data releases), for both image and database repositories  jk: refer to loading spreadsheet for pmcs assignments",45,train
DM-1034,W15 Initial version of image and file archive,"build initial (alpha) version of system for tracking existing image data sets. (a) build a web form where users enter information about existing data sets. (b) design and build a mysql backend (maybe through datacat). (c) implement crawler that automatically fetches metadata from fits headers for data set registered by users.   note that support for additional formats (eg config files) will be added in the future, not through this epic. note that this initial version will not be production ready, we will make it more bullet proof and feature rich during s15 cycle.  jk: refer to loading spreadsheet for pmcs assignments",30,train
DM-1035,W15 Butler (v2) for local data sets,improvements and tweaks to the butler as needed based on the feedback from the apps team  jk: refer to loading spreadsheet for pmcs assignments,19,train
DM-1036,W15 Management of distributed Qserv databases and tables,"this includes tasks such as creating and deleting distributed databases managed by qserv, creating and deleting partitioned tables based on the information that is stored in the css. design and exploratory prototype.  jk: refer to loading spreadsheet for pmcs assignments",39,train
DM-1037,W15 Loading data into distributed Qserv-managed databases,"improvements to the existing data loader, to simplify data loading into qserv. that includes integration with information in the css. this version will include nonparallel, singlenode loader.  relevant document (describing a sketch of much more advanced version): https:/dev.lsstcorp.org/trac/wiki/db/qserv/dataloading  jk: refer to loading spreadsheet for pmcs assignments",28,train
DM-1038,S15 Implement Query Mgmt in Qserv,"initial version of system for managing queries run through qserv. this includes capturing information about queries running in qserv. note, we are not dealing with query cost estimate here, (it will be covered through dm 1490).",40,train
DM-1041,eliminate confusing config side-effects in CalibrateTask,"calibratetask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps.  in particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post psf measurement is to feed those algorithms.  this (as well as poor test coverage) made it easy to break calibratetask in the case where those options are disabled a few branches back.  after conferring with simon and andy, we think the best solution is to remove this sort of conditional processing from calibratetask, which should also make it much easier to read.  instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within calibratetask itself.",1,train
DM-1045,Create a permanent and accessible mapping of the BB# and the bNNN. ,create a permanent and accessible mapping of the bb# and the bnnn.   the users are interested in the bb# since is is used to point to the stdio file form the entire stack build. the bnnn is needed because the daily life of the developer revolves around the stack tagged alternately by the bnnn tags and/or the dm release tags. ,2,train
DM-1046,"W15 Central State System, support for db/table/query metadata","improvements to the css system to make it more thread safe, more compact representation of keys, and adding information that will be needed to handle distributed databases, tables and likely queries (if we decided to use css for query management).  jk: refer to loading spreadsheet for pmcs assignments",49,train
DM-1051,W15 Image Cutout Service,first version of the images cutout service. it will rely on butler. the bulk of the work in this epic includes building a webbased frontend.  jk: refer to loading spreadsheet for pmcs assignments,19,train
DM-1052,Add / improve tests and examples for Log package,nan,1,train
DM-1054,init.d/qserv-czar needs LD_LIBRARY path,"with the addition of log we now need to find some shared libraries from stack. current version of qserv czar init.d script does not capture ldlibrarypath, so we should add it there. ",1,train
DM-1055,Remove unnecessary pieces from qserv czar config,"the config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css.  this ticket includes:  removing these items from the installation/configuration templates  removing these items from sample configuration files  removing these items from the code that reads in the configuration file and sets defaults for these items  fixing things that seem to break as a result of this cleanup.  danielw volunteers to assist on the last item, as needed.  ",2,train
DM-1058,"fix SubSchema handling of ""."" and ""_""","subschema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator.  as part of fixing this, we should also be able to simplify the code in the slot definers in sourcetable.",1,train
DM-1059,track down difference in SdssShape implementation,"the measbase version of sdssshape produces slightly different outputs from the original version in measalgorithms, but these should be identical.  we should understand this difference rather than assume its benign just because it's small.",2,train
DM-1060,S15 Data Distribution & Replica Mgmt Design,"this is a continuation of dm779, continue research related to data distribution and replication. there are still many option questions: should it be centralized or peertopeer? is bittorent useful? how much should be home grown vs offtheshelf (and which offthe shelf). there are no direct deliverables in fy2015, but it is a complex topic, so we should not wait until the last minute. this epic covers the r&d / design.",100,train
DM-1061,W15 Qserv Stability / bug fixes,"tweaks, improvements, fixes to bugs discovered over the course of w15, to improve qserv stability.  jk: refer to loading spreadsheet for pmcs assignments ",83,train
DM-1067,move algorithm implementations out of separate subdirectory,"we should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  they should generally go into anonymous namespaces there.  after doing so, we should do one more test to compare the measbase and measalgorithms implementations.",1,train
DM-1068,audit and clean up algorithm flag and config usage,"check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics).",1,train
DM-1070,switch default table version to 1,"now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw.  code that cannot handle version > 0 tables should continue to explicitly set version=0.",2,train
DM-1071,Switch default measurement tasks to meas_base,"we should set the default measurement task in processimagetask to singleframemeasurementtask, and note that sourcemeasurementtask and the old forced photometry drivers are deprecated.",2,train
DM-1072,create forced wrappers for algorithms,we have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin.  we should go through the algorithms we have implemented and create forced plugin wrappers for these.,1,train
DM-1073,remove old forced photometry tasks,"after measbase has been fully integrated, remove the old forced photometry tasks from pipetasks",1,train
DM-1074,Measurement - Calibration and Ingest,"create command line tasks to transform raw measurement quantities (e.g. fluxes, positions in pixels) to global units (e.g magnitudes and positions in celestial coordinates).  also requires interfacing with the measurement plugin system, as only plugins can be authoritative on how to transform their outputs.  jk: in pmcs this would be bosch j 50% gee p 50%",20,train
DM-1076,convert afw::table unit tests to version 1,"most afw::table unit tests explicitly set version 0.  we should change these to test the new behaviors, not the deprecated ones.",2,train
DM-1077,Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.,"audit tct recommendations to ensure that all standards updates were installed into standards documents.  it was found that the meeting recorded in:  failed to include two recommendations:    recommended: 330: i find the error suffix to be usually more appropriate than exception.  current: 330. exception classes should be suffixed with exception.   recommended but not specifically included: namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or lessqualified) names within those blocks over globalnamespace aliases.  rule 3 6 is an amalgam of namespace rules which doesn't quite have the particulars desired. fyi: the actual vote was to:  ""allow namespace blocks in source code (cc) files.""  to simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards.",2,train
DM-1078,add batch flag to newinstall.sh,in some cases (installing in some script environment) it is nice to be able to run without any interaction.  this will add a flag to tell the script to install without asking about anaconda and git.  it will assume the answer is 'yes' to any question it would have asked.,1,train
DM-1079,Organize the content of the DM Developer Guide,"the dm developer guide is presently a placeholder for information relevant to developers. it is time to reorganize the content, and to provide authoritative guidance to developers. this will involve contributions from various members of the dm team, and likely some selected harvesting of the trac/wiki. ",10,train
DM-1083,Fix overload problems in SourceCatalog.append and .extend,"this example fails with an exception:  import lsst.afw.table as afwtable schema = afwtable.sourcetable.makeminimalschema() st = afwtable.sourcetable.make(schema) cat = afwtable.sourcecatalog(st) tmp = afwtable.sourcecatalog(cat.gettable()) cat.extend(tmp)   expected behavior is that the last line is equivalent to cat.extend(tmp, deep=false).",1,train
DM-1085,Update the DMS and Astro Glossaries,"the dms and astro glossaries in confluence define a set of technical terms used in their respective domains. some of the definitions are placeholders, and other terms used in the swug, dm space, and dm developer guide have yet to be defined in one glossary or the other. it is time to update these docs.",2,train
DM-1086,Determine problem with Mac OS X processCCD output when compared to 'identical' dataset generated on RHEL6,the benchmark file for the summer2012 demo is generated on  rhel6. when using the dataset to compare stack output generated on mac osx 10.8.5 (and 10.9)  there are significant deviations.   find out where the problem arises...in the stack during processing or in the comparison.,4,train
DM-1087,Research how to kill query in mysql,"in the port to the new xrootd ssi api, workerside squashing was lost in the shuffle. the plumbing is different, and reimplementing squash functionality is not entirely straightforward, especially because the new api is still missing documentation and examples for implementing cancellation.  the consequences of not implementing this are minorsome extra work may be done by the worker, but not a whole lot, because user level cancellation has not been implemented.  first step is to understand how mysql code is handling query killing.",4,train
DM-1088,Investigate HTCondor config settings to control speed of ClassAd propagation,"with default settings we do not have good visibility as to whether an updated classad on a compute node (e.g., cachedatalist now has ccd ""s00"") will be in effect on the submit node in time for a job to be matched to an optimal htcondor node/slot.   there are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new classad on a worker node to 'propagate' back to the submit side. we investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions.",2,train
DM-1089,Enhance SWUG chapter on measurement,"a basic description of measurement in the lsst stack was created for the swug (see dm692). however, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. this task is intended to provide the next level of detail, with references to sourcelevel descriptions that exist or are in progress. ",6,train
DM-1099,afw::table - finish interface transition,jk: in pmcs bosch j and gee p,36,train
DM-1100,Measurement - Convert Old Algorithms,jk: in pmcs bosch j and gee p,31,train
DM-1101,Measurement - Finish Framework Overhaul,jk: in pmcs bosch j and gee p,51,train
DM-1103,Design unit-test data package,gather requirements for unit test datasets and determine how to organize and generate the data.,4,train
DM-1104,Generate test data,write scripts to generate the test data (as needed) and run them.,10,train
DM-1105,Custom mapper for unit test data package,"we need a custom mapper for the test data package, so we can run unit tests against it without depending on any particular obs_  package.",6,train
DM-1106,Convert existing tests to use new package,"after the new unit test data package is available, we should convert all tests that currently use afwdata or obstest to use the new package instead, and then remove obstest and afwdata.  should have subtasks and a re estimate of effort after auditing how much there is to be done.",30,train
DM-1107,afw - Footprint Improvements,"this epic contains several improvements to the footprint class, including api refactoring, performance related reimplementations, and some new features.  jk: in pmcs this would be bosch j and swinbank j",30,train
DM-1108,Galaxy Fitting - Shear Precision Experiments,"nb some of the below stories will actually be lauren, not perry.  breakdown: pgee 58%; jbosch 21%; krughoff 6%; lauren 15%",100,train
DM-1109,Minimal MultiFit system prototype,"experiment with a framework for running ""multifit""style fitting algorithms (i.e. fit a model convolved with the psf to multiple exposures simultaneously).    earlier prototypes (meas_multifit) have demonstrated executable multifit code on a single system: repeating that work is not the priority here. rather, we will produce a concept for the framework for running multifit at scale using large datasets on a cluster. it is not a requirement that it be possible to run an atscale multifit job and produce useful results by the end of this epic; however, there should be proofofconcept code which demonstrates the structures within which future multifit work will be carried out. this will be supplied to both science pipelines and process middleware developers.",100,train
DM-1110,Galaxy Fitting - Sampling Algorithm Plugin,nan,60,train
DM-1111,CModel robustness,"the cmodel code is currently a mixture of work ported from hsc and some new development on lsst. it needs to be validated on real data (hsc, sdss, cfht). all known failure modes should be eliminated. where possible, improve its performance.",65,train
DM-1112,Create utilities to allow camera testing team to use CameraGeom,"the camera team can use the camerageom classes to reduce lab data for testing purposes.  since the camera is relatively flexible, a good way of doing this is to create a camera at runtime from the header keys in the files to be reduced.    jk: in pmcs this would be krughoff s",30,train
DM-1113,Make the API for ISR explicit,the run method of the isrtask currently takes a dataref which has getters for calibration products.  this makes the task hard to re use because one needs a butler and because the interface is opaque.  this task will make the isrtask api more transparent.  jk: in pmcs this would be krughoff s,20,train
DM-1114,Re-write astrometry task to remove dependency on astrometry.net,"the astrometry task currently depends as astrometry.net as the only possible algorithmic back end.  this also has created a dependency on the astrometry.net file format.    this task will remove the dependency on astrometry.net file format and will implement the current hsc astrometry solver in the astrometry task.  this will focus on solving single chips, not the full focal plane.    note that dm 167 (which was closed with reference to this ticket) requests that we split photometric and astrometric catalogues, and this is still needed.    jk: in pmcs this would be krughoff s and owen r",80,train
DM-1115,Command-line Tasks - Unit tests for ProcessCcd,we can get a long way toward improved test coverage in pipe_tasks by having unit tests for processccd.  jk: in pmcs this would be owen r,20,train
DM-1116,Add numpy typemaps to afw interfaces so numpy types pass correctly to C++,currently swiged interfaces do not handle numpy types correctly.  this can be fixed with the appropriate typemap.  jk: in pmcs this would be owen r,11,train
DM-1117,Extend Exposure classes to contain background models,"exposures carry many things: psfs, fits metadata, detectors, etc.  another object algorithms may want to access with an exposure is the set of background models subtracted from the exposure to that point.  this will involve porting afw.math.backrgoundlist to c and extending exposure classes to contain the new backgroundlist object.  jk: in pmcs this would be krughoff s",20,train
DM-1118,Improve documentation of code from DM-70,"dm70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code unmerged.  this ticket exists as the second half of dm 70 to clean up its documentation.",6,train
DM-1120,Improve fault tolerance as demonstrated in nightly computing simulator,"adjust simulator parameters, mechanisms, and implementation to minimize time to recover and unprocessed/unarchived data resulting from a machine or network failure.  for w15: pietrowicz, s  100% start oct 2014, finish nov 2014  for s15 rollover: pietrowicz, s  12% start march 2015, finish april 2015",50,train
DM-1121,Simplify and refactor the Event Services API,"remove old fixed metadata; refactor using more generic metadata interface.  for w15: pietrowicz, s  100% start dec 2014, finish dec 2014  for s15 rollover: pietrowicz, s  11% start march 2015, finish april 2015  ",18,train
DM-1122,Write a log4cxx handler that emits events for log messages,"write a log4cxx handler that emits events for log messages.  for w15: pietrowicz, s  100% start jan 2015, end mid jan 2015  for s15 rollover: pietrowicz, s  11% start march 2015, finish april 2015",18,train
DM-1123,requirements and specifications for initial version of inter-Task communication,"gather requirements and specification for implementation of  a gather/scatter mechanism for a set of tasks executing in parallel using the event services, providing for eventual extension to support mpi or other communication mechanisms.  start mid jan 2015, finish feb 2015.    jk: in pmcs this would be pietrowicz s",50,train
DM-1124,Chebyshev approximation object for aperture corrections,"using the interface defined in dm740, implement a chebyshevbased implementation to be used to interpolate aperture corrections across ccdlevel interfaces.  a prototype is available on the hsc fork, which can be copied directly, modulo any interface changes on dm740.  for more information, see the hsc jira issue (which also includes the work associated with dm740): https:/hscjira.astro.princeton.edu/jira/browse/hsc796 and the hsc git commits https:/github.com/hypersuprimecam/afw/compare/releases/s14a_0...tickets/dm 796",3,train
DM-1125,avoid usage of measurement framework in star selectors,"at least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points.  with the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the sdssshape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image.",3,train
DM-1126,design new Footprint API,"this issue is for planning (not implementing) some changes to footprint's interface, including the following:   make footprint immutable   create a separate spanregion class that holds spans and provides geometric operators does not hold peaks or a ""region"" bbox (footprint would then hold one of these).   many operations currently implemented as free functions should be moved to methods   we should switch the container from vector/ to simply vector/, as span is nonpolymorphic and at last as cheap to copy as a shared_ptr.  the output of this issue will be a set of header files that define the new interface, signed off by an sat design review.  other issues will be responsible for implementing the new interface and fixing code broken by the change.",8,train
DM-1127,implement new Footprint API,"this issue is the implementation for dm1126, including fixing code broken by the api changes.  this issue should be given subtasks for discrete pieces of work as part dm1126, as it has a lot of story points.  because it's likely all of this needs to be merged at the same time, it should probably be done on a single branch, unless some of the earlier work can be done in a backwards compatible way..",20,train
DM-1128,Span-based grow operations for Footprint,the current grow operation for footprints is very inefficient for isotropic grows.  a better algorithm can be found in the attached paper.,10,train
DM-1129,Span-based topological set operations for Footprint,"implement spanbased overlap tests and spatial union, intersection, and difference operators for footprints.  should be split into subtasks; first task would be to come up with the interface, and then each operation to be implemented should have one subtask each.  if complete after dm1126 and dm 1127, these operations should be implemented in the spanregion class, and footprint should delegate to that.",20,train
DM-1130,refactor C++ Algorithm classes,"using one of the prototypes of dm 828, refactor the c algorithm code in meas_base and any other packages.",10,train
DM-1131,create PSF simulations for shapelet approximation test,"to determine the number and order of shapelet expansions needed to approximate the lsst psf, we need simulations of lsst psfs, presumably generated using phosim.  i think we want something like 50100 extremely highsnr stars, drawn from realistic distributions of anything that would affect the psf, including position on the focal plane and center position within a pixel.  i only care about getting these out as postage stamps, so i'll leave the question of how best to run phosim to get these up to someone else.  while this is part of a mostly princeton epic, i'm assigning this issue to simon as he'd be able to do it much faster than i could.  i'll also let him review and update the story points estimate.",6,train
DM-1132,create galaxy simulations for shapelet approximation and truncation tests,"using the psf simulations generated in dm1131 and the galsim package, generate ~20k galaxies for each of 3 shear values for each input psf.  will need to be divided into subtasks  figure out what the outputs should look like, write the code to generate the simulations, estimate the time need to run the simulations, run the simulations at scale, etc. ",30,train
DM-1133,implement shear measurement driver for simulations,we'll need driver code to run the galaxy fitting algorithms on the simulations from dm 1132.  efficiently saving all the samples could be challenging.,10,train
DM-1134,implement SDSS PSF residual trick,"sdss galaxy fitting approximated the convolution of a galaxy model with the psf as the convolution of the galaxy model with a simplified approximation to the psf added to the difference between the psf approximation and the true psf.  we should do the same, as it'll be no worse than ignoring the difference between the psf approximation and the true psf, and it may greatly reduce the complexity needed in the approximation.",8,train
DM-1135,test how large pixel region used in galaxy fitting needs to be,"using simulations built on dm1132 and driver code from dm1133, test different pixel region sizes and shapes, and determine at what point shear bias due to finite fit region drops below a tbd threshold.",20,train
DM-1136,test number/order of shapelet expansions needed to approximate PSF,"run driver from dm1133 on simulations from dm1132 with different configuration for shapelet psf approximation, increasing complexity until change in shear estimate drops below a tbd threshold.",14,train
DM-1137,Evaluate python/c++ documentation generation and publication tools ,"this epic related to documentation that is provided as part of normal development activities. the desire is to keep this documentation in and near the codebase as this is best practice for it being maintainable. at the other end, we wish to publish this documentation in a coherent and searchable way for users. a number of tools exist in this area and this item requires a preliminary evaluation to be made.   this is part of curating our documentation infrastructure.  [fe 75% doc 100% starting august 20th] ",20,train
DM-1138,Demonstrate & iterate with team on documentation toolchain   ,"following from dm 1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices.   this is part of curating our documentation infrastructure. ",5,train
DM-1139, Stack documentation infrastructure and migration,"[epic retitled and bumped in points to reflect wider scope and change in resource allocation]    this epic containes work on migrating the documentation infrastructure  into sphinx (from doxygen, confluence) subject to rfc, continuous deployment of documentation, readthedocs or  similar presentation, standardisation via teplates of common  information, a proposal for ciing examples and tutorials, release and install note documentation, and visual  design and javascript development for ui/ux    an additional request to migrate wordbased design documentation has been accepted. latex support is being investigated.     [js 100%]    ",63,train
DM-1140,Investigate automatic MacOS X build/deploy,this item is to set up the same continuous integration process we have on linux on a macosx test server.   jk: in pmcs this would be economou f and new hire ls3,4,train
DM-1141,Evaluate merits of alternative CI and RFC,"evaluate whether we wish to continue buildbot development or use a different (or additional) continuous intergration system.  this is part of a continuous occasional process of evaluating whether our current toolchain choices are still meeting our needs.  [fe at 75%, jh at 75%]",30,train
DM-1142,Regularise Nightly and Weekly builds ,"the intent here is to create two seperate automated deployment environments, one based on a nightly (or ad hoc) build, one one  aslower cadence (eg weekly). this will allow us to do intergration/qa runs on a bleeding or trailing edge as required.   jk: in pmcs this would be economou f and new hire ls3",6,train
DM-1143,Investigate candidates for Verification and Integration Data Sets,"the task here is to develop a data set that can be used both for continuous integration (build tests) and automatic qa (integration tests). we want to maximise the richness of the data set in terms of its usefulness, but minimise it in terms of its size. dn to co ordinate contributions.     [dn 95%  fe 5%]",40,train
DM-1145,Remove code made obsolete,nan,4,train
DM-1147,Create a top-level qserv_distrib package,"qservdistrib will be a meta package embedding qserv, qservtestdata and partition.",2,train
DM-1148,SUI: Research system framework for SUI development,"current ipac development utilizes gwt, is it the right system for lsst sui in 2022? i think this will be an on going activity for the first two years.     10% of goldina, zhang, ciardi, surace 20% of roby, rector, ly, wu 40% groom",100,train
DM-1149,W15 Metadata Store for production tracking (prototype),build a first prototype of the metadata store.   jk: refer to loading spreadsheet for pmcs assignments,45,train
DM-1150,Fast image search,a lot of users will search for images using some (often advanced) spatial criteria. need to implement something similar to what we have in udfs/scisql to efficiently support this class of search. check gis support in mariadb,10,train
DM-1151,Fix example of IsrTask to be callable with data on disk,currently the example of the isrtask takes a fake dataref.  this is hard to use with real data.  in dm1113 we will update isrtask to not take a dataref.  this will make it easy to update the example script to work with real data.  this ticket will also include removing from the unit tests any fake datarefs that have become unnecessary as a result of dm1299.  ,2,train
DM-1152,Css C++ client needs to auto-reconnect,"the zookeeper client in c that the czar uses doesn't autoreconnect. this is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c client.  the zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.).  this ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnectionis impossible"" condition is met.",2,train
DM-1153,"Minor problems in lsstsw, related to Qserv offline install procedure"," on lsstsw master branch tip, ./stack/linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups1.3.0 instead od eups1.5.0)    on fedora19, flock from utillinux 2.23.1 doesn't support next options :  $ flock w 0 200 flock: timeout cannot be zero  but  flock w 1 200  works,   newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. for example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by dm1078)   loadlsst.sh appends automatically http:/sw.lsstcorp.org/eupspkg to eupspkgroot, and if first url in eupspkgroot isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a workaround in qserv offline mode install scripts. would it be possible to define a lightly different behaviour for loadlsst.sh ?   in newinstall.sh, l. 167, if python version isn't correct, then exit with error code. ",3,train
DM-1154,SUI web user interface prototype  ,"sui team want to use this time to study the dm system and all the other design documents to come up with preliminary design of the sui infrastructure with some prototyping along the way to help the proof of concept.  40% wu 30%  rector, roby, ly 10% zhang, goldina 80% ciardi 90% surace 60% groom",100,train
DM-1155,SUI Interface through Qserv with database and prototype ,working closely with database group to define the interface needs between sui and qserv. prototyping the functions will help us understand the interface better.  50% rector 10% ly 10% wu,32,train
DM-1156,SUI:  Query and display LSST image ,define the apis for image query with database group.   depend on the implementation of apis  exercise the image cutout service dm 1977 been developed in slac.     ,6,train
DM-1157,SUI catalog query interface prototype,define the apis for image query with database group  depend on the implementation of apis  30% goldina 20% ly 10% wu,34,train
DM-1158,SUI image visualization prototype (without searching LSST images),using the current software components developed in ipac to put together a prototype of visualization capabilities. the purpose is to get feedback from dm people and potential users of the tool.  20% roby 30% zhang,34,train
DM-1159,SUI 2D plot for catalog prototype,using the current software components developed in ipac to put together a prototype of visualization capabilities. the purpose is to get feedback from dm people and potential users of the tool.    30% goldina 10% ly,38,train
DM-1160,SUI catalog and image interactive visualization with LSST data,"using the current software components developed in ipac to put together a prototype of visualization capabilities. the purpose is to exercise the data access apis developed by slac and get feedback from dm people and potential users of the tool.  20% goldina, zhang 10% roby, ly, wu, ciardi ",20,train
DM-1161,Cleanup SdssShape,"we should do a comprehensive cleanup of the sdssshapealgorithm class.  this includes removing the sdssshapeimpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  we should also ensure that the source from which the algorithm is derived is clearly cited  that's bernstein and jarvis (2002, http:/adsabs.harvard.edu/abs/2002aj....123..583b); see also dm 2304.",8,train
DM-1162,Port meas_algorithms unit tests for plugins,"while test coverage isn't complete, there are many unit tests for measurement plugin algorithms in measalgorithms that have not yet been ported to measbase.  we should make sure any of these tests that aren't already covered in measbase are moved over before we remove the measalgorithms versions of things.",6,train
DM-1164,remove dependency between sconsUtils and eups,"sconsutils currently depends on eups, (and as far as i understand it, it should not...)",4,train
DM-1167,experiment with building MyISAM as a shared library,"per monty (phone call aug 28, 2014), this should be easy, code to look at is in storage/myisam/mi_test ",2,train
DM-1168,zookeeper port numbers should be configurable,"the test programs in core/modules/css/ has hardcoded zookeeper port numbers. that needs to be fixed, it needs to be configurable.",1,train
DM-1169,User-friendly install/configure/test scripts,"  stdout, stderr output should be colorized, whereas redirecting it into a file shouldn't.",3,train
DM-1170,FY19 Add Support for ForcedSource Table in Qserv,"forcedsources will needs special handling   ra/decl columns are not part of forcedsource table, we will need them for partitioning   we might want to store them in subchunks? ",53,train
DM-1176,Make default image origin PARENT in all cases,"after dm840 is finished, and code updated for it (dm846), make the new default image origin parent, remove the undefined image origin enum, and update at least some of the code that explicitly specifies parent (there is little harm in leaving this explicit, other than it does not set an optimal example).",4,train
DM-1177,Implement sharable just-in-time subchunk management on the worker,"the current subchunk management on the worker is very simple. when a query fragment is selected for execution, it builds exactly the subchunks it needs, performs the queries, and destroys the subchunks. this presents a problem when we have concurrent queries that need the same subchunks. thus an earlier query will destroy its subchunks (some/all of which are needed by another concurrent query) and cause the later queries to fail.  the suggested approach is as follows:  each query, instead of building subchunks on its own, delegates responsibility for subchunk creation/deletion to another class (e.g. subchunkmanager, [but try to find a better name]).  each query requests a subchunks and releases subchunks using a lock()/unlock() mechanism (in the subchunkmanager api), or wrapped in an interface like boost::lockguard wraps boost::mutex where acquisition and deletion can be handled by scoping. (it might even be possible to use boost::lockguard mechanism.)   now subchunkmanager blocks calls to lock() until it has created the necessary subchunks and modified its records to note that these subchunks are needed by one additional query). a subsequent unlock() call from the client indicates that scm can decrement its counter and destroy the appropriate subchunks.  i (danielw) think the data structure to manage this is straightforward and the code for creation/deletion of subchunks already exists, but it might not be obvious how to route the plumbing so that queries can request/release appropriately, to an instance (probably only one per worker instance) owned by an object somewhere else.  please feel free to use a better or more maintainable approach. ",15,train
DM-1185,Channel was inactive for too long error,"there as an issue with the receiveevent call that can cause an exception ""channel was inactive for too long"".  the fix for this (according to the activemq users list and archives) is to either completely disable the inactivity monitor or to increase the inactivity limit to something extremely high.  the fix is adding:  ""wireformat.maxinactivityduration=0"" to the url in establishing the connection.  there might also be a way of doing this directly in the activemq broker, but so far i haven't seen anything that would let me specify that.",2,train
DM-1187,Learn the OCS middleware,"ocs will deliver the ocs middleware software in november 2014.  there will be a workshop held at slac the week of november 10th.    i spoke with k t and we should be able to attend this remotely, if necessary.  we need to spend some time getting familiar with this software and how it will integrate with the base dmcs for ap.",6,train
DM-1188,rewrite low-level shapelet evaluation code,"while trying to track down some bugs on dm641, i've grown frustrated with the difficulty of testing the deeplyburied (i.e. interfaces i want to test are private) shapelet evaluation code there.  that sort of code really belongs in the shapelet package (not meas_multifit) anyway, where i have a lot of similar code, so on this issue i'm going to move it there and refactor the existing code so it all fits together better.",2,train
DM-1192,Write a transition plan to move gitolite and Stash repositories to GitHub,"as recommended by the sat meeting on 20140916, we need this document to promote the use of github by other subsystems within the project and to understand the impacts on dm.  the plan should include, but is not limited to:  whether and how the repositories should be reorganized.  how existing commit attributions will be translated.   moving comments in stash to github",20,train
DM-1195,There is a bug in the prescan bbox for megacam.,the bounding box of the prescan region in the megacam camera should have zero y extent (i think).  instead it goes from y= 1 to y=2.  this is either a bug in the generation of the ampinfotables or in the way the bounding boxes are interpreted.,1,train
DM-1196,exampleUtils in ip_isr is wrong about read corner,"https:/dev.lsstcorp.org/cgit/lsst/dms/ip_isr.git/tree/examples/exampleutils.py#n95 says that the read corner is in assembled coordinates.  this is not true, it is in the coordinates of the raw amp.  that is, if the raw amp is in electronic coordinates (like the lsstsim images) it is always ll, but if it is pre assembled, it may be some other corner.  this should probably use the methods in camerageom.utils to do the image generation.",1,train
DM-1197,Support some mixed-type operations for Point and Extent,"the current lack of automatic conversions in python is pretty irritating, and i think it's a big enough issue for people writing scripts that we should fix it.  in particular, allow  point2d  extent2i point2d  extent2i point2d  point2i  extend2d  extent2i extend2d  extent2i  (and the respective operations in the opposite order where well defined) it would also be good to allow the all functions expecting pointd to accept pointi, but i'm not sure if swig makes this possible.  it's probably not worth providing c overloads for all of these functions (and to be consistent we should probably do all or none).  i realize that you invented these types to avoid bare 2tuples, but i'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwgeom objects.",2,train
DM-1199,Revisit log integration with Xrootd,"the integration of logging with xrootd needs to be revisited, for both the czar and the worker, after a discussion with xrootd devs about api changes. we want to accomplish 2 things:   logging from xrootd client should go through the qserv czar's logger, so it can be saved in the same file and enabled/disabled as a component.  logging from the xrootd server may also find a benefit in using our logger as a backend",20,train
DM-1201,add ColumnView support to FunctorKeys,"functorkeys currently only work with individual records, but at least some could work with columns as well.  need to add an interface for this and decide how to handle cases where it the functorkey doesn't support it.",2,train
DM-1210,Invert buffering for czar in row-based result handling,"the xrdssi api performs some impedancematching in buffering transfers. the current code (introduced in dm199) doesn't leverage this because its flow model is based on the older mysqldumpbased results passing. with dump files, we don't know the size of the fragments expected, so we are just passing buffers of bytes and building up a text blob to ingest. with the rowbased protocol, we have sizes specified, hence the receiver can specify exactly what sizes of bytes are needed. implementation of this ticket should simplify and reduce the code overall. ",10,train
DM-1211,anaconda is too outdated to work with pip,"the version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). the issue is an unsafe version of ssh.  a workaround is to issue this command while anaconda is setup:  conda update conda  warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda.  i think what is required is an obvious change to ups/eupspkg.cfg.sh  the current version of anaconda is 2.0.1 based on http:/repo.continuum.io/archive/  note: there is no component for anaconda. i will submit another ticket.",2,train
DM-1213,cleanup order/grouping of header files,"we want:  header for the class  then system  then third party  then lsst   then qserv  we currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places.",1,train
DM-1215,makeMaskedImage leaks memory,"calling afw.image.makemaskedimage leaks memory when called from python, because it returns a raw pointer without telling swig %newobject.  to fix it, it'd be better to just have it return by value instead of by pointer, though this might involve fixing some downstream c code (python code should not be affected).",1,train
DM-1216,compute linear parameter derivatives more intelligently in optimizer,"the numerical derivatives computed by the optimizer currently don't distinguish between the linear parameters (for which derivatives are trivial) and nonlinear parameters (for which they're hard), because we don't pass the information that distinguishes them to the object that computes the derivatives.  if we move the computation of derivatives from the optimizer class to the objective class, we should be able to compute the derivatives much more efficiently.  while this doesn't matter much when fitting single component galaxy models (because there's only one linear parameter in that case), it should matter quite a bit when fitting high order shapelets to psf models.",4,train
DM-1217,Refactor meas_base Python wrappers and plugin registration,"measbase currently has a single swig library (like most packages), defined within a single .i file (like some packages).  it also registers all of its plugins in a single python module, plugins.py.  instead, it should:   have two swig libraries: one for the interfaces and helper classes, and one for plugin algorithms.  most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily.  the package init_.py should import all symbols from both libraries, so the change would be transparent to the user.   have separate .i files for each algorithm or small group of algorithms.  each of these could %import the interface library file and the pure python registry code, and then register the plugins wrapped there within a %pythoncode block.  that'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins.",3,train
DM-1218,Support multiple-aperture fluxes in slots,"we should be able to use multiple aperture flux results in slots.  while this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in sourcetable and the sourceslotconfig in meas_base).  after addressing this, we should remove the old sincflux and naiveflux algorithms, as the new circularapertureflux algorithm will be able to do everything they can do.",2,train
DM-1226,Refine Butler prototype,a prototype of the new butler is part of the s14 delivery.  this needs to be refined into a production package and the rest of the code needs to be ported to use it.,15,train
DM-1227,S15 Multi-node Multi-query Integration Testing Harness,build endtoend integration test harness for qserv that will run queries on multi node system.,56,train
DM-1228,LSE-68: Bring pull interface to CCB approval,"bring a longpending set of changes, primarily the adoption of the ""pull interface"" for all dmcamera image requests, to ccb approval",6,train
DM-1229,LSE-68: Major phase-3 details (W15),"deepen isd to cover: supply of crosstalk constants, deletion policy details, content of the newdata notification, availability of a passthrough tag in data, and other phase 3 matters.  edit isd to ensure that it covers wfs and guider requirements as well.  deliverables:  markedup lse68 with combination of dm proposals for phase 3 details and, where that is not a realistic approach, specific questions for camera daq team.  bring a proposal to the ccb by the end of the cycle.",16,train
DM-1230,LSE-69: Bring Summer 2014 work to CCB approval,"bring summer 2014 work on lse69 to ccb approval, ideally by 20 october in time to be part of the cd2 document package.",6,train
DM-1232,LSE-72: Bring Summer 2014 work to CCB approval,"remaining work is to proofread the sysmlization by brian selvy of the lse72 draft, do any required cleanup in conjunction with the ocs team, and advocate for lcr 202 (already exists) at the ccb.",3,train
DM-1233,Refine requirements and use cases for Level 3 facilities,"refine the requirements and use cases for the three branches of level 3 capabilities exposed to users:   level 3 programming toolkit (user reconfiguration / extension of dm pipelines and stack)   level 3 compute cycle delivery (user access to 10% of compute base)   level 3 data product storage    deliverables:   refinement, if necessary, to level 3 requirements in dmsr    flowed down requirements as a separate document.  sufficient detail to allow a breakdown of the deliverables in the three areas of level 3 by annual release cycle through construction period.",20,train
DM-1234,LSE-72: Remaining Phase 2 details,"sort out remaining phase 2 details of lse72, especially:   details of the configuration mechanism (yet to appear even in lse70), the publication of available configuration keys, the efficient publication of configuration contents, ocsdriven configuration ""knobbing""  specific list of events to be published by dm  more specific efdquery language  efd deployment model (understanding of distributed design envisioned by ocs and its implications for the icd)",10,train
DM-1235,LSE-72: Phase 3 details,advance phase 3 details as needed to eliminate obstacles to ocs and dm development during summer 15.,10,train
DM-1236,LSE-75: Bring Summer 2014 work to CCB,"bring a small set of technical changes (e.g., pointing notice moved to ocs icd) and the addition of psf reporting to t&s into the sysml version of lse 75 and submit a change request to get this approved.",6,train
DM-1237,LSE-75: Refine WCS and PSF requirements,clarify the data format and precision requirements of the tcs (or other telescope and site components) on the reporting of wcs and psf information by dm on a perimage basis.  depends on the ability of the t&s group to engage with this subject during the winter 2015 period.  can be deferred to summer 2015 without major impacts.  current pmcs deadline for phase 3 readiness of lse75 is 29sep2015.,8,train
DM-1238,LSE-130: Bring Summer 2014 work to CCB approval,"bring the current listoriented version of lse130 into the cameratestplanoriented format requested by the camera group, moving the old list(s) into ldm272 for reference.  negotiate what further work is required to make the document acceptable to the camera group for ccb approval in time for cd2.  nominally this requires ccb approval by 20oct2014 in order to have an approved document properly released to the cd2 committee.",16,train
DM-1239,LSE-130: Make requests more quantitative,"proceed, as enabled by input from the dm apps group and the camera, to make the individual data requests in lse 130 more quantitative.",12,train
DM-1240,LSE-140: Bring Summer 2014 work to CCB approval,"convert the existing lse140 draft to sysml, produce a docgen, and review with jacques sebag.  bring to ccb meeting on 8 october under existing lcr201.",6,train
DM-1241,Complete data entry of LSE-140 revised draft into EA,nan,2,train
DM-1242,Risk Register refresh 1/2015,"periodic review of dm risk register contents.  covers preparation for a review expected at the end of january 2015, the only one during winter 2015.",3,train
DM-1244,TOWG - Contributions to the Operations Concept Document,covers contributions to the writing and editing of the operations concept document during the winter 2015 cycle.  deliverables:  timescales diagram and explanatory text  framework for nominal operations chapter  contributions to ktl's writing of the data processing operations chapter  general proofreading,10,train
DM-1245,Install scisql plugin (shared library) outside of eups stack.,scisql plugin is currently deployed in eups stack (i.e. $mysqldir/lib/plugin) during configuration step. nevertheless eups stack should be immutable during configuration step.  mysql plugin dir option may allow to deploy scisql plugin outside of eups stack (for example in qservrun_dir).,3,train
DM-1246,add per-exposure callbacks for measurement plugins,"we should give measurement plugins an opportunity to do some work whenever we start processing a new exposure.  this give them a good time to throw exceptions when there's something obviously wrong with the exposure (e.g. it's missing a psf).  it also gives them an opportunity to do work that could be used to speed up persource processing.  one specific case i have in mind is in shapelet psf approximation for galaxy fitting, where it could be very valuable to use an initial fit to an average psf to initialize (and speed up) the first to persource psfs.  one question here is how to to allow information to be passed from the perexposure method to the persource methods  we should not do that via plugin instance attributes, which means we probably want to have the perexposure method return an arbitrary object that would be passed to the persource methods.  unfortunately, i don't see a way to avoid having that change the signature for those methods for all existing plugins, so this should be done relatively early, before we have too many usercontributed plugins.",3,train
DM-1249,"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 1","transient cache for css, populated from python, no c interface to zookeeper. pack multiple keys using json. tenative packing spec is in dm705  the v1 implementation transitions the zk access from the c layer into python, with code that understands how to unpack jsonencoded data. this can coexist with the existing qservadmin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qservadmin. ",10,train
DM-1250,CSS design for query metadata v2,nan,3,train
DM-1251,CSS design for query metadata v1,the goal of this ticket (and dm1250) is to try to understand what kind of perquery metadata is necessary to provide client transparent query processing in case when czar/proxy could die or be restarted.  some relevant info is in the trac: https:/dev.lsstcorp.org/trac/wiki/db/qserv/css/runtimestate,3,train
DM-1252,Implement structure for DB/table metadata in CSS,nan,4,train
DM-1253,Requirements gathering for Metadata Store,nan,10,train
DM-1254,Metadata Store - design v1,research potential offtheshelf candidates. propose initial version of metadata design. ,5,train
DM-1255,Metadata Store - experimental prototype v1,"this first prototype will involve capturing metadata from a small set of image files.  create a new schema file in the cat package (lsstschema4mysqlw15.sql) with definition of tables that will be used to capture image metadata (see exposure tables in other schema files), and some basic structure for capturing information about the entire repository. decide which keywords are standard, and which should go to flat key value area  pick a directory (or several) with images, candidates: /lsst7/dr7/runs, /lsst7/releasew13ep, /lsst3/dc3/data/obs/imsim/pt1_2, /lsst/dc3/data/pt1.2.3k/datarel/imsim  create test database, load schema  extract metadata for all images in a given directory and load metadata from fits headers.",10,train
DM-1256,Metadata Store - design v2,nan,5,train
DM-1257,Metadata Store - experimental prototype v2 (DataCat),"integrate prototype v1 with fermi's datacat (e.g., reuse logic for reading headers using afw, store in datacat). experiment with foreign tables.",6,train
DM-1258,Update documentation and automatic install script w.r.t. Qserv 2014_09.0 release,creation of qserv_distrib and distribution of qserv via official lsst repositories have to be taken into account in qserv documentation and automatic install script.,4,train
DM-1259,evaluation of Cinder as OpenStack storage cache in the LSST Middleware,"deliverable: evaluation of cinder as openstack storage cache in the lsst middleware daues g 100%"" ",100,train
DM-1260,"implementation of data movement in the production system in the existing hierarchy of NFS disk, condo, tape","deliverable: implementation of data movement in the production system in the existing hierarchy of nfs disk, condo, tape freemon m 50% ",54,train
DM-1261,incorporation of sizing model into data center requirements,deliverable: incorporation of sizing model into data center requirements freemon m 100% ,7,train
DM-1262,data center requirements document,deliverable: data center requirements document petravick d 10% ,11,train
DM-1263,completed governance of security plan for review,deliverable: completed governance of security plan for review petravick d 10% ,1,train
DM-1264,security plan october.,deliverable: security plan ephibian 10% ,1,train
DM-1265,addition of procurement of physical goods to contract,"deliverable: addition of procurement of physical goods to contract petravick d 5%, gelman m 10% ",6,train
DM-1266,insertion of wide area simulator into test stand,deliverable: insertion of wide area simulator into test stand freemon m 100% ,18,train
DM-1267,report of traffic shaping,deliverable: report of traffic shaping freemon m 100% ,7,train
DM-1268,upgraded KVMs,deliverable: upgraded kvms mather b 50% ,9,train
DM-1269,sizing model critique report,deliverable: sizing model critique report perez a 50% ,29,train
DM-1270,more efficient VMware infrastructure,"deliverable: more efficient vmware infrastructure  w15: glick b 50%, elliott m 25%, mather b 10% 38 sp estimated  s15: mather b 40%, glick b 25% 6 sp estimated",6,train
DM-1271,technical roadmap critique report,deliverable: technical roadmap critique report perez a 50%,29,train
DM-1272,Upgrade NFS Storage Servers with new hardware,"deliverable: new hardware, zfs filesystem, 3x 100 tb  1 spare 100+ tb elliot m 75% ",55,train
DM-1273,deployment plan for version 1 of OpenStack,"deliverable: deployment plan for version 1 of openstack glick b 75%, elliot m 15%, mather b 10%, wefel p 10%",56,train
DM-1274,Procure replacement development infrastructure,"procure, install, test, and deploy hardware to replace existing lsst development cluster infrastructure.    assignees: bill glick, matt elliot, bruce mather, paul wefel, jason alt  duration: november   december 2015",45,train
DM-1275,level of effort,"deliverable: level of effort wefel p 100%, freemon m 5%",100,train
DM-1276,level of effort,deliverable: level of effort voiciu l 100%,28,train
DM-1277,level of effort,"deliverable: level of effort petravick d 50%, gelman m 50%, glick b 30%, mather b 50%",100,train
DM-1278,written plan for next period epics,"deliverable: written plan for next period epics petravick d 50%, gelman m 50%, glick b 50%",10,train
DM-1279,Fix Scisql deployment test error (doc.py),"deployment test in tools/docs.py fails due to a wrong ""scisql_index"" path in scisql documentation.  fortunately, qserv configure.py doesn't stop on this error.",2,train
DM-1280,meas_base ResultMappers should be FunctorKeys,"the resultmapper classes in meas_base should inherit from functorkey, and support bidirectional transfers involving the result structs and records.",3,train
DM-1281,add Schema method to join strings using the appropriate delimiter,"delimiters in schema field names are version dependent.  one can currently use schema[""a""][""b""].getprefix() to join fields using the appropriate delimiter, but this is confusing to read.",1,train
DM-1282,multi-level replacement in Schema aliases,schema aliases should support more than one level (i.e. an alias may resolve to another alias).,2,train
DM-1283,remove meas_extensions_multiShapelet from release packages/buildbot,nan,1,train
DM-1284,"rename meas_multifit to meas_modelfit, and add to lsst_apps",nan,1,train
DM-1285,Improve Startup of HTCondor Jobs,adjust configuration parameters of htcondor config and/or submission files to improve speed at which htcondor jobs start in both the replicator pool and worker pool.,2,train
DM-1286,Improve worker fault tolerance of missing distributor data,"instances of worker jobs might ask the archive dmcs which distributor to connect to, only to connect to that distributor which has recently been rebooted, so the distributor might not have that information.   at this point, the distributor could send an “expired” notice of some kind to the archive dmcs to clear it’s cache, and also tell the worker that it has no file of the type it’s looking for.  the worker would then go back to the archive dmcs. ",6,train
DM-1287,Propose and document a recipe to build Qserv in eups,in place build is available and documented.,2,train
DM-1291,Identify test data and camera,to test processccd we need to identify a set of data to process (possibly mocked).  this implies that there will also need to be a minimal camera to go with the minimal data.  this task is to identify the minimal data and find a location for it.,4,train
DM-1293,Implement designed tests for processCcd,implement the designed tests with the installed data.,8,train
DM-1294,Generate use case,we should first sit down with a camera team rep (jim c. maybe) to define the tool they need.  how dynamic are the data?  what is the layout of the data?  we should also get some example test data to work with.,6,train
DM-1295,Sync test data headers with standards,the headers will need to be vetted against known fits standards.  the headers should be valid and should not contain any non standard keywords.  there should also be a census of fits standards that may be used for defining sensor layout.,8,train
DM-1296,Implement a tool to generate a Camera from FITS images,implement the tool to generate the camera object.  it should be a command line tool that will take a path to a file.  it should have an option to persist the generated camera.  it should also have the ability to plot the camera.,10,train
DM-1297,Verify use of the tool with the camera team,we will need to run the tool in the camera team's system to make sure the interfaces are as they expected.  it will also help to make sure that the tool addresses all of the cases.,6,train
DM-1298,Design the API,"the api will have to be able to accommodate data we know about, so will need to deal with reasonable missing data.  it should also not preclude extension.  this will need to be rfc'd since it is an api change.",8,train
DM-1299,Implement and test the new API,"once the api is designed and signed off on, the api will need to be implemented and tested.  this will require updating all obs_  packages that use the current interface.",10,train
DM-1302,C++ code changes required for --std=c++11,"some c code requires changes for modern c compilers if it is to be compatible with c11 and the older standard. here is my list, so far (ignoring the known issue of macos having two different standard c libraries)  implicit conversion of sharedptr to bool no longer works: / in c11, sharedptr has an explicit operator bool which means that a sharedptr can't be implicitly converted to a bool. this is likely to show up in a lot of packages. so far found in:  pexpolicy  measalgorithms  warning: adding 'int' to a string does not append to the string seen in dafpersistence  warning: 'vastart' has undefined behavior with reference types seen in pexlogging trace.h and one other place. fixed by not using references (and thus copying the arguments), rather than using pointers, to avoid changing the apis. this is an old issue; but not all compilers warned about it: see /  warning: 'register' storage class specifier is deprecated seen in:  boost 1.55.0.1  eigen 3.2.0  a pexlogging .i file i silenced this warning in sconsutils because it's too much trouble to upgrade boost and eigen, and the warnings are very obtrusive  in dafpersistence:  src/dbstorageimpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqltype' outside namespace 'persistence' is a c11 extension [wc11extensions]     dafper::integertypetraits/::mysqltype = mysqltypetiny;  i have not figured out how to fix this one.  clang 6 is pickier about the order of instantiation: in afw (fixed in dm1302) many errors such as the following:  include/lsst/afw/table/flag.h:27:8: error: explicit specialization of 'lsst::afw::table::fieldbase/' after instantiation struct fieldbase/  fixed by including flag.h in key.h  clang 6 warns about ambiguous unsequenced modifications. seen in shapelet, for example:  src/hermitetransformmatrix.cc:107:59: warning: multiple unsequenced modifications to 'kn' [wunsequenced]         for (int kn=jn, koff=joff; kn /::simpleimpl' must occur in namespace '' instantiate(float);   measalgorithms produced this warning because the class member wcsptr was a reference (which was unecessary):  src/shapeletkernel.cc:188:34: warning: binding reference member 'wcsptr' to a temporary value [wdanglingfield]         interp(interp), wcsptr(wcsptr>clone())                                  ^   clang 6 warns about expressions such as if (!a == 0) because the ! is applied to ""a"", not the result of the ""a == 0"". fixed in the obvious way in several places.  measalgorithms segfaults on loading unless boost is built with c11 support. see ticket dm 1361 ",4,train
DM-1305,Tests fail in shapelet when building on OS X 10.9,"when building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:   pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/ .failed tests/testmatrixbuilder.py  .f..... ====================================================================== fail: testconvolvedcompoundmatrixbuilder (main.matrixbuildertestcase)  traceback (most recent call last):   file ""tests/testmatrixbuilder.py"", line 310, in testconvolvedcompoundmatrixbuilder     self.assertclose(numpy.dot(matrix1d, coefficients), checkvector, rtol=1e14)   file ""/users/mjuric/test/lsstsw/stack/darwinx86/utils/9.28/python/lsst/utils/tests.py"", line 328, in assertclose     testcase.assertfalse(failed, msg=""\n"".join(msg)) assertionerror: 1/50 elements differ with rtol=1e14, atol=2.22044604925e16 0.175869366369 != 0.175869366369 (diff=1.99840144433e15/0.175869366369=1.13629876856e14)   ran 7 tests in 0.323s  failed (failures=1) tests/testmultishapelet.py  ...f... ====================================================================== fail: testconvolvegaussians (main.multishapelettestcase)  traceback (most recent call last):   file ""tests/testmultishapelet.py"", line 88, in testconvolvegaussians     self.comparemultishapeletfunctions(msf3a, msf3b)   file ""/users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in comparemultishapeletfunctions     self.compareshapeletfunctions(sa, sb, rtolellipse=rtolellipse, rtolcoeff=rtolcoeff)   file ""/users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareshapeletfunctions     rtol=rtolellipse)   file ""/users/mjuric/test/lsstsw/stack/darwinx86/utils/9.28/python/lsst/utils/tests.py"", line 328, in assertclose     testcase.assertfalse(failed, msg=""\n"".join(msg)) assertionerror: 1/5 elements differ with rtol=1e14, atol=2.22044604925e16 2.44929359829e16 != 1.13310777953e15 (diff=8.881784197e16/1.13310777953e15=0.783842839795)   ran 7 tests in 0.131s  failed (failures=1)   ===============  more info on pugsley.ncsa.illinois.edu:   pugsley:lsstsw mjuric$ swvers  productname: mac os x  productversion: 10.9.5  buildversion: 13f34   pugsley:lsstsw mjuric$ clang v  apple llvm version 5.1 (clang503.0.40) (based on llvm 3.4svn)  target: x8664apple darwin13.4.0  thread model: posix  ============  the files are in /users/mjuric/test/lsstsw/build/shapelet/.",1,train
DM-1306,Pre-CCB review of LSE-140 docgen,nan,2,train
DM-1309,Edit agreed-upon changes into Word version of LSE-69,"a meeting around 9/26/2014 agreed on a set of revisions to lse69, with some language still needed from .  this action is to edit the trackedchanges word version of lse 69 containing the notes from that meeting into a final copy that can be reviewed by the camera team and used as input to editing the sysml version of the icd.",3,train
DM-1310,Create change request for LSE-69,create a change request to bring lse 69 up to date and capture the summer 2014 work.,1,train
DM-1311,Enter LSE-69 update into EA as SysML,"covers entering the contents of the lse 69 update into ea as sysml, with associated updating of diagrams, and the creation of a docgen'ed version for ccb action.",1,train
DM-1312,Proofread docgen'ed version of LSE-72,brian selvy is producing a sysml version of the lse 72 updated edited by .  the action here is to proofread the docgen of that version once it is ready.,2,train
DM-1313,Identify Conditions information in LSE-130 that is required for Alert Production,"lse69 declares that there are two categories of conditions data (telemetry) required by dm from the camera: those items that are needed for alert production (for which the ap components at the base will need a whitelist, and for which the camera has a tighter latency requirement), and those that are not (but are then presumably needed in drp or other deferred productions).  it states that the subset needed for ap should be enumerated in lse130.  the action here is to create an initial version of that list.",2,train
DM-1314,Publish Qserv S14 version on lsst distribution server,"in order to publish this version please tag qserv master tip with ""201409.0"" and then run:  ssh lsstsw@lsstdev # command below can't be runned in buildbot, as it doesn't support qservdistrib build rebuild t 201409.0 qservdistrib # bxxx is provided by previous command publish t qserv b bxxx qservdistrib publish t 201409 b bxxx qserv_distrib  ",1,train
DM-1315,advance to assigning tier-2 and tier-3 reliability levels ,accommodated ron lambert's input on networking equipment. assigned tier2 and tier3 levels to processing systems. ,4,train
DM-1316,Deploy LSST stack within OpenStack instances on ISL testbed,"deploy the lsst stack within openstack instances within the isl testbed  this could be for multiple flavors centos, ubuntu, etc, and this could be done by pulling docker images to the instances.   there will also likely be some initial debugging of starting instances within the isl platform as a new installation has been stood up sept 2014. ",1,train
DM-1317,Create Docker Image / Dockerfile for LSST Stack for ubuntu,"create an installation of the lsst stack v9_2  within a docker image for ubuntu for easing the import of lsst software into an openstack instance,  we create  the image utilizing a dockerfile to make systematic  the creation of such images. ",2,train
DM-1318,update expected results file in SDSS demo test,update the expected outputs in the sdss dm stack demo repo to match what we expect from the new meas_base framework.,1,train
DM-1319,Prepare quotes to order new VMware hardware & licenses,nan,2,train
DM-1320,Setup temporary NFS datastore for VMs,"we expect the primary datastores will be local to the vm compute node, but this nfs datastore will allow us to live migrate a handful of vms between compute nodes.  we may also use this or another datastore as place to create backup snapshots of particular vms.",2,train
DM-1321,Setup new VM compute nodes,"once the new vm compute nodes arrive, the hardware needs to be installed, esxi installed, and networked for use by vsphere.",6,train
DM-1322,Expire Workers that receive no files,"in instances where the files the worker expected to get never arrive, there should be a way of the worker to recognize this (say, a timeout after waiting for the archive dmcs for some time), and exit.",4,train
DM-1323,Configure and test new VM infrastructure,"this would be configuring to work vsphere, setting up permissions and groups, adding datastores, and testing the use of the new setup.",10,train
DM-1326,Automatic expiration of replicator jobs,"replicator jobs that receive no data specifically for the visit, raft, and exposure sequence id within a certain amount of time should self expire to prevent future jobs from running.  if this is not done, jobs will back up in the htcondor queue.",4,train
DM-1328,Order new NFS servers,we are waiting on the uiuc business offices to provide us a new account number for ordering these servers.  as soon as we get that account number we need to order the hardware from koi computing.,6,train
DM-1329,Plan new NFS mounts & data organization,this is a task of deciding the new client mounts for the 3 new nfs servers.  these should match up with the new storage hierarchy and classification /.,1,train
DM-1330,Install new NFS servers,nan,10,train
DM-1331,squash edge errors in SdssCentroid,"sdsscentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  instead, it should catch the lowlevel exception and rethrow as measurementerror, after defining a flag field for this specific failure mode.",1,train
DM-1332,address no-shape warnings in GaussianFlux,"gaussianflux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails.  however, we probably don't want to add a new flag for gaussianflux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag.  we should figure out some other way to squash this warning   how we do that may depend on whether this is addressed before or after the c redesign.  we should also consider having gaussianflux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag.  that's probably not worth doing within the current c interface, however, as it'd require some unpleasant mucking around with resultmappers.",2,train
DM-1333,resolve factor of two difference in GaussianFlux,"after changing the implementation of gaussianflux to use the shape slot rather than estimate the shape itself by rerunning the sdssshape code, perry saw a 515% difference in the fluxes (i'm not sure of the sign).  the new behavior (using the shape) is consistent with what we'd have gotten with the old code when the littleused ""fixed"" config option was enabled (not surprising, as that just looked up the sdssshape measurement by name, instead of via slots).  i suspect the difference is coming in because of the factor of two between sdssshape's ""raw"" measurements  the actual gaussianweighted moments  and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments.  the correct weight function to use for gaussianflux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't.  we need to determine which one, and if necessary, fix the new code.",2,train
DM-1334,Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse],"a new version & implementation of the isl openstack testbed is up and running. the new cloud is using icehouse, the ninth openstack release. we get started on this platform by verifying that basic instance creation is working.  we target the creation of an instance through the (horizon) gui interface,  and via the nova cli. ",1,train
DM-1335,Create instance with a Floating IP Associated through the nova CLI,"we see that in working with the horizon gui, it is fairly straightforward to give an instance a public ip address by associating a floating ip with the current local ip.    however, we will want to be able to accomplish this task both remotely and programmatically within workflow.  as a step towards this, we target the solution of this via the nova cli.",2,train
DM-1336,Review advance doc to review with the data center working group,"consult with m. freemon on the sizing model given the generic description of the computing needed tier3 and tier2 support. make a pass through the underlying data center standards doc for things  ""any competent designer should know""  for example wall plugs on separate circuits form computer circuits.  add that to the document, and call meeting of working group for review in late oct.",4,train
DM-1337,Security Plan advancement for October,"get a draft project office detail plan consistent with the level of devleopment of the master plan. (ephiphian). draft a data classification plan, refer to plan and classes of data in drat materials. obtain some central place in lsst documentation framework to hold materials being drafted (if supported by the lsst system) recieve feedback based on portion of plan submitted to lsst project office. incorporate feedback. (if minor) re plan (if major)",18,train
DM-1338,Make QSERV_RUN_DIR scripts able to detect qserv install paths using eups,"ticket related to mario email (subject: [qservl] some points/actions from the discussion today) :  making your ""qserv data"" directory independent of where qserv is installed   i think this is a big one, and largely independent of eups. you have a problem where you want to use one set of test data potentially with different qserv binaries (not at the same time, of course). i'd argue you should refactor the scripts generated by qservconfigure to either:    get all their information about various paths from a single file, for example, from etc/paths.cfg.sh. then you can easily regenerate just that file when you need to switch to a different qserv (or zookeper, or what not), or...  refactor the generated scripts to learn from the environment which binaries to run. i.e., if $qserv_dir is defined, use that qserv, etc. this will let you switch binaries by simply setuping the new one with eups.   the two are not mutually exclusive  e.g., all of this logic could be in etc/paths.cfg.sh, and depending on whether this is a development build or a ""noneups"" build, it can either pick up the paths from the environment or hardcode them.   assuming you did that, your development loop may look something like this:        # assuming that qservconfigure.py has already been run     # in ../qservrun       # do something with qserva clone     cd qserva     setup r .     ... do some edits ...     scons       # now do the tests     cd ../qservrun     ./bin/qservstart.sh     ... do tests ...     ./bin/qservstop.sh       # now switch to qservb clone     cd ../qservb     setup r .       # and do the tests again     cd ../qservrun     ./bin/qservstart.sh     ... do tests ...     ./bin/qservstop.sh    that is, as qservstart picks up the relevant products from the environment, there's no need to rebuild/reconfigure the qservrundirectory each time. ",7,train
DM-1340,Read through log4cxx documentation and log.git code,read through the log4cxx documentation and become familiar with how the log.git package is set up.,2,train
DM-1341,Write/configure tests with existing configurations and appenders,"in order to be come more familiar with how to use the log.git package, write some tests to see how existing configurations and appenders are used by the log.git package.",4,train
DM-1342,Write DM message appender class,write dm message appender class to be used with log.git package.   this might entail writing a configurator class as well;  that depends on the investigation of how configurations/appenders are used.,16,train
DM-1343,Write Unit test for new DM message appender class,"write unit tests for dm message appender class.  this might also require some tests for a configurator class, if that class is created.",2,train
DM-1344,Define gather/scatter mechanism for tasks with generic API,"define gather/scatter mechanism for tasks with generic api.  this will use event services (dm messages) initially, but we would like to support mpi or other communication mechanisms as well.  this will involve consulting with paul about how the existing code is structured. ",8,train
DM-1345,OCS Middleware workshop,attend ocs middleware workshop.  this will probably have be done remotely because i have a personal conflict with that time that will prevent me from attending in person.,4,train
DM-1346,Write example programs for OCS Middleware,"write some example programs to get familiar with the ocs middleware software. the ocs middleware will later be integrated with the ap, in the base dmcs and replicator jobs.",4,train
DM-1347,Refine Event base class to allow ActiveMQ filterable settings,"the current event.cc base class needs to be refined to remove and old style data release terms that aren't used anymore.  plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side.",2,train
DM-1348,Update tests to use unit test framework,the tests for this package predate the unit test framework that other package use.  update the tests to uses the unit test framework and get rid of any duplicate  or obsolete tests.,5,train
DM-1349,Change marshaling code to use json,"the marshaling code for nonstandard (i.e., nonfilterable) components of messages is custom and not standard.  change this to use json.",10,train
DM-1350,General cleanup of Events package,"there are some obsolete classes and code in the ctrl_events package that needs to be removed and/or updated.  pipelinelogevent, for example.   that not only is no longer used, but it is applications specific, and should have been part of another package in the first place, subclassed from this package.",35,train
DM-1352,ORIGINATORID value can churn too quickly.,"the originatorid is a 64bit word consisting of an ipv4 host address, 16bit process id, and 16bit local value.   in addition to the 16bit process id not being standard across platforms (mac os >leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  this could be fixed to changing originatorid to a 32 bit process id and a separate value for the local value, which would be specified together in the dm event selector.   i have to look into this more to see if this is a viable solution.  this might need to go to three separate values to future proof it (i.e., ipv6).",8,train
DM-1354,"Install docker 1.1.2 in an ISL OpenStack CentOS  instance, perform basic checks","install docker 1.1.2 in an isl openstack centos 6.5 instance, and perform  basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images,  pulling  standard images from docker hub, starting containers from these images, etc. ",2,train
DM-1355,Re-arrange how Qserv directories are installed,"we already touched question about installation directory structure at a meeting, maybe we can improve things by rearranging how things are installed      we are currently installing stuff into four directories: cfg, bin, lib, proxy     to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more rootinstall friendly in case we ever want to install under /usr)     this change (if you want to do it) deserves separate ticket, do not do it in this ticket ",3,train
DM-1358,End-to-end demo  fails to exit with the correct status  when Warning would be correct.,"the output  of demo2012 results in an output file which is compared against a benchmarked file. currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic"";  that  number is currently set to 11.  an example using that setting is: @ absolute error = 5.9973406400e1, relative error = 9.1865836000e4 ##2566    #:25  / 29.7478269835  additionally, the current use returns: a 'fatal' error if the code itself fails to execute correctly; a ""warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'success' if the comparison meets all criteria.  it is noted that the buildbot 'warning' color indicator is not currently being displayed when the comparison fails. that is a coding error.  this issue will:   ensure that buildbot_warning(s) causes the correct display color when appropriate.   this ticket has been split into two parts. this is part 1.  part 2 is dm 1379 ",2,train
DM-1359,improvements to PsfFlux,"while using it as an example for the redesign of dm829, i came up with some ideas for how to improve psfflux's handling of edge/bad pixels:   add a flag indicating that at least one pixel was rejected   add thresholds (number? fraction of psf model size?) for how many nonrejected, non edge pixels we need to even attempt a fit.",1,train
DM-1360,Fix minor loose ends from new result plumbing,"some of the more minor issues raised in comments from dm 199 were left undone prior to merging. this ticket addresses them.  for more information, please see:  https:/github.com/lsst/qserv/pull/2 . ",1,train
DM-1362,Edit pull interface and other Summer 2014 work into LSE-68 in Word,"deliverable: circulate a wordbased draft of lse68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other summer 2014 work.  note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted.",2,train
DM-1363,Avoid use of ~/.my.cnf (used by css watcher),"see https:/jira.lsstcorp.org/browse/dm1258?focusedcommentid=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment29230.  /.my.cnf is used by css watcher, an optional tool used for monitoring css. it is a symlink to $qservrundir/etc/myclient.cnf.  the css watcher could use mysql credentials located in /.lsst/qserv.conf (used by integration tests wich are a qserv/mysql client)",2,train
DM-1364,"replace ""bad data"" flag in SdssCentroid","sdsscentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong.  this should be replaced with one or more flags that provide more information.",3,train
DM-1372,Errors in testHealpixSkyMap.py,"there is a failing unit test when healpy is supplied.  the problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist.  if i replace boundary with boundaries, the test passes.",1,train
DM-1375,Create Docker Image / Dockerfile for LSST Stack for CentOS6.5,make a dockerfile for systematic generation of docker images using a centos6.5 base image  containing the lsst stack (v9_2 at the moment) and library dependencies.,2,train
DM-1376,Ensure that the partition package is C++11 clean and compiles on OSX 10.9,"the lsst buildbot infrastructure recently changed to building everything with std=c0x, which broke the partition package, and hence automated qserv builds. while debugging this, i discovered that the partition package does not build on osx 10.9, and considering how minimal its dependencies are, it really should. the osx issue can be fixed by avoiding using boost::makeshared.  the partition package should be cleaned up to avoid all use of using. if we decide to use c11 in qserv, then the codebase should also be modernized (in particular, there are use cases for staticassert, nullptr, etc... ). ",2,train
DM-1377,"Develop expanded and updated Information Security Program from NSF guidance/templates, using existing documents (e.g. LSE-99) as starting point ","develop expanded and updated information security program from nsf guidance/templates, using existing documents (e.g. lse 99) as starting point.  create master document and subordinate documents, allowing for existing aura, site, and institutional polices to be incorporated as appropriate.  jk: in pmcs this is petravick d and ephibian (leclair l)",56,train
DM-1383,Investigate deblending in one band followed by using the resulting templates in all bands,"(see also  https:/hscjira.astro.princeton.edu/jira/browse/hsc1025)  one poorman's approach to deblending multiple bands and visits is  deblend in one band (or a combination, e.g. chi^2 band) using an sdsslike algorithm that produces templates for each child  take the templates (or possible model fits to those templates) and use them to deblend the objects in all other bands  it will probably be necessary to include undetected objects in this fit (cf. https:/hscjira.astro.princeton.edu/jira/browse/hsc1023), using point source models.  note that this does not allow for the different seeing in each band.  this is not obviously catastrophic for reasonably wide blends as the total flux in each pixel is conserved, and if only one template is important near an object's centre the exact form of the template is unimportant.",40,train
DM-1384,Investigate deblending in one band followed by linear fits of models,"(cf. https:/hscjira.astro.princeton.edu/jira/browse/hsc1024)  one poorman's approach to deblending multiple bands and visits is   deblend in one band (or a combination, e.g. chi^2 band)   fit models in that band   in all bands separately fit those models simultaneously to all object in the blend, allowing only a minimum of coefficients to float (ideally only amplitudes, but given the realities of real galaxies the bulge and disk will need to both be fit, or the sloan swindle components, or the lensfit swindle, or ...).  the correct psf for each image would be used.  it will probably be necessary to include undetected objects in this fit (cf. https:/hscjira.astro.princeton.edu/jira/browse/hsc1023), using pointsource models.  it is not clear how much of a poorman's solution this actually is, or whether it will work quite well.",40,train
DM-1386,Merge Footprints from different bands/epochs,"the current concept of the deblender assumes that the inputs are   a merged set of footprints that define which pixels are part of the blend   a merged set of peaks within that merged footprint  please generate these merged footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system). ",5,train
DM-1387,Generate a master list of Objects given detections in multiple bands/epochs,"once we've detected sources in multiple bands we need to merge the positions to generate objects.  this is a little complicated (or at least messy):   the positions have errors   if the seeing is different in different visits, objects may be blended in some but not all exposures   if we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similarbutdifferent consequences (but we should probably deal with in the perband processing)    objects move, so even if the positions are within the errors the motion may still be detectable ",5,train
DM-1388,Submit LCR for LSE-68,"create an lcr, including a summary of changes, for lse 68.",2,train
DM-1389,Edit LSE-68 changes into EA,nan,2,train
DM-1390,Improve output from integration tests,"currently the integration tests print pages and pages of output, it is hard to see what failed and how. it'd be nice to clean that out, and instead, print some short summary, perhaps in a form of a summary table what queries has run, what succeeded, what failed etc. html format might be a reasonable way to display it.",6,train
DM-1394,Eups 1.5.4 requires each new shell to source the eups setups.sh,"eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh.  this requires the buildbot scripts: runmanifestdemo.sh, create_xlinkdocs.sh, be updated to individually  do that task.  add  demo2012: bin/demo.sh . ",2,train
DM-1395,"Reimplement CSS using JSON-packing, czar kazoo + c++ snapshotting phase 2","the v1 implementation (dm1249) transitions the zk access from the c layer into python, with code that understands how to unpack jsonencoded data. this can coexist with the existing qservadmin zk schema. v2 applies packing/unpacking logic in the creation/manipulation code in qservadmin. v2 scope will be defined further, once dm 1249 enters review.  update: phase 2 includes removal of c zk code and unification of qserv css modules into a single place.",10,train
DM-1396,Design CSS schema to support database deletion,"need to implement deleting databases. deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. it should be possible to ""create database x"" at any time later.",2,train
DM-1400,Improve documentation of pixel systems in obs_lsstSim,"there is not much documentation of the coordinate systems in use by camerageom.  this is by nature documentation that is instrument specific, so should go in the obs_ package for each instrument.",1,train
DM-1404,"Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS","building on issues dm1317 and dm1375 where initial images and dockerfile's were constructed, we can now use these dockerfile's as prototypes to extend the set of dockerfiles & images.  we observe that by making  simple (scriptable) edits to the initial dockerfile, we can run 'docker build' to make docker images for several combinations of os and base compiler gcc version.",2,train
DM-1405,Prepare a SL6x openstack image for with Qserv development environment,qserv packaging procedure requires to often rebuild qserv and relaunch integration tests.  in2p3 openstack platform offer next virtual machines :   [fjammes@ccage030 ~]$ nova flavor list   name               disk  swap  rxtxfactor   m1.tiny            0           1.0           cc.windows.small   20          1.0           cc.windows.xlarge  50          1.0           m1.small           10          1.0           m1.medium          10          1.0           m1.large           10          1.0           m1.xlarge          10          1.0           cc.lsst.medium     20          1.0           cc.lsst.large      20          1.0           cc.lsst.xlarge     20          1.0           cc.lsst.xlarge would allow a quick build/test of new qserv release.,5,train
DM-1407,Add return code for integration tests,"integration test have to returns non zero when failing. this will ease use of ci or debugging tools (git bisect, buildbot).",1,train
DM-1411,Stand up netem server with Dell PE 1950,nan,4,train
DM-1424,Create persistent volume of Cinder block storage and attach to instance,"we create a persistent volume of cinder block storage and attach to working instance.  when it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  we want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. ",1,train
DM-1426,Document how to switch Qserv or dependency in eups,"dm1338 may allow to switch qserv or dependency version using eups, without having to reconfigure qserv (i.e. without any change in qservrundir).  there may have conditions for this to work. that's why it's reasonable to get some user feedback on dm 1338 before documenting this feature.",2,train
DM-1428,Add return codes for qserv-check-integration.py and qserv-testunit.py,nan,1,train
DM-1429,Improve Qserv Configuration Procedure,"based on input from review of dm1338, and discussions at hangout https:/confluence.lsstcorp.org/display/dm/lsstdatabasehangout+20141029: implement keepdata option.   keepdata will preserve qservmeta.conf and my.cnf. regarding the latter, this is necessary because user might have customized datadir.  the structure of qserv_meta.conf and my.cnf must remain the same between existing version of qserv and the tobeconfigured version of qserv. it is users's responsibility to ensure the structure did not change. note in particular, the template files (with the exception of my.cnf) should not be customized",7,train
DM-1431,Process sample sdss data with LSST Stack in a Docker container in OpenStack,"process sample sdss data, starting with the lsstdmstack_demo, with lsst stack in a docker container in an openstack instance. ",2,train
DM-1432,SUI define local hardware needs to host the test DB and application servers,"we need have qserv and the test db in ipac to do development and test locally, to access the  data through qserv apis.",10,train
DM-1433,"SUI install Qserv and test DB, enable access to catalogs for SUI team members",install qserv and test db in local hosts.  enable sui team members to access the catalogs. ,6,train
DM-1434,State diagram for jobs in progress,build a state diagram showing job progress throughout a run.,13,train
DM-1439,afw tests use the same name for a dummy file: test.fits,"the afw package  uses a file named: test.fits in multiple testers.  if the user sets up the build to use multiple cpu ( j #), then there is the risk that the shared filename will be affected by more than one tester at a time.   in the case which provoked this issue, the tester: testsimpletable.py, reported that the file was missing.  a simple rerun managed to get past the error.  i recommend that the different testers use uniquely named demo files.",1,train
DM-1440,Github Transition: Naming conventions for repositories,"the simulations team has requested that repos in general and the numerous dm repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_ )  this would be an evident useability aid to dm developers and outside contributors too.   obtain a decision on how to allow users to quickly isolate repositories they are interested in. ",3,train
DM-1441,Github Transition: Storage of large data files,github currently has fixed 100mb per blob or 1gb per repo limits. sims has at least one file in its repo whose history exceeds this (sims_meas) and this has been raised as an issue before.  obtain a decision on a suitable way forward for the time being that would allow the upload of the repositories on github. experienced to be reviewed in a few months time to consider whether has proved satsifactory.,4,train
DM-1442,"Github Transition: Stash-stored pull requests, extraction",extract comments from atlassian stash made during pull requests /code reviews and their associated sha1s if it all possible.   ,4,train
DM-1443,"Github Transition: pull request discussion, retention - proof of concept","store review comments / pr discussion into relevant git repositories  code & process to do this on a continuous basis post transition will be a different issue, ",1,train
DM-1444,Test absence of individual components,"test absence of individual components of the ap simulator.  bring each down, run the system, and restart just those components to see if the system still operates as expected",2,train
DM-1445,check if Qserv compiles with C++11,nan,1,train
DM-1446,Fine tune czar and worker database initialization,"for now, qserv databases are the same on the master and on the worker. it means that czar tables are created on the worker and viceversa.  this ticket aims at creating the right tables at the right places, and also at sharpening permissions on these tables.  this is done by splitting configuration script qservczar.sh, which configure a mononode instance, in two scripts:  qservczar.sh, for configuring the czar  qservworker.sh, for configuring workers.  see https:/confluence.lsstcorp.org/display/dm/lsstdatabasehangout+20141029  please note that this code can't be validated with mononode integration tests (whereas it doesn't break them): indeed it requires a mononode installation with 2 mysql instance, one for the worker and one for the master. updating the mononode configuration and integration tests with such a feature would make them far more complex.  a quick and dirty, hardcoded, testbed is available in u/fjammes/dm1446 test. it has been used successfully to test this ticket.  this ticket will also be validated during next qserv install on in2p3 clusters.",4,train
DM-1447,Improve spatial-selection flexibility by parsing ptInSphBox-like syntax instead of qserv_areaspec_box,"note: it is not clear that we should do this.  this is an idea to change the syntax for spatial area selection. currently, we have select...from...where qservareaspecbox(...) ...  this forces the area selection to apply the box (or appropriate) cut on the tables referenced in the from list that are partitioned, with those columns. it would be nice to allow spatial selection based on tables (and columns?) explicitly specified by the user, and then compute the appropriate chunk coverage and where clause conditions and predicates. this is a pretty big deal, and would affect all spatial select user queries, though the compatibility can be mitigated by supporting both syntaxes.",100,train
DM-1448,Move code for mock images into afw so it reusable.,"there is some code in the exampleutils in $ipisrdir/examples that could be of wider use.  specifically there is code to generate mock darks, flats, and raw data from a mock camera.  there is also code to generate a mock dataref.  it could be used more widely if moved someplace else.  russell suggested afw.camerageom.utils.",1,train
DM-1449,newinstall.sh should check that python2 is available,the standards now suggest using:  #!/usr/bin/env python2  instead of  #!/usr/bin/env python  this doesn't work with (at least) mac osx 10.9 system python.  newinstall.sh should check if this works and suggest a fix (creating a symlink).,1,train
DM-1452,Detect lua version in admin/templates/configuration/etc/init.d/mysql-proxy,nan,1,train
DM-1453,Add data versions to Zookeeper,"track versions of data inside zookeeper, and detect from qserv code if qserv code is compatible with given format of data.",2,train
DM-1454,LOE - Week ending 10/31/14,nan,8,train
DM-1455,LOE - Week ending 10/24/14,nan,6,train
DM-1456,finish porting meas_algorithms unit tests,nan,4,train
DM-1461,C++ Redesign -- Result definition for custom algorithms,additions to jim's redesign to make it easier to define custom results.,3,train
DM-1462,Add NaN check to PixelFlags,the test of pixelflags in measuresources.py (from measalg) requires a check to be sure that the center inputs are not nan.,1,train
DM-1463,SdssShape shiftMax config item is being ignored,the code we ported from meas_algorithms sets the maxshift to 2 without regard to the config item which is supposed to set that value.,1,train
DM-1464,Design Review prep for C++ redesign,write up the design developed on dm 829 and push it through review.,1,train
DM-1469,Github Transition Plan: Reverse mirror for beta-tester repositories,"test the reverse mirror for betatesters.  straw man:  1. break the mirror for anybody betatesting github workflow 2. mirror back to new gitolite area: ""mirror"")  method:  https:/help.github.com/articles/duplicatingarepository/  ",1,train
DM-1471,Rewrite obs_test,"i want to use obstest to test processccd, but it contains no imaging data. i will replace the current data with new imaging data (a portion of one lsstsim ccd), add a camera and rewrite the mapper (subclassing cameramapper).  this also requires rewriting the unit tests in obstest and in one other package that uses obs_test.",10,train
DM-1473,Execute multi-platform lsst_dm_stack_demo test with Fig orchestration,"in dm1431 we demonstrated processing of sample data within docker containers within an openstack instance.  data was processed for containers based on centos6.5, ubuntu 13.10, ubuntu 14.10.  we consider a multiplatform ""testing"" scenario, where we process the sample data in numerous containers based on various platforms/oss all simultaneously on an openstack instance.   this scenario entails starting up and managing multiple docker containers.  fig (http:/ is a new tool for starting up and managing services via docker containers.   it is a common use case with docker to have individual components of an application each run separately in a container (resulting in numerous  containers running on a node or cloud env, i.e., a 'docker stack'), with the containers having linkages/dependencies to be managed. fig may be used to encode the relationship between the containers in the docker stack, and to start up such a set of containers.  we install and examine fig, and apply fig to our multi platform ""testing"" scenario.   ",4,train
DM-1475,Fix 2014_09 documentation,replace newinstallurl=http:/sw.lsstcorp.org/pkgs/ with: newinstallurl=http:/sw.lsstcorp.org/eupspkg/  and eups distrib install qservdistrib 201410.0 with:  eups distrib install qservdistrib  t 201410,1,train
DM-1476,Secure MySQL root password in configuration templates,"mysql password in written in multiple file during configuration procedure. one single file (qservrundir/tmp/my.cnf) should be used, and removed at the end of configuration procedure. qservmeta.conf also contains mysql password and should be also secured (move password to qservconfigure.py cmd line?).",4,train
DM-1477,Study fig to manage Qserv cluster,http:/  coordinate with @gregdaues who is also investigating fig.,5,train
DM-1480,Buildbot master takes exception when exiting from mail notifier after dynamic email sent.,buildbot master exits without posting the required staticallyaddressed email notification if a dynamicallyaddressed was sent.   this fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead letter box (which it is).,3,train
DM-1481,Discover/learn what others are doing in astronomy software,"attend the annual  adass conference to keep up with the software development in the astronomy community.  trey roby, tatiana goldina, xiuqin wu plan to attend the adass 24.",20,train
DM-1482,SUI: study other plot packages,firefly uses clientsidegchart at https:/code.google.com/p/clientsidegchart/ for xy plot. this package has not been updated since 2010. we need to find out more about other plotting packages and consider if we need to switch. the candidates are:  1. flot plotting library for jquery. http:/ 2. highcharts js at http:/,10,train
DM-1488,Finalize the Design of Query Metadata,finetune the design outlined in dm1251 (metadata for capturing information about running queries in qserv.).,6,train
DM-1489,Modify czar to interact with query metadata,"modify czar code: it should interact with the query metadata (store information for long running queries, retrieving).",6,train
DM-1490,FY18 Design and Implement Query Cost Estimate,"design and implement system that will estimate query cost. in particular, we will need to know if the query is interactive, or should be scheduled on shared scan, which shared scan etc. estimating cost will likely involve looking at number of chunks involved, number of joins, and complexity of math operations. consider extending api and allow users to specify a hint. ",79,train
DM-1493,Add queries on partitioned table for Qserv integration test dataset #3,"qserv test dataset #3 runs only queries on nonpartitioned tables. adding queries on partitioned table would increase drastically the coverage of these tests.  please note that a longterm solution could be to launch all the  .fixme queries and to have a more detailed report. for example define queries which must pass for the integration test to succeed, and test queries which may pass and log their results in a report.",4,train
DM-1495,Allow newinstall.sh to run in batch mode without installing Anaconda,"installing qserv on a cluster without internet access requires newinstall.sh to run in batch mode without installing anaconda (whose install requires internet access). so anaconda should have a anaconda=yesno option, against full batch option answering ""yes"" everywhere could also be studied.",2,train
DM-1497,Package Qserv mono-node instance in Docker,learn docker basics and then package a qserv mono node instance.,5,train
DM-1498,Package Qserv master and worker instance in Docker,learn docker basics and then package a qserv mono node instance.,5,train
DM-1505,confusing error message when enabling unregistered items in RegistryField,"pexconfig seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist:    file ""/home/lam3/tigress/lsst/obssubaru/config/processccd.py"", line 51, in /     root.measurement.algorithms.names |= [""jacobian"", ""focalplane""]   file ""/tigress/hsc/lsst/lsstsw/anaconda/lib/python2.7/abcoll.py"", line 330, in ior     self.add(value)   file ""/tigress/hsc/lsst/lsstsw/stack/linux64/pexconfig/9.0+26/python/lsst/pex/config/configchoicefield.py"", line 72, in add     r = self.getitem(value, at=at) attributeerror: 'selectionset' object has no attribute 'getitem' ",1,train
DM-1506,Support new version of newinstall.sh,newinstall.sh now creates loadlsst.bash instead of loadlsst.sh. this has to be taken in account in qserv automated install script: qserv install.sh and in qserv documentation.,1,train
DM-1508,Reverse image slicing doesn't work as expected,"the lsst.afw.image imagelike classes support slicing, but reverse slicing does not work as expected. here are some examples:  from lsst.afw.image import imagef im = imagef(10, 10) im[:,:].getdimensions() # is (10,10) as expected im[0:10, 0:10].getdimensions() # is (10,10) as expected im[::1, ::1] # should be the data with x and y reversed, but fails with: box2i(point2i(2,2),extent2i(12,12)) doesn't fit in image 10x10 im[9:1:1, :].getbbox() # starts at 0,0, as expected, but is 10x10 instead of 9x10 ",1,train
DM-1509,Remove unnecessary config/install steps,"some existing installation configuration steps are not needed any more. for instance, we don't need $run/q or $run/result any more these paths are handled directly by qserv code, so the filesystem lookup shouldn't ever take place.  completion of this ticket should remove unnecessary steps and concepts from installation, simplifying future maintenance. ",2,train
DM-1510,C++ Standards Rule 3-9 needs rewrite,"jim bosch reviewed section 3 of the c standard.  he noted this change which i felt warranted sat clarification:  39: this is a confusing conflation of two entirely different concepts: it seems to discourage all global variables, regardless of whether they're in a namespace, but only to discourage free functions when they aren't in namespace. if that reading is correct, i think both are sensible recommendations, but they need to be clarified, and probably split up. if the reading should be that all free functions are discouraged, that'd be a terrible rule we violate all over the place. if the reading should be that global variables are only discouraged when not in a namespace, that needs to be clarified (and imo it could be bumped up to a complete prohibition).  refer to: https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908685 rule 3.9.   the current rule states: 39. global variables and functions should be avoided and if used must always be referred to using the '::' operator.  ::mainwindow.open(), ::applicationcontext.getname(), ::erf(1.0) _ in general, the use of global variables should be avoided. consider using singleton objects instead. only use where required (i.e. reusing a framework that requires it.) see rule 57 ( .https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908706#cstatements57 ).  global functions in the root namespace that are defined by standard libraries can often be avoided by using the c versions of the include files (e.g. ""#include /"" instead of ""#include /""). since the c include files place functions in the std namespace, ""using namespace std;"", which is permitted by rule 541, will allow these functions to be called without using the '::' operator. in cases where functions are only available in the c include files, the '::' operator must be used to call them. this requirement is intended to highlight that these functions are in the root namespace and are different from class methods or other namespaced free functions. ",1,train
DM-1514,calling extend with a SchemaMapper should support positional arguments,"calling catalog.extend(other, mapper) isn't equivalent to catalog.extend(other, mapper=mapper) because the second argument is the boolean deep.  when a schemamapper is passed as the second argument, we should recognize it for what it is.",1,train
DM-1515,sconsUtils fails to identify Ubuntu's gcc,"on ubuntu 12.04, gcc version says:  gcc (ubuntu/linaro 4.6.31ubuntu5) 4.6.3 copyright (c) 2011 free software foundation, inc. this is free software; see the source for copying conditions.  there is no warranty; not even for merchantability or fitness for a particular purpose.   this apparently isn't quite what sconsutils expected, because it says:  scons: reading sconscript files ... checking who built the cc compiler...(cached) error: no result cc is unknown version unknown   happily, everything seems to work anyway, as the fallback options for the unknown compiler work fine with this one.",1,train
DM-1516,Use eups swig instead of system swig during Qserv build,"qserv build system use system swig, it should be fixed to use eups swig. ",3,train
DM-1518,"Add support for ""SET @@session.autocommit""","the sui team is using jdbc connector and queries like  ""set @@session.autocommit = "".format(switch)  are derailing everything. we should add support for these queries. ",2,train
DM-1519,"check out FIrefly package, build and run an applicaiton","get familiar with firefly package, build and run an application.  understand the coordinate grid overlay on an image. ",10,train
DM-1520,Fix confusing error message,"selecting from a table that does not exist, e.g. something like:  select count( ) from whdfd  produces a strange error:  error 4110 (proxy): qserv error: 'unknown error setting querysession' ",1,train
DM-1521,"Fix ""stripes and substripes must be natural numbers"" bug","on user side:   error 4110 (proxy) at line 1: qserv error: unexpected error: (/, configerror(), /)    in czar log:   1114 22:30:10.155 [0x7fafcefab700] error root (app.py:370)  unexpected error: (/, configerror(), /) traceback (most recent call last):   file ""/usr/local/home/becla/qservdev/linux64/qserv/201409.0/lib/python/lsst/qserv/czar/app.py"", line 360, in init     self.prepareforexec()   file ""/usr/local/home/becla/qservdev/linux64/qserv/201409.0/lib/python/lsst/qserv/czar/app.py"", line 437, in prepareforexec     self.addchunks()   file ""/usr/local/home/becla/qservdev/linux64/qserv/201409.0/lib/python/lsst/qserv/czar/app.py"", line 535, in addchunks     self.computeconstraintsashints()   file ""/usr/local/home/becla/qservdev/linux64/qserv/201409.0/lib/python/lsst/qserv/czar/app.py"", line 529, in computeconstraintsashints     self.pmap = self.makepmap(self.dominantdb, self.dbstriping)   file ""/usr/local/home/becla/qservdev/linux64/qserv/201409.0/lib/python/lsst/qserv/czar/app.py"", line 478, in _makepmap     raise lsst.qserv.czar.config.configerror(msg) configerror: ""partitioner's stripes and substripes must be natural numbers.""    to reproduce:   qservcheckintegration.py caseno 01 load   then  mysql port 4040 lsst  e ""select count( ) from object""    ",2,train
DM-1524,Switch readMetadata' return value from PropertySet to PropertyList,"it'd be useful if readmetadata would return propertylist instead of propertyset, because in many situations order matters. for details, see discussion in dm 1517",2,train
DM-1525,Investigate halos around stars,"andrew becker reports on dmusers:  we have been using the lsst stack to reduce cfht data at uw, and have come across a potential bug in the dm software.  i don't see how this could be something intrinsic to the data, instead it seems like it could be a bug in the software triggered by its application to new data.  i have a how to reproduce at ncsa at /lsst/home/becker/cfht.  follow commands in the setup file to get an uptodate version of the stack (b449) and a modified version of ipisr (a hack to avoid overscan correction) and a private branch of obscfht (u/krughoff/ossossupport).  if you run the commands in sourceme it will run the processccd on a single ccd, in 2 modes, yielding 2 output directories.  the second call merely sets the bin sizes to be the same amongst all the various background subtractions, but is sufficient to trigger the bug.  a basic script included in the directory differences the 2 output calexps (a straight =) and ds9 is used to view the diff.  if you pan to e.g. 1845,546 you'll see a feature like attached in the difference (out1 on the left, out2 in the middle, diff on the right).  this can be seen in various other parts of the image, as irregular annuli around bright blended objects.  it suggests to me that during measurement some substamp manipulations are leaking into the original image.  but you can kind of see in the center image that the second run seems to trigger the issue.   and then:  so to summarize:     export lsstsw=lsstsw    source lsstsw/bin/setup.sh    setup lsstapps t b449    setup k r /nfs/lsst/home/becker/lsst/dms/obscfht    setup k r /nfs/lsst/home/becker/lsst/dms/ip_isr    setup k r /nfs/lsst/home/becker/lsst/dms/processfile    cd /nfs/lsst/home/becker/cfht   will set up your environment.  the following is sufficient to recreate the bug, and create postisrccd images for processfile:     processccd.py input/ id visit=1612606 ccd=8 output out1/ config isr.doassembledetrends=false isr.fringeafterflat=false isr.fringe.pedestal=false isr.dobias=false isr.doflat=false isr.dofringe=false isr.dowrite=true calibrate.dophotocal=false calibrate.doastrometry=false dodeblend=true     processccd.py input/ id visit=1612606 ccd=8 output out2/ config isr.doassembledetrends=false isr.fringeafterflat=false isr.fringe.pedestal=false isr.dobias=false isr.doflat=false isr.dofringe=false isr.dowrite=true calibrate.dophotocal=false calibrate.doastrometry=false dodeblend=true detection.background.binsize=512 calibrate.detection.background.binsize=512 calibrate.background.binsize=512   following are the calls i made to processfile.py (using my modified version of the processfile.git package), and which return images whose difference is exactly 0.  so i don't really get whats happening with processfile, but it doesn't seem to be accepting my commandline config changes.     processfile.py out1/postisrccd/13ap06/e00/20130311/r/postisrccd161260608.fits outputcalexp calexp1.fits c calibrate.dophotocal=false calibrate.doastrometry=false dodeblend=true  calibrate.repair.docosmicray=false     processfile.py out2/postisrccd/13ap06/e00/20130311/r/postisrccd161260608.fits outputcalexp calexp2.fits c calibrate.dophotocal=false calibrate.doastrometry=false dodeblend=true  calibrate.repair.docosmicray=false  detection.background.binsize=512 calibrate.detection.background.binsize=512 calibrate.background.binsize=512     processfile.py out2/postisrccd/13ap06/e00/20130311/r/postisrccd161260608.fits outputcalexp calexp3.fits  c calibrate.dophotocal=false calibrate.doastrometry=false dodeblend=true  calibrate.repair.docosmicray=false  detection.background.binsize=1024 calibrate.detection.background.binsize=1024 calibrate.background.binsize=1024 ",2,train
DM-1526,Update processFile to use new measurement framework,"processfile.py uses the old measurement framework in measalgorithms, but all the other components expect it to be using the new measurement framework in measbase.",4,train
DM-1527,Draft security risks into the Center's template,"the cyber security center has provided a risk template consistent with their templates,  the center attempted to populate their templates with material from lse 99  to the templates,   the impedance mis match was too large, the way forward is seem as  attempting a high level decomposition of risk into the templates.",4,train
DM-1528,Identify potential KVM hardware,identify potential kvm hardware that would meet our needs.  e.g. a current version of the avocent or dell kvms used at ncsa.,1,train
DM-1529,Reorganize docker image repositories and align with github,"a heterogeneous collection of docker images have been accumulating within the public docker repository  daues/lsstdistrib . such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.)     thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective.  we also make  github repositories of matching names to hold the dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.) ",2,train
DM-1530,Test if IPMI can replace KVM,can we use ipmi in place of a kvm?  can we reliably do the following across our various server vendors?   power cycle hung systems   view and use text console   view and use remote gui console   boot from a remote iso,3,train
DM-1531,Create a LSST base CentOS image,nan,2,train
DM-1532,Document use of LSST image for CentOS,nan,1,train
DM-1533,Gather Open Stack needs/requirements from DM team,nan,9,train
DM-1534,Assist DM team in accessing test open stacks,nan,1,train
DM-1538,Fix qserv_testdata documentation,"qserv_testdata relies on sconsutils, and its build procedure has to be clearly documented.",1,train
DM-1539,Add support for mysql JDBC driver,"sui which rely on jdbc fail because they internally issue some queries that are not yet supported by qserv. need to patch it (in the short term), and add proper support (in the long term). this story covers the patching only. the queries that upset qserv are listed below.    show variables where variablename ='language' or variablename = 'netwritetimeout' or variablename = 'interactivetimeout' or variablename = 'waittimeout' or variablename = 'charactersetclient' or variablename = 'charactersetconnection' or variablename = 'characterset' or variablename = 'charactersetserver' or variablename = 'txisolation' or variablename = 'transactionisolation' or variablename = 'charactersetresults' or variablename = 'timezone' or variablename = 'timezone' or variablename = 'systemtimezone' or variablename = 'lowercasetablenames' or variablename = 'maxallowedpacket' or variablename = 'netbufferlength' or variablename = 'sqlmode' or variablename = 'querycachetype' or variablename = 'querycachesize' or variablename = 'license' or variablename = 'initconnect'  select @@session.autoincrementincrement  set names latin1  set charactersetresults = null  set autocommit=1  set sqlmode='stricttranstables' ",2,train
DM-1540,Add support for BIT columns,"the mysql/schemfactory code has incomplete logic for dealing with columns that have bit type   in particular, it doesn't properly handle the length of a bit field, and it sets the sql type to ""bit?"", which causes table creation failures later on. fixing this requires modifying schemafactory, and making sure that bit values are  transmitted properly.",4,train
DM-1541,Add support for transmitting [VAR]BINARY column data,"the code that pulls data out of a mysqlrow and puts it into a protobuf rowbundle does not handle binary data correctly. see https:/github.com/lsst/qserv/blob/master/core/modules/wdb/queryaction.cc#l217 for the relevant code.  the issue is that the generated addcolumn(const char) member function of rowbundle treats the input as cstyle nullterminated strings. but in the case of binary column data (and varbinary/blob variants, maybe also bit\(n\)), the contents can contain embedded nulls. we are currently using such columns for user defined types in mysql (e.g. image bounding polygons), so it's important to get this right. on the protobuf side the fix is as simple as calling addcolumn(const char value, int size) instead. i'm not a mysql c api expert, but the size will presumably have to be obtained/derived from the corresponding mysql_field.  ",8,train
DM-1542,Drawing speed will detect and optimize for large datasets,nan,10,train
DM-1543,Finished Background conversion,nan,2,train
DM-1544,Set up GIT hub for ipac firefly,nan,18,train
DM-1545,Span-based shrink operations for Footprint,"analogous to dm 1128, but shrinking rather than growing footprints.",5,train
DM-1547,"Install docker in an ISL OpenStack Ubuntu instance, perform basic checks",nan,1,train
DM-1551,Prototype HTM-based spatial binning to visualize large number of catalog sources,"currently we are using a generic 2d binning algorithm, that is finding minimum and maximum values of the two columns to be visualized and bins the values into 2d grid with the specified number of grid cells.  this algorithm distorts data in the pole regions and whenever data are on both sides of ra=0. more smart binning based on a spatial index is necessary when reducing the number of (ra,dec) entries intended for visualization.  ",16,train
DM-1552,Resolving QServ database configuration/connectivity issues,"to start the development i should be able to connect to the qserv development database via vpn and run simple queries.   ideally, i'd like to be able to connect to qserv with jdbc, view the data, run spatial queries. i also need an access to data dictionary to interpret data.",10,train
DM-1553,Evaluate SDSS catalog access,"sdss allows two ways to access their catalog data: via http post service, accessible to anonymous users, and via casjobs services, which require having an account. evaluate how sdss catalog data can be accessed from an application by prototyping single and multiple target searches accessing sdss services. ",10,train
DM-1556,FY17 Add Support for User Upload Tables for Qserv,"users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. in ipac terminology, this is called ""table upload queries"". note, we will need to assign some sort of unique id to such list. results must contain information which result correspond to which ""thing"". e.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point.  in qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.   we need to evaluate and agree on how generic these upload tables can be.  we might need to do some special optimizations (e.g., related to spatial searches). ",53,train
DM-1557,FY17 User Upload Tables for ImgServ,"users will need to be able to upload a list of ""things"" to workspace, then request ""repeat a given query for each ""thing"" from the uploaded list"". example of ""things"": ra/dec points, ra/dec points + distances, object ids, object names, bounding boxes, etc. in ipac terminology, this is called ""table upload queries"". note, we will need to assign some sort of unique id to such list. results must contain information which result correspond to which ""thing"". e.g, if user asks for neighbors near (1,1), (4,4), it is not enough to just return list of neighbors, we need to tell which neighbor is for which point. in qserv, that means that user will upload a special table with these ""things"", and then issue a query that involves join against that table.  we need to evaluate and agree on how generic these upload tables can be.  we need to decide on format / location: csv? table in database?   we might need to do some special optimizations (e.g., related to spatial searches).",53,train
DM-1560,A better coordinate grid overlay,some of the grid lines are not drawn right and the labels are not in the locations.,10,train
DM-1561,Research Javascript Frameworks: General Overview,"begin looking into js frameworks. start to look into angularjs, try to write some sample code.  attempt to understand the main concepts. gat an overview of the others out there.  ",12,train
DM-1562,Work with the database group to define initial concepts for backend interface API ,we are starting to gel around some ideas. ,10,train
DM-1563,Improve management of integration test datasets description,"currently data set description (i.e. data file extension, compressed extension, schema file extension) is hardcoded in python/lsst/qserv/tests/datareader.py  this could be improve (standard format for test dataset, meta service, metaconfiguration file, ...)",5,train
DM-1564,Implement drawing only active tab with new data model,nan,6,train
DM-1566,LOE - Week ending 11/07/14,nan,7,train
DM-1567,LOE - Week ending 11/14/14,nan,6,train
DM-1568,LOE - Week ending 11/21/14,nan,8,train
DM-1569,LOE - Week ending 11/28/14,nan,12,train
DM-1570,Create integration test case using data duplicator,integration tests should provide a new test case which use sph duplicate in partition package.,6,train
DM-1571,Setup Qserv for SUI tests,setup qserv on lsst db2 with and load some reasonable data set (perhaps pt 1.2). one potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.,2,train
DM-1572,Test deblended CModel colors,"using hsc data, examine colorcolor and colormagnitude diagrams with deblended cmodel magnitudes.  investigate outliers by looking at images and deblended heavyfootprints.",8,train
DM-1573,Basic validation of LSST pipeline on HSC data,get the pipeline running on hsc data to the point where nothing is obviously wrong.,8,train
DM-1574,"add support for ""freeze-drying"" measurement failures","we should make it easy for users to inspect and capture problems in measurement algorithms into an on disk format that allows them to be reproduced later with minimal setup (ideally, the package would require no access to the original data or configuration).  while this issue is mostly concerned with capturing problems in measurement algorithms, an extension to capture deblender failures should be considered as a future extension.",6,train
DM-1575,Support Mac OS in scisql-deploy.sh,cf. andy connolly message:  just as an fyi on my mac scisqldeploy.py was looking for 0.3.4/lib/libscisqlscisql0.3.so but there is only 0.3.4/lib/libscisql scisql0.3.dylib ,1,train
DM-1576,Sanitize AstrometryTask interface,currently the astrometrytask and astrometry class share work.  e.g. distortion is done in astrometrytask but matching is done in astrometry.  astrometrytask also makes assumptions about what fields are available in the solver config.    the astrometrytask interface should be sanitized so that it can be used as a thin wrapper for calling any astrometry solver.  top level config params should go in the astrometrytaskconfig and solver level work should be done in the solver class and configured at the solver level.,10,train
DM-1577,Rework Astrometry class,"the astrometry class shares information upstream with astrometrytask.  this should be factored out so that the astrometry class can be configured and called via a single well known method (solve?).  one thing the astrometry class that is needed by down stream processing (photocal) is to match sources.  this is currently a private method, but should be made public so that  it can be used without running astrometrytask.",4,train
DM-1578,Move photocal out of meas_astrom,it is confusing that photocal is in measastrom.  i assume that is historical.  i think it could probably live in pipetasks.,2,train
DM-1579,Implement replacement for A.net index files,"astrometry.net index files are hard to generate and hard to read.  we need another, more flexible, more standard system for storing reference files.  we should also be able to read fits files and other formats, but having a standard format with the utilities to create and query them is a must.  coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non blind solver may not need. however, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once).  it will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscastrom appears to use a catalog of star positions, which is nice and simple.",5,train
DM-1580,Implement a replacement solver to the current A.net solver,"this should be further refined.  the solver will be required to work with several input formats.  it will only be required to solve in the in the presence of a reasonable starting point with reasonable pointing errors.  failure should be graceful.  if multiple, equivalent solutions are found, this should be reported (for situations including perfect grids of sources).",15,train
DM-1582,Qserv spatial restrictor names are case sensitive,"the sql grammar treats qserv spatial restrictor names case insensitively, but qana/qservrestrictorplugin.cc does not, which means that one must use e.g. qservareaspecbox rather than qservareaspecbox. we are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. also, case insensitivity is consistent with mysql behavior for udf names.",1,train
DM-1584,Research how to integrate different image metadata stores with DataCat,"data release production will generate highly structured image metadata (exposure tables). if we decide to use datacat (e.g., for keeping more ad hoc metadata), the question arises if/how to integrate all this together:  should we integrate all exposure tables from all releases into datacat? (eg via foreign tables)  should we keep them distinct, and integrate at higher level (e.g., metadata service)",4,train
DM-1585,Design system for tracking existing images/files,"we have a lot of files/images already brought in or generated through data challenges done to date. we need a system for cataloging them. this story will define such system, eg, sketch of ui, underlying technology used (datacat, plain mysql, schema etc).",6,train
DM-1587,Define structure of web form for collecting metadata about existing data sets,web alpha version of the form (using django or fermi java webservices code) that collects input from users about data repositories. authentication not covered in this version.,2,train
DM-1588,Implement FITS header crawler and integrate it with the form,implement crawler that walks through registered repos (through the form) and loads metadata from fits headers into the mysql backend,7,train
DM-1589,Research and experiment with building form for capturing user input,"need to build a form that will be used to capture user input about existing image repositories (users will be registering their files/repositories). options to consider: pythonbased django, javabased system that is part of fermi datacat. ",4,train
DM-1590,Break down & discuss DM-1074,"i will take the lead on dm 1074. first step will be to sit with , get a feeling for what needs to be done, and sketch out a set of stories.",1,train
DM-1591,Convert test_qservAdmin.py into a real unit test,"need to turn ./admin/tests/test_qservadmin.py into a real unit test. in the past it was broken and it went unnoticed, see dm 1395",1,train
DM-1594,Remove check for stack dir write access in qserv-configure.py,"qservconfigure.py checks for write access to stack dir, this should be replace by check for read access, or removed.  fjammes@qservbuildserverxlarge:~/src/qserv$ qservconfigure.py all info: qserv configuration tool =======================================================================  warning : do you want to erase all configuration data in /home/fjammes/qservrun/201410.0 ? [y/n] y info: copying template configuration from /home/fjammes/stack/linux64/qserv/201410.0/cfg/templates to /home/fjammes/qservrun/201410.0 info: creating metaconfiguration file: /home/fjammes/qservrun/201410.0/qservmeta.conf info: reading metaconfiguration file /home/fjammes/qservrun/201410.0/qserv meta.conf info: defining main directory structure info: no write access to dir /home/fjammes/stack/linux64/qserv/201410.0 : [errno 13] permission denied: '/home/fjammes/stack/linux64/qserv/201410.0/writetester' ",2,train
DM-1595,Research Javascript Frameworks: Understand Angular & React,write some prototype code with angular and then try to do the same thing with react,10,train
DM-1596,Clean up multi-component Footprints,"following the landing of dm1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components.  here, we should provide a ""clean up"" operation that takes a multiplecomponent footprint and splits it into a set of contiguous footprints with appropriate peak lists.",4,train
DM-1597,init script fails to start xrootd after crash,"i think we saw this issue in the past, not sure it was actually fixed back then or just was to reintroduced one more time.  after crash of xrootd the regular etc/init.d/xrootd start fails to start it:  [salnikov@lsstdbdev2 dm621]$ /qservrun/dm621/etc/init.d/xrootd status xrootd is dead but pid file exists                         [failed]   see /usr/local/home/salnikov/qservrun/dm621/var/run/worker/xrootd.pid [salnikov@lsstdbdev2 dm621]$ /qservrun/dm621/etc/init.d/xrootd start starting xrootd: (already up)                              [  ok  ] [salnikov@lsstdbdev2 dm621]$ ~/qservrun/dm621/etc/init.d/xrootd status xrootd is dead but pid file exists                         [failed]   see /usr/local/home/salnikov/qservrun/dm621/var/run/worker/xrootd.pid  i can start it with restart but i think that start should detect that xrootd is dead (status does that) and start service correctly.  this issue may exist for other services but i did not check. ",1,train
DM-1598,Design of calibration and ingest system,produce a confluence page describing the approach to be taken to solve dm1074. ensure that all the relevant folks have reviewed that page and are happy. break down dm1074 into stories appropriate to that design.,10,train
DM-1600,Determine if Astrometry class is desired,"the question is whether the astrometry class is the thing that is overridden or if the astrometrytask has component configurables that are overridden.  also, determine location default implementation.",2,train
DM-1601,Add support for c-style comments in front of queries sent to qserv,"connecting to qserv from java fails because the jdbc driver inserts comments. ""/ ... /"" in front of queries (example pasted below). the fix involves removing the comments at the proxy level    sqlexception: qserv error: 'parseexception:antlr parse error:unexpected token: /:'  query being executed when exception was thrown: / mysqlconnectorjava5.1.34 ( revision: jess.balint@oracle.com20141014163213 wqbwpf1ok2kvo1om ) /show variables where variablename ='language' or variablename = 'netwritetimeout' or variablename = 'interactivetimeout' or variablename = 'waittimeout' or variablename = 'charactersetclient' or variablename = 'charactersetconnection' or variablename = 'characterset' or variablename = 'charactersetserver' or variablename = 'txisolation' or variablename = 'transactionisolation' or variablename = 'charactersetresults' or variablename = 'timezone' or variablename = 'timezone' or variablename = 'systemtimezone' or variablename = 'lowercasetablenames' or variablename = 'maxallowedpacket' or variablename = 'netbufferlength' or variablename = 'sqlmode' or variablename = 'querycachetype' or variablename = 'querycachesize' or variablename = 'license' or variablename = 'init_connect' ",1,train
DM-1602,Prevent collisions in /tmp related to scisql deployment,deploying scisql involves creating a file in  /tmp. the file never gets removed. this can cause the following error when qserv is installed later on the same machine:  error: failed to open output file /tmp/scisqldemohtmid10.tsv for writing   we should switch to using a temporary file instead of file with fixed name.,2,train
DM-1603,Qserv scons scripts do not pick up the version of swig provided by eups,"see the summary. the consequence is that the qserv integration tests fail on systems that provide swig 2.x+   there seems to be some implicit dependency on swig 1.x. the reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules).",2,train
DM-1604,Sanitize configs,"some solver specific information is stored in the astrometrytask config.  further, the solver config is accessed inside the astrometrytask run method.  this mixing of information make it hard to make the framework pluggable.  solver configuration should be confined completely within the solver class (whether it be part of the astrometry class or a configurable of its own).",2,train
DM-1607,Add unit tests to test c-style comments in/around queries,i should have thought about it when doing dm 1601 but i didn't... it'd be good to add test queries that test comments before/after/inside query.,1,train
DM-1608,Move meas_algorithms unit tests to meas_base framework,the following tests in measalgorithms need to be ported to the measbase framework:  measure.py psfselecttest.py testpsfdetermination.py ticket2019.py testcorrectfluxes.py (though this cannot be done until the algorithm exists),2,train
DM-1609,Research off-the-shelf solutions for harvesting metadata,relevant links:   http:/  there’s a list of metadata in the document   caom (common archive object model): http:/,4,train
DM-1610,Integrate Metadata Store prototype v1 with cat and db  ,"integrate prototype developed through dm 1255 with cat and db as appropriate (don't hardcode schema, don't hardcode credentials, etc).",4,train
DM-1611,Experiment with DataCat foreign tables,"this is a placeholder story, we should break it down into more smaller stories...",15,train
DM-1614,Add support for mysql JDBC driver (v2),"mysql client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in dm 1539 are not sufficient.",1,train
DM-1615,Design and implement CSS structure for distributed Qserv setup,for management of the distributed databases/tables we need info in css about all workers and tables. the info will be created by data loader and updated by replicator which do not exist yet. for this issue we need to provide python api which can fill the same information in css so that we can build and test other pieces needed for this epic.,5,train
DM-1616,Implement remote host access for management framework,to manage remote workers we need a way to access services on remote machines that run workers. services may be something like mysql (which we would prefer to run without tcp port open) and optionally xrootd. this ticket will implement python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.,5,train
DM-1617,Client library for accessing distributed database/table information from CSS,"provide python interface for accessing information in css which is relevant to distributed management, such as database/table/node data. this interface can be used also by data loader and replicator.",8,train
DM-1618,Implement distributed database creation,implement python library which creates databases on all active workers based on info from css.,10,train
DM-1619,Implement distributed table creation,implement python library for creating mysql tables on all active workers.,11,train
DM-1620,Move CSS documentation close to code ,"css documentation about the structure is currently in trac (which is readonly), at https:/dev.lsstcorp.org/trac/wiki/db/qserv/css. we need to move it close to the source code, e.g., to a doc file.",2,train
DM-1621,Add unit test to to verify zookeeper versioning works,"this is a follow u pto dm 1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled.",1,train
DM-1622,Add unit test for queries from DM-1539,need to add unit test for queries listed in dm 1539,1,train
DM-1624,Qserv should report it's version,"it should be possible to determine which version of qserv we are running by looking at information from log files. so, in practice, we probably should generate in scons a unique id (from sha from git?) and compile it into the code.",1,train
DM-1625,SciSQL should report its version,it should be possible to determine which version of scisql we are running.,1,train
DM-1626,Build 2014_12 Qserv release,the title says it all. please open ticket for next release when closing this one.,1,train
DM-1627,"Integrate metaserv, imgserv and dbserv with Data Access Services"," create dbserv for handling database related webbased queries, support running queries through web (initial version).   integrate dbserv, metaserv and imgserv restful api into the webserv  want to run all services (meta, image, db) under one server.   also want to be able to run them separately, so have some handy servers for each service  proof of concept for supporting multiple formats (html, json)",22,train
DM-1629,Adopted/Retired RFCs are not counted as resolved,"also, marking an rfc as adopted brings up a box with a message applicable to the closed status.",1,train
DM-1630,New RFCs should result in dm-devel E-mails and HipChat postings,email notices of new rfcs filed with a dm component go to dm devel email notices of new rfcs filed with a dm component go to data management chat room email notices of all new rfcs go to bot: rfc room ,1,train
DM-1632,Build 2014_11 Qserv release,"the title says it all. please open ticket for next release when closing this one.  tasks to do:   publish last buildbot build under a temporary eupstag (""qservdev"")and test it, if it works fine:  create gittags for qserv and dependencies  publish the release with eupstags ""qserv"" and ""yyyymm""  generate and publish the doc for release ""yyyymm""  update release number in qserv code and set ""yyyymm1"" as release in dev and ""yyyymm"" as stable release (update admin/bin/qservversion.sh, doc/source/conf.py, doc/source/toplevel.rst)  generate and publish the doc for release ""yyyy_mm1""   look at the doc  commit  admin/bin/qservversion.sh, doc/source/conf.py, doc/source/toplevel.rst with current ticket number   this procedure should be validated and documented.",1,train
DM-1633,Update build process for Firefly opensource,update build and deploy related scripts to reflects the changes affected by open sourcing of firefly.,7,train
DM-1635,Remove redundant CORS headers from Firefly's http response,make sure cors related headers are not sent when the origin header is missing.  firefox does not like it.,1,train
DM-1636,Research popular web development technologies,research popular web development technologies to prepare firefly for the future with the focus on front end framework. ,6,train
DM-1641,"Document ""getting started"" procedure for new stack developers","document a procedure for building the stack on a new system in a way that is appropriate for both project members and external contributors.  this can be linked as a ""getting started"" guide from http:/dm.lsst.org/.",5,train
DM-1642,LOE - Week ending 12/5/14,nan,8,train
DM-1645,preparation work for FIrefly open source,"1. discuss with ipac director and managers to open source firefly 2. study the major open source license,apache, gpl, bsd 3clause, mit.  3. final decision: bsd3 clause",6,train
DM-1647,Use an OpenStack instance to run an HTCondor Central manager,use an openstack instance to run an htcondor central manager,4,train
DM-1648,W16 Research technologies for Data Access,"research technologies potentially useful for data access / database. this epic covers apache mesos, google kubernetes, maxscale, serf, consul, and memsql.",38,train
DM-1652,The existing FITS reader class needs to be refactored to improve the performance(1), checkout the classes  understand the fits standard and current implementation,16,train
DM-1653,Extend data loading script to support multi-node setup,"implement simplest use case for data loading with one or more worker database separate from czar database. simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular tcp connection. it should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in css doc on trac).",8,train
DM-1655,Working with SLAC on definition of metadata store,"follow up metadata store schema development to ensure sui will be able to use it. define the fields that should go into data definition table. define the fields that must be present in the image metadata table, which sui will be searching.",2,train
DM-1656,hipchat support for maigically urlifying docushare documents,"it would be generally use full if references to official documents in hipchat (or it's successor) automagically generated urls for official document handles.  eg: document1234, lse123",1,train
DM-1657,LOE - Week ending 12/12/14,nan,8,train
DM-1658,Add git bisect tool for Qserv repos," fjammes@clrlsstdbmastervm:~/src/qserv (u/fjammes/dm627 $%) $ qservtesthead.sh h  usage: qservtesthead.sh [options]    available options:     h          this message     q          quick: only rebuild/install new qserv code,                 and perform test case #01    rebuild from scratch, configure and run integration tests against   a qserv git repository.   prerequisite:     source loadlsst.bash     setup qservdistrib t qserv     setup k r $    can be used with 'git bisect' :     cd $     git bisect start     git bisect bad     git bisect good gitcommitid     git bisect run /home/fjammes/src/qservtestdata/bin/qservtesthead.sh   code is in dm 627 ticket branch.",2,train
DM-1659,Aperture Flux back into an abstract class,"the last change to apertureflux to make it work with the new c design changed aperturefluxalgorithm into an instantiatable class.  however, i have now figured out how to make this work with swig while still allowing measure and fail to be defined by default at the apertureflux level..  so this issue is to put things back in order.",1,train
DM-1660,Statistics tests use a constant image,i just noticed that the tests for statistics (clipped mean etc.) use a constant image.  we should be testing a gaussian field (although that makes the tests a little trickier) ,2,train
DM-1661,czar log4cxx link/load bug,"under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::objectptrbase while loading the css python wrapper shared lib.",2,train
DM-1662,Make qserv dependencies build on OS X with clang,fix anything necessary for qserv dependencies to build on os x with clang.  note  making qserv itself build is more complicated and may require a separate ticket.,4,train
DM-1663,fix dependency problems in obs_subaru scons scripts,"when building obssubaru with  j4, it often tries to build files related to the defects before the main python module is built, resulting in import errors (because the scripts it invokes depend on the main python module).    we need to rewrite the scons scripts to ensure this dependency is captured.    a preliminary look indicated that this is not entirely trivial, and i'll have to remind myself a bit of how some things in scons work to get it done, so i'm putting this off for a future sprint.    in the meantime, the workaround is to build obssubaru with no parallelization.",2,train
DM-1664,Understand historical written docushare materials dealing with the operational security environment.,"researched the docushare traversing the plans for materials to be embedded in the ocs and similar systems, as well as extant operational plans. the goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams  to mender ea smooth integration with it security systems upon delivey and integration of their sub systems in chile.",3,train
DM-1667,Install PgMySQL and use to connect to local Qserv.,"used ""pip"" to install it. ""conda"" should work as well. therefore, it should be easy to make it part of the delivered system: vm, container, tar file, after the fact download, etc. it has documentation, uses the mit license, under active development and available from pypi. db connection is straight forward and requires little experience to get meaningful work done.",2,train
DM-1668,Create SQL code to read Qserv into Python Pandas data frame,"this works well, at least for a simple case. you can move directly from a query statement to a pandas data frame for analysis in just a few lines of code. here is the start of an ipython qserv session showing how easy it is.  in [6]: import pandas as pd in [7]: import pymysql as db  in [8]: conn = db.connect(host='lsstdb1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='lsst')  in [11]: df = pd.read_sql(""select deepcoaddid, tract, patch, ra, decl from deepcoadd"", conn)  in [12]: df out[12]:     deepcoaddid  tract   patch        ra      decl 0      26607706      0  406,11  0.669945  1.152218 1      26673242      0  407,11  0.449945  1.152218 2      26804242      0   409,2  0.011595 0.734160 3      26673154      0   407,0  0.449945  1.152108 … ",2,train
DM-1669,Explore queries for Qserv database.,"use a local database here at ipac with 5 qserv tables in it. looked at several python query interfaces. used the pymysql interface for testing because it's pure python and because i found reports suggesting it was almost as fast as the mysqldb interface that requires c language libraries.  ad hoc queries can be constructed in three lines of code, so useable in a science environment.  found a couple of bugs in qserv that were reported.",6,train
DM-1670,Begin looking at how Python Pandas can be used for LSST data analysis.,"pandas is well integrated with the other parts of scipy: numpy, matlibpy, etc.  it’s a good candidate for data analysis, especially where time series are involved. however, there are no multidimensional columns, poor metadata support for fits files and a need to use masks instead of nan values. these may, or may not, be problems.  there is a 400 page book about pandas, so it will take some further time to learn its value, especially with astronomical data in different situations.",5,train
DM-1673,Allow SWIG override for broken SWIG installations,"dependency on swig 2.0+ was introduced into qserv, and this broke qserv building on systems relying on swig 1.3.x.  this ticket introduces basic code to override swig_lib on those systems to allow use of the broken installation (some swig search paths are fixed during its build process otherwise).",1,train
DM-1674,Data ingest scripts cleanup,nan,10,train
DM-1675,Identify dead code,nan,1,train
DM-1676,Write documentation for SSI interface,nan,5,train
DM-1677,multi-error utility class,"queryaction::impl currently has a handful of private members/methods related to maintaining a collection of errors that occurs while processing a query (adderrormsg, intstring, intstringvectory, errors, etc.)  it would be useful to split this stuff out into a separate, re usable utility class, and extend, then extend with some additional functionality (output stream operator, subclass from std::exception, maybe capture line and file, integrate with logging, etc.)  [fabrice: discuss requirements and basic design w/ fritz and/or daniel]",10,train
DM-1685,Minor bug in a test,"tests/centroid.py has a bug in testmeasurecentroid: ""c"" is undefined in the following bit of code:  if display:     ds9.dot(""x"", c.getx(), c.gety(), ctype=ds9.green) ",1,train
DM-1692,"Implement ""unlimited"" result size handling","dm854 exposed an issue in handling large results. result rows are returned from worker to czar in protobufs messages. however, protobufs messages should not be larger than some number of megabytes, according to protobufs documentation. iow, protobufs is not designed to handle messages on the order of hundreds of megabytes. there may be some code in the protobufs implementation that does not scale beyond messages of a few megabytes. hence, in order to send larger amounts of result rows, we need to use multiple messages. the current protobufs definition for result messages includes a placeholder for chaining result messages, but there is no code on the czar or worker that implements result chaining.   the scope of this ticket is to implement message chaining for results on the worker and on the czar, in such a way that it places no limits on the overall number of result rows (if there is a limit, it should be no smaller than quadrillions and be welldocumented).  ",15,train
DM-1694,Define interfaces for Data Access Services,nan,8,train
DM-1695,Implement interfaces for Data Access Services,"implement proof of concept, skeleton of the prototype. the work will continue in follow up stories in february and in s15.",8,train
DM-1696,Integrate image cutout service interfaces with butler,nan,5,train
DM-1697,Finish image cutout service implementation,define appropriate interfaces and connect them with the restful api (see dm 1695).,9,train
DM-1698,Finish metadata store prototype, a quick proofofconcept prototype of loading tool for loading database information into metadata store.    handling mysql credentials through auth file in home dir instead of hardcoded values.,4,train
DM-1700,Create read only OpenStack volume and execute processing scenario,create read only openstack volume and execute processing scenario,4,train
DM-1701,Clone OpenStack volume for use against multiple instances,clone openstack volume for use against multiple instances,2,train
DM-1702,Examine fqdn/hostname assignment for OpenStack instance,examine fqdn/hostname assignment for openstack instance,4,train
DM-1703,S15 Implement Database & Table Mgmt,"continuation of dm 1036, making the code for managing distributed databases and tables more feature reach, including features such as deletion.",53,train
DM-1704,S15 Run Large Scale Qserv Tests,run large scale tests to uncover unexpected issues and bottlenecks.,48,train
DM-1705,S15 Tune Qserv,fix scalability and performance issues uncovered through large scale tests dm 1704,100,train
DM-1706,S15 Analyze Qserv Performance,"final analysis of qserv performance, measure kpis. based on ldm240, we are aiming to demonstrate:   50 simultaneous low volume queries, 18 sec/query   5 simultaneous highvolume queries, 24 h/query   data size: 10% of dr1 level.   continuous running for 24 h with no software failures.  ",5,train
DM-1707,S15 Refactor Qserv,"ongoing refactoring of qserv   code cleanup, tightening interfaces etc.",100,train
DM-1708,W16 Improve Query Coverage in Qserv,currently qserv supports only a limited subset of queries. we need to make sure it supports all queries that users need to run.,46,train
DM-1709,Implement result sorting for integration tests,"we need to be able to sort results, because we can't always rely on order by. so we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.)    the following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re enabled them prior to closing this ticket:    case02/queries/0003selectmetadataforonegalaxywithusing.sql  case02/queries/3001query035.sql  case02/queries/3008selectobjectwithcolormagnitudegreaterthan.sql  case02/queries/3011selectobjectwithmagnitudes.sql  case02/queries/3011selectobjectwithmagnitudesnoalias.sql  ",8,train
DM-1710,ValueError in lsst.afw.table.Catalog.extend()," from lsst.afw.table import basecatalog, schema  s = schema() c1 = basecatalog(s) c2 = basecatalog(s)  c1.extend(c2)   the above fails, saying:   traceback (most recent call last):   file ""test.py"", line 7, in /     c1.extend(c2)   file ""/users/jds/projects/astronomy/lsst/stack/darwinx86/afw/10.0+3/python/lsst/afw/table/tablelib.py"", line 6909, in extend     tablelib.basecatalogextend(self, iterable, deep) valueerror: invalid null reference in method 'basecatalog_extend', argument 3 of type 'lsst::afw::table::schemamapper const &' ",1,train
DM-1711,"S15 Improve MetaServ: RESTful, Basic Image Search, DDL, Config Files","implement beta version of the metadata service. this version will support basic image search, ddl, config files, and restful interfaces for metaserv and database (qserv).",57,train
DM-1712,"S15 Add Support for Image Stitching and Rotation, RESTful APIs","implement image stitching and rotating, including restful apis.",35,train
DM-1713,S15 Image & File Archive v2,system for tracking existing image data sets integrated with metadata services.,5,train
DM-1714,Integrate MetaServ with Schema Browser,"schema browser displays detailed info about schema, including custom fields like ucd, units etc. this information is stored as comments embedded in the master version of the schema (in ""cat"" repo). currently we are generating ascii from the master schema for schema browser, and we load it into mysql, then schema browser reads it from mysql. this story involves changing schema browser such that it will read the schema information directly from metaserv.",6,train
DM-1715,Disable query killing,apparently killing a query through ctrl c is confusing xrootd. disable query killing (which seems to be only partly implemented).,1,train
DM-1716,Implement query killing through Ctrl-C,need to properly implement query killing through ctrl c,16,train
DM-1720,Make secondary index for director table only,"following discussion on qserv l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. need to modify data loader to recognize which table is director table and generate index only for that table. ",2,train
DM-1721,S15 Improve Query Coverage in Qserv,"query coverage in the qserv integration testing is very limited, we have been turning off more and more queries and we were making the qserv code and the data loader more strict. this epic covers work (fixes and improvements) related to  re enabling test queries marked as ""fixme"" (when it make sense, some queries are for features that are not implemented yet)  adding more queries to test interfaces and features that are implemented but are not currently tested.",40,train
DM-1722,LOE - Week ending 12/19/14,the system administration team at ncsa worked on the following loe tasks this week:  rma'ed raid card for lsstdev /  updated user's updated public ssh keys /  rebuilt failed drive in lsstdbdev2 /  increasing drive size of lssteval /  researched & repaired lsst20 nfs issues /  upgraded lsst xfer to 10g networking,14,train
DM-1725,stack build fails on gcc 4.8 with opt=3,"the stack fails to build with gcc 4.8, with a test failure in meas_base (though a similar problem on the hsc fork suggests the problem is actually in afw).   reports that on opensuse 13.1, the failure goes away when compiling with opt=1 instead of the default opt=3, indicating that the problem is overly aggressive optimization.  this, and the fact that a traceback of the hsc side failure implicates maskedimage::getxy0, leads me to guess that something is going wrong with the alignment of the eigen data members in afw::geom::point.  until that theory is disproven, this probably belongs in my court, though i'd be happy to let someone steal it from me if they're interested in working on it.",4,train
DM-1726,Stabilize Firefly Repository, make github firefly ready but not public  add firefly to lsst git  clean up any lingering issues from separation  break up irsa viewer from firefly viewer,14,train
DM-1727,Research Javascript Frameworks: Work toward future proposal,take prior research and come up with rough firefly migration proposal.  include how to use a hybrid system for foreseeable future. write some prototype code.,16,train
DM-1731,fix table file handling of MANPATH in dependencies,"as discussed on dm1220, the table files for:   mysqlproxy   protobuf   lua   expat should have the manpath entry removed entirely, while:   xrootd should have "":"" added to the end of its manpath value, to allow the default paths to be searched as well.",1,train
DM-1732,Fix error on duplicate result_id_m table while launching qserv integration tests,"next command:   qservcheckintegration.py case=01 load;  qservcheckintegration.py case=02 load  fails, most of the time, with next error in logs:  qserv@clrinfoport09:/src/qserv (u/fjammes/dm627 +)⟫ cat /qservrun/201412/var/log/qservczar.log | grep result1211906m 0103 16:38:18.576 [0x7f23c4ff9700] debug root (build/rproc/infilemerger.cc:432)  infilemerger create table:create table qservresult.result1211906m (`ra` double) 0103 16:38:18.661 [0x7f23c4ff9700] debug root (build/rproc/infilemerger.cc:354)  infilemerger sql success: create table qservresult.result1211906m (`ra` double) 0103 16:38:18.661 [0x7f23c4ff9700] debug root (build/rproc/infilemerger.cc:445)  infilemerger table qservresult.result1211906m is ready 0103 16:38:18.686 [0x7f23c77fe700] info  root (build/rproc/infilemerger.cc:299)  merging w/create table qservresult.result1211906 select ra as ra from qservresult.result1211906m order by ra 0103 16:38:18.720 [0x7f23c77fe700] debug root (build/rproc/infilemerger.cc:354)  infilemerger sql success: create table qservresult.result1211906 select ra as ra from qservresult.result1211906m order by ra 0103 16:38:18.720 [0x7f23c77fe700] info  root (build/rproc/infilemerger.cc:305)  cleaning up qservresult.result1211906m 0103 16:38:18.721 [0x7f23c77fe700] info  root (build/rproc/infilemerger.cc:317)  merged qservresult.result1211906m into qservresult.result1211906 0103 16:38:24.716 [0x7fe3cf7fe700] debug root (build/rproc/infilemerger.cc:432)  infilemerger create table:create table qservresult.result1211906m (`qs1count` bigint(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] error root (build/rproc/infilemerger.cc:351)  infilemerger sql error: error applying sql. error 1050: table 'result1211906m' already exists unable to execute query: create table qservresult.result1211906m (`qs1count` bigint(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] error root (build/rproc/infilemerger.cc:438)  infilemerger error: error creating table (qservresult.result1211906m) 0103 16:38:34.672 [0x7fe3deffd700] info  root (build/rproc/infilemerger.cc:299)  merging w/create table qservresult.result1211906 select sum(qs1count) as objcount from qservresult.result1211906m 0103 16:38:34.673 [0x7fe3deffd700] error root (build/rproc/infilemerger.cc:351)  infilemerger sql error: error applying sql. error 1054: unknown column 'qs1count' in 'field list' unable to execute query: create table qservresult.result1211906 select sum(qs1count) as objcount from qservresult.result1211906m 0103 16:38:34.673 [0x7fe3deffd700] info  root (build/rproc/infilemerger.cc:305)  cleaning up qservresult.result1211906m 0103 16:38:34.674 [0x7fe3deffd700] info  root (build/rproc/infilemerger.cc:317)  merged qservresult.result1211906m into qservresult.result1211906   so it seems test case #01 create table .result1211906m, and then test case #02 try to reuse this name for an other query.  qserv result tables aren't cleaned (here qserv has be stopped):  ls  ~/qserv run/201412/var/lib/mysql/qservresult/ db.opt                result1211336m.myi  result1211340m.myi  result1211343m.myi  result1211346m.myi  result1211382m.myi  result1211385m.myi  result1211902m.myi  result1211905m.myi result1211334m.frm  result1211337m.frm  result1211341m.frm  result1211344m.frm  result1211347m.frm  result1211383m.frm  result1211386m.frm  result1211903m.frm  result1211906m.frm result1211334m.myd  result1211337m.myd  result1211341m.myd  result1211344m.myd  result1211347m.myd  result1211383m.myd  result1211386m.myd  result1211903m.myd  result1211906m.myd result1211334m.myi  result1211337m.myi  result1211341m.myi  result1211344m.myi  result1211347m.myi  result1211383m.myi  result1211386m.myi  result1211903m.myi  result1211906m.myi result1211335m.frm  result1211339m.frm  result1211342m.frm  result1211345m.frm  result1211381m.frm  result1211384m.frm  result1211901m.frm  result1211904m.frm result1211335m.myd  result1211339m.myd  result1211342m.myd  result1211345m.myd  result1211381m.myd  result1211384m.myd  result1211901m.myd  result1211904m.myd result1211335m.myi  result1211339m.myi  result1211342m.myi  result1211345m.myi  result1211381m.myi  result1211384m.myi  result1211901m.myi  result1211904m.myi result1211336m.frm  result1211340m.frm  result1211343m.frm  result1211346m.frm  result1211382m.frm  result1211385m.frm  result1211902m.frm  result1211905m.frm result1211336m.myd  result1211340m.myd  result1211343m.myd  result1211346m.myd  result1211382m.myd  result1211385m.myd  result1211902m.myd  result1211905m.myd   changing idcounter to next value in appinterface.py:  self._idcounter = int(time.time() % (606024365)   10)  solves the problem, but a deeper explanation will allow to bring a more robust fix.",8,train
DM-1733,Build 2015_01 Qserv release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-1734,refactor fftools viewer to have derived viewers / start LSST SUI git repo,do the following:   refactor fftools to allow for specialized viewers.  make a generic fftools viewer.  make an irsaviewer package in ifenew that will build.   generalize the catalog search and add factories to access the lsst search process. ,10,train
DM-1735,Have newinstall.sh check itself against distrib version,we want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.  ,1,train
DM-1736,Migrate qserv modules (python code) to new logging system,nan,8,train
DM-1738,deblender artifacts in noise-replaced images,"we still see noise artifacts in some deblended images on the lsst side when running the m31 hsc data.  they look like the result of running noisereplacer on heavyfootprints in which the children can extend beyond the parents.  this was fixed on the hsc side on dm 340 (before the hsc jira split off), and i think we just need to transfer the fix to lsst.",1,train
DM-1739,Catch-all epic for essential fixes during DM-W15-4,nan,10,train
DM-1743,CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly,"both the noquote and noescape csv formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. furthermore, when quoting is turned off, the reader incorrectly treats embedded nul characters as a quote character.",1,train
DM-1744,Fix SWIG_SWIG_LIB empty list default value,"see serge message to qservl ""xrootd premature death"":  however, there are bigger problems. first of all, master doesn’t build for me. i get this error:    file ""/home/lsstadm/qserv/sconstruct"", line 104:     env.alias(""distcore"", getinstalltargets())   file ""/home/lsstadm/qserv/sconstruct"", line 90:     exports=['env', 'arguments'])   file ""/home/lsstadm/stack/linux64/scons/2.3.01/lib/scons/scons/script/sconscript.py"", line 609:     return method(args, kw)   file ""/home/lsstadm/stack/linux64/scons/2.3.01/lib/scons/scons/script/sconscript.py"", line 546:     return sconscript(self.fs, files, substkw)   file ""/home/lsstadm/stack/linux64/scons/2.3.01/lib/scons/scons/script/sconscript.py"", line 260:     exec file in callstack[1].globals   file ""/home/lsstadm/qserv/build/sconscript"", line 39:     canbuild = detect.checkmysql(env) and detect.setxrootd(env) and detect.checkxrootdlink(env)   file ""/home/lsstadm/qserv/sitescons/detect.py"", line 225:     xrdlibpath = findxrootdlibpath(""xrdcl"", env[""libpath""])   file ""/home/lsstadm/qserv/sitescons/detect.py"", line 213:     if os.access(os.path.join(path, fname), os.rok):   file ""/home/lsstadm/stack/linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77:     elif path == '' or path.endswith('/'):  which is caused by the fact that env[“libpath”] looks like:  [[], '/home/lsstadm/stack/linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/linux64/log4cxx/0.10.0.lsst12/lib', '/home/lsstadm/stack/linux64/xrootd/4.0.0rc4qsclient2/lib', '/home/lsstadm/stack/linux64/zookeeper/3.4.6/c binding/lib', '/home/lsstadm/stack/linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/linux64/log/10.0+3/lib']  the first element is [], which comes from https:/github.com/lsst/qserv/blob/master/sitescons/state.py#l173 where a pathvariable called swigswig_lib is given a default value of []. i can fix the build by changing the default to an empty string… but i don’t know enough scons to say whether that’s the right thing to do. can one of the scons gurus confirm that’s the right fix? ",1,train
DM-1753,make lsst-sui & firefly git repo / make an lsst viewer,nan,4,train
DM-1754,Update auto build tool to work with new split repositories ,"after the repository split, changes are required to get the auto build tool to work properly. firefly and firefly based applications are built using gradle system.  ",8,train
DM-1755,Create an integration test case with GB-sized data,"it's difficult to load manually data in qserv, so a way to do that is to use integration test framework to automatically do this.  big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests.",4,train
DM-1761,Provide input data for exampleCmdLineTask.py,"pipetasks/examples/examplecmdlinetask.py reads data from a repository. the comments in pipetasks/python/lsst/pipe/tasks/examplecmdlinetask.py suggest that   # the following will work on an ncsa lsst computer: examples/examplecmdlinetask.py /lsst8/krughoff/diffimdata/sparsediffimoutputv72 id visit=6866601   there are a few problems with that:   external contributors don't have access to lsst;  even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it;   the mention of this data is fairly well buried  it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon.  at least the first two points could be addressed by referring to a publicly available data repository. for example, the following works once afwdata has been set up:   examples/examplecmdlinetask.py $/imsim id visit=85408556   although this has the downside of only providing a single image.",1,train
DM-1762,Export SUI data (DC_W13_Stripe82_subset)," import sui.sql.bzip2.out (produced by serge) into mysql for deepsource and deepforcedsource tables:  remove columns chunkid and subchunkid for each chunk table  merge all chunk table into the main table  join deepsource and deepforcedsource to add coordinates of deepsource (director) object in deepforcedsource table. then dump  deepsource and deepforcedsource  to files deepsource.csv and deepforcedsource.csv  select f. , coalesce(s.ra, f.ra), coalesce(s.decl, f.decl) from deepforcedsource f left join deepsource s on (f.deepsourceid = s.deepsourceid) into outfile '/db1/dump/deepforcedsource.csv' fields terminated by ',' optionally enclosed by '""' lines terminated by '\n';    load this file using qserv loader.  a sample should be made and tested first to validate this procedure. this sample could be added in qserv_testdata",3,train
DM-1764,overhaul slot and alias system,"while working on dm1218 and dm464, i've grown quite dissatisfied with the current state of the slot and alias mechanisms, and we now have a concrete proposal for largerscale changes on rfc11.  unfortunately, i don't think we'll be in a good position to do much about this until we've completed the transition to measbase and removed the old measurement framework in measalgorithms.",6,train
DM-1765,move SourceRecord/Table/Catalog to meas_base,"we can make address a lot of dependency issues if we move the source classes to meas_base, because we'll no longer have lowlevel code (e.g. afw::table persistence) in the same module as very highlevel code (e.g. slots).   it will also put all the slot code in the same place, instead of spreading it across two pacakges.  this should be straightforward, except that we'll have a lot of downstream code to (trivially) change, and there's a good chance swig will get confused somewhere along the way.",2,train
DM-1766,Remove in-memory support of old-version afw::table objects,"after removing the old measurement framework in meas_algorithms, we should also end support for version=0 schemas in memory, and instead convert version=0 schemas to version 1 when we unpersist them.",11,train
DM-1767,afw::table - post-transition improvements,breakdown: jbosch 40%; swinbank 60%,23,train
DM-1769,Measurement - Framework Improvements,"add new features in measbase that are desirable, but not required to replace functionality in measalgorithms.  breakdown: pgee 70%; jbosch 30%",60,train
DM-1770,Support DDL in MetaServ - design,"ddl information is embedded as comments in the master version of the schema (in ""cat"" repo). currently we are only using it for schema browser. this story involves designing the procedure involving loading ddl information into metaserv. we need to be ready to support a variety of scenarios:  we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not)  we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv   we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database).",2,train
DM-1771,move executionOrder from plugin config class to plugin class,"we originally put the executionorder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  but it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change.",1,train
DM-1772,Create a search processor to do cone/box search on a QSERV catalog,"create a search processor which accepts cone and box spatial constraints, queries a catalog, stored on qserv (deepsource), and returns the relevant rows as an ipac table or rawdataset.  the implementation is querying deepsource catalog using qservareaspeccircle and qservareaspecbox spatial restrictors. ",11,train
DM-1773,"Read SUI requirements, send a list of questions to the group scientist","read the requirements document on archive browser and query tools, make a record of unclear items, send questions to our scientist (d. ciardi) ",5,train
DM-1774,Add support for running unit tests in qserv/admin,"this came up during review of dm 370: ""we do not run tests scripts in admin/ during regular build, sconscript in admin/ does not support that unfortunately."" this story involves tweaking sconscript to enable running unit tests automatically.",1,train
DM-1775,Setup work environment / test builds for refactored repositories,"after the repository was split into firefly and new ife, it was necessary to understand the changes, check for inconsistencies, test builds, set up new idea project, etc. ",3,train
DM-1776,Obtain and use catalog dd (data definition),"get catalog metadata, which should include column description, units, and type. use it in tool tips and possibly to create flexible constraints.  a temporary solution is to get metadata from an internal lsstschemabrowsers12lsstsim database on lsst db.ncsa.illinois.edu  a permanent solution would be querying catalog metadata from metadata store.",10,train
DM-1777,XYPlotter should be caching and restoring plot metadata,"for efficiency, xyplotter is designed to create up to 4 cards. when the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. when card is reused the previous plot metadata (like column selections, grid option, etc.) are lost.",4,train
DM-1778,Remove optimisation flag management in partition SConstruct," eups build with g and o3 by default.   developpers can build their sources with next options:  scons debug=false opt=3   nevetheless partition sconstruct add o2 option if debug is disabled, which is orthogonal. ",2,train
DM-1782,The existing FITS reader class needs to be refactored to improve the performance(2)," fits reader class refactoring, improve readability  validation of the new code against the old one  unit test  performance improvement recording if possible",16,train
DM-1783,fix faint source and minimum-radius problems in Kron photometry,"this transfers some improvements to the kron photometry from the hsc side:   hsc983: address failures on faint sources   hsc989: fix the minimum radius   hsc865: switch to determinant radius instead of semimajor axis   hsc962: bad radius flag was not being used   hsc121: fix scaling in forced photometry  the story points estimate here is 50% of the actual effort, as the work (already done) also benefited hsc.",5,train
DM-1784,Fix errors in parsing or rendering nested expressions,qserv has errors rendering nested expressions in predicates of the where clause. it is unclear whether the problem is in constructing the predicate representation or in rendering the representation (or both).  example:  select  o1.objectid from object o1  where abs( (scisqlfluxtoabmag(o1.gfluxps)scisqlfluxtoabmag(o1.rfluxps))               (scisqlfluxtoabmag(o1.gfluxps)scisqlfluxtoabmag(o1.rfluxps)) ) < 1  yields:  select o1.objectid from lsst.object100 as o1 where abs((valueexp factor functionspec scisqlfluxtoabmag(valueexp factor columnref o1.gfluxps)factor functionspec scisqlfluxtoabmag(valueexp factor columnref o1.rfluxps)))<1   i probably left out a more general implementation one/both of those parts of query parsing/analysis.,4,train
DM-1785,Add rotAngle to baseline schema,"add ""rotangle double"" to every table that has image ra/decl.  ",1,train
DM-1786,Implement using multiple disk spindles,"qserv should be able to take advantage of multiple disk spindles (jbod type architecture). in practice that means either relying on something like native mysql partitioning, or tweaking loader so that it can distribute chunks across multiple disks (and patch symlinks in mysql data_dir).  ",20,train
DM-1787,OpenStack automation via Python scripts : Launch an Instance,"in our introductory work with openstack we have been utilizing the horizon gui interface for first steps, followed by the use of command line tools (the 'cli') (e.g., nova, cinder, etc) as shown in dm1334  dm1700 , dm1701. while it is possible to write automation scripts that utilize the cli, an approach based on 'pure' python scripting would fit more seamlessly into the lsst software development process. enabling openstack automation via python offers the opportunity to integrate provisioning of resources into the overall flow of lsst workflow & processing (e.g., drp.)  the openstack services expose native python apis that expose the same feature set as the commandline tools.  the required python packages (pythonkeystoneclient, pythonnovaclient, ..) are installed on the head node 'vlad mgmt' of the ncsa isl openstack, and so initial python scripting can be executed/tested there.  a first python script will perform required authentication and launch an instance. ",4,train
DM-1788,OpenStack automation via Python scripts : Software installation/test on an LSST node,"the set of packages that will enable us to write against the native python apis of the openstack services is   pythonkeystoneclient pythonglanceclient pythonnovaclient pythonquantumclient pythoncinderclient pythonswiftclient   we begin testing these in dm 1787  on the ncsa openstack head node, but eventual use within lsst orchestrating workflow would entail these being installed on lsst nodes in the lsst stack.  in this issue we perform a basic installation of these packages into the system space on an lsst node/vm for testing.  these are managed in github, and we install these via 'pip install' onto an lsst vm for initial tests.",4,train
DM-1789,"Experiment with afwtable, meas_base, pipe_{tasks, base}",nan,4,train
DM-1790,Produce detailed prototype & accompanying documentation,nan,4,train
DM-1791,Produce straw-man prototype,nan,3,train
DM-1792,Update documentation and automatic install script w.r.t. new newinstall.sh script,newinstall.sh script has evolved and breaks qserv install procedure.,1,train
DM-1794,Pull distEst package into obs_subaru,"reducing hsc data requires an estimate of the distortion, which is provided by the hsc package distest.  this can be pulled into obssubaru to consolidate code and reduce dependencies.  i propose to treat distest as legacy code, which means i will pull it into obssubaru without major changes to the code style.",6,train
DM-1795,Use anonymous NCSA rsync server to distribute large test datafiles.,"dm1755 has been done before this feature was available. it uses rsync over ssh which require use to have an sshkey on lsstdev.  nsca rsync server can now be accessed with next syntax:  rsync av lsstrsync.ncsa.illinois.edu::qserv/qserv_testdata/datasets/case04/data/deepsource.csv.gz . # list files available in the module named ""qserv"" rsync lsstrsync.ncsa.illinois.edu::qserv  # to add content to this module/group, you can copy files into the following path:   /lsst/rsync/qserv/   rsync over ssh feature should be kept, in order to distribute private data are distributed.",4,train
DM-1796,Improve test coverage for case04,"most of queries used in gbsized case04 return empty results.  these queries should be added:  fjammes@lsstdb2:/src/qservtestdata (u/fjammes/dm1755) $ mysql host 127.0.0.1 port 4040 user qsmaster qservtestcasesuiqserv e ""select count() from deepforcedsource""      fjammes@lsstdb2:/src/qservtestdata (u/fjammes/dm1755) $ mysql host 127.0.0.1 port 4040 user qsmaster qservtestcasesui_qserv  e ""select  from deepforcedsource limit 1""   but, even better, sui team could provide some more interesting query to qserv team in order to improve case04 quality.",3,train
DM-1797,Package flask,"the data access webservice apis are relying on flask, so we need to package flask according to the lsst standards. for my initial testing, i just run ""sudo aptitude install python flask"".  ",1,train
DM-1798,Regression testing of AP Simulator,run the ap simulator to make sure that none of the changes to the ctrl_events package break anything.,1,train
DM-1799,Regression testing of Orca,do some test runs using orca to make sure orca still works after the the changes to ctrl_events.,1,train
DM-1802,remove unused local typedefs,gcc 4.8 now warns about locally defined typedefs that aren't used.  we have a few of these in ndarray and afw::gpu that should be removed.,1,train
DM-1803,S15 Explore Qserv Authorization,"explore authorization centrally: use information generated by parser. either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized.  note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept.",8,train
DM-1804,Study the current SUI requirement ,study the current requirement carefully to make sure they all make sense and we can do it.,4,train
DM-1805,Study the current SUI requirement ,nan,4,train
DM-1806,Study the current SUI requirement,nan,2,train
DM-1807,Study the LSST data products document and give a summary to the team,nan,6,train
DM-1808,Study the current SUI requirement,nan,2,train
DM-1809,Study SUI requirement and summarize all the input from other team memebers,nan,10,train
DM-1810,segfaults in ip_diffim on gcc 4.8,"i'm seeing test segfaults in ipdiffim on gcc 4.8, similar to those resolved on dm 1725, but with no similar smoking gun yet.  preliminary indication is that the problem is actually in measalgorithms.",2,train
DM-1811,Prepare initial content,prepare lse130 content as far as possible without input from the new collimatedprojector calibration plan.,4,train
DM-1812,Determine LSE-130 impact of collimated projector calibration plan,"during a working meeting with robert lupton and chris stubbs, determine the impact on lse 130 of the introduction of the collimated projector for calibration.",8,train
DM-1813,Prepare draft of LSE-130 for Camera and CCB review,produce a reviewable draft of lse 130 based on decisions on calibration operations,4,train
DM-1814,Support Camera CD-2 (mainly re: LSE-130),"provide slides and other information needed for cd2, mainly relative to the open questions around lse130",2,train
DM-1815,Support LSE-130 review by CCB (mainly Camera),"respond to comments, perform revisions to lse 130 as necessary based on feedback from ccb review of the document",4,train
DM-1816,Convert LSE-130 to SysML,"following ccb recommendation of approval of lse 130 draft, convert word draft to sysml and provide a docgen to robert mckercher for final posting. ",2,train
DM-1817,Create and post docgen of LSE-68,"to support discussions with the camera, post a provisional docgen of lse 68 to the appropriate confluence page.  use knowledge from ea training to improve template.",4,train
DM-1818,Support completion of final document,"based on ccb approval of lse 72 on 10 october, support the completion of the final copy of the document for posting on docushare.",1,train
DM-1819,Complete LSE-140 work as needed to produce final document,complete any reviewdriven revisions of lse140 and support the ccb meeting and following final document preparation.,2,train
DM-1820,LSE-140: Collect desired changes for future release,"prepare for a future revision (phase 3) of lse140.  collect issues to be addressed in the revision.  determine if any affect phase 2 scope (which would require a prompt revision).  it is not anticipated that there will be an actual revision of lse140 during the winter 2015 cycle, because additional detail on calibration requirements will not be available in time.",1,train
DM-1822,Fix czar assertion failure,"reported by tatiana: i am encountering this once in a while.   qservczar.log  python: build/rproc/protorowbuffer.cc:69: int lsst::qserv::rproc::escapestring(iter, citer, citer) [with iter = gnucxx::normaliterator/ > >, citer = gnucxx::normaliterator/, std::allocator/ > >]: assertion `srcbegin != srcend' failed.  czar is dead and qserv stops responding after that.    for more details, search in qservl archives mails with ""czar assertion failure"" subject: https:/listserv.slac.stanford.edu/cgi bin/wa for complete description.",4,train
DM-1824,Define issues to be addressed,"work with tcs contacts (jacques sebag, paul lotz, etc.) to define the principal issues",1,train
DM-1825,Produce draft of LSE-75 with agreed revisions,"produce a draft of lse75 with the following agreed revisions:  remove reference to advance notice of pointing, now in lse72  add reference to psf reporting",1,train
DM-1826,"Catch-all epic for essential fixes during DM-W15-1,2,3 ",nan,10,train
DM-1828,Create primary calibration plugins,nan,9,train
DM-1829,Develop General Acceptable Use Policy,don petravick and lee leclair,6,train
DM-1830,Develop Information Classification Policy,"don petravick, lee leclair",6,train
DM-1831,Develop Incident Response Policy,"don petravick, lee leclair",6,train
DM-1832,Develop Data Management Sub-Project Plan and Risk Table,"don petravick, frossie economou",6,train
DM-1833,Develop PMO Sub-Project Plan and Risk Table,"lee leclair, iain goodenow",6,train
DM-1834,Develop EPO Sub-Project Plan and Risk Table,"don petravick, frossie economou",6,train
DM-1835,Develop Camera Sub-Project Plan and Risk Table,"don petravick, richard dubois",6,train
DM-1836,Develop Telescope and Site Sub-Project Plan and Risk Table,"lee leclair, german schumacher",6,train
DM-1837,Package ISP Documents into LSST Standard Format for Control and Delivery,robert mckercher,4,train
DM-1839,Deblend post-merge objects,nan,4,train
DM-1840,Task-level processing for merged objects,nan,5,train
DM-1841,"Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" ","xrootd prevents the worker to return more than 2mb data.  on gbsized data:  mysql host=127.0.0.1 port=4040 user=qsmaster batch  e ""select scienceccdexposureid from scienceccdexposuremetadata""                                                                                                                                                                     error 4120 (proxy) at line 1: error during execution: 1 ref=1 resource(/chk/qservtestcase03qserv/1234567890): 2015012316:27:45, error merging result, 1420, result message md5 mismatch (1)   on integration test case 04:  qserv@clrinfoport09:~/src/qserv (u/fjammes/dm1841 )⟫ mysql host=127.0.0.1 port=4040 user=qsmaster qservtestcase04qserv  e ""select  from deepforcedsource""   error 4120 (proxy) at line 1: error during execution: 1 ref=1 resource(/chk/qservtestcase04qserv/6970): 2015020416:23:43, error merging result, 1420, result message md5 mismatch ref=2 resource(/chk/qservtestcase04_qserv/7138): 2015020416:23:43, error merging result, 1420, result message md5 mismatch ref=3 ( 1) ",5,train
DM-1843,Permit PropertySets to be represented in event payloads,"in the old marshalling code, property sets were representable within the payload of the event.   this was removed in the new marshalling scheme.   there are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code.  at the same time, new new filtering code can not allow this to be added, because the jms headers only take simple data types.",2,train
DM-1844,Test Qserv on SL7,needed to run qserv on cc in2p3 cluster.,2,train
DM-1845,Coordinate implementation of web form for collecting data about existing data sets,"the form is being implemented by the datacat team. need to coordinate (including with the ncsa team which parts are covered by which team), test, fine tune etc.",4,train
DM-1847,Add support for large results in XrdSsiRequest::GetResponseData,getresponsedata needs to handle data sets beyond 2 mb. the problem is discussed in more details in story dm 1841,12,train
DM-1848,SUI work with DB team to define the image query API,ipac sui team will work with slac database team to define the image query apis.  ipac needs to make sure the apis are sufficient to satisfy the ui needs. slac will implement them. .,10,train
DM-1849,Study RESTful API and work with SLAC team  to define image query APIs,nan,2,train
DM-1850,"Discuss, review, and define image query APIs with SLAC team",nan,2,train
DM-1851,Image query API discussion and review,nan,2,train
DM-1852,Image query API discussion ,nan,1,train
DM-1853,Image query API discussion,nan,1,train
DM-1854,SUI propose a structure definition for user workspace,workspace is an integral part of sui. we want to start the discussion and definition of workspace concept and structure.     sui team had several discussions and xiuqin presented the results at the dm ahm at slac. the slides and the discussion notes are here: https:/confluence.lsstcorp.org/display/dm/workspace+discussion,20,train
DM-1855,Specify mechanism for periodic (nightly/weekly) build distribution,nan,3,train
DM-1856,Implement nightly/weekly release automatic distribution - Part I,"this ticket covers code in sqre codekit to do migrate as much of the process of special machines, and git tag repos on the basis of eupspkg manifests.",4,train
DM-1857,Github Transition Plan: Write document for CCB,nan,7,train
DM-1858,Release engineering W15 bucket,bucket epic for activities related to stack releases during w15,24,train
DM-1859,Publish v10_0 release,nan,12,train
DM-1860,Update documentation for v10_0 release,all done bar obtaining some release notes. ,2,train
DM-1861,Workflow tool improvements w15 bucket,"bucket epic for w15 for improvements with jira, hipchat, etc ",12,train
DM-1862,JIRA project for RFCs,nan,3,train
DM-1864,Implement Github transition plan,nan,10,train
DM-1865,Review existing Level 3 documentation,review existing requirements in this area.  find all relevant existing project controlled and other key documents.,4,train
DM-1866,Document as-is Level 3 requirements and conceptual design,"produce a single jumpingoff point for documentation on all aspects of level 3, on confluence.  ensure that flowdown for existing level 3 requirements in sysml is modeled.  describe the highlevel conceptual design.",4,train
DM-1867,tighten control over heterogeneous DictFields,"dm1218 added support for dictfields with heterogeneous item types, which probably allows a bit too much freedom (the rest of pex_config is much more stronglytyped).  instead of passing none to allow any type to be used, we should pass a tuple of supported types.",1,train
DM-1868,Define JSON Results for Data Access Services,"as discussed at https:/confluence.lsstcorp.org/display/dm/dataaccesshangout+20150223, we should support json format. this story covers defining structure of json results for data access services (dbserv, imgserv, metaserv) ",3,train
DM-1871,SUI requirement refinement to define many unclear areas,"in the current sui requirement document, many areas are not clearly defined. we want to put more description and definition for those areas, and  identify and define the missing functions.  the goal is to generate a requirement document for dm review and put it under version control. ",50,train
DM-1872,SUI User workspace specification,"at the 2015 deb dm ahm at slac, sui led a discussion of workspace. https:/confluence.lsstcorp.org/display/dm/workspace+discussion  we want to continue the discussion, understand the user needs, identify the dm groups involved.  the goal is to generate a document to capture the functions requirement of workspace.     the first version of document is here https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=41783931",30,train
DM-1873,SUI 2D data visualization (XY plot),better algorithm in spatial binning to visualize large number of catalog sources plot histogram for tabular data plot basic light curve ,40,train
DM-1874,SUI the alert subscription system specification,"identify parties involved in the alert system generation, broadcast, subscription.  understand the flow of the alert from generation to notifying users. understand the requirement for sui subsystem ""alert subscription and notification"". ",30,train
DM-1875,SUI infrastructure implementation,identify the hardware resources needed at ncsa for short term development and  set up the basic git repository and build system explore multi resolution images display for background iamge,40,train
DM-1876,SUI functional design ,"understand current use cases, collect and define  more use cases. design the major functions of major components in sui, mainly firefly package.",60,train
DM-1877,SUI web interface and Python interaction,"to facilitate  users to interact with firefly visualization components in ipython notebook, to allow users to control the display with python script. ",60,train
DM-1878,"Collect, understand, and define more use cases",this is an on going effort. the collected use cases will be posted at confluence page https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=41784036. ,20,train
DM-1880,Implement RESTful interfaces for Database (GET),"implement restful interfaces for database (see all d  in https:/confluence.lsstcorp.org/display/dm/api), based on the first prototype developed through dm 1695. the work includes adding support for returning appropriately formatted results (support the most common formats). this covers ""get"" type requests only, ""post"" will be handled separately.",5,train
DM-1881,Improvements to web form ,nan,5,train
DM-1883,organize the workspace discussion and present a good proposal,sui team had several discussions and xiuqin presented the results at the dm ahm at slac. the slides and the discussion notes are here: https:/confluence.lsstcorp.org/display/dm/workspace+discussion,10,train
DM-1884,summarize WEBDAV capabilities and past experience using it,"webdav could be a candidate for managing the user workspace.  summarize its capabilities and past experience, collect some use cases will help us to make a better decision.",4,train
DM-1885,Contribute to the workspace capability discussion ,"this include past experience, collection of use cases. ",2,train
DM-1887,HDF5 file format study,"xiquin, loi, trey, and myself discussed hdf5 as a default format to return result set and metadata from lowerlevel database services vs. traditional ipac table. here is the summary:  advantages of ipac table format   simple and humanreadable, contains a single table  fixed length rows (easy to page through)  supported by many astronomical tools   provides a way to pass data type, units, and null values in the header  more metadata can be added through keywords (attributes)  disadvantages of ipac table format    steaming can not be started before all data are received – need to know column width before the table can be written (csv is better alternative)  only alphanumeric and '' characters are allowed in column names (small subset of available characters)  only predefined datatypes and one attribute type (string)  ascii representation requires about twice as much storage to represent floatingpoint number data than the binary equivalent.  advantages of hdf5   can represent complex data and metadata (according to lofar, good to represent time series)  structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes  flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes  access time and storage space optimizations  partial i/o: “chunked” data for faster access  supports parallel i/o (reading and writing)  builtin compression (gnu zlib, but can be replaced with others)  existing inspection and visualization tools (hdfview, matlab, etc.)  disadvantages of hdf5   complex  tuned to do efficient i/o and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes.  requires native libraries (available in prepackaged jars, see below)  not human readable  (?) not yet widely supported by astronomical tools (counterexamples: astropy, idl, more at hdfgroup site)  tools and java wrappers:   jhi5  the low level jni wrappers: very flexible, but also quite tedious to use.  java hdf object package  a highlevel interface based on jhi5.  hdfview  a javabased viewer application based on the java hdf object package.   jhdf5  a highlevel interface building on the jhi5 layer which provides most of the functionality of hdf5 to java. the api has a shallow learning curve and hides most of the housekeeping work from the developer. you can run the java hdf object package (and hdfview) on the jhi5 interface that is part of jhdf5, so the two apis can coexist within one java program. (from stackoverflow answer, 2012)    netcdfjava is a pure java library, that reads hdf5. however, it's hard to keep pure java version uptodate with the standard, does not support all the features.  a way to set up native libraries (3rd option from jhdf5 faq):      ""use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. jar files with the appropriate structure are cisdjhdf5batteriesincluded.jar and lib/nativejar/.jar (one file for each platform). this is the simplest way to use the library.""       ",1,train
DM-1889,S15 Butler (v3),improvements and tweaks to the butler as needed.,19,train
DM-1890,Research support for integrated view of all databases,"metadata store needs to contain information about all databases (data release databases, level 1, level 3 user databases).",11,train
DM-1891,Add support for IPAC table format,implement support for result formatting in ipac table format.,6,train
DM-1893,Research supporting cutout from images with overlaps,research producing cutout from images with overlaps.,10,train
DM-1894,SUI refactor Firefly package,"the team has studied and researched the web ui framework (dm 1148) in w15. a new framework will be decided in february 2015. many classes in firefly package need to be refactored to use the the new framework. this epic in s15 will be our first attempt for it. developers will be involved in this effort: trey roby, loi ly, tatiana goldina, lijun zhang, xiuqin wu",100,train
DM-1896,Design CSS schema to support table deletion,"table/chunk deletion can be an extended process as some worker nodes may be temporarily down. we need to define a process and its supporting structures in css to allow gradual deletion of individual chunks and full tables.  deliverable: a design of a system capable of deleting a distributed table (all chunks, all replicas). it should be possible to create a table with the same name after deletion.",4,train
DM-1897,Modify CSS structure to support table deletion,"modify css structures to support drop table, as defined in dm 1896.",2,train
DM-1898,Consistency checking for table data CSS ,"css data on tables/chunks/nodes is supposed to be consistent at all times. would be nice to have a tool that verifies consistency, probably including checking actual worker state.",4,train
DM-1899,Tool to dump CSS information,css information tree may become large and it would be nice to have a tool to examine that tree or parts of it. something that dumps the tree in user friendly way and allows filtering or summarizing.,2,train
DM-1900,Worker management service - design,"we need to replace direct workermysql communication and other administrative channels with a special service which will control all worker communication. some lightweight service running alongside other worker  servers, probably http based. data loading, start/stop should be handled by this service.",5,train
DM-1901,Re-implement data loading scripts based on new worker control service,once we have new service that controls worker communication we'll need to reimplement workeradmin class based on that.,8,train
DM-1903,Implementation of calibration transformation framework,"following dm 1598 there will be a detailed design and prototype implementation for the calibration & ingest system. this issue covers cleaning up that code, documenting it, having it reviewed, and merging to master.",2,train
DM-1904,Continued footprint improvements,a redesigned api and support for topological operations within the footprint class.  this continues the work started in dm 1107 in w15.  breakdown: jbosch 15%; swinbank 85%,8,train
DM-1905,QSERV issues when working with scisql_s2PtInCircle,"this problem has been adressed un u/fjammes/dm1841.  here's tatiana report:   this error happens on all deepforcedsource queries. (it happens on deepsource too, but not always.)  [20150115 11:02:29] [proxy][4120] error during execution: 1 ref=1 resource(/chk/lsst/6970): 2015011511:01:05, complete (success), 0, ref=2 resource(/chk/lsst/7138): 2015011511:02:19, complete (success), 0, ref=3 resource(/chk/lsst/7140): 2015011511:02:18, error merging result, 0, ref=4 resource(/chk/lsst/730 (1)  query examples:  select  from deepforcedsource where ra>0.4 and ra/0.9 and decl<1.1;  select  from deepforcedsource where scisql_s2ptincircle(ra, decl, 0.5, 1.1, 0.138) = 1; ",2,train
DM-1906,Outline an expandable Python framework for advanced users ,"outline an expandable python framework for use by advanced users in interactive or batch mode. many users should be able to contribute software to the framework following simple api guidelines. also look at configuration and delivery systems for the software. start by looking for existing python software that could be used in the areas of:  database & file access  data analysis frameworks  display of data,, especially in an astronomical context.  reading and writing data in different formats  graphing data locally or over the web  science and astronomy data analysis  event interfaces  vo functionality and connecting with existing software like ds9,, aladin, topcat, etc.  ways to integrate modules with an lsst focus like  agn  large scale structures  galaxies  local volume  solar system  astrostatistics  stellar pops  strong lensing  supernova  transients  weak lensing  camera  configuration and delivery system  ",12,train
DM-1907,Backport HSC multi-band deblend processing,"breakdown: lauren 60%; price 40%  we need to transfer the recent hscside multiband deblender changes to the lsst side, including all hsc issues in this query: https:/hsc jira.astro.princeton.edu/jira/issues/?filter=11603",35,train
DM-1909,Interface design for full focal plane PSF estimation,"breakdown: swinbank 40%; jbosch 30%; rhl 30%  this is mostly design work, but i'd like the goal to be a new python commandline task that repeats as much of the current processccd as necessary to run full focal plane psf estimation, and would serve as a starting point for a futureproof visit processing script.  some issues include:   gather requirements from e.g. desc people as to needed inputs and data flow.   get details of what camera/telescope systems will provide, and figure out how those related to what desc needs.   design classes for camera/telescope engineering data and wavefront information.  figure out how they'll be managed on disk (stored with exposure, part of camerageom, with flats, biases, etc.).   discuss parallelization needs with middleware team, and determine a way forward that lets us design interfaces using future parallelization schemes that don't exist yet.   design python task interface for full focal plane psf estimation.   implement commandline task that calls the full focal plane psf estimation task.   implement placeholder psf estimation subtask that just uses existing psf determiners on single ccds.  this may be too ambitious for the 40 story points we've allocated, and should discuss either adding more effort or reducing the scope.",45,train
DM-1910,"DRP DM-S15-1,2,3 Bugs and Papercuts",breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%,20,train
DM-1912,"DRP DM-S15-4,5,6 Bugs and Papercuts",breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%,20,train
DM-1913,Prototype command interaction with Firefly,nan,10,train
DM-1914,Research Javascript Frameworks: Finish new framework proposal,nan,18,train
DM-1915,Create a deployable installation package for Firefly,"with firefly being opensource, we should provide a simple allin one installation package so a user can quickly setup and deploy an instance the firefly tools web application.",10,train
DM-1916,Fine-tune data access interfaces,"brought up by gregory via comments on the api page.  data release selection in queries: i see that the /db/... queries take a ""?"" query parameter ""db"" with an example value of ""dr1"", i.e., a data release selector.  a couple of remarks:  will this query parameter be provided for all the level 2 image data products, e.g., for retrievals of coadded images? if so, then it needs an equivalent to the m4 ""get /meta/v0/db/"" query.  i assume the ""db"" query parameter defaults to the most recent data release.  will the m4 query return an indication of which ""?db="" value is the current default?  i assume that the numericidentifier components of the various paths are unique only within a single data release.  that means that eventually, in user documentation, we should make sure that they understand that they can't scan through different releases' versions of the same image (for example) just by varying the ""?db="" parameter.     are the identifiers also unique within a particular type (i.e., ""raw"", ""template"", ""coadd"", ""calexp"", etc.)?    distinguishing l1 and l2 versions of reprocessed data products  since most or all of the l1 data products will be regenerated in each data release, the catalog and image apis should presumably allow the user to distinguish between the two.  i see how this could be done for catalogs  the ""?db="" parameter presumably allows selecting something like ""l1"" (for the actively updated level 1 database) in addition to the abovedocumented ""dr1"", ""dr2"", etc.  will the l2 table names for the reprocessed l1 data products be generally expected to be the same as for l1?  (barring the discovery of a serious issue that requires revision of the schema for the reprocessing.) how will the l1 and reprocessed l1 image data products be distinguished?",9,train
DM-1917,Fix missing virtual destructors,the compiler is warning about some derived class hierarchies that are lacking virtual destructors.  we should add at least empty implementations to the base classes of these hierarchies.,1,train
DM-1918,write Unit test and validation classes to validate the FITSreader refactoring ,nan,8,train
DM-1919,Address misc. compiler warnings,"fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change.  this helps keep compiler noise down so its easier to notice ""real"" warnings.",1,train
DM-1920,update shapeHSM wrappers to latest external version,"the hsm shape code has undergone many improvements and bug fixes as part of being included in the galsim package, and we've recently included those in the hsc fork of measextensionsshapehsm (hsc129, hsc1093).  we should transfer those changes to the lsst side before tackling dm 981 (or at least before finishing it).  the story point estimate here is for the work already done on the hsc side (with the usual 50% factor for shared work).  the transfer to the lsst side should be essentially no effort.  to the extent that evm cares about this, the credit should go to [price], even though i ([jbosch]) am doing the transfer.",6,train
DM-1921,Make unit tests use shared libraries,"many (all?) unit tests are currently built as static executables which include all needed object files. this has several issues associated with it:  many files are compiled twice, once as .os files for shared libraries, second time as .o file for unit tests  unit tests do not test actual code in the shared libraries but instead separately built copy of the same code  we should change our procedure and make unit test to link against shared libraries to avoid these problems.",4,train
DM-1923,Setup network for IPMI,nan,2,train
DM-1924,Setup IPMI bastion hosts,nan,5,train
DM-1925,Document how to use IPMI with LSST infrastructure,nan,1,train
DM-1926,Base configuration of NFS servers,install and configure os,3,train
DM-1927,Test new NFS servers,test to confirm servers are configured optimally,15,train
DM-1928,Deploy first of the NFS servers,nan,20,train
DM-1929,LOE - Week ending 12/26/14,nan,4,train
DM-1930,LOE - Week ending 1/9/15,nan,8,train
DM-1931,LOE - Week ending 1/16/15,nan,4,train
DM-1932,LOE - Week ending 1/23/15,nan,8,train
DM-1934,LOE - Week ending 2/6/15,nan,8,train
DM-1935,LOE - Week ending 2/13/15,nan,8,train
DM-1936,LOE - Week ending 2/20/15,nan,8,train
DM-1937,LOE - Week ending 2/27/15,nan,8,train
DM-1939,Define instrumental inputs to PSF estimation,"define inputs needed to build physicallymotivated psf models beyond what's contained in the image data and the current camerageom.  this includes:   static engineering data from lab tests   slowlyvarying engineering data (measured daily, weekly, etc.)   pervisit auxiliary data from telescope and camera (including, but not limited to wavefront sensor data).  the focus here should be on apis, not the technical details of the data; we want to define class hierarchies (perhaps some polymorphic ones) that can be used to pass this data around in the future.  we also want to identify and characterize any preprocessing that needs to be done before they can be used for psf estimation.  experts who should be consulted include  (who is in charge of making sure the camera and telescope interfaces to dm are welldefined), camera/telescope people he recommends, and people from the lsst desc who have worked on physical psf estimation and know something about the things they'll want (e.g. michael schneider, aaron roodman).  this is essentially a requirementsgathering task, so the output should be a confluence page, not code.",10,train
DM-1940,API for instrumental inputs to PSF estimation,"following the requirementsgathering in dm1939, come up with classes that can be used to pass the needed inputs to the psf estimation code, and at least sketch out roughly how they will need to be managed by the butler and loaded by the framework code.  the output of this ticket should be an api design in confluence (possibly on the same page used for dm 1939), and an associated rfc.",10,train
DM-1941,Organize SUI design discussions,organize the sui team for web ui discussions to capture as much functions and components as possible.  a draft design document should be produced out of those discussions. ,10,train
DM-1942,Test data development and HSC stack integration,"breakdown: price 20%; lauren 80%  this epic is focused on general stack testing and integration on hsc data, with a goal of getting lsstside reductions of hsc data to the same level of quality  and robustness currently present on the hsc fork of the stack, which will involve a combination of backporting minor fixes from the hsc codebase and tuning parameters for lsstside algorithms that supercede their hscside counterparts.  this will allow sciencelevel algorithms to be tested using hsc data, and will provide functionality important for sciencegrade tests on other real data (such as the ongoing cfhtls reprocsessing at in2p3).  this effort will be focused on singleepoch processing (i.e. processccdtask), with coadd level processing a stretch goal dependent somewhat on the completion of astrometric calibration work at uw.",45,train
DM-1943,HSC backport: convert Peak to PeakRecord,"this issue covers transferring all changesets from https:/hscjira.astro.princeton.edu/jira/browse/hsc1074 and its subtasks, as well as:   an rfc to propose the api change, and any requested modifications generated by the rfc.   additional fixes to downstream code that's broken by this change (hscside changesets should be present for most of downstream fixes, but perhaps not all).",8,train
DM-1944,HSC backport: guarantee consistent handling of peaks in deblender,this issue covers transferring changesets from:   https:/hscjira.astro.princeton.edu/jira/browse/hsc134   https:/hscjira.astro.princeton.edu/jira/browse/hsc1109   https:/hscjira.astro.princeton.edu/jira/browse/hsc1083  ,4,train
DM-1945,HSC backport: multiband processing for coadds,"this issue includes transferring changesets from many hsc issues:   hsc1060   hsc1064   hsc1065   hsc1061  most of this is in multiband.py in pipetasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obssubaru).  however, before we make these changes, we'll need to open an rfc to gather comments on the design of this task.  we should qualify there that this is not a longterm plan for consistent multiband processing (which we'll be starting to design on dm1908), but a step towards better processing in the interim.  note: while i've assigned this to [lauren], as i think it will be very helpful for her to get familiar with this code by doing the transfers, the rfc will have to involve a collaboration with [jbosch], , and bob armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it.",8,train
DM-1946,HSC backport: low-level Footprint merge code,"this is a transfer of changesets from the follow epics:   https:/hscjira.astro.princeton.edu/jira/browse/hsc1020   https:/hscjira.astro.princeton.edu/jira/browse/hsc1075: only the afw changes  because this is purely an addition to the interface, and we're planning to redesign that interface in dm 1904, i don't think we need an rfc here.",4,train
DM-1947,Add support for async request cancellation to xrdssi,nan,12,train
DM-1948,S15 Implement Query Cancellation,add support for query cancellation.,32,train
DM-1950,Add abstraction in czar for unit tests,add hooks in czar that will let us build unit tests.,8,train
DM-1951,Unit test for query cancellation,nan,8,train
DM-1952,"Change log priority for message ""Unknown column 'whatever' in 'field list'""  ",next message should be logged with error priority:   0204 15:08:03.748 [0x7f1f4b4f4700] info  foreman (build/wdb/queryaction.cc:250)   [1054] unknown column 'whatever' in 'field list'   ,1,train
DM-1953,Post meas_base move changes to Kron,"these are to note leftovers from dm982.  they could be done in a single issue. 1.  i commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework.  2.  jim asked me to replace the computesincflux which is currently in psfimage.cc in measalgorithms with a similar call in measbase/apertureflux.cc.  i did not do this because it became rather complicated, and can just as easily be done when the measalgorithms routine is moved or removed.  basically, the templating in apertureflux is on pixel type, whereas in measalgorithms it is on imaget (where imaget is not necessarily a single class hierarchy   e.g., image and maskedimage).  so i left this for now.",1,train
DM-1954,HSC backport: deblended HeavyFootprints in forced photometry,"this is a transfer for changesets for https:/hscjira.astro.princeton.edu/jira/browse/hsc1062.    unlike most of the hsc backport issues for multiband deblending, these changes will require significant modification the lsst side, because we need to apply them to the new forced measurement framework in measbase rather than the old, hsconly one in measalgorithms and pipe_tasks.    also include https:/hscjira.astro.princeton.edu/jira/browse/hsc1256, https:/hscjira.astro.princeton.edu/jira/browse/hsc1218, https:/hscjira.astro.princeton.edu/jira/browse/hsc1235, https:/hscjira.astro.princeton.edu/jira/browse/hsc1216.",20,train
DM-1955,International Network Design and Implementation,fiu/amlight is expected to provide a full capacity 3 x100 gbps link by fy17.  ,12,train
DM-1956,REUNA will provide a “pre-operations” link between La Serena and Santiago,reuna will provide la serena   santiago links at full capacity 100 gbps link by fy17 and a 40 gbps secondary link by fy20.  this link will support testing and development prior to that time.,14,train
DM-1957,Chilean National Network Design and Implementation,refer to reuna mrefc sub award contract for deliverable details.  this covers non contract work by the aura/lsst.,20,train
DM-1959,S15 Data Distribution & Replica Mgmt Prototype,this epic covers building an initial prototype of the data distribution and replication system. the design is covered through dm 1060,30,train
DM-1961,Prepare data set for large scale tests,"load data set for large scale tests (duplicate, partition, load)",10,train
DM-1962,Run large scale tests,coordinate running large scale tests,6,train
DM-1964,Parallelization requirements for PSF estimation,"we need to gather algorithmic ideas for how full focal plane psf estimation will work from a parallelization and data flow standpoint, and discuss with the middleware team how these should be handled from an interface standpoint.  questions include:   will we need to do significant crossccd image processing or require significant memory for these tasks?  if so, should we structure this via message passing between ccdlevel processes, or scattergather?   assuming a scattergather approach, will we need multiple scatter/gather iterations when processing a single visit?   how much data will be passed between threads/processes?  would this include complex serializable objects, or just pod arrays?   will different psf estimation plugins will have different parallelization requirements?  or, can we define the plugin interfaces at a low enough level that parallelization can be handled by the framework?  if plugins do need to control their own parallelization, how do we make parallelization interfaces accessible to the plugins?",8,train
DM-1965,Python interface for full-visit PSF estimation,"create a python interface for a pluggable psf estimation system that supports algorithms that will operate over full images.  this should include a sketch of how a calling command line task, and placeholders for parallelization interfaces that may not yet be finalized.  the output of this issue is a completed rfc on the design.",6,train
DM-1966,Command-line driver and placeholder implementation for PSF estimation,"create a commandline task that makes use of the new psf estimation interface, duplicating as much of processccdtask's functionality as necessary to provide the inputs to psf estimation (i expect this new task to ultimately replace processccdtask).  this may have to include workarounds or temporary implementations for parallelization features that are not yet available.  create a simple psf estimation placeholder that simply uses existing singleccd psf determiners.",6,train
DM-1968,CModel flux validation and testing,"investigate the performance of the new version of the cmodel code on various test datasets, including hsc data (following dm 1942), sdss, and possibly cfht data.  breakdown: lauren 100%",35,train
DM-1969,Create a kind of Wcs that encapsulates a TAN WCS and a distortion model,we can simplify the astrometry solver if we have a wcs that encapsulates a pure tangentplane wcs and a distortion model that takes converts between pixels and tan_pixels. this is useful because at the early stages of processing raw data we have a tan wcs from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry).  the result will be a wcs whose sky/pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent plane wcs that is currently available. this will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.,4,train
DM-1973,Build 2015_02 Qserv release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-1974,"Fix enclose, escape, and line termination characters in qserv-data-loader","add this string to mysql loader 'load data infile' command:     q += ""enclosed by '%s' escaped by '%s' lines terminated by '%s'"" % (enclose, escape, newline)  and add params in cfg file.",2,train
DM-1977,S15 Implement Fully-RESTful Data Access Web Service,"improvements to the skeleton of data access web service built in w15. make all responses fully restful (including results, and errors, setting response headers). fine tune api, add support for versioning, unit testing.",39,train
DM-1978,Research and Document API Versioning,"research and document versioning of the restful api (through flask blueprints). in particular, need to understand how to avoid code duplication between different versions of api.",4,train
DM-1979,Add SQLite-based v0.1 unit testing for metaserv,"add unit tests for the restful flask based api. i think it'd be most useful if we could  a) load some test data into underlying metaserv  b) run programmatically things like ""curl  h accept:text/html http:/127.0.0.1:5000/meta/v0/db/l2/dcw13stripe82/tables"" etc (more examples in dax_ serv/readme.txt) and verify that we got what we expected. do it for both json and html.",4,train
DM-1980,Add error handling for webserv,add basic error handling for the restful flask based api,6,train
DM-1981,Improve security for mysql in python,"revisit all code that talks to mysql from python to use parameter bindings instead of direct string substitutions. in practice:  conn.execute(""select  from t where name=%s"" % thename)  should be replaced with  conn.execute(""select  from t where name=%s"", (thename,))   for details, see http:/mysqlpython.sourceforge.net/mysqldb.html#someexamples ",5,train
DM-1982,Fix JDBC timestamp error,jdbc driver returns an error on next query:   sql> select   from scienceccdexposure [20150206 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [20150206 13:39:37] [s1009] cannot convert value '00000000 00:00:00' from column 32 to timestamp. [20150206 13:39:37] [s1009] value '[b@548997d1' can not be represented as java.sql.timestamp ,1,train
DM-1987,Redesign/Refactor WCS and Coord,"%50 ksk, %50 ro currently wcs is mutable and coord objects are heavyweight.  refactor wcs to be immutable and make coord less heavyweight.  include lists of coord objects.  it's possible astropy could inform in that area.  also, remove tanwcs in favor of tansipwcs since tanwcs can have sip terms.",40,train
DM-1988,Update analysis tasks: diffim and snap combination,"50% ksk 50% ro  a small amount of this work is in 02c.03.01 the diffim task needs to be looked at.  it needs to be updated to use current mechanisms.  it should also be refactored to split out some of the procedural code into methods.  in a similar task, the lsstsimisrtask needs to include a real snap combine step.  currently, one of the snaps is dropped on the floor.  for this round just implement naive snap addition and morphological cr rejection.",43,train
DM-1989,Define API for Stack Astrometric Calibration,"70% ro 30% ksk this should also include a minimal implementation.  this should be done with an eye toward photometric calibration.  prerequisite: get multi epoch (multiband?) catalogs of centroids from some trusted source (cfht, hsc?).  1. load all stars that overlap a patch for all epochs. 2. associate all stars on each chip.   2a. implement kd tree 3. fit model for rigid chip system  optics  atmosphere.  eigen for sparse model fitter.   3a. allow for external catalog.   3b. this could include a class to fit xytransforms 4. turn result into wcs.  so maybe a down scope for a single cycle epic is to get matches and interfaces for models and solvers. ",75,train
DM-1990,Research DCR in the context of DiffIm including possible algorithms for mitigation.,"it is not clear how template coadds will be built.  this includes understanding the data necessary to generate a template for the entire sky.  this epic is to identify possible techniques as well as the risks associated with each technique.    this does not need to pin down the exact algorithm or the specific selections, but should inform what further development is necessary to avoid putting alert generation at risk.",19,train
DM-1991,Refactor Approximate and Interpolate classes,100% ro a base class for this will be created in dm740 as a part of epic dm85.    this epic is to implement the classes to replace the original functionality.,34,train
DM-1992,SQuaRE Support,50% ksk 50% ro square has asked that we leave 20 sp free per cycle to help out.,20,train
DM-1994,Story point display and roll-up in epic display,"i understand that there is a pending request to display the story points for individual story issues in the minitable in which they are displayed for an epic.  it would also be useful to see a rolledup total of the story points for the defined set of stories  so that, among other things, this could be compared to the story point value for the epic.  ideally the story points for the rollup might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already   i don't care which as long as the definition is clear).",1,train
DM-1996,Define faulty/consistent states and recovery process,"whiteboard session(s) to gather a list of invariants / principles that define a consistent state, a list of fault conditions, and the steps in failure recovery.",12,train
DM-1997,Data transport mechanism for data distribution,"decide on a transport mechanism for data (bit torrent, scp, or ?). we must take into consideration whether the data source matters in this choice (e.g. tape vs known good node), as well as how to identify that a data source is the correct one (e.g. via checksums and sequence numbers).",12,train
DM-1998,Architecture for failure detection and resolution,"come up with an architecture for detecting failure or non nominal conditions (e.g. under replication). the core question to resolve is whether we go with a distributed approach, or with centralized control.",16,train
DM-1999,Research existing theory and prior art,"peruse the distributed systems literature for prior approaches to this problem, and examine existing system implementations for components/ideas we could reuse.",10,train
DM-2000,Document data distribution/replication plan,"produce an overview document that explains our definitions, architecture and strategies for dealing with data distribution and replication. ",10,train
DM-2001,Define strategy for adding and removing data ,define a strategy (push/pull distributed/decentralized) for recognizing incoming data and cleaning up/removing stale/deleted data. is data ingest just another form of failure recovery?,10,train
DM-2002,Define data distribution/replication testing strategy,"once we decide on a design for data distribution / replication, we should come up with a test plan.",4,train
DM-2003,Package Reorganization (Science Pipelines),"breakdown: jbosch 50%, swinbank 50%",38,train
DM-2005,switch ndarray to external package,there is already an external ndarray project on github (we've been using a fork of that).  we should merge the forks and switch to using the external package. ,2,train
DM-2006,"merge ""basics"" packages","create detailed rfc and implement merge of base, utils, and daf_base.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",2,train
DM-2007,separate pex_exceptions from base and rename,"remove dependency on base from pex_exceptions and rename to just ""exceptions"" (after rfc).  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",1,train
DM-2008,Move Wcs from afw::image to afw::coord,create rfc and implement move of wcs from afw::image to afw::coord.  see https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning,1,train
DM-2010,move Jarvis/shapelet code to legacy package,"create rfc and remove jarvis/shapelet package from meas_algorithms, into new legacy sci package.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",1,train
DM-2011,"move Psf, Kernel code to new afw::convolution subpackage",create detailed rfc and implement move for these packages.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning,2,train
DM-2012,split pipe_base into command-line and non-command-line components,"create detailed rfc and implement package split, to separate basic tasks (to be used as e.g. subtasks) from cmdlinetask and argumentparser.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",1,train
DM-2013,Implement image stitching,"this story involves implementing code that stitches images, simple case that does not involve tract boundaries. more advanced case in covered in separate ticket. we will need to determine wcs information for the target images.",6,train
DM-2014,Create interface and utility package for single-frame/forced processing,create detailed rfc and implement move of interface and utility code from multiple existing packages to new package.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning,2,train
DM-2015,Design and implement RESTful API for image stitching and rotation,nan,4,train
DM-2016,Split PSF estimation and PSF model code into separate package,create detailed rfc and implement move of concrete psf estimation code and psf subclasses from meas_algorithms to separate package.  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning,1,train
DM-2017,rename packages with minimal reorganization,"measdeblender, ipisr, measastrom, measmodelfit, and ipdiffim do not require major refactoring to fit into the new package reorganization, but they should be renamed (with sciprefixes).  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",1,train
DM-2018,Split measurement plugins into separate packages,"create detailed rfcs and implement splitting measurement plugins into separate package.  may want one package for extremely basic plugins, always used plugins (pixelflags, transformedcentroid, etc).  see also https:/confluence.lsstcorp.org/display/dm/summer2015packagereorganization+planning",2,train
DM-2019,Split coaddition code and single-frame/forced command-line drivers,this should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).,2,train
DM-2020,Research how to support L3,research implications of having to deal with updatable level 3 data.,12,train
DM-2021,Architecture for supporting small non-partitioned tables,"some tables, like exposure, provenance, will not be partitioned, and the current plan is to either replicate them on each node, or store on shared file system. need to decide how it will be dealt with.",6,train
DM-2022,Research software deployment on the qserv cluster,discuss and decide how to deploy and upgrade qserv software,10,train
DM-2023,Investigate procedures for package reorganization,e.g. develop script to handle bulk namespace changes.,4,train
DM-2024,DRP S15 support for SQuaRE,breakdown: jbosch 16%; lauren 16%; rhl 20%; pgee 16%; price 16%; swinbank 16%,20,train
DM-2025,FY15 Key Performance Metrics,"collect data, compile scripts, perform measurements as necessary to report figures in respect of the fy15 key performance metrics.  breakdown: lauren 50%; rhl 50%",16,train
DM-2026,Support Exposure Use Cases,development in support of exposure use cases,19,train
DM-2027,Implementing stitching multiple patches across tract boundaries in a coadd,nan,4,train
DM-2030,refactor afw Swig to improve build times,"i have an idea for how to improve swig build times that we should get vetted (and possibly improved upon) by a true swig expert (even if that costs a bit of $$).  this issue includes vetting that idea (splitting up classes into multiple per package module builds), getting it through the rfc process, and implementing it in afw.",10,train
DM-2031,Add image-query related KPIs to the plan,"existing plan in ldm 240 does not mention image related kpis. need to come up with a road map, and propose kpis. this should be synchronized what realistically ncsa cluster can deliver in any given fy.",5,train
DM-2034,FY19 Setup Database for Deep Drilling,nan,53,train
DM-2035,FY18 Setup Calibration Database,"need to think through issues related to supporting calibration. schema, requirements that will require special optimizations.",39,train
DM-2036,S17 Build Prototype of AlertProd and L1 User Database,"build a working, nonoptimized prototype of the http:/ldm135.readthedocs.org/en/master/#alertproductionanduptodatecatalog.    deliverable: working, non optimized prototype of alertprod database.",79,train
DM-2037,X16 Revisit Design of AlertProd & L1 Db,"revisit the http:/ldm135.readthedocs.org/en/master/#alertproductionanduptodatecatalog, including schema, indexing, partitioning, synchronization, replicating, fail over. verify that the latest requirements match the design. this epic will likely involve experimenting and lightweight standalone prototyping related to parts of the system that might be non trivial to scale or to implement.    deliverable: a refreshed design document for alert production and l1 user database.",19,train
DM-2038,FY17 Design Internal DRP Database,"internal drp db will be used to store   all bookkeeping (provenance, what run what did not, etc)  intermediate data products (might be larger than final data products),    a subset of data (what we need by drp), eg foorprints of objects  internal drp db might need its own spatial engine.  it is expected that sdqa will run on that database.   need to think through issues related to supporting internal data release production. schema, requirements that will require special optimizations.  need to define reliability requirements.  in limited cases pipelines might want to use internal db (instead of files). example: select all objects from a given region. need to understand query load and complexity coming from drp.",79,train
DM-2039,FY18 Revisit Design of L3 Support in Qserv,need to think through issues related to supporting level 3 databases,100,train
DM-2040,FY19 Design Next-to-database Data Analysis System,need to design the system that will allow users run their own custom data analysis next to database.,100,train
DM-2041,FY19 Implement Next-to-database Data Analysis,need to implement the system that will allow users run their own custom data analysis next to database.,100,train
DM-2042,W16 Improve Data Provenance Design,"we have a detailed design of the provenance, described at https:/dev.lsstcorp.org/trac/wiki/db/provenance. work covered by this epic involves:  1. revisiting the design and tweaking it as necessary. in particular:   describing in more details interactions with key data producers (drp, alertprod, calibration, l3 data brought in by users).   estimating the size of provenance data    considering querying the provenance data    2. evaluating existing offtheshelf provenance systems/tools.    deliverable: a document describing data provenance architecture / schema supported by a standalone proofofconcept prototype.",69,train
DM-2044,Catch all epic for essential fixes in Science Pipelines DM-W15-5,nan,10,train
DM-2045,server side preparation for  histogram plot (1),convert necessary code to make it possible for a javascript component to place a json request to the server and to parse the resulting rawdataset.  ,6,train
DM-2046,Client side plot display for histogram," create a react javascript component, which takes the data and renders histogram.   make it possible to call this component from gwt code, using experimental jsinterop technology in gwt 2.7",10,train
DM-2047,"SUI Investigate L3 data/tools requirements, evaluate potential tools ",there are many overlap areas in l3 data analysis tools with the general science user tools. we want to identify those requirements and needs to help making sui components adaptable for l3 data production and analysis.,30,train
DM-2048,Start requirements gathering for pipeline QA visualization needs  ,gather use cases for pipeline qa visualization tools. we want to build the sui components in such a way that they could be used to support qa needs. ,36,train
DM-2049,SUI Build the visualization components that could be used independently,"currently we identified three basic components: image visualizer,  tabular data display,  2d xy plot. all three could share the data model and provide inter activities between the components.  ",80,train
DM-2050, Integration and test monitoring architecture Part I,"[retitled to better capture cycle scope]    develop and deploy a layer to capture the outputs, initially numeric,  of integration testing afterburners such as sdssdemo, hscdemo, and  others developed this cycle. also capture meta information such as  execution time and memory footprint. propose log format to standardise  production of such informations. investigate notification system based  on trending away from expected values. investigate data provisioning  of integration tests such as storage of test data in githublfs.    [75% jmp 25% jh]        ",100,train
DM-2051, Firefly-based data display for SQuaSH - Part I,"[epic retitled to better reflect cycle scope]    this epic covers work relating to working on the visualisation side of  the science qa analysis harness. it is a timeboxed effort to come up  to speed with firefly in particular, evaluate it against our needs,  and provide any feedback to the firefly team. some prototyping of  visualising integration dataset products will also be involved.     [af 100%]         ",45,train
DM-2052,Maintain list of OSes that pass build and integration testing ,"provide an automatiically generated and updated pages showing operating systems that are successfully building  and integrating the stack from source.   [fe at 75%, jh at 75%]",20,train
DM-2053,Specify system for performing CI on Docker stack containers,investigate how we can ci first party docker containers with runnable stacks    [jh 100%],30,train
DM-2054,Release engineering  Part One,"bucket for public stack releases  [fe at 75%, jh at 75%]",40,train
DM-2055,Miscellaneous service support improvements,"jira, comm toos,  etc for dm and  non dm teams (indicate)    in order to avoid fractional story points, some 0    [fe at 75%, jh at 75%]",16,train
DM-2057,Attend Scale 13x conference,"attend database talks, in particular the maxscale proxy talk (http:/  if anyone has questions they would like me to ask, please post them here as well.  i will post notes to this issue. ",2,train
DM-2058,Data loader should always create overlap tables," we have discovered that some overlap tables that are supposed to exist were not actually created. it looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. situation is actually symmetric, there could be non empty overlap table but empty/missing chunk table. when we create one table we should always make another as well. ",2,train
DM-2059,Clean up QuerySession-related code in czar,"(created in response to dm211) this ticket should address the following inelegancies in the qservczar.    querysession>querypipeline. the ""session"" abstraction has moved to a better place. the iterator portion should be shifted into its own separate class, though perhaps still associated with querypipeline. the iterator portion's new home should be amenable to eventually moving the actual query materialization to the worker, though we shouldn't introduce new abstractions until we are actually ready to move the substitution/materialization to the worker.   querycontext needs to be split into incoming external querycontext and a sort of queryclipboard for passing information between analysis/manipulation plugins. eventually, i imagine a chain/tree of them attached to the select statements themselves in order to represent subquery scope nesting (which is complicated to represent and to reason about nesting and the resulting namespace resolution is tricky), but i don't think we should try doing the chaining in the first phase. for this ticket, create queryclipboard to hold the portion for interchange between analysis plugins. query analysis plugins would then pass this object (which points at an immutable? querycontext) between themselves. queryclipboard probably should live in qana, querycontext in query (unless there is a good reason to move it).   ",8,train
DM-2060,Rename TaskMsgFactory2,rename taskmsgfactory2 to taskmsgfactory.    please see dm 211 for more information.,1,train
DM-2061,Port fault-recovery testing code to XrdSsi,"please see dm211 for the origin of this ticket.  billc put in code to introduce random errors in query dispatch as part of working on code to recover from faults. in the port to the xrdssi api, we did not port this code. this story is to introduce the ability (compiletime configurable, if not commandline or dynamically configurable) to simulate these sorts of faults to exercise the faultrecovery (confined to retrying on transient ish failures) code.",12,train
DM-2063,Creates overlap tables even if empty while loading data,"query execution expects all chunk and overlap tables to exist, even if they are empty. in short term, that means loader should:  look at all chunks and add corresponding overlap chunks,  look at overlap chunks and add missing empty chunk table ",2,train
DM-2065,FY17 Implement Data Verification Tool,"need a tool for verifying whether data is in consistent stage (e.g., right after loading, after some upgrades, in general at any given time).  the list of things to check include:  empty chunk file,  xrootd exported db,  data tables  overlap tables,  data_0123456789 tables  chunkid, subchunkid columns existence  some of the above can be automatically fixed on the spot when problem is discovered.",90,train
DM-2066,Add test case to catch missing empty chunks or overlaps,discussed at db hangout 20150218.   we need a use case to test for missing empty overlap chunk tables and/or empty chunk tables.,2,train
DM-2069,FY17 Design L2 Catalog Swap/Release Automation,need to think through the issues related to releasing l2 catalog / swapping a new one in place of an old one,54,train
DM-2070,FY17 Build AP-ready Data Provenance System,"improvements to the first version of the standalone prototype built through dm2042. discussions with the application team on capturing provenance and integrating drp with the provenance system. add scaffolding / unit tests that will simulate data producers, in particular drp.  deliverable: drpready system for capturing provenance.",79,train
DM-2071,FY18 Integrate AlertProd with Data Provenance,integrate alert production with the provenance system.,56,train
DM-2072,FY17 Implement Async Queries in Data Access Web Services,"work includes:   asynchronous requests, request status, retrieving results for dbserv and imgserv",26,train
DM-2073,FY19 Implement Partial Query Results,nan,100,train
DM-2075,S17 Improve ImageServ,nan,9,train
DM-2077,W16 Add Support for Multi-table Shared Scans,implement multitable shared scans. ensure that sharedscans are not delaying interactive queries. the baseline architecture of the shares scans are described in http:/ldm135.readthedocs.org/en/master/#shared scanning.,100,train
DM-2078,F16 Qserv KPMs,nan,24,train
DM-2079,F17 Run Large Scale Qserv Tests,nan,26,train
DM-2080,F18 Run Large Scale Qserv Tests,nan,26,train
DM-2081,FY19 Implement Missing Features in Qserv for L3,need to think through issues related to supporting level 3 databases,100,train
DM-2082,FY20 Improve Design of Next-to-database Data Analysis,need to implement the system that will allow users run their own custom data analysis next to database.,100,train
DM-2083,FY18 Demonstrate Fault Tolerance,nan,100,train
DM-2084,FY18 Implement Basic Resource Mgmt for DB,"includes things like query throttling per user for all databases (l1, l2, l3)",54,train
DM-2085,FY19 Optimize Resource Mgmt for DB,nan,79,train
DM-2086,FY18 Implement Basic Resource Mgmt for Images,nan,54,train
DM-2087,FY19 Optimize Resource Mgmt for Images,nan,79,train
DM-2088,W16 Distributed Loader - Research,"in production, we will need a distributed loader that will be capable of loading entire data set produced by drp within 2448 hours. this epic involves researching all the needs, requirements and constraints, and exploring what the best architecture for a distributed loader would be. related doc: http:/ldm135.readthedocs.org/en/master/#dataloading",28,train
DM-2089,W16 Distribution and Replica Mgmt Prototype v2,"this epic involves building a complete, working prototype of the qserv data distribution and replica management.",100,train
DM-2090,FY18 Implement L2 Catalog Swap/Release Automation,need to think through the issues related to releasing l2 catalog / swapping a new one in place of an old one,100,train
DM-2091,FY17 Add Support for Managing Per-user Access for DB,nan,53,train
DM-2092,FY18 Add Support for Managing Per-user Access for Image and File Archive,nan,79,train
DM-2093,FY18 Integrate Qserv with EFD,nan,79,train
DM-2094,Port metaREST.py to db,"metarest_v0.py in metaserv is currently using mysqldb instead of going through the db api, because we need to use parameter binding for security reasons. we should switch to using db, once the db interfaces will support it. ",1,train
DM-2095,Port dbREST.py to db,"dbrest_v0.py in dbserv is currently using mysqldb instead of going through the db api, because we need to use parameter binding for security reasons. we should switch to using db, once the db interfaces will support it. ",1,train
DM-2096,Long term database work planning,long term planning (updating ldm 240).,8,train
DM-2097,Package andyH xssi fixed version (>2MB answer pb) in eups,"see dm1847  andy made a patch, it'd be good to the xrootd we use for our stack.",1,train
DM-2101,FY18 Revisit L2 Catalog Schema,revisit the baseline schema,53,train
DM-2103,FY18 Implement Internal DRP Database,implement internal drp database as designed in dm 2038,60,train
DM-2107,FY17 Improve ImageServ,nan,80,train
DM-2108,FY19 Demonstrate Qserv Fault Tolerance,including multi master failover,100,train
DM-2110,FY19 Optimize Partitioning Granularity,"we have been always talking about having ~20k chunks per table, and it was driven primarily by spreadsheet based analysis. we need to look in more details into that, and perhaps even change the model if needed, e.g., introduce  different partitioning for larger tables, like forcedsource.",53,train
DM-2111,FY17 Improve Query Coverage in Qserv,currently qserv supports only a limited subset of queries. we need to make sure it supports all queries that users need to run.,53,train
DM-2112,FY18 Improve Query Coverage in Qserv,currently qserv supports only a limited subset of queries. we need to make sure it supports all queries that users need to run.,79,train
DM-2113,"FY17 Support Explain, Show, List Commands","implement http:/dev.mysql.com/doc/refman/5.0/en/explain.html and http:/dev.mysql.com/doc/refman/5.0/en/show.html commands for qserv. also, commands such as ""list tables"" will need to be intercepted and overloaded. ",53,train
DM-2115,FY19 Implement Multi-master for Qserv,nan,100,train
DM-2116,FY19 Make Database Secure,"revisit security issues, such as sql injections, detecting and shielding from dos attacks, etc.",79,train
DM-2117,FY19 Build/Setup Basic Qserv Monitoring,frontend/worker health monitoring (and management?),79,train
DM-2118,FY18 Implement Failover for L1 Database,"need to implement and test failover  a failure of the master copy of l1 database, and automatic fail over to a replica. the design of the alert production l1 database is covered http:/ldm135.readthedocs.org/en/master/#alertproductionanduptodatecatalog.",80,train
DM-2119,W16 Optimize Secondary Index - Research,"work on the secondary index (objectid > chunkid / subchunkid mapping). this needs to be scalable to 40b entries. since we are planning to ingest all data from drp in <2 days, building should take <2 days. this epic involves researching applicable technologies (including experimenting with most promising ones). deliverable: proposed technology / architecture along with measures performance at production scale (40 b entries). ",48,train
DM-2123,FY18 Add Support for Non-partitioned Tables,"non partitioned tables will need special attention. options include: a. replicating them on each worker node b. keeping them on a shared file system c. federating  need to thing through these issues, pick the best architecture and implement it.",79,train
DM-2124,FY17 Revisit Qserv Deployment on Cluster,nan,60,train
DM-2125,FY18 Design Qserv Software Upgrading,need to understand how to do software update for qserv ,26,train
DM-2129,FY19 Improve Query Coverage in Qserv,nan,90,train
DM-2130,FY20 Improve Qserv Monitoring,frontend/worker health monitoring (and management?),79,train
DM-2131,Resolve compiler warnings in new measurement framework,"when building meas_base, or any other measurement plugins which follow the same interface, with clang, i see a bunch of warnings along the lines of:   in file included from src/apertureflux.cc:34: include/lsst/meas/base/apertureflux.h:197:18: warning: 'lsst::meas::base::aperturefluxalgorithm::measure' hides overloaded virtual function       https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20284390; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should.  while we're at it, we should also fix:   include/lsst/meas/base/apertureflux.h:233:1: warning: 'aperturefluxresult' defined as a struct here but previously declared as a class       [wmismatchedtags] struct aperturefluxresult : public fluxresult ",1,train
DM-2136,W16 Understand Async Queries in Qserv,"understand how disruptive the changes related to implementing asynchronous queries will be for qserv.    delivarable: brief description outlining changes needed, with story point estimate.",10,train
DM-2137,Add parameter binding to db interface,nan,1,train
DM-2138,Validate user query in dbREST,need to validate query (from security standpoint that user enters through rest api.,1,train
DM-2139,Support DDL in MetaServ - implementation,"ddl information is embedded as comments in the master version of the schema (in ""cat"" repo). currently we are only using it for schema browser. this story involves building tools that will load the ddl schema into metaserv. design aspects are covered in dm 1770.",8,train
DM-2141,"Add meas_extensions_shapeHSM to lsstsw, lsst_distrib","measextensionsshapehsm has just been resurrected from bitrot, and should be included in our distribution.    contrary to dm 2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and galsim there.",1,train
DM-2148,General OpenStack Learning,nan,6,train
DM-2149,Setup spare test hardware for OpenStack testing,nan,20,train
DM-2150,Test Ubuntu OpenStack,nan,6,train
DM-2151,Test Mirantis OpenStack & Fuel,nan,6,train
DM-2152,"Figure out OpenStack networking (vLAN, routing, etc)",nan,6,train
DM-2153,Figure out OpenStack integration with LDAP,nan,17,train
DM-2155,Log fails on uniccode string,"log is currently failing if we pass unicode string, it is easy to reproduce by doing: log.info(u""hello""). it fails with:     file ""/home/becla/dataarchdev/linux64/log/mastergfab0203bd33/python/lsst/log/log.py"", line 103, in info     log("""", info, fmt, args, depth=2)   file ""/home/becla/dataarchdev/linux64/log/mastergfab0203bd33/python/lsst/log/log.py"", line 94, in log     getfuncname(depth), frame.flineno, fmt % args)   file ""/home/becla/dataarchdev/linux64/log/master gfab0203bd3+3/python/lsst/log/loglib.py"", line 648, in forcedlogiface     return loglib.forcedlogiface(args) typeerror: in method 'forcedlogiface', argument 6 of type 'std::string const &' ",1,train
DM-2157,Data loader crashes on uncompressed data.,"vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data:  root  critical  exception occured: local variable 'outfile' referenced before assignment traceback (most recent call last): file ""/home/vaikunth/src/qserv/bin/qservdataloader.py"", line 312, in / sys.exit(loader.run()) file ""/home/vaikunth/src/qserv/bin/qservdataloader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) file ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 168, in load return self.run(d atabase, table, schema, data)   file ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 192, in run     files = self.gunzip(data)   file ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 388, in gunzip     result.append(outfile) unboundlocalerror: local variable 'outfile' referenced before assignment   it looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data. ",1,train
DM-2158,Add support for JSON - define structure,"as discussed at https:/confluence.lsstcorp.org/display/dm/dataaccesshangout+20150223, we should support json format. this includes defining the exact format, and implementing it. this story covers defining the format.",2,train
DM-2159,Implement Image Response for ImgServ,"this story covers implementing proper response, and the header metadata for the fits image response.",3,train
DM-2161,Setup webserv for SUI tests,"we need to setup a service (eg on lsstdev) that can be used by the ipac team to play with our webserv/metaserv/dbserv/imgserv.  the server runs on lsstdev machine, port 5000. to sshtunnel, try:  ssh l 5000:localhost:5000 lsstdev.ncsa.illinois.edu   an example usage:     curl 'http:/localhost:5000/db/v0/query?sql=showdatabaseslike""%stripe%""'   curl 'http:/localhost:5000/db/v0/query?sql=showtablesindcw13stripe82'   curl 'http:/localhost:5000/db/v0/query?sql=describedcw13stripe82.deepforcedsource'   curl 'http:/localhost:5000/db/v0/query?sql=describedcw13stripe82.scienceccdexposure'   curl 'http:/localhost:5000/db/v0/query?sql=selectdeepforcedsourceid,scienceccdexposureidfromdcw13stripe82.deepforcedsourcelimit10'   curl 'http:/localhost:5000/db/v0/query?sql=selectra,decl,filternamefromdcw13stripe82.scienceccdexposurewherescienceccdexposureid=125230127'   curl 'http:/localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=0.299951669961&filter=r&width=30.0&height=45.0'  ",2,train
DM-2163,Refactor Geom class in Firefly,the geom class was ported from c code 20 years ago.  it needs to refactor to comply with java oo design.  ,8,train
DM-2164,Review at DM leadership team meeting,"review document  with kantor, kt, hobblit, and lambert,  including prep time ",3,train
DM-2165,Refactor document for that specifications are clearer,1) have one basic definition of racks and other components in the specifications.  2) fully write up first full draft  specification for the supporting material  handing area.,3,train
DM-2166,receive / process comments  from Jeff Barr a,"receive edits from jeff barr,  accept the formatting and mechanical l edits. compose separate email list issues related to non lsst tenants in the  room.  ",1,train
DM-2167,Investigate  Commerical  vendor to deal with comments on requirements. ,process email discussion about the need to liaison with the putative chilean design contractor.    kantor suggests a contractor to support requirements may be apropos. ,1,train
DM-2168,Work inside NCSA to connect procurement contract Modification to OSPRA contract officet,"work jeff's proposal until it reached university contract officer.  january meeting   clarify  purchasing rules   internal discussion of property management,   general work within  contract modification process. ",6,train
DM-2171,Implement JSON Results for MetaServ and DbServ,"implement json results for metadata service (see all m in https:/confluence.lsstcorp.org/display/dm/api),  and database service (see all d) as defined in dm 1868",3,train
DM-2173,Disable testDbLocal.py in db if auth file not found,tests/testdblocal.py can easily fail if required mysql authorization file is not found in user home dir. skip the test instead of failing in such case.,1,train
DM-2175,Adapt integration test to multi-node setup v2,following dm595 we can start qserv in multinode configuration. next step is to be able to run integration tests in that setup. this needs a bit of understanding how to distribute chunks between all workers in a cluster and how to load data in remote mysql server.,10,train
DM-2176,Worker management service - impl,"we need to replace direct workermysql communication and other administrative channels with a special service which will control all worker communication. some lightweight service running alongside other worker  servers, probably http based. data loading, start/stop should be handled by this service.",10,train
DM-2177,Implement worker-side squashing,"in the port to the new xrootd ssi api, workerside squashing was lost in the shuffle. the plumbing is different, and reimplementing squash functionality is not entirely straightforward, especially because the new api is still missing documentation and examples for implementing cancellation.  the consequences of not implementing this are minorsome extra work may be done by the worker, but not a whole lot, because user level cancellation has not been implemented.",12,train
DM-2178,Migrate Qserv to external sphgeom,"migrating qserv to the new c geometry api required porting a fair amount of code from the python layer and updating the plumbing in the czar. during implementation, the sphgeom was in the process of finding a home, so the sg code was temporarily placed under core/modules.  this ticket covers:  removing core/modules/sg  updating code to point at the external sphgeom   updating build logic to properly depend on and link with external sphgeom.",4,train
DM-2181,S17 Design Prototype EFD Schema for DRP,"the epic involves understanding the structure of the efd database produced by the engineering and facility team, and designing schema that will be best suited for data release production. note that the original efd database may not even be in mysql, there were discussions to store it in postgresql.  deliverable: alpha version of the efd database schema for drp with ""real"" data loaded (if available).",52,train
DM-2182,FY18 Design DRP-ready EFD Schema,nan,79,train
DM-2186,Move astrometry_net wrapper code from meas_astrom to meas_astrometry_net,we would like to remove all astrometry.net wrapper code from measastrom and put it in a new package with a name such as measastrometrynet.  this will also require moving any abstract task base classes into a lower level package such as measastrom.,6,train
DM-2187,FY17 Data Loader for Large Tables with No Position Information,"we need to load some tables (e.g., forcedsource) that lack director positioning, we will only have the director's primary key. the general case is very expensive (lookup position and chunk for each position), however the fact such tables will be spatially ordered when loading helps.",60,train
DM-2188,Update the astrometry.net astrometry solver to use the new standard schema,"dm 1576 provides a new astrometry solver and a new schema for reference objects. however, the old astrometry.net astrometry solver still uses the old schema. it would be wise to convert the old solver to the new schema so that the match list returned by it is in standard format.",4,train
DM-2189,Large scale test planning,"need to come up with a plan which data set we will use for large scale tests, and how we will produce it.",10,train
DM-2190,Documentation for data loader,"vaikunth had some ""expected"" troubles playing with data loader options for his dm 1570 ticket. main issue i believe is the absence of the documented use cases and their corresponding data loader options. i'll try to add a bunch of common use cases to rst documentation and also verify that all options behave as expected.",2,train
DM-2191,Define command line tasks for pre-ingest transformation,"dm 1903 provided a command line task which would transform a src catalogue into calibrated form. here, we build on that to provide command line tasks for all source catalogues which will need to be ingested; will include at least deepcoaddsrc, goodseeingcoaddsrc, chisquaredcoadd_src.",6,train
DM-2192,"Provide transformations for ""big three"" measurements","provide standard calibration transformations for each of shape, flux and centroid and make sure they are returned as the default transformation for all algorithms measuring those quantities.",10,train
DM-2193,Add assertXNearlyEqual to afw,"we often want to compare two wcs for approximate equality. afw/image/testutils has similar functions to compare images and masks and i would like to add one for wcs    this ended up being expanded to adding functions for many afw classes (not yet including image like classes, though existing functions in image/testutils for that purpose should probably be wrapped or rewritten on a different ticket)",5,train
DM-2194,Ensure proper functioning of HSC distortion correction within obs_subaru,there may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels).  this needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_xxx representations.,6,train
DM-2195,Create form framework in React,we want to create a new frame work for entering data for forms and dialogs. this is in javascript based on react.js.  this is the first step in our javascript conversion.,10,train
DM-2197,Prototype HTM-based spatial binning to visualize large number of catalog sources,see story dm 1551.,8,train
DM-2199,Build 2015_03 Qserv release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe. ,1,train
DM-2201,Add typemaps for numpy scalars,"add typemaps so that we can use numpy scalars to call c functions that take plain old scalar types (e.g. float, double or int). at present attempting to pass numpy scalars will fail unless the type is one of a restricted subset, e.g. float and numpy.float64 succeed but numpy.float32 is rejected as being an incompatible type, and similarly for integer types.",4,train
DM-2202,Acquire development data,"we'll need a reference set of data to work against.  this could be sdss, cfht, or simulated.  should be 10? epochs with realistic atmospheric conditions taken at similar airmass and hour angle.  single band is fine for now.",4,train
DM-2203,Produce task API,"this will require a new task, so will require a new interface and associated rfcs.  the interface should take an arbitrarily large stack of catalogs with or without a reference catalog.  it should return a stack ow wcss that map from the individual coordinate systems to the reference.",6,train
DM-2205,Break down monster DM-1108 stories,"[pgee]  after finishing the measurement work, your next priority is to get started on dm1108. however, the stories you have been assigned there are currently too big for useful scheduling (2030 sps is a mini epic; we're looking for less than 10 sps per story). the first task therefore is to work with [jbosch], and others if required, to break them down and come up with a set of stories which usefully reflect the work which needs to be done.",4,train
DM-2206,Deploy and test network emulation for nightly processing testbed,"deploy and test network emulation for nightly processing testbed.    assignees: paul wefel, steve pietrowicz, james parsons  duration: january   february 2016",19,train
DM-2207,Alert Production Simulator,"start march 2015, finish july 2015 pietrowicz s   100%",96,train
DM-2208,Complex Event Processing,"start may 2015, finish june 2015",31,train
DM-2209,OCS Software,"start july 2015, finish august 2015 pietrowicz s   100%",10,train
DM-2210,Configuration Management (Puppet),"start march 2015, finish may 2015 mather b   40%",9,train
DM-2211,Setup qserv prototype for qserv & SUI teams,"start july 2015, finish august 2015 glick b  25%   qserv requirements:  sui will be testing against lsst10 (or ipac qserv) for now ?   sui requirements:  xiuqin's 'short term' version:  1 vm  sui build server      4gb memory and 200gb hard disk should be good enough.  1 vm  apache server as a proxy and web front end      4gb memory and 100gb hard disk should be enough      port 80 accessible from outside  2 vms  tomcat servers       each has 16gb memory, access to 1tb of shared hard disk      port 8080 should be open for apache server to access      port 8009 should be open to each other so they can replicate cache. (first 2 vms are not absolutely needed. we can always use one of the hosts in number 3 to do build and host apache server.)  trey roby's 'long term' (in 2+ years) sui requirements:  2 vm/machines for tomcat servers, they are fairly large        each 100 gb mem        each 16 processors        1 tb disk space shared and accessible between.  1 vm/machine for web server, can be small   16 gb mem        30 gb disk        4 processors  2 small vm/machines for playing around with workspaces/l3 concepts        each has 8 gb mem        each 10 gb disk        4 processors         can share the tomcat servers disk  space",10,train
DM-2213,Storage Policies and Alignment,"start march 2015, finish august 2015 freemon m   100%",50,train
DM-2214,File System Research and Prototyping,"start march 2015, finish september 2015 freemon m  100%, glick b  25%, daues g  40%, elliot m  25%",100,train
DM-2215,File Management Technology,"start march 2015, finish september 2015 daues g  40%, freemon m  100%",50,train
DM-2216,Understand GPFS and commercial filesystems between data centers,"start may 2015, finish june 2015 petravick d   50%, tbd from set group",6,train
DM-2217,Update Sizing Model,"start march 2015, finish september 2015  alt j  50%, petravick d  10%",22,train
DM-2218,Base Data Center Requirements,"start march 2015, finish may 2015 petravick d   50%",18,train
DM-2221,Start understanding inheritability and reusability of dataset types,"in order to allow for onthefly task creation of dataset types, the essentials of each type need to be encapsulated in code.  that code should be reused across all similar dataset types, and there are opportunities for inheritance and specialization, particularly in cases like simple file oriented mappers.  investigate this by prototyping a number of possibilities.",4,train
DM-2224,Wide-Area Network Work,"start march 2015, finish september 2015 wefel p   25%",20,train
DM-2225,LOE - S15 (sys admin),"glick b  25%, mather b  40%, elliot m  25%, freemon m  100% wefel p   25%",80,train
DM-2226,LOE - S15 (management),"petravick d  50%, gelman m  50%",20,train
DM-2227,LOE - S15 (misc),all ncsa team,10,train
DM-2228,LOE - Week ending 3/6/15, backup issues with lsststor141 (https:/jira.ncsa.illinois.edu/browse/lsst632)  setup jumbo frames on lsstxfer (https:/jira.ncsa.illinois.edu/browse/lsst628)  crashplan reconfig on lsstxfer (https:/jira.ncsa.illinois.edu/browse/lsst 629),8,train
DM-2229,LOE - Week ending 3/13/15, crashplan issue with lsstnetem (https:/jira.ncsa.illinois.edu/browse/lsst633)  yum/glibc issue with lsstdbdev1 (https:/jira.ncsa.illinois.edu/browse/lsst631)  account for jacques sebag (https:/jira.ncsa.illinois.edu/browse/lsst624),8,train
DM-2230,LOE - Week ending 3/20/15, lsstdbdev2 drive failure (https:/jira.ncsa.illinois.edu/browse/lsst636)  account for colin slater (https:/jira.ncsa.illinois.edu/browse/lsst634)  disable robyn allsman's accounts (https:/jira.ncsa.illinois.edu/browse/lsst 623),26,train
DM-2231,LOE - Week ending 3/27/15,nan,21,train
DM-2232,LOE - Week ending 4/3/15, researched buildbot slowness on lsstdev /  researched buildbot slowness on lsstdev /,16,train
DM-2233,LOE - Week ending 4/10/15,nan,16,train
DM-2234,LOE - Week ending 4/17/15,nan,17,train
DM-2235,LOE - Week ending 4/24/15," researched how to monitor network drops, errors, etc /  opened up sui ports on lsstdev /  moved /nfs/admin/ to /condo/admin/  review of el 6.x kernel security patch  fixed jumbo frame issues, primarily with old vms that needed new version of nic /",15,train
DM-2236,Prototyping with Puppet,nan,5,train
DM-2237,Test Puppet with base configuration manifests,nan,38,train
DM-2239,Develop use cases for TOWG,"start march 2015, finish may 2015 petravick d  50%, glick b  25%, gruendl r   5%",20,train
DM-2240,ISO Work,"start march 2015, finish september 2015 withers a   25%",40,train
DM-2243,Extend API: expose cursor,extend api to expose cursor. this was brought up by andy in dm 2137. ,1,train
DM-2245,Define ntermediate plan for MacOSX builds, we have  1. obtain a dedicated colo osx server  2. have done some testing using the square vagrantsandbox harness  it is therefore a plausible avenue forward to do at least a nightly build/deploy/intgerationtest on osx pending more extensive arrangements requiring purchase of hardware.  ,4,train
DM-2246,Github transition for DM,dm's transition for code repositories to github is complete.  outstanding are data repositories; a cleanup of contrib/externals; and supporting the stash move. ,10,train
DM-2247,Workflow improvements for Sims / PST projects,new wokflow for sims merge of opsim and catsim new workflow for pst ,5,train
DM-2248,Prototype automated system for release preparation builds, prototype an environment that allows automatic    provisioning of a vm for a certain os  install the stack prerequisites for that os  build the stack via newinstall.sh from the production server  run integration tests (in the curent case the sdss test  https:/github.com/lsstsqre/sandboxstackbuild,22,train
DM-2250,"Galaxy Fitting via ""ngmix""","provide wrappers that let us run erin sheldon's https:/github.com/esheldon/ngmix as part of the dm pipeline.  issues so far only cover getting the a single frame (visit or coadd) version of the code running.  ngmix can also to simultaneous fitting to multiple exposures, but it's not yet clear how we'll want to handle the i/o and that interacts with a future multifit plugin framework.",45,train
DM-2251,Implement SExtractor's SPREAD_MODEL,"the new sextractor star/galaxy classifier, spreadmodel, is popular with everyone who has tried it, and should be simple to implement by building on code in measmodelfit.  see definition and discussion here: http:/arxiv.org/abs/1306.4446",2,train
DM-2252,Define common interface for star/galaxy classifiers,"we need some common fields for star/galaxy classifiers so they can participate in a slotslike mechanism once we have several of them.  most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability.  we may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hardcode such a threshold into the processing either   especially when we could also use a functorkey to get a boolean from the floating point value.",2,train
DM-2253,add third-party package builds for ngmix dependencies,"in addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages.  while it can build without them, we probably want the full functionality.  i also see some undeclared dependencies on the ""esutil"" and ""fitsio"" packages (all from esheldon's github), and there may be a few more dependencies on some of his own packages.    this issue includes creating a third party build for ngmix itself.",6,train
DM-2254,Add SFM plugin for ngmix MCMC sampling,"add an sfm plugin for ngmix mcmc fitting, as in the example in the ngmix readme.    this should depend on dm 5429 (or a suitably configured modelft_shapeletpsfapprox) for psf approximation.    for now, we should just take the mean of all parameters in the mcmc samples and write those to the record, as we currently don't have any way to save all of the samples.    testing and tuning this algorithm to get it working well should be deferred to another issue.  the only requirement here is that it be able to run without crashing (even if that means setting the number of samples small).",10,train
DM-2256,make a simple build for Firefly package,we want to have a out of box build for users of firefly package. it will include a simple firefly viewer. ,6,train
DM-2257,Allow eups xrootd install script to be relocatable,"xrootd lib/ directory should be s relative symlink to lib64, no a full path link.",1,train
DM-2258,Setup in2p3 cluster for Qserv team, create accounts  update umask on stack  to each account  provide easy ssh config if possible  setup up build procedure (each developer can build qserv using tag git and 'git' version is set up by default on all the qserv if ti exists),2,train
DM-2259,remove PSFAttributes,"psfattributes has long been deprecated, and we just need a little more work to remove it:   add an effective area accessor to the psf interface, and implement it in imagepsf.   replace usage of psfattributes with usage of psf accessors.  this may require a little work if code depends on the details of how the shape was calculated, as psfattributes provided support for more algorithms than we will going forward.",2,train
DM-2262,Improve build system for sphgeom,nan,2,train
DM-2263,Create pilot condor jobs,"create long running jobs to reduce the startup time for new htcondor jobs.   this can be implemented as a parent/child, or as a onexitremove=false directive in htcondor.  i suspect it will be a combination of the two.",21,train
DM-2264,Implement task switching between work job machines,ap requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.,4,train
DM-2265,Refactoring,the initial prototype of the ap simulator needs to be refactored to improve how tasks are handled by the components for further development.,25,train
DM-2268,Implement API for reading simulated camera data,"currently this is generated by the replicator and sent to the distributor.  the idea where is to put the api in place so that the data will be transferred from outside of the replicator to it, and then passed on.",10,train
DM-2269,Implement file transfer API ,create file transfer api so we can easily test different types of file transfer mechanisms to/from the ap.,4,train
DM-2270,Move VMs to Docker containers,we anticipate being able to move from the vms that we currently use to using docker.  this will require some coordination with greg daues to see how htcondor is configured.  ,2,train
DM-2272,Unify logging strategy for python scripts," add vvv option   remove default value for configuration file in logger, provide it at each script level (i.e. integration test, data loader).    if it exists, provide configuration file option explicitly to all called submodules which uses it.    see admin/python/lsst/qserv/admin/logger.py     14 def getdefaultlogconf():                                                                                                                                                                  15     defaultlogconf = ""/.lsst/logging.ini"".format(os.path.expanduser('~'))                                                                                                               16     return defaultlogconf                                                                                                                                                                  17                                                                                                                                                                                              18 def addlogfileopt(parser):                                                                                                                                                                 19     """"""                                                                                                                                                                                      20     add option to command line interface in order to set path to standar                                                                                                                     21     configuration file for python logger                                                                                                                                                     22     """"""                                                                                                                                                                                      23                                                                                                                                                                                              24     parser.addargument(""v"", ""logcfg"", dest=""logconf"",                                                                                                                                  25                         default=getdefaultlogconf(),                                                                                                                                      26                         help=""absolute path to yaml file containing python"" +                                                                                                                27                         ""logger standard configuration file"")                                                                                                                                28     return parser                                                                                                                                                                            29                                                                                                                                                                                              30                                                                                                                                                                                              31 def setuplogging(path='logging.ini',                                                                                                                                                        32                   defaultlevel=logging.info):                                                                                                                                               33     """"""                                                                                                                                                                                      34     setup logging configuration from yaml file                                                                                                                                               35     if the yaml file doesn't exists:                                                                                                                                                         36      return false                                                                                                                                                                           37      configure logging to defaultlevel                                                                                                                                                     38     """"""                                                                                                                                                                                      39     if os.path.exists(path):                                                                                                                                                                 40         with open(path, 'r') as f:                                                                                                                                                           41             logging.config.fileconfig(f)                                                                                                                                                     42         return true                                                                                                                                                                          43     else:                                                                                                                                                                                    44         logging.basicconfig(level=defaultlevel)                                                                                                                                  45         return false    ",6,train
DM-2277,Document HOW-TO setup-up krb5 for easy cluster access, su aptitude install krb5user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx   /etc/krb5.conf  [libdefaults]  defaultrealm = in2p3.fr  ...  allowweakcrypto = true   ... [realms]  in2p3.fr =   sshconfig:  host ccqservbuild gssapiauthentication yes gssapidelegatecredentials yes forwardx11 yes hostname ccqservbuild.in2p3.fr #proxycommand ssh w %h:%p cc   host ccqserv1  gssapiauthentication yes gssapidelegatecredentials yes forwardx11 yes hostname %h.in2p3.fr proxycommand ssh  w %h:%p ccqservbuild ,2,train
DM-2279,Fix problems with mysql timeout,we added some code for supporting reconnecting (see https:/dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. this needs to be addressed.,1,train
DM-2280,The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane,"the tanpixels camerageom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector  i.e. a star at the center of the detector will have the same position in pixels and tanpixels coordinates. that is a mistake. tanpixels should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not.  fixing this will help measastrom match stars. the effects of not fixing it are making the matcher search farther for a fit. as long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal.",2,train
DM-2281,Implement connection pool,"implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout.",1,train
DM-2282,Switch to using db connection pool,"switch to using the db connection pool. note, in addition to getting auto reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously.",1,train
DM-2286,Participate in design process,"participate and guide the sui design process, generate charts and documents as appropriate",9,train
DM-2287,Move javascript code into firefly repo and begin creating a real input form,nan,10,train
DM-2288,Work with Camera & Pipeline team to spec out  proof of concept tools,nan,14,train
DM-2289,Personnell requisitions ,"work though recruiting for software effort.  investigated and filled the ""kenton"" recruiting pattern at ncsa  (few explicit requirements, many desirable)  began discussion to break down hires for ""2nd"" floor  work  to be in the lsst group v.s support groups  ads and doug's group",3,train
DM-2290,Arrange for commercial object store presentation,"arrage for presentations next week w.r.t commercial object store.  the vendor in question is know to ncsa and has claims to have produced a commercial object store having both nfs,  gpfs  and swift interfaces. ",1,train
DM-2291,Begin WBS review ,begin  comprehensive review of the wbs.   forced on overall framework and begin  workflow systems  ,1,train
DM-2292,Security officer orientation,begin orientation of lsst iso alex withers. ,1,train
DM-2293,Internet2 TIER investigation,nan,1,train
DM-2294,Unable to start cmsd on Qserv worker node,"some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3  here's the log:   [fjammes@ccqserv101 ~]$ cat /qserv/qservrun/var/log/worker/cmsd.log 150309 21:19:46 9794 starting on linux 3.10.0123.8.1.el7.x86_64 copr.  20042012 stanford university, xrd version v20140617203cf45  cmsd worker@ccqserv101.in2p3.fr initialization started. config using configuration file /qserv/qservrun/etc/lsp.cf =====> all.adminpath /qserv/qservrun/tmp =====> xrd.port 1094 =====> xrd.network nodnr config maximum number of connections restricted to 4096 config maximum number of threads restricted to 2048 copr.  2007 stanford university/slac cmsd.  worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so  =====> oss.localroot /qserv/qservrun/xrootdrun =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qservrun/var/run =====> all.adminpath /qserv/qservrun/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock the following paths are available to the redirector: w  /    worker@ccqserv101.in2p3.fr phase 1 server initialization completed.  worker@ccqserv101.in2p3.fr phase 2 server initialization started. plugin unable to find  required version information for xrdossgetstoragesystem in osslib libxrdoss.so  worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 xrdprotocol: protocol cmsd could not be loaded  cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed ",2,train
DM-2295,Read through Don's SCADA notes and comment,nan,1,train
DM-2299,"Revisit db and dbPool, separate connection from utilities","revisit whether we need something better than a very basic db connection pool.    it may be worth looking at http:/docs.sqlalchemy.org/en/rel09/core/pooling.html (or even sqlalchemy in general). note the pooling plain dbapi connections section  one can use sqlalchemy pooling independently of the other library features.    separate utilities like createdb(), dbexists() and such from core part that deals with connections / sqalchemy.",12,train
DM-2300,Improve Webserv/Metaserv,"work includes implementing features requested by sui (schema metadata, units etc)",35,train
DM-2301,Support metadata for databases without description,"the metaserv should be able to support databases for which we don't have the ascii schema with descriptions and special tokens (ucd, units etc). this story involves implementing it. in practice, the metaserv/bin/metabackend will need to be extended to implement ""add db""",4,train
DM-2305,Measurement transforms for Flux,provide calibration transforms for flux measurements to magnitudes.,3,train
DM-2306,Measurement transforms for centroids,provide calibration transforms for all algorithms measuring centroids.,5,train
DM-2307,Measurement transforms for shapes,provide calibration transforms for algorithms measuring shapes.,2,train
DM-2309,Update dev quick-start guide to new git repositories,the quick start documentation for developers still points to the old git repositories. the rst document needs to be updated to the github repos.,1,train
DM-2312,obs_test's table file is out of date,"obstest's table file is somewhat out of date. problems include:   afw is required but missing   measalgorithms and skypix are used by bin/geninputregistry.py, which is only used to create the input repo so these can be optional   dafpersistence is not used   dafbase is only used by bin/geninputregistry.py, so it can be optional (though it is presumably setup by daf_butlerutils in any case)",1,train
DM-2314,Improve xssi API to send a few bytes with the message informing the client that a response is available on  the server,"this would allow qserv no to send the first protobuf header as a xrootd in band message, and save some resources (network and cpu due to xrootd/tcp/ip encapsulation)",6,train
DM-2316,Clarify expectations for unauthenticated user data access,"short version:  clarify what existing community practices, notably including vo interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives.  details:  at the february dm all hands, [frossie] raised an objection when it was mentioned that there is a presumption that all user access to lsst data through the dm interfaces (as opposed to through epo) will be authenticated.  we don't appear to have ever documented an explicit requirement that all access be authenticated.  the basic controlling requirement is ossreq0176, ""the lsst data management system shall provide open access to all lsst level 1 and level 2 data products, as defined in the lsst system requirements and herein, in accordance with lsstc board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for nonus/chile access was still being developed.  however, this presumption has been around for a long time.  it is inherent to the project policy that access to the nonalert data will be limited to individuals who are entitled to it.  no matter what we think the final policy might be, we do have to design a system that can be consistent with this policy.  [frossie] stated that the astronomical community relies on certain types of data and metadata  she mentioned coverage maps, among others  being available through unauthenticated interfaces.  this ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design.",2,train
DM-2320,Remove deprecated merging code: rproc::TableMerger,"rproc::tablemerger seems to be replaced with rproc::infilemerger, so this class could certainly be removed easily. ",2,train
DM-2322,Revisit exceptions in db module,revisit db/python/lsst/db/exception.py. perhaps get rid of the numbers.,5,train
DM-2323,KT reading list for operational requirements,nan,2,train
DM-2324,Observatory site requirements reading,nan,2,train
DM-2327,"Setup hosts for SUI (2x Tomcat, Apache, and build)","xiuqin's 'short term' version: 1 vm  sui build server 4gb memory and 200gb hard disk should be good enough. 1 vm  apache server as a proxy and web front end 4gb memory and 100gb hard disk should be enough port 80 accessible from outside 2 vms   tomcat servers each has 16gb memory, access to 1tb of shared hard disk port 8080 should be open for apache server to access port 8009 should be open to each other so they can replicate cache. (first 2 vms are not absolutely needed. we can always use one of the hosts in number 3 to do build and host apache server.)  the 2 tomcat servers are larger than we can currently support as vms.   we've discussed repurposing 2 of the older lsst ""cluster/condor"" nodes (e.g. lsst14 & lsst15) for this purpose.  but, ideally these could be implemented with the new vsphere hardware if the timeframe works.",4,train
DM-2329,"review """"data center in a box"" mali.  Recover consultant's contact into ","reviewed the data center in a  box, recovered consultant's name prior to drafting a sow.",1,train
DM-2330," attend DDN WOS briefing, write summary note. ","as described above.  summary note is attached. also looked for use of this product in doe labs, who would be consumers  of lsst data.  discovered that it had been investigated for use in hep a few years earlier, but that is was not adopted because, at that time the hardware and software were coupled.",1,train
DM-2331,misc for week of march 9,finalize job descriptions. meet with kantor additional hour of  orientation for the iso. group meeting  misc.,1,train
DM-2334,Simplify interactions with XrdOss,"the qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. for  details, see  https:/listserv.slac.stanford.edu/cgibin/wa?a1=ind1503&l=qservl#3",5,train
DM-2335,Setup IRODS zone on ISL OpenStack,"we begin an examination of irods for managing data collections. we perform initial testing using resources available on ncsa's isl openstack.  to mock up a zone or 'data grid' managed by irods, we set up an icat server, a resource server (this is a data storage resource that does not run the central database), and a client host. ",4,train
DM-2336,Save iRODS installations/servers as docker images,"we install and configure irods servers (an icat server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository. ",4,train
DM-2337,develop/propose storage policies,nan,6,train
DM-2338,develop/propose storage procedures,nan,10,train
DM-2339,develop/propose storage implementation,nan,3,train
DM-2340,Reprise SDRP processing metrics,"in support of an sdrp based science talk of yusra alsayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the us side of processing of the split drp.  this information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the sdrp science results. ",2,train
DM-2341,Use parallel ssh to manage Qserv on IN2P3 cluster,in2p3 sysadmin won't manage qserv through puppet. so qserv team has to provide ssh scripts to do this.  ,5,train
DM-2342,Integrate changes from Events code review,nan,6,train
DM-2343,Move afw_extensions_rgb functionality into afw proper,see rfc 32 ,1,train
DM-2347,(In)equality semantics of Coords are confusing,"viz:   in [1]: from lsst.afw.coord import coord in [2]: c1 = coord(""11:11:11"", ""22:22:22"") in [3]: c1 == c1, c1 != c1 out[3]: (true, false) in [4]: c2 = coord(""33:33:33"", ""44:44:44"") in [5]: c1 == c2, c1 != c2 out[5]: (false, true) in [6]: c3 = coord(""11:11:11"", ""22:22:22"") in [7]: c1 == c3, c1 != c3 out[7]: (true, true)   c1 is simultaneously equal to and not equal to c3!",1,train
DM-2348,useValueEquality and usePointerEquality fail to fail,these swig macros return a class instead of raising an exception instance when the equality operation fails.,1,train
DM-2349,Add unit tests to SchemaToMeta,"add unit tests, also improve variable names as suggested by kt in comments in dm2139",1,train
DM-2352,Install and learn to use iPython notebook,nan,4,train
DM-2353,Participate in design discussion,participate  in the design discussions three times weekly for two to three months. ,9,train
DM-2354,Participate in design discussion,participate in the design discussions three times weekly for two to three months. ,9,train
DM-2355,Participate in design discussion,participate in the design discussions three times weekly for two to three months. ,9,train
DM-2356,Identify the hardware resources needed at NCSA for short term development ,supply the hardware resources needed at ncsa for short term development. it is captured in dm 2327  ,1,train
DM-2357,make PixelFlagsAlgorithm fully configurable,"pixelflagsalgorithm currently hard codes the mask planes it considers.  this should be fully configurable instead.  it also overloads the ""edge"" flag to mean both ""edge mask plane was set"" and ""centroid was off the edge of the image"".  these should be different flags.  we may also want to have this algorithm use safecentroidextractor.'  finally, the algorithm is woefully undertested.",2,train
DM-2358,standardize handling of missing peaks in centroiders,gaussiancentroid has a no_peak flag that it sets when there is no peak to use as an input.  sdsscentroid does not.  this behavior should be standardized.  maybe we should use safecentroidextractor here?,1,train
DM-2363,RGB code introduces dependency on matplotlib,"while the new rgb code looks like it's just calling numpy, numpy is actually delegating to matplotlib under the hood when it writes rgb(a) arrays.  it also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who  like me  are trying to use slightly older system python packages).  i think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version).  i'm happy to do this myself (since i'm probably the only one affected by it right now).",1,train
DM-2364,Revisit the choice of using flask,"we should quickly revisit if flask is the right choice for us.  related: reportedly, our simple flask based webserver is using more cpu in an idle state than expected. it might be useful to profile things, and look into that. ",1,train
DM-2367,run lsstswBuild.sh in a clean sandbox,"the ""driver"" script, lsstswbuild.sh, used by the buildbot slave on lsstdev to initiate a ""ci run"" has a number of environment assumptions (binaries in the $path, paths to various components, hostnames, etc.).  this prevents it from [easily] being invoked on any other host.  as lsstswbuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined.  in addition, the current version of lsstswbuild.sh and related scripts on lsstdev are not version controlled.",8,train
DM-2370,Move QuerySession::_stmtParallel from query::SelectStmtPtrVector to query::SelectStmtPtr,"querysession::_stmtparallel is a vector but it seems only it's first element is used, so storing it in a vector doesn't seem necessary.  code can be easily simplified here. this should lead to mode understandable code.  querysession public members and method comments could also be improved here.",4,train
DM-2371,run lsstswBuild.sh under Jenkins on EL6, demonstrate lsstswbuild.sh being invoked by jenkins on el6 (same os as lsst dev).  experiment with a single build slave attached to a jenkins master   investigate configuration management of jenkins builds.,6,train
DM-2373,Improve logger use in qserv,"qserv logger must be easily configurable. next technique, based on log4cxx documentation allows to do it easily.  example:  in querysession.cc, initialize a static logger:  namespace lsst   then use it in query session member functions:           if (logchecklvl(logger, loglvldebug))    and use log4cxx.property to easily configure, at runtime, logging for each qserv module class:   # logger for all module will inherit this one log4j.logger.lsst.qserv=error # this could be generalized to all qserv modules: log4j.logger.lsst.qserv.qproc=info # can also be done at the class level for advanced debugging #log4j.logger.lsst.qserv.qproc.querysession=debug   and then in the log:  /home/qserv/qservrun/2015_03/var/log/qservczar.log:0319 17:08:48.786 [0x7f208da93740] debug lsst.qserv.qproc.querysession (build/qproc/querysession.cc:118)   query plugins applied:   this proposal is a draft and should be improved before implementing it.",8,train
DM-2375,evaluate NCSA proposal to investigate CEPH in the context of NCSA Integrated systems lab,"the integrated systems lab (isl) is the orgianizational vehicle used to investigate pre production technologies at ncsa.   since  we still lack the ability to procure goods,  i evaluated and commented on an isl proposal to investigate the ceph file system for its properties as an alternative to the lsst baseline file system gpfs. ",1,train
DM-2376,revise and circulate data center requirements note,"reconvert the ~10 tbd's  in the priori version of the note  — i’ve kept the stipulation that end of servicelifee stuff will leave these spaces  but added an appendix that “this is what the central space should provide”  my understanding is  there are now discussions on whether that central space will exist of not.   the requirements  can be promoted to center requirements no central space is evient. — the maximal average weight for a rack was computed from ldm144 and is given.  — there are more cu ft estimates for  the need to dispose of dunnage and packing material. — tbd’s w.r.t overhead cabling are specificed. — the non  lsst requrements are in there, and have been as far as i am able to ascertain them from   champaign urbana.   ron has been stating requirements as “rows”   i have never fixed row length  thinking  that is for the designer to do.   we’ve stated that rows are shareable, but racks are not. so what’s in the docs is definitive unless/until non lsst requirements can be stated in the same terms.     ""shall support 16 racks for the noao tenant"". is what i had.  power requirements as per  the common space, becasue we want a maximally flexible space. ",2,train
DM-2377,Management,"meetings  monday cam meeting, friday standup and infrastructure.  internal ncsa group meeting,  internal ncsa ""comp pol"" technical coordination meeting.  screen existing candidate pool for likely people to fill opening,  interviewed one person checked references + misc.",3,train
DM-2380,Retrieve HSC engineering data,"hsc data becomes public 18 months after it was taken, so data taken during commissioning are now available.  we would like to use this data for testing the lsst pipeline.  it needs to be downloaded from japan.",2,train
DM-2382,Make sure the command-line parser warns loudly enough if no data found,a user recently got confused when calling parseandrun didn't call the task's run method. it turns out there was no data matching the specified data id. make sure this generates a loud and clear warning.,1,train
DM-2383,migrate package deps from sandbox-stackbuild to a proper puppet module,there is a growing list of known package dependencies in the sandbox stackbuild repo and a need to use this information for independent environments (such as ci).  this list of packages should be lifted out into an independent puppet module that can be reused.,2,train
DM-2385,Implement data loading in worker manager service,this is a separate ticket for implementation of the data loading part of the worker management service (started in dm 2176). some ideas and thoughts are outlined in that ticket.,6,train
DM-2387,Build testQDisp.cc on ubuntu,testqdisp.cc needs flags lpthread lboost_regex to build on ubuntu.,1,train
DM-2390,Errors need to be checked in UserQueryFactory from QuerySession objects,userqueryfactory doesn't check its querysession object for errors after setquery. thus it continues setting things up after the querysession knows the state is invalid.,1,train
DM-2391,Migrating to GWT 2.7,"to use javascript interop functionality, we need to migrate to gwt 2.7",10,train
DM-2392,React components for Form,"to familiarize myself with react and to prepare the ground for moving to react based user interfaces, we need to create react components for the form. this story includes checkboxgroup, radiogroup, and listbox input fields.",10,train
DM-2396,FY17 Enable DC Analyses through Qserv,load data challenge data into qserv and enable analytics of the dc data through qserv.,53,train
DM-2397,FY18 Enable DC Analysis through Qserv,load data challenge data into qserv and enable analytics of the dc data through qserv. ,53,train
DM-2400,FY17 Fix Qserv Bugs,bucket epic for unexpected bug fixes.,53,train
DM-2401,FY18 Fix Qserv Bugs,bucket epic for unexpected bug fixes.,53,train
DM-2402,FY19 Fix Qserv Bugs,bucket epic for unexpected bug fixes.,53,train
DM-2403,FY20 Fix Qserv Bugs,bucket epic for unexpected bug fixes.,100,train
DM-2404,W16 Butler (v4),nan,100,train
DM-2411,Allow qserv-admin.py to delete a node,"registered workers in css with qserv admin.py are currently not able to be removed (no delete node type command). also, changing node status from active to inactive needs to be fixed.",1,train
DM-2412,Change integration test user from root to qsmaster,currently integration tests use root account as default user   this should be changed to qsmaster for the future.,2,train
DM-2414,investigate configuration management for jenkins,the most popular puppet module for managing jenkin's jenkinsci/puppet jenkins is able to create a master and build slaves but is missing the functionality to manage several master configuration options that otherwise require manual setup.  we need to investigate the difficulty of managing a jenkin's master configuration values in an idempotent manner.,4,train
DM-2415,convert Statistics to use ndarray natively,"the statistics class predates ndarray, and hence uses some hackish imageclass emulators/wrappers to deal with 1d arrays.  it'd clean things up considerably to have it use ndarray under the hood, and have the image based interfaces interact via their ndarray views.",3,train
DM-2417,Data loader script crashes trying to create chunk table,"vaikunth discovered a bug in data loader when trying to load a data into object table:  [critical] root: exception occured: table 'object7480' already exists traceback (most recent call last):   file ""/usr/local/home/vaikunth/src/qserv/bin/qservdataloader.py"", line 318, in /     sys.exit(loader.run())   file ""/usr/local/home/vaikunth/src/qserv/bin/qservdataloader.py"", line 254, in run     self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data)   file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 171, in load     return self.run(database, table, schema, data)   file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 209, in run     self.loaddata(database, table, files)   file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 586, in loaddata     self.loadchunkeddata(database, table)   file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 653, in loadchunkeddata     self.makechunkandoverlaptable(conn, database, table, chunkid)   file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataloader.py"", line 727, in makechunkandoverlaptable     cursor.execute(q)   file ""build/bdist.linuxx8664/egg/mysqldb/cursors.py"", line 176, in execute     if not self.deferwarnings: self.warningcheck()   file ""build/bdist.linuxx8664/egg/mysqldb/cursors.py"", line 92, in warningcheck     warn(w[ 1], self.warning, 3) warning: table 'object7480' already exists  it looks like i did not do enough testing after my recent improvement in creating chunk tables. it tries to create the chunk table with ""create table if not exists ..."" but that actually generates ""warning exception"" on mysql side when table is already there. need to catch this exception and ignore it.",1,train
DM-2419,Implement authentication mechanism for worker management service,we need some reasonable security for access to new worker management service. it should be lightweight and not depend on complex things that require infrastructure. something based on a shared secret should be adequate for our immediate needs and likely for the long term.,4,train
DM-2420,Document API for worker management service,"new worker management service exposes its api as an interface to restful web service. many or all ""methods"" will be wrapped into some sort of python api, but it would still be useful to document every web service ""methods"" independently. there is basic documentation in the design document (https:/dev.lsstcorp.org/trac/wiki/db/qserv/wmgrdesign), this needs to be extended with detailed description of what those methods do and what kind of data they accept and return.  this story involves selecting the right tool.",4,train
DM-2421,Improve support for Python modules in Scons,"it seems we have two tools to manage python modules:   sitescons/pytarget.py and  sitescons/site_tools/pymod.py (grep for installpythonmodule) used by admin tools. we could unify this, isn't it?",1,train
DM-2423,Weighting in photometric calibration is incorrect,"dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point.  git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/photocal.py (robert lupton the good 20101213 05:03:12 +0000 353)     return np.average(dmag, weights=dmagerr), np.std(dmag, ddof=1), len(dmag)  please fix this.  at the same time, we should add a config parameter to soften the errors. ",1,train
DM-2424,Create transitional duplicate of Span,"one challenge in switching from ptr(span) to span in footprint is that swig won't generate wrappers for std::vector/ (or any other container) if %sharedptr(span) is used anywhere else in the codebase.  so, to allow both the old footprint class and the new spanregion to coexist (temporily), we need to have two span classes, one wrapped with %sharedptr and one wrapped without it.  since we don't want to disrupt the old footprint class yet, we should call the new span something else, and make it the one that's wrapped without %shared_ptr.  this ticket can be considered complete once we have a unit test demostrating a usable swig wrapped std::vector/ while all old footprint tests continue to pass.",1,train
DM-2425,Implement SpanRegion core functionality,"implement the core of the spanregion class, as prototyped in rfc37.  this includes the following:   the private implementation object and copyonwrite utilities (see schema for an example of copyonwrite, but note that spanregion's implementation object can be private, while schema's is not).   all stl container methods and typedefs, and their pythonic counterparts.   all constructors and assignment operators, except for spanregionbuilder.  this includes the ability to detect and fix overlapping spans.   all simple accessors.   iscontiguous()    the shift and clip methods.",6,train
DM-2426,Implement SpanRegion+ellipse operations,"implement the following spanregion operations:   construct from an ellipse  note geom::ellipses::pixelregion; this should do most of the work.   compute centroid  see old footprint implementation   compute shape (quadrupole moments)  see old footprint implementation  one complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses.  that's easy to address at the c level, but it's tricky in python (which package imports the other?)  i'll be emailing dm devel shortly to start a discussion on how to address this problem.",2,train
DM-2427,Implement SpanRegion applyFunctor methods,"implement methods that apply arbitrary functors to pixels within a spanregion, as described on rfc 37.  the only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently.  i'd be happy to consult on this; i have a rough idea in my head, but it needs to be fleshed out.",3,train
DM-2429,Add aperture corrections to meas_extensions_photometryKron,"when transitioning measextensionsphotometrykron to the new measurement framework, aperture correction was omitted pending the completion of dm85. it needs to be reenabled when that epic is complete.",1,train
DM-2430,Make qserv server-side log messages more standard,"qserv serverside python logging appears to mostly use a common format: ""%(asctime)s %(name)s %(levelname)s: %(message)s"".  it also mostly uses a common date format: ""%m/%d/%y %i:%m:%s"".  but i see instances of:  ""%(asctime)s %(levelname)s %(message)s""  ""%(asctime)s  %(name)s  %(levelname)s  %(message)s""   ""%(asctime)s \ %(levelname)s %(message)s""  and now, after dm2176, ""%(asctime)s \[pid:%(process)d\] \[%(levelname)s\] (%(funcname)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s""  unless these are used in very different contexts, it will aid automated log processing for them to be more standardized.  in addition, the date format is unacceptable as it does not use rfc 3339 (iso8601) format and does not include a timezone indicator (which means the default datefmt is insufficient).  this must be fixed.  see also dm1203.",1,train
DM-2431,Fork GREAT3 sim code and integrate with LSST stack,"get the great3 simulation code running with lsstprovided thirdparty packages of python, galsim, etc, and figure out where we're going to put our modified scripts on github:   do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo?  (i think probably the latter, but we should determine how many repos, and for what purposes.)   where in github space do we put them (lsst?  lsstdm? user spaces?)  this is one of several issues that together will replace dm1132 (which was just a planning stand in for these more detailed issues).",4,train
DM-2432,Increase postage stamp size in simulation scripts,the great3 simulations have a fixed postage stamp size (though this may differ between branches).  a first step at modifying the simulation scripts to meet our needs would be to try to change the postage stamp.  this is one of several issues that together will replace dm1132 (which was just a planning standin for these more detailed issues).,2,train
DM-2433,Create simulation script with different constant PSF per galaxy.,"modify the great3 simulation scripts to create a branch in which each galaxy gets a different constant psf, rather than one constant psf per subfield or a spatiallyvarying psf that spans multiple subfields.  this could be done by modifying the control/ground/constant branch or the variablepsf/ground/constant branch, or creating an entirely new branch, or anything else (since we don't actually need multiple branches in our simulations).  at this point, the source of the psfs doesn't really matter  as long as we have a class that can provide a different one to every image.  this is one of several issues that together will replace dm1132 (which was just a planning stand in for these more detailed issues).",6,train
DM-2434,Draw simulated PSFs from a library of on-disk files,modify the simulation code to draw psfs at random from a library of ondisk files (whose format and ondisk layout should be specified here).  the psfs chosen should be deterministic via a random number generator seed specified via config.  this is one of several issues that together will replace dm1132 (which was just a planning standin for these more detailed issues).,4,train
DM-2435,Reading an Exposure from disk aborts if the Psf is of an unknown type,"attempting to read an exposure (in this case via the butler) fails if the psf class isn't available.  an exception would be reasonable, but an assertion failure is not.  running the attached script on tigersumire with bq. setup python anaconda; setup t v101rc2 lsstapps; setup j distest t hsc; setup j r ~/lsst/obs/subaru    warning: could not read psf; setting to null: persistablefactory with name 'psfexpsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because python calls were not available from c). ; loading object with id=4, name='psfexpsf' ; loading object with id=28, name='coaddpsf'  python: src/table/io/inputarchive.cc:109: boost::sharedptr/ lsst::afw::table::io::inputarchive::impl::get(int, const lsst::afw::table::io::inputarchive&): assertion `r.first >second' failed. aborted ",1,train
DM-2436,"Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC","hsc 1196 includes fixes and test cases for afw. after review on hsc, they should be checked/merged to lsst.",1,train
DM-2437,Port HSC-side functionality to allow showCamera to display real data via the butler,"one of the things that exists on the hsc side of things but not lsst is the ability to use showcamera to create fullfocalplane mosaics.  please convert the code to run with the new camerageom    not only is this generically useful, but it's part of the effort required to make the dm side visualisation work for the camera group  ",4,train
DM-2439,iRODS test: Replicate data between servers,"a fundamental feature of using irods would be to prevent file loss/corruption incidents by replicating data to different physical servers, possibly in geographically disparate locations. we verify that we can replicate data within out test zone/grid.",2,train
DM-2440,iRODS test:  Virtual collection ,"irod manages data as a 'virtual collection', that is, one can have a single logical/virtual view of a collection of files (the appearance of a single file system/tree) while the data with the collection is stored on separate physical servers. we demonstrate this by creating a collection with data targeted/uploaded to different physical resources.",2,train
DM-2441,iRODS test: Register data in place,"in our first tests of irods, we have used ""iput"" to load data into irods cache spaces (the irods vault).  for large collections already in a well known location on a server, one may want to leave the data in place but still manage it with irods. to do this one can use ""ireg"" to register the data with irods without the upload process.",2,train
DM-2442,"iRODS usage, devel survey",read up on current irods usage and development track. ,3,train
DM-2444,Fix and test CheckAggregation," class checkaggregation   inline void operator()(query::valueexpr::factorop const& fo)    return is missing here. .get() is not needed, shared_ptr is like regular pointer which is convertible to bool, so whole thing should probably be: if (! fo.factor) return;   we should have a unit test to show us we have problem here ",4,train
DM-2445,"Fix query ""SELECT * FROM Object o, Source s WHERE  o.objectId = s.objectId AND o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint""","next query fails:    mysql host=127.0.0.1 port=4040 user=qsmaster qservtestcase01qserv e   ""select    from object o, source s   where  o.objectid = s.objectid   and    o.objectid = 390034570102582   and    o.latestobstime = s.taimidpoint""      it seems there's several problems here:     objectid field is duplicated, zookeeper could be used to know all the fields involved by  in a query, but then it has to know each columns.   subchunkid and chunkid are also duplicated, this isn't the case in the plainmysql query.    this duplicated columns prevent the creation of the result table on the czar.  ",6,train
DM-2447,v10_1_rc2 build test,"test v101rc2 + tickets/dm 2303 on el6, el7, fedora 21, ubuntu 12.04, & ubuntu 14.04.  results to be reported in http:/ls.st/faq .",1,train
DM-2448,Write additional test for duplicate fields check,"alongside:   boostautotestcase(getduplicateandposition)   add test for:  no duplicate strings  triplicate   more than one string duplicated  and alongside:   boostautotestcase(samenamedifferenttable)   test more than one duplicated column, and a column duplicated more than twice.",3,train
DM-2450,Fix cmsd-server logger configuration,"cmsd server logger configuration is incorrect:  see cmsd.log on qserv worker:   plugin loaded unreleased qservossgeneric unknown from osslib libxrdoss.so log4cxx: could not instantiate class [org.apache.log4j.xrootdappender]. log4cxx: class not found: org.apache.log4j.xrootdappender log4cxx: could not instantiate appender named ""xrdlog"". log4cxx: no appender could be found for logger (qservoss). log4cxx: please initialize the log4cxx system properly. qservoss (qserv oss for server cmsd) ""worker"" ",4,train
DM-2451,Fix interface between QservOss and new cmsd version,"qservoss gives an error when attempting to run queries on the worker from the czar. error log snippet:   qservoss (qserv oss for server cmsd) ""worker"" 150331 16:06:17 9904 meter: unable to calculate file system space; operation not supported 150331 16:06:17 9904 meter: write access and staging prohibited.  worker@lsstdbdev3.ncsa.illinois.edu phase 2 server initialization completed.  cmsd worker@lsstdbdev3.ncsa.illinois.edu:36050 initialization completed.  ",1,train
DM-2452,Replace toString() function,"see  comment:    fabrice, anything is possible in c, if you can define tostring() for vectors it should also be possible to define some other construct to format vector into a stream :)  my objection to tostring() is based on couple of of observations:      most of the time in our code converting complex objects to string is done to push them to streams or to logging system (logging is also usually based of streams)     methods like tostring() are usually implemented using temporary streams.  so if you write code like cout / std::ostream& operator/& cins)  } template / detail::continserterhelper/ continserter(const cont& cont) ; }   and after that you can do:   std::vector/ v; std::cout << continserter(v);   and this has no overhead or any temporary objects created. ",3,train
DM-2454,investigate github oauth integration for jenkins ,"we need a means of authenticating and authorizing users to interact with the ci system.  the current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively.  jenkin's has support for ldap and there is a plugin available for github oauth.  administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new dm or the exist lsst ldap instance.",7,train
DM-2455,uncaught exceptions in GaussianFlux,"sdssshapealgorithm::computefixedmomentsflux, which is used to implement gaussianflux, now throws an exception when the moments it is given are singular.  that shouldn't have affected the behavior of gaussianflux, as it contains an earlier check that should have detected all such bad input shapes.  but that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs.  we need to investigate what's going wrong with these objects, and fix them, which may be in sdssshape or in the safeshapeextractor gaussianflux uses to sanitize its inputs.",1,train
DM-2456,Participate in April design process,most work here was with designing firefly tools api related details.,8,train
DM-2457,Prepare firefly for GitHub,nan,8,train
DM-2458,Finsh pushing firefly to GitHub,nan,3,train
DM-2459,Begin actual conversion of parts of firefly to pure javascript,nan,10,train
DM-2460,Develop next gen Firefly JavaScript API Tools,nan,10,train
DM-2461,Develop external http api that can control Firefly viewer,nan,14,train
DM-2462,Implement client side of mask layers in FITS image Viewer,nan,20,train
DM-2463,Prepare v10_1 release candidate,candidate is v101rc2 based on eups tag b949,6,train
DM-2466,lsstsw ./bin/deploy needs LSSTSW set to install products in the right place,"i  cloned lsstsw into /desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into /lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw.  the short term workaround is to manually set lsstsw before running ./bin/deploy, but this should not be necessary; bin/deploy should either set lsstsw or not rely on it. i don't recall this problem with earlier versions of lsstsw; i think this is a regression.  for now i updated the instructions at https:/confluence.lsstcorp.org/display/ldmdg/thelsstsoftwarebuildtool but i look forward to being able to revert that change.",1,train
DM-2467,Implement stitching multiple patches across tract boundaries in a coadd v2," find region that returns multiple tractpatchlists for testing.   request region via central point (ra, dec) with width and height definable in arcseconds and pixels.    may be extend web interface to other data sets, and/or good seeing skymaps. ",8,train
DM-2468,Qsev Documentation,nan,10,train
DM-2469,Turn on C++ 11 flag for Qserv,nan,4,train
DM-2470,Handle all exceptions coming from worker,nan,7,train
DM-2475,Build 2015_04 Qserv release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-2477,Design API and RFC design,"use the hsc implementation of the base class as a point of reference for designing an integrated approximate and interpolate class.  the design take into account chebyshev, spline, and gaussian process mechanisms.  want to take into consideration client code.  i.e. it shouldn't make current consumers more complicated (background and aperture correction to name two).  rfc the designed api.",8,train
DM-2478,Edit background class,make fixes to background class to use new approximate/interpolate class.,10,train
DM-2479,Fix-up any code that uses approximate/interpolate,the background matcher is one area where the approximate/interpolate class will be used.  this story will find all places (including examples and unit tests) where the old approximate/interpolate mechanisms are used and update them to use the new interface.,4,train
DM-2480,Delete old approximate/interpolate classes,"once all updates are done to code and unit tests pass with the new approximate/interpolate interface, the old ones should be completely removed.",2,train
DM-2484,"Justify level of staff at La Serena, to the level of justifying for office space",help with the specifications for the buildings in la serena. the request si enough prove to justify office space for the dm administrators. and for  other staffing needed for dm.  this work will span two weeks and is due this thursday  april 9.    this story is for the orienting work  kickoff phone call.,1,train
DM-2485,recieve and begin to process document from SET about scalability of CEPH,in the context of isl investigations into stogie systems the set group has produced a document  that goes into the scaling of the meta data services.  the concern is that there is a central meta data service ins ceph.   began to process this analysis and to think about feasibility of testing program.,1,train
DM-2486,management for week March 30.,"investigated invoicing fro storage condo  appears to be annual fee, ok by jeff. investigated attaching  effort breakdown to invoke  this seems hard as u of i invoicing occurs at quite a distance (procedural) distance from the ncsa business office.  decided to look at improvements in recording effort in jira so as to be able to generate report.  capture all actuals.  business office transition  support is transitioning from matt s. to new person.  review amcl sides,  kept tradition generating exponentially more comments, but reduced the exponent.  process to bill out effort applied to project, but not in standing assignments in the staffing plan.  internal strategy meeting about agenda items w.r.t vao given rap plante is leaving ncsa. prep for dm leadership meeting   synergies at ncsa.  ",4,train
DM-2487,security weekly meeting ,"met with the iso, looking for ways to more actively engage.  idea was to focus on the scada enclave, and the need was to engage with  german etc",1,train
DM-2489,Draft SCADA security plan,nan,10,train
DM-2491,Initial survey of Datacat for LSST ,"jacek, brian van klaveren have sent along some initial overview/description of their work on datacat;      https:/confluence.slac.stanford.edu/display/~bvan/lsstdatacatoverview  we start examining this in the context of our studies of managing data collections at ncsa.",1,train
DM-2492,shapelet unit tests attempts to access display on failure,"when tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  these plots should be disabled, as they obscure the real error when the display is not available.",1,train
DM-2493,prepare v10_1_rc3 release candidate,need for rc3 identified ,10,train
DM-2497,Fix g++ 4.9 return value implicit conversion incompato,"g 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context.  this mean bool checkers along the lines of  bool isvalidfoo()   require an explicit cast to compile under g 4.9 with  std=c0x.  there were a handful of these in our code; found and fixed.",1,train
DM-2498,run jenkins builds on multiple platforms,"demonstrate a jenkins build matrix running lsstswbuild.sh on a number of platforms including; el6, el6, f21, u12.04, & u14.04.",28,train
DM-2500,Mountain - Base fiber path design and installation method,design path and installation method for mountain   base fiber cable.  path will run from cerro pachon to cerro tololo to aura gate.  installation method will define where the fiber cable will be on poles or underground.,40,train
DM-2502,Improve db.createTable,"dm 2417 revealed that the current implementation of createtable in db module behaves differently that mysql: mysql will issue a warning if table exists, and db module will fail with an error. we should make the db behave similarly to how mysql behaves. ",2,train
DM-2503,Doxygenize db,the db module needs to be doxygenized.,1,train
DM-2504,Optimize support for many identical database schemas - design,it is likely we will have 100s or 1000s of identical databases (identical in terms of schema). it'd be good to not repeat the schema information in metaserv. this ticket include coming up with a plan how to implement it.,1,train
DM-2505,Optimize support for many identical database schemas - impl,"it is likely we will have 100s or 1000s of identical databases (identical in terms of schema). it'd be good to not repeat the schema information in metaserv. this ticket include implementing a clean solution, proposed through dm 2504",2,train
DM-2506,Document structure of our custom ddl ascii schema,need to better document what is supported / accepted by schematometa.py. we are currently relying on cat/sql/baselineschema.sql as the guide.,2,train
DM-2507,Information exchange between processes - research,"we need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd).   this story involves understanding key requirements (structures, scale), and researching what mechanism would be best).   deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design.",4,train
DM-2508,Information exchange between processes - implementation,"implement system for information exchange between cmsd and xrootd, per instructions in dm 2507",8,train
DM-2510,Research feasibility of using SQLite as backend to the db module,this story involves plugging in sqlite and dealing with issues that arise as a result of using sqlite in places that depend on the db module.,10,train
DM-2511,The distance field of match lists should be set,"the meas_astrom astrometrytask returns a match list that has distance = 0 for all elements. neither the matcher nor the wcs fitter are setting this field, and both ought to.",2,train
DM-2512,FY17 Integrate Web Services with NCSA Authentication System,we need to integrate data access web services with authentication mechanisms used by ncsa.,40,train
DM-2513,W16 Improvements to db,improvements to db wrapper.,31,train
DM-2514,Migrate to new WBS for 02C.06,migrate to the new wbs structure for 02c.06. work include:  revisiting wbs assignment for all epics  updating https:/confluence.lsstcorp.org/display/dm/s15planning4dbteam  updating ldm240 spreadsheet  updating associated budget accounts   tweaking https:/github.com/jbecla/experimental/blob/master/buildldm240.py,1,train
DM-2515,"Catch ""address in use""","i noticed when running integration tests, it failed with the error pasted below. it'd be good to catch it and print something useful. i am not entire sure what port number is in use, and what to kill...      file ""/usr/local/home/becla/stack201502/repo/qserv/bin/qservwmgr.py"", line 89, in /     sys.exit(main())   file ""/usr/local/home/becla/stack201502/repo/qserv/bin/qservwmgr.py"", line 85, in main     app.run(host)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/sitepackages/flask/app.py"", line 772, in run     runsimple(host, port, self, options)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/sitepackages/werkzeug/serving.py"", line 710, in ru nsimple     inner()   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/sitepackages/werkzeug/serving.py"", line 692, in in ner     passthrougherrors, sslcontext).serveforever()   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/sitepackages/werkzeug/serving.py"", line 486, in ma keserver     passthrougherrors, sslcontext)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/site packages/werkzeug/serving.py"", line 410, in  init     httpserver.init(self, (host, int(port)), handler)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/socketserver.py"", line 419, in init     self.serverbind()   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/basehttpserver.py"", line 108, in serverbind     socketserver.tcpserver.serverbind(self)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/socketserver.py"", line 430, in serverbind     self.socket.bind(self.serveraddress)   file ""/usr/local/home/becla/stack201502/linux64/anaconda/2.1.0/lib/python2.7/socket.py"", line 224, in meth     return getattr(self._sock,name)( args) socket.error: [errno 98] address already in use ",1,train
DM-2518,Add a CFHT-based post-build integration test to the sandbox build,"from     i have installed some simple stack validation tools working on cfht data in /lsst8/boutigny/validcfht    here is the content of the readme file :      this directory contains a set of utilities to validate a stack release with cfht data    at the moment, only validation plots for the astrometry are produced    directories :    rawdownload     : contain raw cfht images (flat, dark, bias, fringe,... corrected)  referenceplots : contain reference plots corresponding to the best results obtain so far.    files :    setup.cfht       : stack environment setup  validcfht.sh    : run processccd taks on the cfht images     validcfht.sh init : create the input/ouput directories, ingest raw images and run processccd     validcfht.sh      : without the ""init"" argument, runs processccd assuming that the directory structure exists and that the raw images have been ingested.  validcfht.py    : run some analysis on the output data produced by validcfht.sh  processconfig.py : configuration parameters for processccd  run.list         : list of vistits / ccd to be processed by processccd    requirements :    obscfht : tickets/dm1593  astrometrynetdata : sdssdr9 reference catalog corresponding for cfht deep field #3       basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stackv10_0    i hope that this is useful. just be careful that i wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. if it is useful, i can certainly add more plots to validate the psf determination, photometry, etc.    comments, suggestions and criticisms are very welcome.",1,train
DM-2519,Check for Qserv processes at configuration tool startup,configuration tool has to check for qserv processes before removing configuration directory (which may contains init.d scripts for these running processes),4,train
DM-2520,Proof of concept Python APIs to access Firefly components,the pipeline needs to visualize the images using firefly. we want to provide a few python apis for proof of concept that we could do this in python and ipython notebook. ,6,train
DM-2521,Update repo.yaml for first set of Sims Stash repo moves,the repos.yaml file needs to be updated with correct repository locations once sim 1074 is completed.,1,train
DM-2522,Implement distributed database deletion,"implement database deletion based on the process defined in dm1396. need to deal with situations like worker is offline  might need some infrastructure e.g., running something in background to act when affected workers come back online.  deliverable: a demonstration of system that deletes a distributed database: user issues ""drop database x"" and all copies of that database on all workers, all replicas of all chunks are deleted. it should be possible to ""create database x"" at any time later.",6,train
DM-2523,Ensure we can delete/create table with the same name,test / ensure that we can create a table with the same name as the table we just deleted.,4,train
DM-2524,Margaret's mgmt. activities in March,catchall story for loe activities in march 2015.  20 working days  7 days vacation = 13 actual days  13 days  2 sp/day  .5 fte = 13 sp.,13,train
DM-2526,Tom Durbin on board as a consultant for the Base site data center.,"prep/attend/follow through for meeting with tom durbin, the facility manager of the national petascale computing facility, to discuss participating as  consultant to the project as it finds a design contractor, and as the design matures.    ",1,train
DM-2528,Mgt activity summary for week of April 6," made inquires about the status of materials contracting  ball in aura's court.  prepare for visit to lyon.  consult with atoll, researched collaborative structures,  articulated and vetted hardware process, made some slides for the visit.  spent time thinking about vo protocols and such in prep for the dm f2f discussion.  edited job descriptions for the ads department ,who will recruit for our systems engineers to include lsst, and lsst concerns.  management / leading by walking around  n.b. margaret in cam training (or associated travel)  tu f. n.b don was off 1 1/2 days  ",3,train
DM-2530,Resolve outgoing port issues on Blue Waters/Cray systems ,"pro data system scaling tests on cray system were limited by the number of outgoing ports on a cray node. the limitation had been  20 ports, participated in tests of new system software,limit relaxed to at least 2000 in tests. likely greater.",3,train
DM-2532,discussed the request from the SUI group for authentication guidance ,"responded to ticket from jacek, on behalf, i think  of the sui group asking for guidance on authentication at ncsa.   so far, consulted with alex withers,  contemplating the extent of policies so far (not much) an authentication mechanism worth investigtaing and likely policies.  drew figure for discussion, wrote up in hip chat.",1,train
DM-2533,Remove version attribute from Schema,"remove the schema attribute and its getters and setters.  this change won't be something we can merge to master on its own, as it doesn't provide backwards compatible fits reading that will added in future tasks.",1,train
DM-2534,Rewrite afw::table FITS reading to be more flexible,"in order to support backwards compatible fits table reading, we need to break the current assumption that everything we need to know about how to read a record from a fits file is contained in the record's schema.  this issue involves that refactoring, without actually adding the backwards compatibility support.",4,train
DM-2535,Backwards compatibility for reading compound fields from FITS,"read old style afw::table compound fields in as scalar fields, using the new functorkey conventions.",2,train
DM-2536,Backwards compatibility for reading slots and measurements from FITS,rename fields to match the new slot and measurement naming conventions.,2,train
DM-2537,Contextual error handling,"there are cases when an empty result might have different errors than the top error, and it would be good to unwrap the context in which the error occured. example: get /meta/v0/db/l3/joe_mydb/tables/object, the result might be empty because the database does not exist, or the v0 is not a supported version, etc.",4,train
DM-2538,RESTful python client,develop basic abstractions for restful apis in a python client,3,train
DM-2541,Research Ceph file system,"research ceph as possible networked filesystem for lsst usage to replace nfs. estimate spending 10 20 hours of work with result being a wiki page of suggestions, limitations, etc.  (implementation will be a different task, presuming we want to implement.)",6,train
DM-2543,Python APIs for Firefly ,we need python apis to interface with firefly visualization components.  this is the first set of many functions.  ,8,train
DM-2544,ctrl_events build issue,had a problem where ctrl_events was having build issues.,1,train
DM-2545,LaTeX support in Doxygen broken,"latex markup in doxygen documentation ought to be rendered properly for display in html. it isn't: it's just dumped to the page as raw text. see, for example, https:/lsst web.ncsa.illinois.edu/doxygen/xlinkmaster2015041507.01.28/classlsst11afw11geom11affine_transform.html#details.",1,train
DM-2546,Host.cc doesn't find gethostname and HOST_NAME_MAX under el7,el7 gives an error that it can't find hostnamemax.,1,train
DM-2547,Fix again interface between QservOss and new cmsd version,"qservoss gives an error when attempting to run queries on the worker from the czar. error log snippet:   qservoss (qserv oss for server cmsd) ""worker"" 150331 16:06:17 9904 meter: unable to calculate file system space; operation not supported 150331 16:06:17 9904 meter: write access and staging prohibited.  worker@lsstdbdev3.ncsa.illinois.edu phase 2 server initialization completed.  cmsd worker@lsstdbdev3.ncsa.illinois.edu:36050 initialization completed.  ",8,train
DM-2549,The string repr of Coord should show the coordsys and angles in degrees,"the default string representation of coord (e.g. std::cout << coord in c and str(coord) in python) is to show class name and a pair of angles in radians.  it would be much more useful if the default display showed the angles in degrees, as that is what people are used to. also, it would be very helpful if the display included the name of the coordinate system. this is especially needed for the base class, as it is quite common to get shared_ptr to coord and have no idea what coordinate system it is.  at present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. but it seems silly to have to do that.",2,train
DM-2551,ANetAstrometryTask's debug doesn't fully work,"anetastrometrytask's debug code calls (deprecated) method task.display, which raises an attributeerror on this coce:    try:      sources[0][0]  except indexerror:              # empty list      pass  except (typeerror, notimplementederror): # not a list of sets of sources    ",1,train
DM-2552,xrootd can't be started via ssh," qserv@clrinfopc04:/src/qserv$ ssh localhost vvv ""qserv/qservrun/201502/etc/init.d/xrootd start"" ... debug3: ignored env  debug1: sending command: ~qserv/qservrun/201502/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channelinputstatusconfirm: type 99 id 0 debug2: exec request accepted on channel 0 starting xrootd.. debug1: clientinputchannelreq: channel 0 rtype exitstatus reply 0 debug1: clientinputchannelreq: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: closeread debug2: channel 0: input open > closed   here ssh command freeze, it is possible to lauch xrootd with this (example) script:  set e set x  . /qserv/run/etc/sysconfig/qserv export qswxrdquerypath=""/q"" export qswdbsock=""$"" export qswmysqldump=`which mysqldump` qswscratchpath=""$/tmp"" qswscratchdb=""qservscratch"" export qswresultpath=""$/result"" export lsstlogconfig=""$/etc/log4xrootd.properties""  eval '/qserv/stack/linux64/xrootd/xssi1.0.0/bin/xrootd c /qserv/run/etc/lsp.cf l /qserv/run/var/log/xrootd.log n worker i v4 &'  echo ""script started""  and the same problem occurs. so the problem seems to be with xrootd, and not the startup scripts.   ",5,train
DM-2554,Remove most compound fields from afw::table,"remove all point, moment, coord, and covariance compound fields.  array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable length arrays if we do.",2,train
DM-2555,Create and advertise Firefly mailing list,create an ipac mailing list for all users of firefly.  advertise it to the interested communities (including the lsst camera group) and through the github site.  the mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.,1,train
DM-2556,Make dbserv async,nan,5,train
DM-2557,Vectorize methods for locating objects on detectors,vectorize transformsinglesys and finddetectors in afw.camerageom so that the sims_coordutils method findchipname (which finds the chips that an object lands on) runs faster.,2,train
DM-2558,Migrate qserv code to reworked db/dbPool,"migrate code to the new implementation of sqlalchemy based db module, including removal of dbpool.",11,train
DM-2563,Chilean Network LOE ,nan,40,train
DM-2572,Addressing File corruption in iRODS 3.3.1,in this issue we examine how file corruption would be detected and repaired with irods tools and rules/microservices.,16,train
DM-2573,read and understood proposal to consider CAS/crowd system ,"the fermi telescope has an authentication system based on cas/crowd. the  benefit of the system is that it can be use as an authentication system for both web and command line.      download materials, and acquire the  understanding from a review of documentation . discuss with the iso,  propose discussion for vtony's visit to ncsa (may 21).",1,train
DM-2574,management activities for week of April 13,"read proposed  ""hardware"" contract amendment, sent marked up comments to julie robinson, u of i contract negotiator.  major points are that hardware is not descriptive of all purchases  needed to fulfill sow.  the procurement approval process needs spelling out. detailed guidance in comments inserted into contract.  along with m. gelman met with the ncsa business people to fully understand the u  of i invoicing process, and the information in the existing business processes. prior to inventing processes for the  supplementing the u of i invoice with the more detailed annotations (hours by wbs) agreed to in the lsst contract.  obtained help from the nscs it group. documented in tow page note.   met concerning seemingly large amount of effort to respond to hip chat take about slowness in the ncsa development system.     miscellaneous and meetings.  ",5,train
DM-2578,Research BeeGFS file system,"research beegfs as possible networked filesystem for lsst usage to replace parts of nfs. estimate spending 10 hours of work with result being a wiki page of suggestions, limitations, etc. (any implementation will be a different task, presuming we want to implement.)  http:/  beegfs (formerly fhgfs) is a parallel cluster file system, developed with a strong focus on performance and designed for very easy installation and management. if i/o intensive workloads are your problem, beegfs is the solution.  likely not good replacement for formal/managed data, but perhaps great option for shared scratch file systems.",5,train
DM-2579,"Calling AliasMap::get("""") can return incorrect results","it looks like empty string arguments can cause aliasmap to produce some incorrect results, probably due to the partial match logic being overzealous.",1,train
DM-2580,Implement user-friendly template customization,"qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each qserv services.    an optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory.    this should be thinked alongside configuration management inside docker container.",5,train
DM-2581,log4cxx build failure on OS X, writes:   i have a log4cxx failure on a macp while building lsst_distrib. attaching file in case someone has any bright ideas for me in the morning ,1,train
DM-2582,Research MaxScale as a mysql-proxy replacement,"we have been told by monty that maxscale is the replacement of the mysqlproxy. based on dm2057 the sentiment is that it won't work for our needs. we should very briefly document what our needs are, how we use the proxy now, and if we think maxscale is not good enough, say it why, and discuss with monty and his team.",5,train
DM-2585,Purchase of network equipment for use in Chile,"jkantor, rlambert",40,train
DM-2586,Base LAN Network Design ,"design of the network at the base to to provide services for the ""tenants"" telescope, camera and dm",20,train
DM-2587,Base Network LOE,nan,40,train
DM-2589,Design the Network from NCSA to Ampath in Florida,nan,20,train
DM-2590,Comparison of ALMA and Reuna/AURA costs on National link,nan,4,train
DM-2591,Comparison of ALMA summit to base link with LSST,nan,2,train
DM-2592,Remove obsolete hinting code in proxy,remove now dead code related to sending hints from proxy to czar,1,train
DM-2593,Client API for new worker management service,"we have new worker management service which has http interface, now we need to provide simple way to access it from python basically wrapping all http details into simple python api. ",8,train
DM-2594,Change repos.yaml for next set of Simulations Stash repos,the next set of simulations stash repository migrations is laid out in sim 1121.,1,train
DM-2595,Symlink data directory at configuration,"we decided to introduce symlinks in order to protect data. this is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. this story introduces symlinks to data: when qserv is reinstalled, only the symlink is destroyed, and the data stay untouched.",5,train
DM-2597,Fiber installation on AURA property from Gate to Pachon,aura and reuna oversee telefonica in installing the fiber from the aura gate to cerro tololo to cerro pachon.,20,train
DM-2599,afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]' ,it seems that afwimage.exposuref ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. this is not the case with afwimage.maskedimagef which correctly uses the input extension number passed in the same way.  the problem has been checked on osx yosemite 10.10.3 with  the is illustrated in  the following code https:/gist.github.com/anonymous/d10c4a79d94c1393a493  which also requires the following image in the working directory: http:/ ,3,train
DM-2600,FY18 Integrate DRP with Data Provenance,"integrate the data provenance system with the drp. this includes capturing hardware and software configuration, as well as dependencies between data sets.  deliverable: system capable of capturing provenance for drp.",54,train
DM-2601,'eups distrib install flask -t qserv' fails on Ubuntu 14.04,"qserv now depends on flask, so this blocks all qserv install which rely on eups.  comman below works with systempython but not with anaconda:   qserv@clrinfoport09:~/stack/eupsbuilddir/linux64/flask0.10.1/flask0.10.1⟫ python setup.py install home /home/qserv/stack/linux64/flask/0.10.1                                                                   running install traceback (most recent call last):   file ""setup.py"", line 110, in /     testsuite='flask.testsuite.suite'   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/core.py"", line 151, in setup     dist.runcommands()   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/dist.py"", line 953, in runcommands     self.runcommand(cmd)   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/dist.py"", line 972, in runcommand     cmdobj.run()   file ""/usr/lib/python2.7/distpackages/setuptools/command/install.py"", line 73, in run     self.doegginstall()   file ""/usr/lib/python2.7/distpackages/setuptools/command/install.py"", line 82, in doegginstall     cmd.ensurefinalized()  # finalize before bdistegg munges install cmd   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensurefinalized     self.finalizeoptions()   file ""/usr/lib/python2.7/distpackages/setuptools/command/easyinstall.py"", line 274, in finalizeoptions     ('installdir','installdir')   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/cmd.py"", line 298, in setundefinedoptions     srccmdobj.ensurefinalized()   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensurefinalized     self.finalizeoptions()   file ""/usr/lib/python2.7/distpackages/setuptools/command/installlib.py"", line 13, in finalizeoptions     self.setundefinedoptions('install',('installlayout','installlayout'))   file ""/home/qserv/stack/linux64/anaconda/masterg68783b1848/lib/python2.7/distutils/cmd.py"", line 302, in setundefinedoptions     getattr(srccmdobj, srcoption))   file ""/home/qserv/stack/linux64/anaconda/master g68783b1848/lib/python2.7/distutils/cmd.py"", line 105, in getattr     raise attributeerror, attr attributeerror: install_layout ",5,train
DM-2602,FY18 Integrate Calibration Pipe with Data Provenance,nan,26,train
DM-2603,FY19 Integrate L3 with Data Provenance,integrate l3 (images and databases) with data provenance.,80,train
DM-2605,Package flask dependencies,"we packaged flask (see dm 1797) and we are using it via eups, but we have not packaged flask dependencies, and we are still relying on anaconda to get them. this story involve packaging the dependencies.",3,train
DM-2606,HSC backport: recent Footprint fixes,this is a backport issue to capture subsequent hscside work on features already backported to afw.  it includes (so far) the following hsc issues:    https:/hscjira.astro.princeton.edu/jira/browse/hsc1135    https:/hscjira.astro.princeton.edu/jira/browse/hsc1129    https:/hscjira.astro.princeton.edu/jira/browse/hsc 1215,2,train
DM-2607,W16 Qserv Refactoring,refactoring of qserv as found necessary in w16.,60,train
DM-2608,FY17 Qserv Refactoring,nan,100,train
DM-2609,FY18 Qserv Refactoring,nan,100,train
DM-2611,API key case study,nan,1,train
DM-2612,Margaret's mgmt. activities in April," weekly dmlt phone meetings  weekly security meetings  weekly local group meetings  t/cam training meeting  1 week w/ travel  attended leadership meetings about ncsa/uiuc receiving and inventory policies and procedures and grant management for project managers   prepared slides for amcl  updated ldm240 milestones for fy16+  created invoice breakdown (incorrectly!)  prepared march technical progress report  prepared travel expense report for t/cam meeting  attempted to update s15 staff plan in pmcs  reviewed risk registry with don  met with julie robinson & aura to work on/discuss procurement contract amendment    met with kaylyn and alan to discuss invoicing, billing process and timeline, wbs activity code breakdown in mis  met with jay and kaylyn about staff planning and budget  met with nathan to figure out how to track a lot of this ev stuff by downloading to a local database and integrating mis information  developed swimlane diagram to understand roles and responsibilities of reporting  started working on gantt chart to wrap my head around tracking resource loading and activity progress",31,train
DM-2613,weekly liaison with ISO,"discussed ""should we piggyback signing of the lsst aup with a capability offered by osg""? with alex.  additional understanding of authorization/authentication.",1,train
DM-2614,Management activities for week of april 21,"met with julie robinson, the illinois contract negotiator, w.r.t. aura ""hardware"" contract amendment.  redrafted long paragraph i aura section, breaking it down into separate items for each party, and addressed what i see a grave flaws so that a discussion could be held.  did work w margaret do arrange for business proscesst discussion relating to monthly reporting to lsst appended to invoices, basic   interviewed martin paegert (one day visit).  further work on other other matters relating to open requisitions of people. other work on personal matters  responded to comments about the ncsa wbs and overall project wbs not being aligned.  on lldm240  provided example of working one case  scheduled for next wee'k lt.  misc.",5,train
DM-2615,LOE - Week ending 5/1/15,"  patch maintenance (kernel, zfs, intel nic, esxi nics) on thursday",12,train
DM-2618,Drop PK on overlap tables in data loader,nan,2,train
DM-2619,Reimplement Data Loader Using Worker Mgmt Service,"current loader depends on ssh, need to switch to the new service, http based.",8,train
DM-2621,Add version stamping in czar and ssi service,"dm2547 will introduce compiletime version generation of a header file that has macros defining version strings. ideally, each running process using qserv code (e.g., czar, cmsd, xrootd, and mysqlproxy), and perhaps oneshot binaries (loader?) would print version information when logging to improve debuggability.  dm 2547 focused on the osslib plugin (libxrdoss) for the cmsd. the next important processes are the czar and xrootd (libxrdsvc). this story covers inclusion of version identifiers (w/ commit hash) in the czar and xrootd logs. hopefully this will end any confusion about versions when reading log files sent from colleagues.",3,train
DM-2622,Modify czar to support table deletion,"czar needs to handle table deletion. in practice that means mysql proxy should let drop table queries through, and czar should modify appropriate tablerelated metadata structures in css. this is part of work proposed in  dm1896.  ",6,train
DM-2623,Design Basic Watcher,"design watcher, including its interactions with other components (mysql, css, etc). in the near term, the watcher will handle deleting tables and databases.",2,train
DM-2624,Implement DROP table in watcher,implement drop table using the watcher designed in dm 2623.,1,train
DM-2625,Create service for managing watcher,we need to be able to start/stop the watcher implemented through dm 2624. this story involves extending our scripts for starting various qserv services to manage watcher.,1,train
DM-2627,Add support for configuring multi-node integration tests,"the multinode integration test software produced through dm2175 has hardcoded node names. this story will allow user to configure it. current plan is to preset integration test for several different configurations, e.g., singlenode, 2node, 8node (and maybe eg 24 node), and user would supply node names through a configuration file.",5,train
DM-2628,Integration test succeeds when individual tests fail,"integration test behaves strangely, it always succeeds even though there may be tests that fail. here is what i ge when i run individual case:  [salnikov@lsstdbdev4 dm2617]$ qservcheckintegration.py i 01 l ............... 20150428 11:26:12,137  lsst.qserv.tests.benchmark  error  mysql/qserv differs for 4 queries: 20150428 11:26:12,138  lsst.qserv.tests.benchmark  error  broken queries list in /usr/local/home/salnikov/qservrun/201504/tmp/qservtestcase01/outputs/qserv: ['0001fetchobjectbyid.txt', '0003selectmetadataforonegalaxywithusing.txt', '0003selectmetadataforonegalaxyclassicjoin.txt', '0003selectmetadataforonegalaxy.txt'] 20150428 11:26:12,138  root  critical  test case #01 failed   but if i run integration test it says everything is ok:  [salnikov@lsstdbdev4 dm2617]$ qservtestintegration.py ................... ok   ran 5 tests in 160.058s  ok   there are actually messages about failed test in the output but you have to look very closely not to miss them. ",1,train
DM-2629,Fix build for gcc 4.7.2 and gcc 4.8.2,#include / is missing in threadsafe.h,1,train
DM-2630,Document configuration tool main use cases," document main use case for qservconfigure.py: install qserv master/worker node with externalized data directory   hide complex configuration options?   configuration steps:   general configuration steps    d, directorytree  create directory tree in qservrundir, eventually                         create symbolic link from qservrundir/var/lib to                         qservdatadir.   e, etc             create qserv configuration files in qservrundir                         using values issued from metaconfig file                         qservrundir/qservmeta.conf   c, client          create client configuration file (used by integration                         tests for example)  components configuration:   configuration of external components    x, xrootd          create xrootd query and result directories   c, csswatcher     configure csswatcher (i.e. mysql credentials)  database components configuration:   configuration of external components impacting data,   launched if and only if qservdatadir is empty    m, mysql           remove mysql previous data, install db and set                         password   q, qservczar      initialize qserv master database   w, qservworker    initialize qserv worker database   s,  scisql          install and configure scisql   ",3,train
DM-2631,"Use WebSocket for communication between client and web server, proof of concept",research and proof of concept code to use web socket for two way communication between client and web server. ,10,train
DM-2632,implementation of Web Socket for two-way communication between client and Web server ,implementation of web socket to be used as the two way communication method between client and web server. ,10,train
DM-2633,"refactor the image stretch code for better, simplified  organization ",nan,8,train
DM-2634,add new image stretch algorithm to Firefly visualization ,"there is a need to include two new stretch algorithms, which are asinh and power law gamma.  the algorithm is as follow:  asinh ## input        zp: zero point of data        mp: maximum point of data        dr:  dynamic range scaling factor of data.  it ranges from 1100,000        bp: black point for image display        wp: white point for image display ## calculate rescaled data value        rd = dr (xpix  zp)/mp ## calculate normalized stretch data value         nsd = asinh(rd)/asinh(mpzp) ## calculate display pixel value        dpix = 255  (nsdbp)/wp       note: the bp, wp values specify how far outside of the scale data one wants the image to display.  by default, setting bp=0 and wp=dr.     power law gamma ## input \br        zp: zero point of data        mp: maximum point of data        gamma: gamma value for exponent ## calculate rescaled data value        rd = xpix   zp ## calculate normalized stretch data value         nsd =  rd(1/gamma) / (mp0zp)(1/gamma) ##  calculate display pixel data value         dpix = 255   nsd       ",8,train
DM-2635,"Provide a function to return the path to a package, given its name","as per rfc 44 we want a simple function in utils that returns the path to a package given a package name. this has the same api as eups.getproductdir, but hides our dependence on eups, as per the rfc.",2,train
DM-2636,Update code to use the function provided in DM-2635,as per rfc44: update existing code that finds packages using eups.getproductdir or by using environment variables to use the function added in dm2635,3,train
DM-2638,Run large scale tests,coordinate running large scale tests.,6,train
DM-2642,missing dependencies in scons builds,"ndarray and afw have some headers generated via m4, and while those are built when the package is installed, if someone just tries to build other targets, they aren't   leading to build failures.  we also need to add a dependency from the ""lib"" target to the ""python"" target, because we can't link the python libraries against the c library until it's built.  that needs to be changed in sconsutils. ",1,train
DM-2643,Migrate Qserv to ssi v2,ssi v2 including comple objectification of the interface. need to migrate qserv to the new interface.,6,train
DM-2646,Switch to using shpgeom and remove duplicate code,qserv is currently relying on a copy of the spherical geometry code (in core/modules/sg) instead of relying on the sphgeom module. this needs to be cleaned once we sort out the build issues with sphgeom (dm 2262).,1,train
DM-2647,TOWG attendance ,remote operations discussion. ,1,train
DM-2648,MGT for balance of April ,"recruiting for open positions work on accounting infrastrucutre. ""hardware"" contact     outline to jeff over the phone what is coming  work through  inventory infrastructure for materials for la sereba  review budget and effort projections.  hear file system invesitigations meeting. ",3,train
DM-2649,Histogram options,"we have studied histogram options supported by exoplanet archive (http:/exoplanetarchive.ipac.caltech.edu/cgibin/tblview/nphtblview?app=exotbls&config=planets)    we'd like to support similar options for our histogram plot. the options are:     column selection   axes options:         linear, log, linear reversed, log reversed        range selection (auto, manual)    binning options        min, max, number of bins        (these can be assigned automatically: nbins = sqrt(npoints))  ",12,train
DM-2652,Prepare external http api for Firefly viewer for beta use,nan,18,train
DM-2653,Fix thread leak in Qserv,qserv is currently leaking a thread per query. executing a simple query list select count( ) from object in a loop results in everything hanging after qserv is up to 67 threads.,6,train
DM-2655,Prepare next gen Firefly JavaScript API Tools for beta,nan,8,train
DM-2656,Look into current transient alert event systems.," research how prior and current alert systems work, skyalert in particular.  install and try current working transient event alert system  catalina. what can we learn from this?  look at new technologies that might help   zeromq messaging, message formats, forms of distribution and archiving, etc.",10,train
DM-2657,Configuration mechanism for GalSim galaxy generation,"this is an additional script for great3sims to allow simple configuration of the great3sims.run().  most of the parameters which need to be set are in great3sims.constants.py, though some additional command line parameters may be needed for the run method.",2,train
DM-2658,Build Psf Libraries from PhoSim Images,"takes output provided by debbie from phosim runs and use them to create libraries of psfs.  warp to remove camera distortion if necessary.  this issue does not include figuring out what different categories of psfs are required, but all of the process issues should be covered in this issue.",4,train
DM-2659,Categorize Psfs and Distributions Required from PhoSim,"request a full focal plane of psf images. write code to allow them to be stored in a way which allows us to sample randomly from a full focal plane.  there will be multiple such focal planes, so we also need to be able to pass the information to the measurement algorithm which will allow us to categorize measurements by visit.  this will be done in the psf library building code, and will then be passes to the measurement algorithm through the great3sims code which constructs the data for the measurement algorithm.",2,train
DM-2660,Produce HSC Psf sample for use in algorithm testing,"produce a set of well distributed psfs from hsc data.  as long as the wcs info is also provided, the code to warp them should have been done in a separate issue.",4,train
DM-2661,Alternative parameterized Psfs from PhoSim,"michael schneider has suggested that he can do a better job of creating realistic psfs from psf models which he is working on, and which he intends to integrate into galsim.  these are intriguing, but depend on work which hasn't been done yet.  when these models are fully available in galsim and supported through the yaml configuration interface, we should work with them.  but this is currently an ""as time permits"" issue.",6,train
DM-2662,Prototype test harness for testing measurement algorithms,"this is a relatively simple task, which will take the galaxy images from the great3sims modifications and run measurement algorithms on the individual postage stamps.  the result will be a catalog of the measurement outputs, cross references against the galaxy and psf parmeters used for a given postage stamp.  to do this, we need to combine information from the galaxy catalog and psf catalog into an input catalog for the algorithm.  a source needs to be created for each galaxy which will contain at least the galaxy centroid and footprint relative to the postage stamp.  the postage stamp with psf appended and the above source much be fed to the measurement algorithm",4,train
DM-2663,Do time tests running measurement algorithms against sample galaxies,"jim has suggest that we use cmodel to run these tests, since he is not committing to completing a complete shape measurement algorithm during the next sprint.  so we will do our timing test using cmodel and shapelet approximation, and switch to the new algorithm from jim when it is available.",5,train
DM-2664,Find an adequate process platform for shape measurement tests,"this issue requires an estimate of how many measurements  we will need to run during s 15.  and it also needs an estimate of how long it will take to measure a single galaxy.  we should be able to guess how many galaxies are required to do an accurate assessment of a single parameterization of the shape measurement algorithm.  we probably cannot accurately estimate how much of the parameter space of the shape measurement algorithm we will have to explore.   the total amount of processing required should tell us whether this can be done with simple multicore systems, or if a more sophisticated parallel process environment is required.  there is a large additional task if ordinary multicore processing isn't adequate, so this task may spawn a rather large additional issue.",4,train
DM-2665,proof of concept types and providers for managing jenkins security settings,"proof of concept level implementation of native puppet types and providers for managing jenkins users, security realm, and authorization strategy.",20,train
DM-2666,Create Analysis code for Constant Shear Tests,"for any test of shear measurement vs. input shear (where input shear is constant), plot the measured shear vs. input shear and fit the multiplicative bias m and additive bias c.",8,train
DM-2667,puppet types & providers for puppet security management,"the current puppet jenkins (and ansible, and chef) can not fully control jenkins users and security realm / authorization strategy settings.  we should develop a proof of concept level limitation of naive puppet types and providers for users, security realm, and authorization strategy.",20,train
DM-2668,Analyze bias vs. postage stamp size of galaxies,"vary the postage stamp size of simulated galaxies and access the effect of that size on the shear bias.  this task will not require additional galaxy image generation, as the intent is to generate all the galaxies at a size which is liberally larger than the likely point where bias does not change with size.  james jee has indicated that 48 pixels on the lsst scale is not large enough enough for this bias to converge.  it seems likely that we will need to generate our images at 64 or 96 pixels to get beyond this limit.",10,train
DM-2669,resolve communication between JavaScript component and java server, we are writing the web application client side code in javascript. js interop will make it much easier to use gwt with javascript libraries. this task is to resolve the issues that may rise with this technology since it is new in gwt2.7.  ,10,train
DM-2671,Build 2015_05 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-2672,Build 2015_06 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-2674,Get meas_mosaic working on HSC data with LSST stack,"we have an old, bitrotted version of measmosaic on the lsst side, created in a failed attempt to get it running on lsst phosim data.  now that we're making a serious effort to get hsc data running through the lsst pipeline, we'll need to get it running with the lsst pipeline at least on hsc data, which will probably involve just merging everything from the hsc side over, and then fixing it until it builds and runs.  for this issue, we'll assume that we're going to use the eigen backend for the matrix solver, rather than the mkl one we use on the hsc side.  that will make it much slower (since mkl is multithreaded and we can't make eigen multithreaded for just measmosaic), but hopefully still usable.",10,train
DM-2675,Research Serf and Consul,serf: https:/serfdom.io  consul: https:/  ,4,train
DM-2678,Investigate loading of binary data,"the binary entries in qservtestdata are stored as binary values in text files and there is no reason to believe that they are being read into the database correctly, see qservtestdata/datasets/case01/data/scienceccdexposure.tsv.gz.     binary data from text files needs to be in hex format along with whatever other changes need to be made to reliably load the data into the database. note that this story involves just investigating, it is not yet clear how much work will be needed to properly implement it. this story will help up understand the effort needed.",5,train
DM-2679,Fix default LOAD DATA options,"integration tests in multinode produced the following error during data loading:  20150501 17:03:03,030  lsst.qserv.admin.dataloader  critical  failed to load data into nonpartitioned table: data truncated for column 'poly' at row 60 20150501 17:03:03,031  root  critical  exception occured: data truncated for column 'poly' at row 60   the default options for mysql load data need to be fixed for this.",1,train
DM-2680,v11.0 release,nan,20,train
DM-2681,Fix race condition in userQueryProxy,"in userquerykill, depending on timing, the call ""uqmanager.get(session)>kill()"" can fail if kill is called more than once by user, because the session might get deleted by the earlier kill. to simulate this, i modified the code to delay the second kill as follows:   void userquerykill(int session)      uqmanager.get(session)>kill(); }   we need to revisit if other functions in this class might suffer in similar way.",4,train
DM-2682,Add missing empty-chunk-path on Ubuntu 14.04,qservdatadir/var/lib/qserv wasn't created on ubuntu 14.04 and this was breaking loader script. it was working on sl7 for unknown reason. creation of the directory has been added to qserv czar config script.,1,train
DM-2683,Fix case05 3009_countObjectInRegionWithZFlux freeze,this prevents 2014_05 release to pass integration tests.,1,train
DM-2684,F17 Experiment with Non-Partitioned Tables,"test sharing of unpartitioned tables between worker nodes. this is something we claimed would work if we simply stuck them on a san, but never tested. now is a good time to find out whether it actually works. if it fails, we need to rethink that part of the design. shall we put the unpartitioned on a set of std replicated mysql nodes and attach them to the worker mysqld via the connect engine? probably worth it to reconsider the overall architecture so that this can be integrated more elegantly (and maintainably) than just bolting on more moving parts.    this epic involves:  a) revisiting the numbers and checking if we could simply replicate nonpartitioned tables on all nodes  b) estimating realistic load on the nonpartitioned tables  c) playing with bringing the nonpartitioned tables through mechanisms such as https:/mariadb.com/kb/en/mariadb/connecttabletypesmysqltabletypeaccessingmysqlmariadbtables/    if replicating on all nodes turns out to be too costly, we will arrange appropriate test bed (at ncsa?) and do the testing in s16 cycle.    ",20,train
DM-2687,Clean up FITS binary table writing,"fits binary table is being refactored by necessity on dm 2534, and while there's no similarly urgent need to clean up the writing code, we should do it at some point, as the refactoring of the read code broke some symmetries and made it even harder to follow the writing code that it already was.",4,train
DM-2689,Review with Telefonica revised path Tololo - Pachon,a meeting in santiago  with reuna and telefonica to discuss the difference in price for the new path from tololo to pachon,2,train
DM-2691,May 1 management,met with a. withers first cut read of scada plan. contact session aura  explain gross contract changes accepted i principle),1,train
DM-2692,Rule for automatic replication in iRODS,"maintaining extra copies/replicas on separate resources is an important tenet in irods, with this practice considered key for prevention of data loss. the automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in irods.",2,train
DM-2693,Revisit mysql connections usage in integration tests,"recent changes in integration tests require too many connections. we need to understand what changes that is now requiring so many connections, and fix it.",4,train
DM-2694,Revisit mysql connections from worker,"revisit the code that handles mysql connections in qserv. at the moment qserv will maintain a connection per chunk query, up to a hardcoded limit (groupscheduler: 4, scanscheduler:32).  also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit).",8,train
DM-2695,Prototype Ceph Deployment,"deploy ceph on spare storage servers, with particular emphasis on deploying ceph fs. this should likely take 8 15 story points over a period of about 3 weeks. this does not include a production deployment of ceph for lsst. it is intended to help us gain insight into requirements for the initial production deployment. the story will primarily consist of effort from m. elliott & w. glick, with secondary effort from m. freemon and b. mather. ",20,train
DM-2696,Research GPFS Server for Performant Access to Condo Storage,work with ncsa set to figure out requirements for lsst gpfs server access to our condo storage. implementation will be a different story. expect this to take 3 6 story points over the next couple of weeks.,1,train
DM-2698,Fix connection leak,"fix connection leak: 1 connection is leaking per chunk query, in practice ~30+ connections for a query that touches many chunks.  it is a real blocker, and we need to fix it asap.",6,train
DM-2699,Final cleanup of Query cancellation code,"the query cancellation code that went in through dm1716 works fine, however we feel it'd be good to do another pass and double check we are applying the cancellation consistently. some potential places to clean: 1. in ccontrol/userquery.cc we changed the semantics of discard() 2. queryrequest needs some cleanup: it'd be better to call finished() from one place  more regarding the former (from dm1716 pr): ""if a query is cancelled, none of the cleanup below happens in discard() anymore  presumably we are now waiting for object deletion to do the cleanup.  if object deletion is sufficient to do this cleanup, do we need discard() at all anymore? it would be best if cleanup always occured in the same place rather than having two different control paths for it?""  regarding the latter  see comment in https:/jira.lsstcorp.org/browse/dm1716",4,train
DM-2703,Fix memory leak in Executive,"there is a memory leak, most likely in executive, related to requesters. it looks like the ~mergingrequester() is never during normal operations (it is called when there are abnormal conditions and different parts of the code are triggered).   as a result infilemerger kept inside merginerequester is not called either, which results in 2 connection leaks per query.",6,train
DM-2705,Latin America Infinera rep to give presentation of equipment,nan,4,train
DM-2707,FY17 Research technologies potentially useful for Data Access,nan,26,train
DM-2708,Understand race condition in Executive::_dispatchQuery,"inserting a log (presumably just a delay) in executive::_dispatchquery after the new queryresource but before the provision call causes queries to fail.  the particular test query was ""select count( ) from object"" on test case 01.",2,train
DM-2709,Convert the ds9 interface to follow RFC-42,"rfc42 (provide a backendagnostic interface to displays) being accepted, please implement it.    for now, provide compatibility code so that the old way (import lsst.afw.display.ds9) still works.  ",6,train
DM-2710,Mutex use before creation,"qana/queryplugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. its constructor is not guaranteed to be called before the static registerxxxplugin (see e.g. qana/aggregateplugin.cc) instances use it to register plugin classes.",1,train
DM-2711,Migrate boost:thread to std::thread,we are mixing boost and std threading libraries. this should be cleaned up   use std:thread consistently everywhere.,5,train
DM-2712,Migrate boost::shared_ptr to std::shared_ptr,"we are mixing boost and std sharedptrs. this should be cleaned up   use std:sharedptr consistently everywhere. in a few places we have other types of pointers, (e.g weak_ptr). migrate these too.",2,train
DM-2715,Add missing includes unistd.h for gcc 4.9.2,nan,1,train
DM-2716,Fix connection leak (2nd iteration),fix connection leak (and memory leak and thread leak)  we are leaking 2 per query.,2,train
DM-2717,Add test involving many chunks,"it might be useful to add a test to the integration test suite that involves a large number of chunks per node. i think i'd try something like 200300 chunks. i'd 1. add case06 2. get one table, say object from case05 and configure partitioning to ensure we have 200300 chunks. 3. run several queries that touch all chunks.",5,train
DM-2718,Upgrade EUPS used by lsstsw,"as discussed, bump it up when you get a chance please. ",1,train
DM-2720,Migrate boost::scoped_ptr to std,"we have a few places where we are using boost::scopedptr. given we migrated sharedptrs, we might want to move scopedptrs too (most likely to std::uniqueptr).",1,train
DM-2722,Revisit design of query poisoner,"as we discovered through dm 2698, poisoner tends to hold onto query resources even after the query completes. we should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. this story involves just the planning part, implementation will be done through separate stories.",1,train
DM-2723,LOE - Week ending 5/8/15,nan,12,train
DM-2724,LOE - Week ending 5/15/15,nan,3,train
DM-2725,LOE - Week ending 5/22/15,nan,3,train
DM-2726,LOE - Week ending 5/29/15,nan,12,train
DM-2727,Package Python requests package,to complete dm 2593 we need to package and install `requests` as a separate package instead using one from anaconda.,1,train
DM-2728,Build should fail if node.js is not present,"problem: i built firefly by mistake w/o having node on my path. the build didn't signal any errors, but generated an unusable webapp that wouldn't load.  expected behavior: the build should have failed and warned the user that node.js is missing.",2,train
DM-2729,Fix a few more g++ 4.9.2 compatos,"some of the recent boost > std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including / when we should include /, not explicitly including /, etc.)  also, pthread linker option is required when using std::thread under gcc 4.9.2. ",1,train
DM-2733,Generalize / Simplify Facade ,"daniel started thinking about simplifying facade, here is some unfinished code from him   / unfinished. planned to be a rethinking of facade that collapses some / genericity and simplifies things using the assumption of running on a / snapshot. class facadesnapshot : public facade       virtual bool containsdb(std::string const& dbname) const          string p = prefix  ""/dbs/""  dbname;         bool ret =  (map.find(p) != map.end());         logfdebug(""containsdb(%1%): %2%"" % dbname % ret);         return ret;     }     virtual bool containstable(std::string const& dbname,                                std::string const& tablename) const          if (tablename.empty())          string p = prefix  ""/dbs/""  dbname  ""/tables/""  tablename;         bool ret =  (map.find(p) != map.end());         logfdebug(""containstable returns: %1%"" % ret);         return ret;     }     virtual bool tableischunked(std::string const& dbname,                                 std::string const& tablename) const          string p = prefix  ""/dbs/""  dbname  ""/tables/""                 tablename  ""/partitioning"";         bool ret =  (map.find(p) != map.end());         logfdebug(""%1%.%2% %3% chunked.""                    % dbname % tablename % (ret?""is"":""is not""));         return ret;     }     virtual bool tableissubchunked(std::string const& dbname,                                    std::string const& tablename) const      virtual bool ismatchtable(std::string const& dbname,                               std::string const& tablename) const          string p = prefix  ""/dbs/""  dbname  ""/tables/""  tablename  ""/match"";         stringmap::constiterator m = map.find(p);         bool ret = (m != map.end()) && (m >second == ""1"");         logfdebug(""%1%.%2% is %3% a match table""                    % dbname % tablename % (ret ? """" : ""not ""));             return ret;     } #if 0     virtual std::vector/ getalloweddbs() const ;     virtual std::vector/ getchunkedtables(std::string const& dbname) const;     virtual std::vector/ getsubchunkedtables(std::string const& dbname) const;     virtual std::vector/ getpartitioncols(std::string const& dbname,                                                       std::string const& tablename) const;     virtual int getchunklevel(std::string const& dbname,                               std::string const& tablename) const;     virtual std::string getdirtable(std::string const& dbname,                                     std::string const& tablename) const;     virtual std::string getdircolname(std::string const& dbname,                                       std::string const& tablename) const;     virtual std::vector/ getsecindexcolnames(std::string const& dbname,                                                          std::string const& tablename) const;     virtual stripingparams getdbstriping(std::string const& dbname) const;     virtual double getoverlap(std::string const& dbname) const;     virtual matchtableparams getmatchtableparams(std::string const& dbname,                                                  std::string const& tablename) const;   private: #endif }; ",14,train
DM-2734,Add config file for test dataset 04 tables,"following the changes to default load data settings in dm 2679, two tables in test case 04 need to have a config file to include their in.csv format.",1,train
DM-2735,optimistic matcher may match the same reference object to more than one source,"the optimistic pattern matcher in meas_astrom, adapted from hscastrom, does not check if reference objects have been used before when finding the reference object nearest to each source. as a result the same reference object may be matched to more than one source. this should not happen.",4,train
DM-2736,Log xrootd client debug messages in Qserv czar,"xrootd client print it's debug messages to stdout. this ticket aims at redirecting them to qserv logger, if possible.",4,train
DM-2737,Build a DiscreteSkyMap that covers a collection of input exposures,"this is essentially a rehash of the old trac ticket # https:/dev.lsstcorp.org/trac/ticket/2702, originally reported by [jbosch], which reads:  ""i'd like to add a task and bin script to create a discreteskymap that bounds a set of calexps specified by their data ids. this makediscreteskymap.py could be used instead of makeskymap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually.""  the work was done by [jbosch] &  and exists on branch u/price/2702 in pipe_tasks, but it was never merged to master.  i plan to simply rebase the commits in that branch onto master.",1,train
DM-2738,"Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code","see next emails:  hi fabrice,  absolutely!  andy  on wed, 13 may 2015, fabrice jammes wrote:  > hi andy, > > thanks, > > in my understanding, you're ok if i remove the existing > #include ""xrdouc/xrdouctrace.hh"" > from qserv source code. i'll do it soon. > > have a nice day, > > fabrice > > le 12/05/2015 23:41, andrew hanushevsky a écrit : >> hi fabrice, >> >> well, no. we have a longstanding approach that qserv should not depend on anything outside of xrdssi public interfaces. this is the only way to easily protect sqserv code from infrastructure changes. so, i would not. if you want to copy something like that for >> >> qserv please do, it's simple enough. but in the end qserv needs to be selfcontained in that it does not depend on xrootd code just the public ssi interfaces. >> >> andy >> >> original message from: fabrice jammes >> sent: tuesday, may 12, 2015 9:06 am >> to: andrew hanushevsky >> subject: about xrdssi client logging >> >> hi andy, >> >> hope you're doing well. >> could you please tell me if its usefull to include >> #include ""xrdouc/xrdouctrace.hh"" >> in our xrdssi client code? >> >> indeed client seems to only print dbg macro output, that's why i was >> wondering if xrdouctrace was only use on the server side. >> if yes, i will remove it from our client. >> >> thanks, and have a nice day, >> >> fabrice ",1,train
DM-2740,Make ANetAstrometryTask more configurable,"the current anetastrometrytask has a solver that is not easy to retarget. this makes testing with hscastrom needlessly difficult. my suggestion is to make the solver a true task instead of a task like object, and make it retargetable using a configurablefield instead of a configfield. this is very easy to do because the solver is already a task in all but name. ",2,train
DM-2743,sandbox selection of newinstall.sh source url,"frossie would like the ability to control the source url for the newinstall.sh script in sandboxstackbuild.  the newinstall.sh installation logic needs to be migration to the puppetlsststack module, converted into a defined type, and have unit+ acceptance tests written for it.",1,train
DM-2744,Second Review with Chris Smith AURA head,went over the process relating to aura and nsf,4,train
DM-2745,Design of the summit network computer facility,jeff ,40,train
DM-2748,Add clear message when integration test fails,"integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectdeepcoadd.txt but it isn't printed at the end of tet output.",2,train
DM-2750,Fix case04/0011_selectDeepCoadd.txt,"it seems config=/path/to/table.cfg param can be duplicated (see dbloader l.77 and qservdbloader l. 87)    futthermore there is an enclosing pb and it can be solved for this query by passing correct cfg table (which in.csv.enclose correct parameter), but then next query fails: it seems some cfg parameters of table.cfg aren't managed correctly by the loader in plain mysql mode.     this need further investigations.",6,train
DM-2751,Allow lsst/log library to log PID on the C++ side,lsst/log should be able to log application pid,3,train
DM-2752,db 10.1+4 tests randomly fail with python egg installation error,"the unit tests for db seem to fail at random and always pass on a second build attempt.  my hunch is that multiple tests are running in parallel all attempting to install the mysql module but i haven't investigated.                     db: 10.1+4 error (0 sec).  error building product db.  exit code = 2  log is in /home/build0/lsstsw/build/db/build.log  last few lines: :::::  [20150515t19:12:35.557258z] scons: done reading sconscript files. :::::  [20150515t19:12:35.558276z] scons: building targets ... :::::  [20150515t19:12:35.558409z] scons: nothing to be done for `python'. :::::  [20150515t19:12:35.570007z] makeversionmodule([""python/lsst/db/version.py""], []) :::::  [20150515t19:12:35.686733z] running tests/testdblocal.py... running tests/testdbremote.py... running tests/testdbpool.py... failed :::::  [20150515t19:12:35.695011z] passed :::::  [20150515t19:12:35.698811z] passed :::::  [20150515t19:12:35.706360z] 1 tests failed :::::  [20150515t19:12:35.706703z] scons:  [checkteststatus] error 1 :::::  [20150515t19:12:35.708443z] scons: building terminated because of errors.    [root@ip192168123151 .tests]# cat  tests/testdblocal.py  traceback (most recent call last):   file ""tests/testdblocal.py"", line 53, in /     from lsst.db.db import db, dbexception   file ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in /     import mysqldb   file ""build/bdist.linuxx8664/egg/mysqldb/init.py"", line 19, in /        file ""build/bdist.linuxx8664/egg/mysql.py"", line 7, in /   file ""build/bdist.linuxx8664/egg/mysql.py"", line 4, in bootstrap   file ""/home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py"", line 937, in resourcefilename   file ""/home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py"", line 1632, in getresourcefilename   file ""/home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py"", line 1662, in extractresource   file ""/home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py"", line 1003, in getcachepath   file ""/home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py"", line 983, in extractionerror pkgresources.extractionerror: can't extract file(s) to egg cache  the following error occurred while trying to extract file(s) to the python egg cache:    [errno 17] file exists: '/home/build0/.pythoneggs'  the python egg cache directory is currently set to:    /home/build0/.pythoneggs  perhaps your account does not have write access to this directory?  you can change the cache directory by setting the pythoneggcache environment variable to point to an accessible directory.  tests/testdbpool.py  /home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py:1032: userwarning: /home/build0/.pythoneggs is writable by group/others and vulnerable to attack when used with getresourcefilename. consider a more secure location (set with .setextractionpath or the pythoneggcache environment variable). 05/15/2015 07:12:35 root warning: required file with credentials '/home/build0/.lsst/dbauthtest.txt' not found. tests/testdbremote.py  /home/build0/lsstsw/anaconda/lib/python2.7/sitepackages/setuptools5.8py2.7.egg/pkgresources.py:1032: userwarning: /home/build0/.pythoneggs is writable by group/others and vulnerable to attack when used with getresourcefilename. consider a more secure location (set with .setextractionpath or the pythoneggcache environment variable). 05/15/2015 07:12:35 root warning: required file with credentials '/home/build0/.lsst/dbauthtestremote.txt' not found. ",1,train
DM-2754,Write example-based documentation for multiband processing,"the multiband coadd processing tasks we're porting over from the hsc side don't have the highquality example based documentation we typically provide for tasks on the lsst side, so we need to write it from scratch.",6,train
DM-2755,Improve selection criteria for sources,"dominique boutigny has demonstrated that one reason the new astrometry task is working so poorly is that it is not selective enough about which sources it uses. this ticket is to be used to improve that situation.  another problem dominique discovered is that the tan sip wcs fitter needs to be iterated to work properly, and that work may also be done on this ticket. ",4,train
DM-2756,Configure NCSA LSST Perfsonar Host,nan,3,train
DM-2757,Administrative - 6-2015,meetings and reporting and such,2,train
DM-2758,Create LSST wiki documentation for LHN effort,nan,3,train
DM-2759,Onboarding Humberto,efforts in helping new employee humberto come up to speed in his role as lead on perfsonar deployments,1,train
DM-2762,Avoid leaking memory allocated by mysql_thread_init,"mysql/mysqlconnection.cc contains the following comment:      / dangerous to use mysqlthreadend(), because caller may belong to a     / different thread other than the one that called mysqlinit(). suggest     / using threadlocalstorage to track users of mysqlinit(), and to call     / mysqlthreadend() appropriately. not an easy thing to do right now, and     / shouldn't be a big deal because we thread pool anyway.   the comment is not really correct with regards to thread pooling. instead, each rproc::infilemerger has an rproc::infilemerger::mgr which contains a util::workqueue that spawns a thread, and so we are failing to call mysqlthreadend at least once per user query. this has been verified using the memcheck valgrind tool. ",3,train
DM-2763,Mountain - Base fiber implementation,"aquire, install, and test fiber connecting mountain   base.   the fiber will follow a path along roads from cerro pachon to cerro tololo and down to the aura gate, where it will connect with telefonica fiber bundle to la serena.",40,train
DM-2764,Improve management of ColSchema.hasDefault and ColSchema.defaultValue,"managing default values in protobuf and result table isn't optimized for now. indeed all values are packed in protobuf, whereas default values could be removed from protobuf messages (in .queryaction::impl::_fillrows())  it is interesting to monitor performance when packing default values, and when not, and then improve the code related to default value management, or completely remove it.",10,train
DM-2765,AURA Traffic Utilizing the Fiber Link at 10Gbps,obtain equipment in order to light the fibers up to 10gbps and run live traffic for tololo and pachon over the link,30,train
DM-2766,Fix ORDER BY in integration test query case03 0019.1.0 ,"order by fails sometimes for unknown reason, see  datasets/case03/queries/0019.1.0_selectrundeepsourcedeepcoadddeepsrcmatchrefobject.sql.fixme",4,train
DM-2768,investigate decomposition of stack build into independent packages,"in order to obtain per package build time, test time, coverage, or virtually any per component metric ,the ci build needs to be decomposed from a single large integrated build into per package jobs with an overall work flow representing the dependency graph.    this is also needed for binary artifacts to be passed between builds step and/or binary packages.",10,train
DM-2769,sconsUtil has a hard dependency on EUPS for both tests and installation,"after some discussion on data management, its clear that sconsutils is a hard requirement on eups for both tests and installation.  it was decided by rfc44 that tests should not depend on eups.  however, i'd argue that sconsutils should also not depend on eups as any package that uses sconsutils (the virtual entirety of the stack) can not build or run tests without the presence of eups.  the current situation is that the complete stack has a hard dependency on eups.    attempting to build sconutils without the presence of eups.  the tests fail attempting to import the eups module.    $ sconsutilsdir=. scons q  unable to import eups; guessing flavor  cc is gcc version 4.8.3  checking for c11 support  c11 supported with 'std=c11'  unable to import eups; guessing flavor  doxygen is not setup; skipping documentation build.  importerror: no module named eups:    file ""/home/vagrant/sconsutils/sconstruct"", line 9:      scripts.basicsconstruct.initialize(packagename=""sconsutils"")    file ""python/lsst/sconsutils/scripts.py"", line 106:      scons.script.sconscript(os.path.join(root, ""sconscript""))    file ""/usr/lib/scons/scons/script/sconscript.py"", line 609:      return method(args, kw)    file ""/usr/lib/scons/scons/script/sconscript.py"", line 546:      return sconscript(self.fs, files, substkw)    file ""/usr/lib/scons/scons/script/sconscript.py"", line 260:      exec file in callstack[1].globals    file ""/home/vagrant/sconsutils/tests/sconscript"", line 5:      import eups      attempting to bypass the test failures:    [vagrant@jenkinsel71 sconsutils]$ rm rf tests  [vagrant@jenkinsel71 sconsutils]$ sconsutils_dir=. scons q install  unable to import eups; guessing flavor  cc is gcc version 4.8.3  checking for c11 support  c11 supported with ' std=c11'  error with git version: uncommitted changes  found problem with version number; update or specify force=true to proceed  ",6,train
DM-2770,sconsUtil install target does not respond to either force=True or --force,"i've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out:    https:/github.com/lsst/sconsutils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsutils/installation.py#l92      $ sconsutils_dir=. scons q force=true install   unable to import eups; guessing flavor  cc is gcc version 4.8.3  checking for c11 support  c11 supported with 'std=c11'  error with git version: uncommitted changes  found problem with version number; update or specify force=true to proceed    ",1,train
DM-2775,Improve SIP fitting,"dominique boutigny says ""the tansip fitter is very sensitive to bad matches. this is a weakness of the fitter and i think that it could (should) be rewritten in such a way to reject the outliers internally.""  this has resulted in iteration in the matching (dm2755), which should be unnecessary (or at least minimised).    additionally, it seems the sip fitter fits for x and y in subsequent iterations, which can confuse users.    we should:  1. make the sip fitter fit for x and y concurrently.  2. add rejection iterations in the sip fitter.  3. remove or minimise the iterations in the matching.",6,train
DM-2777,Fix races in BlendScheduler,"_integrityhelper() from wsched/blendscheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. my theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar.",2,train
DM-2778,HSC backport: allow for use of Approximate model in background estimation,this issue involves transferring changesets from the following hsc issues:     https:/hscjira.astro.princeton.edu/jira/browse/hsc145  investigate approximating rather than interpolating backgrounds   https:/hscjira.astro.princeton.edu/jira/browse/hsc1047 background object cannot be loaded with butler   https:/hscjira.astro.princeton.edu/jira/browse/hsc1213 set background 'approximate' control settings when background control is created.   https:/hscjira.astro.princeton.edu/jira/browse/hsc1221 tests failing in ip_diffim   https:/hscjira.astro.princeton.edu/jira/browse/hsc1217 verify backgroundlist io works properly when approximate is enabled in background control  hsc jira    the approximate (chebyshev) approach greatly improves the background subtraction around bright objects compared with the interpolation scheme currently in use (which oversubtracts near bright objects).,6,train
DM-2779,Fix race in Foreman,the foreman implementation passes a taskqueue pointer corresponding to running tasks down to the task scheduler without holding a lock. this means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.,2,train
DM-2780,Document and test how to log PID via lsst/log,nan,2,train
DM-2781,push PID in lsst/log MDC in a C++ plugin (for xrootd),nan,2,train
DM-2782,Firefly Tools API: Add advance region support,"firefly tools api: add advance region support  improve firefly's region functionality to support a ""dynamic region"".  data can be added or removed from this region by api calls.  allow any amount of region lines to be added or removed.  make sure performance is good.  also, document the current firefly region support.",2,train
DM-2783,Control firefly viewer tri-view mode,"when table data is add to firefly viewer, control whether it goes into tri view or just overlay data on fits, or just does an xyplot, etc",6,train
DM-2784,Add Firelfy Tools API controlled Pan and Zoom,nan,2,train
DM-2785,FFTools python wrapper: make launch Browser smarter. ,"fireflyclient.launchbrowser() needs to send an event to the server who will attempt to guess if there is an existing connection.  it will not be launch in that case.  this way it can be called every time without creating tons of tabs, who are all talking to the same channel.    also, launchbrowser really should not return until the tab is ready to receive events from the websocket channel.  both of these feature are going to take some thought on how to do.  this is a multi threaded problem on both the client and the server.    ",4,train
DM-2786,"FFTools api, wrapper: upload region file from memory like fits file",nan,1,train
DM-2787,Footprint dilation performance regression,"in dm1128 we implemented spanbased dilation for footprints. a brief test on synthetic data indicated that this was a performance win over the previous version of the code.    in may 2015, this code was merged to hsc and applied to significant quantities of real data for the first time. a major performance regression was identified:      [may9 00:26] paul price: processccd is now crazy slow.  [may9 00:29] paul price: profiling...  [may9 00:40] paul price: i'm thinking it's the footprint grow code...  [may9 00:44] paul price: and the winner is…. footprint construction:  [may9 00:44] paul price: 2    0.000    0.000  702.280  351.140 /home/astro/hsc/products/linux64/measalgorithms/hsc3.8.0/python/lsst/meas/algorithms/detection.py:191(makesourcecatalog)         2    0.005    0.002  702.274  351.137 /home/astro/hsc/products/linux64/measalgorithms/hsc3.8.0/python/lsst/meas/algorithms/detection.py:228(detectfootprints)       15    0.001    0.000  698.597  46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionlib.py:3448(init)       15  698.596  46.573  698.596  46.573   [may9 00:53] paul price: if i revert hsc1243 (""port better footprintgrow code from lsst""), then the performance regression goes away.  @jbosch @jds may be interested...      the source of the regression must be identified and resolved for both hsc and lsst.",5,train
DM-2789,rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method,"per discussion on this pr related to dm2636:  https:/github.com/lsst/daf_butlerutils/pull/1#issuecomment104785055    the cameramapper.geteupsproductname() should be renamed to getpackagename() and converted to an abstract method.  this will eliminates a runtime, and thus ""test time"", dependency on eups.  as part of the rename/conversion, all subclasses that are not already overriding geteupsproductname() will concurrently need to have getpackagename() implemented.",3,train
DM-2790,meas_modelfit not in full-stack doxygen build,"i'm fairly certain measmodelfit is included in lsstapps and hence in ci, but id doesn't seem to be included in the lsst doxygen build:    http:/lsst web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/search.php?query=modelfit",1,train
DM-2792,Make the new astrometry task the default task,"the new astrometry task should be the default astrometry task, but we need to make sure it is good enough first.",1,train
DM-2793,Improve behavior of new matcher on highly distorted fields,"the optimistic matcher used by the new astrometry task probably does not handle highly distorted fields well. the issue is that it tries to match in x y space, and if that has significant curvature then the match is not optimal.    i suggest matching in ra/dec space, as per tabur's original algorithm (on which this code is based). this is simple and easy to understand    an alternative is to use the old technique of ""undistorting"" source and reference object positions before matching. this works, but is complicated, difficult to understand and adds an unnecessary step.  ",8,train
DM-2794,Fiber from Tololo to Pachon,preparation for fiber cable from cerro tololo to cerro pachon,5,train
DM-2795,Various fixes for broken code within display=True clauses and/or using --debug,running with the display and/or debug options turned on is revealing many instances of code that is now suffering from bit rot.  this ticket will be used to track those encountered while trying to debug issues arising while porting hsc code and running processing tasks on real data.,4,train
DM-2797,Implement HSC improvements to Colorterm,"paul price recommended some hsc changes for the colorterm class. to quote paul: changed colorterms so it's not a global, and it can now be configured using config. see https:/github.com/hypersuprimecam/measastrom/blob/master/python/lsst/meas/photocal/colorterms.py    this sounds useful. note that the hsc colorterms.py is in measastrom but as of dm1578 the lsst version is in pipe_tasks.",6,train
DM-2799,Tests for daf_butlerUtils should not depend on obs_lsstSim,currently two of the tests in dafbutlerutils depend on obslsstsim. they will never run in a normal build because obs packages can not be a dependency on dafbutlerutils.    after discussing the options with  the feeling is that ticket1640 should be rewritten to remove the dependency and ticket1580 can probably be removed.,2,train
DM-2800,Review with Contractors preparing fiber path,hold conversations with the two major fiber laying contractors to prepare the path from tololo to pachon with a trench,2,train
DM-2801,Document NCSA Wide Area Network status now and in the near future,write up a document explaining how the wide area network is evolving at ncsa.,2,train
DM-2802,W16 Finish Implementing Database & Table Mgmt,implement table and database deletion.,29,train
DM-2803,Adapt multi-node tests to latest version of qserv / loader,"the multi node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately.",8,train
DM-2804,Implement query metadata skeleton,skeleton implementation of the query metadata   including the apis and core functionality (accepting long running query and saving the info about it),8,train
DM-2805,Complete Query Metadata Implementation,including query abort,10,train
DM-2806,Run large scale tests,nan,6,train
DM-2807,Chile National Links Contracts Negotiation,the negotiations and conversations that have occurred with firstly entel and finally reuna and telefonica for dark fiber between la serena and santiago,60,train
DM-2808,La Serena - Santiago Dark Fiber Acquisition,acquiring the fiber between la serena and santiago and testing the segments. this will be carried out by reuna with some collaboration with lsst personnel,15,train
DM-2809,La Serena - Santiago Link Equipment installation,reuna will install the amplifiers and the dwdm equipment for the link between la serena and santiago with collaboration with lsst personnel ,20,train
DM-2810,La Serena - Santiago LInk with Live Traffic flowing,the link is established and tested with live lsst data running over the 100g lambda,10,train
DM-2813,Mountain to Base Implementation Plan Feasibility Check/Reevaulation,the installation of the fiber on the aura property from gate to pachon via tololo. testing that portion of the link.  connecting the 4 dark fibers supplied by telefonica and testing end to end.,80,train
DM-2814,Fiber lay between Gate to Tololo,telefonica will install the fiber cable from the gatehouse to tololo. this will be tested once terminated. we will oversee and monitor the installation and be present for testing.,40,train
DM-2815,Fiber Install from Tololo to Pachon ,telefonica will install the cable from tololo to pachon sahred infrastructure building and terminate. testing of this portion will be carried out.,20,train
DM-2816,Connection of Telefonica 4 Fibers at AURA Gate ,the connection will made with the telefonica 4 dark fibres to the 24 fiber cable from the gate to pachon.  tests will then take place over the whole mountain to base link.,10,train
DM-2817,Creation of chunked views in wmgr,current implementation of the creating chunks for views in wmgr is likely not doing right thing. need to find an example of the partitioned views and implement correct procedure.,3,train
DM-2818,Document architecture of the data loader,fabrice requested documentation for the overall architecture of the data loader.,2,train
DM-2819,Install 10Gbs Transceivers at the ends of the fibers and test,purchase 2x10gbs transceivers and install at pachon and la serena.  run tests to confirm integrity of the link  utilize the link for aura live traffic on the fiber backbone,20,train
DM-2820,AURA traffic utilizing 100Gbs Lambda,aura acquires their dwdm end nodes and installs. tests and live traffic flows.,30,train
DM-2821,Contract Negotiations for Chilean links,defining the contracts between aura and reuna and reuna and telefonica who is supplying and installing the fibers for the la serena to santiago link,20,train
DM-2822,Mountain - Base Contract and Execution,defining the contracts and execution between aura and reuna and reuna and telefonica who is supplying and installing the fibers for the mountain to base link,30,train
DM-2823,La Serena - Santiago Early Diverse Path,reuna will provide a 4gbps path via the legacy fiber path to santiago,10,train
DM-2824,La Serena - Santiago Diverse Path Final Capacity,reuna will upgrade the capacity from 4gbps to a minimum of 40gbps over the legacy fiber route to santiago,10,train
DM-2825,La Serena - Santiago Fiber Tests over 3T cable,the 3t cable from la serena to santiago is expected to be completed september 2015 at which time reuna can test the segments along the route.  ,20,train
DM-2826,Mountain - Base AURA link upgrade to 100Gbps,aura will obtain their dwdm equipment end nodes and install on the backbone from the summits to la serena.,30,train
DM-2827,Implement RESTful interfaces for Database (POST),"implement restful interfaces for database (see all d  in https:/confluence.lsstcorp.org/display/dm/api), based on the first prototype developed through dm 1695. the work includes adding support for returning appropriately formatted results (support the most common formats). this covers ""post"" type requests only, ""get"" will be handled separately.",8,train
DM-2830,orphan threads in archive DMCS,"the archive dmcs can experience orphaned threads if a connection is made from external processes waiting for data to arrive.   if the external process goes away, the thread that was created to handle that connection will be waiting on a data structure to be updated.   if the data doesn't arrive, the thread remains alive when it should be checking to see if the connection that created it is still viable, and die if it isn't viable.",17,train
DM-2833,LOE - Week ending 6/5/15,nan,5,train
DM-2834,LOE - Week ending 6/12/15,nan,3,train
DM-2835,LOE - Week ending 6/19/15,nan,3,train
DM-2836,LOE - Week ending 6/26/15,nan,1,train
DM-2837,Add unit tests for the new colorterms code,"the new colorterms code that we adopted from hsc may not have complete unit tests. the existing colorterms test is pretty good, but may have holes. i'm more concerned about the unit test for photocaltask, which does not apply colorterms at all (likely an existing issue).    also, be sure to test that the obscfht config override loads correctly (presumably with a unit test in obscfht) and similarly for obs_subaru.",4,train
DM-2838,Documentation and testing for Firefly Javascript and Python API,"document, polish and test firefly javascript and python apis     proofread and polished all the documentation, added missing docs   tested all the examples and api methods. updated test cases as needed.",10,train
DM-2839,sconsUtils should notice when SWIG python file has been modified,currently scons will not rerun tests if a .i file has been modified if the only outcome of that modification was a change to the .py file. sconsutils should be modified to look for changes in both swig output files.,2,train
DM-2840,Add support for listing async queries,"modify mysql proxy and implement ""show processlist"" command, which should display list of currently running queries.",4,train
DM-2841,Write Qserv User Guide,"it'd be useful to write a document about qserv geared towards users, describing what queries qserv supports now, what will be supported in the future, what restrictions we are imposing and such. ",8,train
DM-2843,Distributed Hash Table prototyping,nan,20,train
DM-2844,reserved,nan,10,train
DM-2845,Near Neighbor Optimizations,"to optimize near neighbor queries we are maintaining overlap tables and subchunking. it'd be useful to revisit that. getting rid of subchunks would simplify qserv code. this epic involves    testing speed of in database near neighbor queries without subchunking, including how sensitive the optimizer is for these types of queries    exploring possibility of precalculating and storing near neighbors, perhaps per subchunk  ",15,train
DM-2847,SUI Firefly server side Python job management,"in order to support camera team needs and l3 data production, firefly server needs to be able to start a python job with proper input data and get the output data as a result of running the python job. this will make the future integration of firefly and dm pipeline stack much easier. ",40,train
DM-2849,Tweaks to OO display interface,"when i wrote the initial version of displayfirefly i found a few minor issues in the way i'd designed the display class; at the same time,  found some missing functions in the backward compatibility support for ds9.    please fix these;  note that this implies changes to afw, displayds9, and display_firefly.  ",2,train
DM-2850,getSchemaCatalogs() breaks Task encapsulation,"the getschemacatalogs() method was added to task to allow cmdlinetasks to introspect their subtasks for schemas they produce, but it requires the subtasks to report the schemas by butler dataset.  this limits subtask reusability by locking them into producing a particular butler dataset (or, as in dm2191, requiring additional arguments from their parent task that they wouldn't need with a better design).    instead, we should have persubtask slot interfaces (i.e. an interface for all subtasks that could fill a particular role in a cmdlinetask) for how the parent tasks should retrieve their schemas.  this will require `cmdlinetask` subclasses to implement the `writeschemas` method themselves, instead of inheriting an implementation from `cmdlinetask` itself.",2,train
DM-2851,Build the recent 10.1 release  & Gather strace logs for file system testing,"we build the recent version 10.1 stack release in a centos 6.6 docker container. as we do so, we also gather strace logs for candidate packages  (for example, afw) for analysis within an effort to create load simulators for file system testing/profiling.   as another product of the effort,  i will make a docker image of the latest release installed on centos 6.6 and push to  docker hub.",4,train
DM-2852,AAA requirements document,nan,2,train
DM-2853,Put together a few slides for NCSA-IN2P3 meeting," i put together a few slides for the ncsa in2p3 meeting describing previous scaling, middleware, and processing efforts of lsst dm. ",1,train
DM-2854,Fix Qserv SsiSession worker race,"the worker ssisession implementation calls releaserequestbuffer after handing the bound request to the foreman for processing. it therefore becomes possible for request processing to finish before releaserequestbuffer is called by the submitting thread, resulting in a memory leak.",2,train
DM-2855,Margaret's mgmt. activities in May,"weekly dmlt meeting  weekly iso meeting  weekly ncsa local group meeting    met with ncsa networking person (paul) to discuss progress and plans  attended endtoend networking meeting    attended remotely the ccsdaqocsdm workshop for scada presentation by iso    worked on/discussed aura procurement contract amendment and to understand property management procedures at aura and ncsa    discussed/planned agenda, made travel arrangements, prepared slides for trip to ccin2p3  2day meeting in france with ccin2p3 group    4day dmlt faceto face meeting and t/cam day    interview with candidate for systems lead    cleaned up jira tickets from april  reviewed reporting requirements and jira procedures with ncsa employee (bruce) and managers (doug, brett)    attended several ncsa leadership development and training courses",22,train
DM-2856,Multi-processing capability for shear test measurements,"a suitable multi cpu capability must be created for measurement tests.  we are hoping to just use a pipetask, but to do so, the butler must be customized to allow it to read our cutouts and psfs from galaxies and psfs generated from galsim and phosim.    this will be a relatively simple story if pipetasks running on 2 or 3 machines at uc davis proves to be an adequate solution for running our shear experiments.",6,train
DM-2858,"Add support for ""ORDER BY f1, f2"" for has-chunks query","   querysession description:    original: select objectid, taimidpoint from   source order by objectid, taimidpoint asc;  has chunks: 1    needs merge: 1    1st parallel statement: select objectid,taimidpoint from lsst.source%cc% as qst1_    merge statement: select objectid,taimidpoint order by objectid,,taimidpoint asc    scantable: lsst.source      merge statement has syntax error",4,train
DM-2859,"Return error for ""SELECT a FROM T ORDER BY b"" for has-chunks query",order by field has to be in result table => it has to be in select list.  return clear error message to user if not.,4,train
DM-2861,Enquiry into MiniSub FO cable,obtaining a quote from a company in canada for a special clad cable for the tololo pachon link,2,train
DM-2862,RFI with vendors in Vina,open day with all interested vendors  to layout the projects for equipment on mountainbase and la serenasantiago links.,8,train
DM-2863,Validate wmgr client / server versions ,"if the client and server are on different versions, unexpected things can happen. (example: we run old version of the server, and use latest client). we need to check the version on both sides. ",4,train
DM-2864,Fix bug related to selecting rows by objectId from non-director table,"the following example illustrates the problem:    let's select one raw from qservtestcase01qserv      select sourceid, objectid from source limit 1;     objectid         29763859300222250         then select it, but use ""sourceid"" in the query, all good here:    select sourceid, objectid from source where sourceid=29763859300222250;     objectid         29763859300222250         but if we add ""objectid"", the row is not found:      select sourceid, objectid from source where sourceid=29763859300222250 and objectid=386942193651348;  empty set (0.09 sec)      similarly, even without sourceid constraint, the query fails:    select sourceid, objectid from source where objectid=386942193651348;  empty set (0.09 sec)      ",8,train
DM-2865,Merge BoundedField from HSC as is,"to make headway on aperture corrections, we are bringing the hsc implementation of boundedfield over.",2,train
DM-2866,Learn about Butler,transferring knowledge from k t to the db team.,2,train
DM-2867,Learn about Butler,transferring knowledge from k t to the db team.,2,train
DM-2868,Learn about Butler,transferring knowledge from k t to the db team.,2,train
DM-2869,Learn about Butler,transferring knowledge from k t to the db team.,2,train
DM-2870,Learn about Butler,transferring knowledge from k t to the db team.,2,train
DM-2871,Document butler v2 and transfer knowledge to Nate,clean up and release prototype implementation of butler v2.,9,train
DM-2872,Setting with CoordKey doesn't support non-IcrsCoord arguments,something in the functorkey template resolution doesn't allow coord arguments to be used when setting record values with a coordkey (only icrscoord arguments work.,1,train
DM-2873,"Handle ""where objectId between""","query in a form:      select objectid   from object   where objectid between 386942193651347 and 386942193651349      currently fails with    error 4120 (proxy): error during execution:  1 ref=1 resource(/chk/qservtestcase01qserv/6630): 2015060220:41:43, complete (success), 0,   ref=2 resource(/chk/qservtestcase01qserv/6631): 2015060220:41:43, state error (unrecognized), 0,   ref=3 resource(/chk/qservtestcase01qserv/6800): 2015060220 ( 1)      we already documented that such queries are not advised, but nethertheless qserv should handle it better, e.g., return a message ""not supported"" ",6,train
DM-2877,demo package should contain the same comparison script used by CI,"the lsstdmstack_demo package currently used in the ci system contains a bin/compare script that doesn't do all of the checks done by the numdiff script that buildbot runs.  these need to be unified, so users can anticipate buildbot results and reproduce buildbot failures locally, especially when making changes to the expected results file.    while the numdiff script currently checks more columns than the compare script, i believe the compare script follows a much better approach and should be extended to be used in both places (numdiff converts everything to ascii then compares text files; compare works directly from the binary results and uses numpy to do the comparisons).",2,train
DM-2879,Add transformation tasks for new Butler dataset types,the new butler dataset types created as part of the hsc deblender merge will need transformation tasks so they can be ingested to the database.    see also dm 2191.,2,train
DM-2881,Revisit Parser / IR,"revisit the existing parser code    consider reusing the code from maxscale or (antrl3) mysql parser from mysqlworkbench, or maybe reuse http:/savage.net.au/sql/sql 92.bnf.html and wrap in bison    separate ir node productions from grammar",94,train
DM-2882,S15 Qserv CSS v2,"revisit qserv common state system. implement mysql based kv interface, and add support for updates. implement ""locking"" mechanism. ",36,train
DM-2883,wcslib is unable to read PTF headers with PV1_{1..16} cards,"scamp writes distortion headers in form of pvinn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14;  there is a discussion at https:/github.com/astropy/astropy/issues/299    the simplest ""solution"" is to strip the values pv1nn (nn=5..16) in makewcs()  for ctypes of tan or tan sip and this certainly works.    i propose that we adopt this solution for now.  ",1,train
DM-2884,LOG() macro fails if message is a simple std::string,lsst:log log() macro crash with fatal error if message is a simple string.,2,train
DM-2885,Improve confusing error message,"selecting a column that does not exist results in confusing error. example:      select badcolumnname  from qservtestcase01qserv.object   where objectid=386942193651348;      error 4120 (proxy): error during execution:  1 ref=1 resource(/chk/qservtestcase01qserv/6630): 2015060516:23:42, error in result data., 1, (1)    similarly,       select whatever   from qservtestcase01qserv.object;      prints  error 4120 (proxy): error during execution:  1 ref=1 resource(/chk/qservtestcase01qserv/6630): 2015060516:23:52, error in result data., 1,   ref=2 resource(/chk/qservtestcase01qserv/6631): 2015060516:23:52, error merging result, 1990, cancellation requested  ref=3 resource(/chk/qservtestcase01qs ( 1)    (note, sourceid does not exist in object table)      ",5,train
DM-2887,Fix broken IN - it now takes first element only,"in is broken  it only uses the first element from the list. here is the proof:      select count() as n from qservtestcase01qserv.source   where objectid=386950783579546;            1 row in set (0.10 sec)    mysql> select count() as n from qservtestcase01qserv.source  where objectid=386942193651348;            1 row in set (0.09 sec)    mysql> select count() as n from qservtestcase01qserv.source  where objectid in(386942193651348, 386950783579546);            1 row in set (0.09 sec)    mysql> select count() as n from qservtestcase01qserv.source  where objectid in(386950783579546, 386942193651348);             1 row in set (0.11 sec)  ",8,train
DM-2890,isrTask assumes that the Exposure has a Detector,while trying to use the isrtask to interpolate over bad columns in ptf data i discovered that the code assumes that the exposure has a detector attached.    please remove this restriction.  ,1,train
DM-2892,Keep track of database of the director table,"an l3 child table might very well have an lsst data release object table as its director, while almost certainly not living in the dr database. to support it, we should keep track of the database name holding director's table. note, this is related to dm2864  the code touched in that ticket should be checking the director's db name.    don't forget to add a unit test that will exercise it!",1,train
DM-2893,Improve qproc unit testing framework,"qproc unit testing framework allow to test the whole query analysis pipeline, it has grow and should be re organized to be easilly understandable, maintainable.",5,train
DM-2895,"treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib","per discussion on rfc 55, it was determined that  lsstapps and lsstlibs and lsstthirdparty maybe be treated as separate top level products that lsstdistrib need not depend on them nor do they need to be included as part of ci builds.",1,train
DM-2897,Travel to CCIN2P3 to establish realtionship,travel to cc in2p3.  of establish operatal coordination between the sites,10,train
DM-2898,Prototype file system loading tools for file system studies.  ,worked on a tool chain to    extract file io patterns from program strike.   represent in a flat file.  generate codes     to make many dependent copies dummy files and directories    to generate a python code to read and write files like the original application.   to generate a c code to read and write files like the original application.    a driver program to run the pseudo codes in parallel.    the system runs on toys and needs to be show to work on traces from       ,10,train
DM-2899,Host/Attend DM LT meeting ,host/attend dm lt meeting ,7,train
DM-2900,Add queries that exercise non-box spatial constraints,qserv has code to support:    qservareaspecbox    qservareaspeccircle    qservareaspecellipse    qservareaspecpoly    but only the first one (box) is exercised in our integration tests. this story involves adding queries to test the other 3.,2,train
DM-2903,Clean up gitolite,"we need to clean up gitolite and cgit:   repositories that have moved to github should be removed (or, possibly, mirrored back from github).   empty repositories (like contrib/eups.git) and obsolete repositories (like lsst/dms/afwextensionsrgb.git) should be removed altogether.   contrib/data_products.git (the data products definition document source) should be moved to github in the lsst org.   contrib/processfile.git should be moved to github in the lsstdm org unless the author adds some test cases and it can be integrated into the ci system as a toplevel product (in which case it can go into the lsst org).   the primary authors of other contrib repositories should be contacted to see if they should be moved to github in the lsstdm or another org (possibly a new lsstcontrib org).  in particular, contrib/plotz/ (paul lotz of the telescope and site subsystem) and contrib/pyreb (in2p3 work for the camera subsystem) contain current work that should be moved.",4,train
DM-2905,Update Scons to v2.3.4,scons has not been updated in over a year. rfc 61 agreed that we should upgrade it now before tackling some other scons issues.,1,train
DM-2908,Add Gaussian PSF example to measurement task documentation,"i see some documentation on how to add a placeholder gaussian psf to an image (to work around the fact that some algorithms require a psf) was recently added to the release notes.  i don't think that's actually appropriate, as the same algorithms also required psfs in the framework  the failure mode was just different (previously, it'd just result in all objects being flagged and the peak position used for the centroid, so it may have been easy to miss  hence the change to a fatal error).    i propose moving the example to the documentation for singleframemeasurementtask, and taking it out of the release notes.  i'll also make sure there's a link from the measurement framework overhaul release notes page to the doxygen for singleframemeasurementtask  i'm not sure if that's sufficient to make up the visibility gap between the doxygen docs and the release notes in confluence, but i don't have any other shortterm ideas.",1,train
DM-2909,Remove unused code from sconsUtils,the code in deprecated.py in sconsutils is not used by anything anywhere.  has indicated that the file can simply be removed.,1,train
DM-2910,obs_cfht is broken with the current stack,"obscfht's camera mapper is missing the new packagename class variable, so it is not compatible with the current stack.    i suggest fixing obssdss and obs_subaru as well, if they need it.",1,train
DM-2911,Build 2015_07 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-2913,Port HSC Curve-of-Growth code,"port content from hsc1144, hsc1236, hsc1223, hsc1219, hsc1203, hsc1153.",6,train
DM-2914,Improve handling of extremely large blends,port hsc code from issues:   https:/hscjira.astro.princeton.edu/jira/browse/hsc1250   https:/hscjira.astro.princeton.edu/jira/browse/hsc1245   https:/hscjira.astro.princeton.edu/jira/browse/hsc1237   https:/hscjira.astro.princeton.edu/jira/browse/hsc1268   https:/hscjira.astro.princeton.edu/jira/browse/hsc1274   https:/hscjira.astro.princeton.edu/jira/browse/hsc1265    https:/hscjira.astro.princeton.edu/jira/browse/hsc 1228,10,train
DM-2915,Port safe coadd clipping from HSC,"we have an algorithm on the hsc fork that modifies assemblecoaddtask to clip outliers in a much safer way, based on detecting contiguous regions in the difference between a nonclipped coadd and an aggressivelyclipped coadd, and only rejecting pixels that are outliers in a single epoch.    one complication for this code transfer is that some of the coadd code has been refactored on the hsc side, and there may be code in hscpipe that duplicates much of what's in pipe_tasks.  we may need help from https:/hscjira.astro.princeton.edu/jira/browse/hsc1166    https:/hscjira.astro.princeton.edu/jira/browse/hsc1202",18,train
DM-2917,obs_cfht unit tests are broken,"obscfht has one unit test ""testbutler"" that uses git:/git.lsstcorp.org/contrib/price/testdatacfht. 4 of the tests fail, as shown below.    in addition, testdatacfht is huge, and the tests barely use any of it. it's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obscfht itself.      localhost$ tests/testbutler.py   cameramapper: loading registry registry from /users/rowen/lsst/code/testdata/testdatacfht/data/registry.sqlite3  cameramapper: loading calibregistry registry from /users/rowen/lsst/code/testdata/testdatacfht/calib/calibregistry.sqlite3  ecameramapper: loading registry registry from /users/rowen/lsst/code/testdata/testdatacfht/data/registry.sqlite3  cameramapper: loading calibregistry registry from /users/rowen/lsst/code/testdata/testdatacfht/calib/calibregistry.sqlite3  ecameramapper: loading registry registry from /users/rowen/lsst/code/testdata/testdatacfht/data/registry.sqlite3  cameramapper: loading calibregistry registry from /users/rowen/lsst/code/testdata/testdatacfht/calib/calibregistry.sqlite3  ecameramapper: loading registry registry from /users/rowen/lsst/code/testdata/testdatacfht/data/registry.sqlite3  cameramapper: loading calibregistry registry from /users/rowen/lsst/code/testdata/testdatacfht/calib/calibregistry.sqlite3  .cameramapper: loading registry registry from /users/rowen/lsst/code/testdata/testdatacfht/data/registry.sqlite3  cameramapper: loading calibregistry registry from /users/rowen/lsst/code/testdata/testdatacfht/calib/calibregistry.sqlite3  e.  ======================================================================  error: testbias (main.getrawtestcase)    traceback (most recent call last):    file ""tests/testbutler.py"", line 122, in testbias      self.getdetrend(""bias"")    file ""tests/testbutler.py"", line 110, in getdetrend      flat = self.butler.get(detrend, self.dataid, ccd=ccd)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasettype, dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataid), write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/cameramapper.py"", line 287, in mapclosure      return mapping.map(mapper, dataid, write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 118, in map      actualid = self.need(self.keydict.iterkeys(), dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 199, in need      lookups = self.lookup(newprops, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 345, in lookup      return mapping.lookup(self, properties, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 168, in lookup      where, self.range, values)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/registries.py"", line 120, in executequery      c = self.conn.execute(cmd, values)  operationalerror: no such column: extension    ======================================================================  error: testflat (main.getrawtestcase)    traceback (most recent call last):    file ""tests/testbutler.py"", line 117, in testflat      self.getdetrend(""flat"")    file ""tests/testbutler.py"", line 110, in getdetrend      flat = self.butler.get(detrend, self.dataid, ccd=ccd)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasettype, dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataid), write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/cameramapper.py"", line 287, in mapclosure      return mapping.map(mapper, dataid, write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 118, in map      actualid = self.need(self.keydict.iterkeys(), dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 199, in need      lookups = self.lookup(newprops, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 345, in lookup      return mapping.lookup(self, properties, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 168, in lookup      where, self.range, values)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/registries.py"", line 120, in executequery      c = self.conn.execute(cmd, values)  operationalerror: no such column: extension    ======================================================================  error: testfringe (main.getrawtestcase)    traceback (most recent call last):    file ""tests/testbutler.py"", line 127, in testfringe      self.getdetrend(""fringe"")    file ""tests/testbutler.py"", line 110, in getdetrend      flat = self.butler.get(detrend, self.dataid, ccd=ccd)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasettype, dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataid), write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/cameramapper.py"", line 287, in mapclosure      return mapping.map(mapper, dataid, write)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 118, in map      actualid = self.need(self.keydict.iterkeys(), dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 199, in need      lookups = self.lookup(newprops, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 345, in lookup      return mapping.lookup(self, properties, newid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/mapping.py"", line 168, in lookup      where, self.range, values)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafbutlerutils/10.13g302a9ed/python/lsst/daf/butlerutils/registries.py"", line 120, in executequery      c = self.conn.execute(cmd, values)  operationalerror: no such column: extension    ======================================================================  error: testraw (main.getrawtestcase)  test retrieval of raw image    traceback (most recent call last):    file ""tests/testbutler.py"", line 101, in testraw      raw = self.butler.get(""raw"", self.dataid, ccd=ccd, immediate=true)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 244, in get      return callback()    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 242, in /      innercallback(), dataid)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 238, in /      callback = lambda: self.read(pythontype, location)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/butler.py"", line 426, in read      location.getcpptype(), storagelist, additionaldata)    file ""/users/rowen/lsst/lsstsw/stack/darwinx86/dafpersistence/10.11g6edbc001/python/lsst/daf/persistence/persistencelib.py"", line 1430, in unsaferetrieve      return persistencelib.persistenceunsaferetrieve(self,  args)  fitserror:     file ""src/fits.cc"", line 1064, in lsst::afw::fits::fits::fits(const std::string &, const std::string &, int)      cfitsio error: could not open the named file (104) : opening file '/users/rowen/lsst/code/testdata/testdatacfht/data/raw/08bl05/w2.22/20081101/i2/1038843o.fits.fz[1]' with mode 'r'   lsst::afw::fits::fitserror: 'cfitsio error: could not open the named file (104) : opening file '/users/rowen/lsst/code/testdata/testdata_cfht/data/raw/08bl05/w2.2+2/20081101/i2/1038843o.fits.fz[1]' with mode 'r''        ran 6 tests in 3.544s    failed (errors=4)  ",1,train
DM-2919,PhotoCalTask mis-calling Colorterm methods,"when i implemented dm2797 i made a few errors in pipe_tasks:   photocaltask miscalls two methods of colorterm by providing filtername, which is not needed   colortermlibrary.getcolorterm mishandles glob expressions (the two arguments to fnmatch.fnmatch are swapped).    we also need a unit test for applying colorterms, but that will require enough work that i have made a separate ticket for it: dm2918. meanwhile i have tested my changes by running dominique's cfht demo. this proves that the colorterm code runs, but does not prove that the terms are correctly applied.",1,train
DM-2920,Clean up code in afw for Approximate background estimation,"the intention is to eventually set useapprox=true (i.e. chebychev approximation)  as the default for background estimation.  however, in looking into the relevant code in afw/math while working on dm2778, there is some cleanup and restructuring that needs to be done before resetting the defaults (which may also require adjusting some defaults in the calibrate stage to be more appropriate for the approximation, as opposed to inperpolation, scheme).  this issue is to clean up the code and make sure it all operates coherently.  a seperate ticket will be to actually reset the defaults and make any other config default changes required.    in particular, the config setting of approxorderx/binsize are not being assessed properly, nor is the behavior of the given undersamplestyle being executed.  the undersampling checks are currently only being done against the interpoation settings, which is not appropriate when useapprox=true.  a temporary check was added in meas.algoritms.detection.getbackground() in dm2778 so that it is currently ""safe"" to run with useapprox=true and any other user overridden config setting (binsize, approxorder, undersamplestyle) (and there currently exists similar checks in pipe.tasks.matchbackground), but these should be removed once this issue has been implemented. ",3,train
DM-2921,Set Approximation as default for background subtraction,"once the approximate code in afw.math has been cleaned up (see dm2920), set the default for background subtraction to be the chebychev approximation (i.e. useapprox=true).  ensure any other relevant config defaults (e.g. binsize, approxorderx) are adjusted appropriately.  this will change the outputs of the lsstdmstack_demo, so the ""expected"" files will need to be replaced along with this change in default settings (see dm2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes).",1,train
DM-2922,Initial DC Base dseign,prepare a document for the initial design of the base and summit networks,6,train
DM-2923,Port HSC-1199 to LSST (UNMASKEDNAN mask propagates to all amplifiers),port issue hsc 1199 to lsst stack to address unmaskednan mask propagates to all amplifiers,4,train
DM-2934,Add RFD issue type to RFC project,"to support the rfd process adopted in [rfc53], an rfd issue type in the rfc project is required.  while we could add rfdspecific fields to it, i think it's simplest if it's just generic with details provided in the description.",1,train
DM-2927,Modernize sconsUtils code to python 2.7 standard,"as part of the work investigating dm2839 i modernized the sconsutils code to meet current coding standards (using in rather than has_key, using items() rather than iteritems etc). since i'm highly doubtful that dm2839 is going to be closed any time soon i will separate out the modernization patches into this ticket.",1,train
DM-2928,move old ingest scripts into and retire old packages,"this ticket implements rfc57, by:    renaming datarel to dafingest (there is already a dafingest package, but it's completely empty, so i'll just forcepush it all away)    removing everything from the renamed package that doesn't relate to ingest (including pruning dependencies)     removing ap and testing_endtoend from the ci system  ",1,train
DM-2929,Some AFW tests are not enabled with no explanation,"running coverage.py on the afw test suite indicated that two test classes in tests/wcs1.py are disabled. wcstestcasecfht was added by [rhl] in 2007 but disabled during a merge a long time ago by [jbosch] in 2010 but with no indication as to why. wcsrotateflip appeared in 2012 (added by [krughoff]) but doesn't appear in the suite list at the end and so does not execute.    similarly testschema.py has two tests that are not run: xtestschema and testjoin. i assume xtestschema is deliberately disabled but could there at least be a comment in the test explaining why?    my feeling is that we should either run the tests or they should be removed. having them their gives the impression they are doing something useful.    less importantly, warpexposure.py has some support code for comparing masked images that was written in 2009 by [rowen] but which is not used anywhere in the test.",2,train
DM-2930,Fix problem with Qserv related to restarting mysql,"i noticed some strange (reproducible!) behavior: if i run:    qservcheckintegration.py case=01    then restart mysqld    /etc/init.d/mysqld restart    then the query:  mysql host=127.0.0.1 port=4040 user=qsmaster   qservtestcase01qserv e   ""select count( ) as objcount   from object   where qservareaspec_box(0.1, 6, 4, 6)""    consistently fails every single time.    to fix it, it is enough to restart xrootd.",5,train
DM-2931,We write truncated Wcs data to  extended HDU tables in Exposures,"when we write wcs to extra hdus in exposures they are truncated if other than tan/tan sip.  please don't write them.    a better long term solution is needed.  in particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. tpv to the tables so as to support coaddpsf.  these issues are not included here.",1,train
DM-2932,Test install of OCS software on CentOS VMs,install current version of the ocs software onto two vms,24,train
DM-2935,qserv-admin CREATE NODE fails,  qserv > create node worker1 type=worker host=worker 1 port=5012 rundir=1;  06/15/2015 05:59:52 qadm error: missing parameter. (mysqlconn)  error:  missing parameter. (mysqlconn)    ,1,train
DM-2936,Refactor Histogram in edu.caltech.ipac.visualize.plot package.,"the histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer,  integer, long integer, float and double.  since fitsread has now only works on float, there the  histogram should be refactored accordingly.",3,train
DM-2938,"CalibrateTask has an unwanted ""raise"" in it","on 20140630 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the calibratetask in pipe_tasks. that change was accidentally merged to master. i confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master.",1,train
DM-2939,fix usage of obsolete astrometry interfaces in ProcessImageTask,"as discussed recently on hipchat (science pipelines standup), there's code in proocessimagetask that assumes an ""astrometer"" attribute on a calibratetask instance.  since this is just needed to match a new set of sources against the reference catalog here, we should probably be using one of the new matcher objects, either by getting one from calibratetask via a documented interface, or by constructing a new one.",4,train
DM-2940,DS9 tests fail if DS9 not running in some configurations,there are a few issues with the robustness of the testds9.py tests in afw.     the tests are skipped if the displayds9 package can not be loaded but they should also skip if ds9 is missing or if ds9 can not be loaded. the latter is especially important during builds that unset $display.   the launching code in initds9 can not notice the simple case of ds9 immediately failing to load. it simply assumes that there are delays in launch. the reason for this is that os.system does not return bad status if the command has been started in the background. another scheme for starting ds9 should be considered. maybe a different exception could be raised specifically for failing to start it.   at the moment each test independently has a go at starting ds9. this makes the tests take a very long time (made worse by mtv also trying multiple times) despite it being clear pretty quickly that ds9 is never going to work.   currently the mtv tests must run early as they are the only tests that attempt to start ds9 if it is not running. if the two tests that call mtv are disabled two other tests fail. ideally the initds9 code should be called in all cases.,1,train
DM-2945,Wmgr refuses to serve queries from remote interface,vaikunth discovered that wmgr returns 404 for all operations. it looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non local interface.,1,train
DM-2948,Remove explicit buildbot dependency on datarel,"the buildbot scripts have an explicit dependency on the datarel package, which we'd like to remove from the stack.  it uses datarel as the toplevel product when building the crosslinked html documentation; lsstdoxygen's makedocs script takes a single package, and generates the list of packages to include in the doxygen build by finding all dependencies of that package.    so, to remove the explicit dependency on datarel, we need to either:    find a new toplevel product with a doxygen build to pass to makedocs (e.g. by adding a trivial doxygen build to lsst_distrib)    modify the argument parsing in lsstdoxygen to take a list of multiple products (it looks like the limitation to one package is only in the argument parsing), and pass it a list of toplevel products in the buildbot scripts.    this is currently a blocker for dm2928, which itself a blocker for dm1766, which has now been lingering for a few weeks now.  i'm going to look for other ways to remove the block on the latter, but i don't have a solution yet.",3,train
DM-2949,remove dead code and dependencies from datarel,"removing the datarel package entirely has proved to be difficult (dm2928, dm2948), so instead i'm simply going to remove noningest code (and dead ingest code) from the package, along with its dependencies on ap and testing_endtoend.  other dependencies will be retained even if they aren't necessary for the code that will remain in datarel, to support lsstdoxygen's use of datarel as a toplevel package for documentation generation.",1,train
DM-2951,Refactoring the class CropAndCenter,this class contains the codes which are not used.  it needs to be simplified and refactored. ,4,train
DM-2952,Crop needs to be refactored,this class needs to be refactored to be in consist with fitsread class which treats all data type as float.  thus the bitpix in this class does not have to be treated based on its value.,3,train
DM-2953,Qserv code cleanup and auto_ptr --> unique_ptr migration,"code cleanup, including migrating some parts to c11 (in particular, autoptr > uniqueptr)",4,train
DM-2954,Add a unit test for aperture corrections in measurement task,dm436 adds code to meas_base that allows one to run a subset of measurement algorithms based on execution order. this addition should have a unit test.    dm436 also tasks to measure and apply aperture correction. those tasks should have unit tests.,6,train
DM-2955,Setting up and running PhoSim for Psf Library,"debbie bard leaving created some new work creating the psf libraries we need.  while this od not a major task except for computer time, there is some setup required.  i will get an account at slac and learn to run the phosim utilities she and michael have developed.    in the short term, simon is going to do some runs for me.  meanwhile, i will get into debbie's account and run her configuration at slac.    the outputs then need to be checked to be sure that the psfs are reasonable.",4,train
DM-2956,Migrate Qserv code to nullptr,nan,3,train
DM-2958,Review ITIL V3.0 as prep for input to IT use case,"itil is a standard breakdown of processes used in an it system.  while full itil may very well be too heavy lsst operations, it provides a useful checklist for he use cases being developed in the towg.   i created an itil type spreadsheet to check against the dump of the workflows in the ea  tool  ",3,train
DM-2959,add metric to  application specific IO benchmarking tool,"iosim is an applicationspecific benchmarking tool that is developed to be responsive to the request from lsst to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load.  the week the software was     optimized to allow for faster test cycles.    threads were supported by squashing all thread io into a single simulation process.   information from the original ""model"" program is propagated to the simulation program, allowing for comparison to the model.    initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added    the goals to deliver this capability in a few week when common systems at ncsa are available for testing.",4,train
DM-2960,Management for Don for week of June 15,on boarded mattais carrasco kind to work in the process execution in the context of level 3 processing.   misc.,4,train
DM-2961,Port the psfextractor external library from HSC to LSST,nan,14,train
DM-2962,Prototype iRODS tiered resource with NERSC HPSS,"irods can support access to a tape archive with the use of a ""tiered resource"" where one resource has the role of the cache, and a second has the role of archive.    use of such a tiered resource construct could be valuable to data management.   because a irods plugin for hpss is readily available, we examine the set up and use of the tiered resource testing against nersc hpss.",4,train
DM-2964,Read through and comment on latest version of LSE-78,nan,1,train
DM-2966,Design CSS that supports updates,"design how to redesign css, we currently take a snapshot when char starts. it is too static. ",2,train
DM-2967,Fix to DM-2883 isn't quite right,"the fix to dm2883 (remove illegal pvij cards) isn't quite right, and the error was masked by a piece of code elsewhere that duplicated the functionality.    the issues is that while pv1[14] cards are indeed valid, the ones that scamp writes are not.  so we should remove them too, if there are any other scamp tpv coefficients.    the masking code was a unilateral removal of pvi_j cards dating back years.  ",1,train
DM-2972,Discourse evaluation (Part 1),work in support of evaluation discourse as a dm platform for internal and external interactions.,3,train
DM-2975,Quantify how much objects are blended,it would be useful to have a parameter that indicates how much any given galaxy is blended. this will be useful for testing how photometry or shears are affected by blending effects.    ports code from hsc 1260.,2,train
DM-2976,SourceCatalog.getChildren requires preconditions but does not check them,this is a code transfer from hsc 1247.,2,train
DM-2977,Miscellaneous CModel improvements from HSC,"this improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for jose garmilla's tests.    includes hsc1288, hsc1284, hsc1228, hsc1250, hsc1264, hsc1273, hsc1240, hsc1249, hsc1238, hsc990, hsc1155, hsc1191",2,train
DM-2978,FootprintMerge: fix bug when identifying existing peaks in a merge.,"if two separate footprints from the same catalog happen to be merged because an existing merged object overlaps both of them, the flags of which peaks are being detected in which bands is not being propagated. this is causing the apparent dropout of some sources in a merged catalog which were detected in single frame processing.    taken from ticket hsc 1270",1,train
DM-2980,refactor coaddition code,"the hsc fork has coaddition code in two places: pipetasks and hscpipe.  the code in hscpipe is what we use (though that depends on the code in pipetasks in places), while the code in pipetasks is more similar to what's currently on the lsst side.    we want to bring the refactored version in hscpipe back to lsst, but we want to put it directly in pipetasks to remove the code duplication that currently exists on the hsc side.    work on this issue should begin with an rfc that details the proposed changes.    note that this should not bring over the ""safe coadd clipping"" code, which is dm 2915.",5,train
DM-2981,polygon masking in CoaddPsf,"we need to create polygonbased masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of psfs and aperture corrections.    this includes hsc issues hsc972, hsc973, hsc974, hsc975, hsc976.    at least some of this will be blocked by dm 833, which is the port issue for coaddition of aperture corrections.",8,train
DM-2982,Updating node status in qserv-admin to INACTIVE fails,"in qservadmin.py when attempting to update a node status from active to inactive the following error is produced:      > update node worker2 state=inactive;  traceback (most recent call last):  file ""/usr/local/home/vaikunth/src/qserv/bin/qservadmin.py"", line 650, in /  main()  file ""/usr/local/home/vaikunth/src/qserv/bin/qservadmin.py"", line 645, in main  parser.receivecommands()  file ""/usr/local/home/vaikunth/src/qserv/bin/qservadmin.py"", line 163, in receivecommands  self.parse(cmd[:pos])  file ""/usr/l  ocal/home/vaikunth/src/qserv/bin/qservadmin.py"", line 180, in parse      self.funcmap[t](tokens[1:])    file ""/usr/local/home/vaikunth/src/qserv/bin/qservadmin.py"", line 380, in parseupdate      self.parseupdatenode(tokens[1:])    file ""/usr/local/home/vaikunth/src/qserv/bin/qservadmin.py"", line 405, in parseupdatenode      self.impl.setnodestate(options)    file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservadmin.py"", line 660, in setnodestate      self.kvi.set(nodekey, state)    file ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvinterface.py"", line 415, in set      self.zk.set(k, v)    file ""/usr/local/home/vaikunth/qserv/linux64/kazoo/2.0b11/lib/python/kazoo2.0b1py2.7.egg/kazoo/client.py"", line 1170, in set      return self.setasync(path, value, version).get()    file ""/usr/local/home/vaikunth/qserv/linux64/kazoo/2.0b11/lib/python/kazoo2.0b1 py2.7.egg/kazoo/client.py"", line 1182, in set_async      raise typeerror(""value must be a byte string"")  ",1,train
DM-2983,Backport HSC parallelization code,"assuming rfc 68 is approved, transfer the hsc code to lsst as described there.",4,train
DM-2984,Compare LSST and HSC pipelines through ISR,"run both the hsc and lsst isr routines on two to three visits worth of hsc engineering data. compare the results. where differences exist, either:     create and work other tickets to resolve them;   explain their origin and why we don't think they are a problem.",10,train
DM-2985,Integrate javascript build with gradle,integrate javascript build tools webpack with gradle.,2,train
DM-2986,Conversion of FITS binary table extension to IPAC table format. ,fits binary table contains data types and structures that cannot map directly to ipac table.  we need to define ways to handle these cases.,6,train
DM-2987,Modify IpacTableParser to support extra wide table.,ipactableparser fail to load ipac table with extra wide headers and columns.  replace the logic for reading headers and columns information so that it will support any file/size.,2,train
DM-2989,XY plot need to be able to handle multiple tables with the same name,"xy plot was relying on a table request object to cache previously loaded tables.  this was done for performance reason.  however, table request is not reliable since the same request may be submitted multiple times.",2,train
DM-2990,Add XYPlot to Python interface,make it possible to add plots (not connected to a displayed table) to standaloneui.  add showxyplot to python api.,6,train
DM-2991,Firefly External Task Launcher,"implement and external task launcher, which forks a [python] process and gets back the results. the results can be a table, an image, or a json.    ",10,train
DM-2992,"Search processors to get image, table, or json from an external task","implement three search processors, which use the external task launcher (dm2991):     to get a table (possibly in binary fits format)   to get an image   to get json",8,train
DM-2993,Products must not depend on anaconda,"setuprequired(anaconda) should be removed from webservcommon.table.    we want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda.",1,train
DM-2996,Understand and improve error code management,"it seems there is several constants to store qserv error code (for example see msgcode.h and util::errorcode, or msgstate::resulterr,jobstatus::resulterror). this could certainly be simplified and clarified?    furthermore in util::error it seems there's a confusion between code and statuses",8,train
DM-2997,Bump eups anaconda package to 2.2,by popular request. ,1,train
DM-2998,Begin to write a note for the TOWG using ITIL as a checklist,"began to work use cases, and found that they were long, considering the number r of aspects that need to be considered,  presented to the towg,  got guidance to think in terms of processes, but to take that the effort estimates would  have a reasonable basis.  got guidance to think about the sites's needs but to congress a central approach.    agree that this would be the struggle for the week.",4,train
DM-2999,Management work for Don in the week of June 22,internal and external recruiting.   input on ncsa re organizaiton to ensure proper placement of lsst activities in the ncsa organization.  meetings.,3,train
DM-3001,Whitepaper submission to NSF Cyber Summit,working on drafting whitepaper and abstract for scada security challenges faced by lsst.,1,train
DM-3002,ISO presentation to all-hands meeting,"presentation giving overview of iso work, esp. w.r.t aup and master security plan.",1,train
DM-3003,Extend the Process Execution Framework to accomodate changes needed by SUI and others,extend the process execution framework to accomodate changes needed by sui and others by changing the task and configuration classes.    covers effort from july 1st   august 31st at 0.25 fte.,22,train
DM-3004,Preliminary Process Execution Framework work,"preliminary work to extend the process execution framework to accomodate changes needed by sui and others.    this story captures work done in june, prior to incorporating the activity into the baseline plan. work will be logged under dm 3003 starting july 1st.",4,train
DM-3005,"prepare jenkins ""demo"" for usage as an interim CI system",we have a working plan of putting the buildbot scripts under jenkins demo into usage as a production ci system as an intermediate step towards a fully decomposed build.,15,train
DM-3012,complete puppet jenkins native type implimentation and merge upstream,nan,20,train
DM-3016,Meeting with CTSC at CLHS Portland OR,"discussed lsst security plan going forward.  specifically work on scada security plan.  meeting held at conference in portland or, june 14th, acm clhs.",1,train
DM-3019,Do more research into Flux modules and bring one in ,nan,16,train
DM-3020,expose stretch to python API,nan,2,train
DM-3021,Improve region support,some parts of the region support has been more testing because of the python interface.  it is now clear what we should do.,4,train
DM-3022,Convert Color Stretch dialog to React/flux/JavaScript,nan,4,train
DM-3023,Review LSE-78,review either the current version #26 and/or the newest version when it become available.,2,train
DM-3024,Discuss US WAN options with NCSA ESnet representative,nan,1,train
DM-3025,Provide network support for ceph and openstack lsst storage server efforts,"work done to provide network connectivity, troubleshoot and monitor connections for the above efforts",1,train
DM-3026,DLP/LDM-240 support chages,"   jira changes to create dlp project     lsstsqre/sqrejirakit to generate ldm240like display       iterate with t/cams, kevin, jeff",6,train
DM-3027,Early access user onboarding and feedback ,"getting comments, testing, hipchat/jira changes",2,train
DM-3028,Display stories in JIRA epic table display,"  solved with issue matrix plugin; unfortunately this removed the ""create issue in this epic"" functionality, so that needs to be a new ticket.",1,train
DM-3029,Set up Slack for evaluation,  free account procured and tested by various volunteers; next step is to apply for non profit status which gives us the first paid tier free to 100 users. ,1,train
DM-3030,Set up Discourse for evaluation.,  server up on do at community.lsst.org. email needs fixing before volunteer users can be invited. ,1,train
DM-3031,Addressing File corruption in iRODS 4.1.x,we examine solutions for repairing corrupt files within an irods 4.1.x zone.,2,train
DM-3032,Read revised LSE-209 and LSE-70,read over the revised lse209 and lse70 documents,4,train
DM-3033,"Add Sdss3Mapper to ingest, convert and map SDSS-III ""frame"" files","sdssiii does not use the fpc file format for science images.  science images are now released as  http:/data.sdss3.org/datamodel/files/bossphotoobj/frames/rerun/run/camcol/frame.html  the primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flatfield conversions included as extensions. the astrometric information is in hdu3 instead of a separate astrans file.     obssdss should be able to ingest frame files and map them to load as dataset ""raw."" it should also optionally replace the backgrounds and de calibrate to convert the units back from nanomaggies to counts.     this will be implemented as lsst.obs.sdss.sdssmapper.sdss3mapper.",5,train
DM-3034,Margaret's mgmt. activities in June,nan,14,train
DM-3035,Check czar->proxy messages size,"these messages are stored in varchar(255) (fyi, memory tables can't contain text). we just need to make sure we have a reasonable fixed size char (and maybe check whether we are hitting the limit, and log it somewhere)",4,train
DM-3036,Move Qserv code comment to LSST documentation standards,lsst documentation standards: https:/confluence.lsstcorp.org/display/ldmdg/documentation+standards#documentationstandards requireddocumentationstyle  is different from the previous standards used by qserv (i.e. / text).     we should convert everything to lsst documentation standards.,6,train
DM-3037,remove lsst/log wrapper from Qserv,"lsst/log api looks stable now, so removing the wrapper would simplify the code.",1,train
DM-3038,Debug problems with Qserv at scale,nan,20,train
DM-3090,Implement test suite for new class SqlTransaction,some test that shows that transactions are properly committed/aborted would be nice to have.,1,train
DM-3091,Remove unused function populateState() ,"qserv doesn't seem to relaunch no more chunk query in case it fails (see dm2643)    and this function is now unused:    qserv@clrinfopc04:~/src/qserv (master)$ grep r populatestate core/  core/modules/qdisp/executive.cc:void populatestate(lsst::qserv::qdisp::execstatus& es,    ",1,train
DM-3093,LOE - Week ending 7/10/15,nan,1,train
DM-3094,LOE - Week ending 7/17/15,nan,1,train
DM-3095,LOE - Week ending 7/24/15,nan,2,train
DM-3096,LOE - Week ending 7/31/15,nan,2,train
DM-3097,Bi-weekly meeting with Victor and Iain.,nan,2,train
DM-3098,Incident response report template,nan,1,train
DM-3099,Incident response security work plan document,nan,2,train
DM-3101,Creation of XML descriptions of messages sent to OCS,create xml descriptions of messages sent to the ocs. upload these to a new github repository.,4,train
DM-3102,Resolve segmentation fault in LoggingEvent destructor,"there seems to be a possible race condition in log4cxx::spi::loggingevent::~loggingevent. i've had multiple segmentation faults in that function. in all cases, another thread was involved in writing. in at least 2 cases, the second thread was in xrdcl::logoutfile::write.  ",5,train
DM-3103,RFI with prospective DWDM vendors for Chile National Networks,hold request for information meetings in chile with equipment vendors. ,12,train
DM-3104,"Add ""ORDER BY"" clause to lua SQL query on result table","if user query has ""order by"", then lua  can't just execute ""select  from result"" because the order for such query is not guaranteed. to fix that, we need to add ""order by"" clause to the ""select  from result"" query on the lua side.    once we have the above, we might want to remove ""order by"" from the query class which runs a merge step on the czar (this has to be done in query analysis step).",8,train
DM-3105,Add assertXNearlyEqual methods for image-like classes,"presently one can compare two imagelike objects using free functions imagesdiffer, masksdiffer and maskedimagesdiffer in lsst.afw.image.testutils. these should be replaced by assertxnearlyequal methods that afw adds to lsst.utils.tests, as per dm2193.    if necessary, we could leave the old functions around for awhile. but i would prefer to simply get rid of them if we can.    one subtlety is that the current functions take numpy arrays, not afw image like class instances. examine the existing users of the code to determine how best to deal with that.",4,train
DM-3106,Add slot for calibration flux,this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1005.,2,train
DM-3108,Use aperture flux for photometric calibration,this is a port of work performed on hsc but without a ticket. relevant commits are:     https:/github.com/hypersuprimecam/measastrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0   https:/github.com/hypersuprimecam/measastrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99   https:/github.com/hypersuprimecam/obssubaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45   https:/github.com/hypersuprimecam/obssubaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99,1,train
DM-3109,Add support for accessing schema from QueryContext,"when we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. it will also be useful for checking if user is authorized to run query, and for queries like ""show create table"". this story involves writing code that will provide access to schema.",3,train
DM-3110,qserv code cleanup,"i made some random cleanup of the qserv code while playing with css v2. i want to push these changes to master, thus i am creating this story for this. it involves improvements to logging in userqueryfactory and facade (both are now per module), removing unnecessary namespace qualifiers, and whitspace cleanup.",1,train
DM-3111,Convert major portion of GWT in Firefly to pure JavaScript (W16),continue to convert gwt portion of firefly to pure javascript,100,train
DM-3114,Enable aperture correction in the integration test,"the present integration test does not enable aperture correction. this should be enabled and the results sanitychecked.    this is a separate ticket rather than dm436 at jim bosch's suggestion, to avoid ticket bloat.    it requires two separate changes:   update obssdss's sdsscalibratetask to measure and apply aperture correction   update the expected results from the integration test lsstdmstackdemo",4,train
DM-3118,Attend CCS-DAQ-OCS-DM Workshop IV,nan,6,train
DM-3119,whitepaper CFP for nsf cyber summit,nan,2,train
DM-3120,Execution Framework prototype,nan,8,train
DM-3121,Graphical communication interface,creating a graphical representation of execution framework,2,train
DM-3125,basic monitoring of jenkins nodes with notification,"this last weekend, the build slaves el62 and el72 ran out of disk space and were causing stackosmatrix build failures.  we should have an active monitoring system that sends notifications via at least one of hipchat/email/pagerduty.  there is disk utilitization information present in jenkins itself, aws cloudwatch, and ganglia (as of v0.2.x of the demo).  however, it may make be more convenient (read: expedient) to use a dedicated monitoring system such as sensu instead of mining existing data sources.    we should also investigate if we can configure jenkins to not schedule jobs on a slaves with low disk space.",6,train
DM-3126,gcc 4.8 package does not create a symlink bin/cc,"i created a new lsst package named ""gcc"" that contains mario's gcc 4.8 package. i used it to build lsst_distrib on lsst dev and it worked just fine. unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the lsst build system uses cc to build c code.    the desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc.",2,train
DM-3133,"add ""dax_"" prefix to data access related packages","as agreed at https:/confluence.lsstcorp.org/display/dm/dataaccessmeeting+20150713, add dax prefix towebserv, webservcommon, webservclient, dbserv, imgserv, metaserv",1,train
DM-3134,Implement Result Streaming in Qserv,qserv supporting streaming results while the query is running to client application.,60,train
DM-3135,FY19 Handling unexpected conditions during query execution,"detect and handle unexpected conditions during query execution (e.g. bad chunk, hit per user resource limit)",53,train
DM-3136,Add & use new mask plane for out-of-bounds regions,"add a new mask plane for regions with no data  fully vignetted, edge patches in coadd.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc669.",2,train
DM-3137,Handle bad pixels in image stacker,"we currently or together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs.    this is a port of work carried out on https:/hscjira.astro.princeton.edu/jira/browse/hsc 152.",1,train
DM-3138,Open day for RFP from Equipment vendors,open day with vendors to describe the needs and requirements of the mountain to base and la serena to santiago networks. this is the first formal meeting in the procurement process. vendors will be invited to propose their solution along with cost.,8,train
DM-3139,"HSC backport: extra ""refColumn"" class attributes in multiband",this is a transfer for changesets for https:/hscjira.astro.princeton.edu/jira/browse/hsc 1283.  ,1,train
DM-3140,add gcc to list of packages in lsstsw,add gcc to the list of packages in etc/repos.yaml in lsstsw,1,train
DM-3141,Reduce verbosity of astrometry,"the astrometry.net solver that runs by default in measastrom 10.1 is very verbose.  here's an example running hsc data with an sdss reference catalog:    $ processccd.py /tigress/hsc/hsc output /tigress/pprice/lsst id visit=904020 ccd=49 clobberconfig  : loading config overrride file '/home/pprice/lsst/obs/subaru/config/processccd.py'  warning: unable to use psfex: no module named extensions.psfex.psfexpsfdeterminer  hscastrom is not setup; using lsst's measastrom instead  cannot import lsst.meas.multifit: disabling cmodel measurements  cannot import lsst.meas.extensions.photometrykron: disabling kron measurements  cannot enable shapehsm ('measextensionsshapehsmdir'): disabling hsm shape measurements  cannot import lsst.meas.extensions.photometrykron: disabling kron measurements  cannot enable shapehsm ('measextensionsshapehsmdir'): disabling hsm shape measurements  : loading config overrride file '/home/pprice/lsst/obs/subaru/config/hsc/processccd.py'  : input=/tigress/hsc/hsc  : calib=none  : output=/tigress/pprice/lsst  cameramapper: loading registry registry from /tigress/pprice/lsst/parent/registry.sqlite3  cameramapper: loading calibregistry registry from /tigress/hsc/hsc/calib/calibregistry.sqlite3  processccd: processing   processccd.isr: performing isr on sensor   processccd.isr warning: cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.maskedimage warning: expected extension type not found: image  processccd.isr: applying linearity corrections to ccd 49  processccd.isr.crosstalk: applying crosstalk correction  afw.image.maskedimage warning: expected extension type not found: image  : empty wcs extension, using fits header  processccd.isr: set 0 bad pixels to 647.04  processccd.isr warning: there were 6192 unmasked nans  processccd.isr warning: cannot write thumbnail image; hsc.fitsthumb could not be imported.  processccd.isr: flattened sky level: 647.130493 / 12.733898  processccd.isr: measuring sky levels in 8x16 grids: 648.106765  processccd.isr: sky flatness in 8x16 grids  pp: 0.024087 rms: 0.006057  processccd.calibrate: installinitialpsf fwhm=5.88235294312 pixels; size=15 pixels  processccd.calibrate.repair: identified 80 cosmic rays.  processccd.calibrate.detection: detected 303 positive sources to 5 sigma.  processccd.calibrate.detection: resubtracting the background after object detection  processccd.calibrate.initialmeasurement: measuring 303 sources (303 parents, 0 children)   processccd.calibrate.astrometry: applying distortion correction  processccd.calibrate.astrometry: solving astrometry  loadreferenceobjects: read index files  processccd.calibrate.astrometry.solver: number of selected sources for astrometry : 258  processccd.calibrate.astrometry.solver: got astrometric solution from astrometry.net  loadreferenceobjects: getting reference objects using center (1023.5, 2084.5) pix = fk5coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  loadreferenceobjects: search for objects at fk5coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  loadreferenceobjects: found 495 objects  loadreferenceobjects: trimmed 257 outofbbox objects, leaving 238  processccd.calibrate.astrometry.solver: fit wcs: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processccd.calibrate.astrometry: 186 astrometric matches  processccd.calibrate.astrometry: refitting wcs  processccd.calibrate.astrometry: astrometric scatter: 0.047945 arcsec (with nonlinear terms, 174 matches, 12 rejected)  processccd.calibrate.measurepsf: measuring psf  /tigress/hsc/lsst/stack101/linux64/anaconda/2.1.04g35ca374/lib/python2.7/sitepackages/numpy/core/methods.py:59: runtimewarning: mean of empty slice.    warnings.warn(""mean of empty slice."", runtimewarning)  /tigress/hsc/lsst/stack101/linux64/anaconda/2.1.04g35ca374/lib/python2.7/sitepackages/numpy/core/methods.py:71: runtimewarning: invalid value encountered in doublescalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/lsst/meas/algorithms/python/lsst/meas/algorithms/objectsizestarselector.py:143: runtimewarning: invalid value encountered in less    update = dist < mindist  processccd.calibrate.measurepsf: psf star selector found 163 candidates  processccd.calibrate.measurepsf: psf determination using 114/163 stars.  processccd.calibrate.repair: identified 92 cosmic rays.  processccd.calibrate: fit and subtracted background  processccd.calibrate.measurement: measuring 303 sources (303 parents, 0 children)   processccd.calibrate.astrometry: applying distortion correction  processccd.calibrate.astrometry: solving astrometry  processccd.calibrate.astrometry.solver: number of selected sources for astrometry : 258  solver:    arcsec per pix range: 0.153025, 0.18516    image size: 2054 x 4186    quad size range: 205.4, 4662.78    objs: 0, 50    parity: 0, normal    useradec? yes, (320.343, 0.500178), radius 1 deg    verifypix: 1    code tol: 0.01    dist from quad bonus: yes    distractor ratio: 0.25    log tuneup threshold: inf    log bail threshold: 230.259    log stoplooking threshold: inf    maxquads 0    maxmatches 0    set crpix? no    tweak? no    indexes: 3      /tigress/hsc/astrometrynetdata/sdssdr9finkv5b/sdssdr9finkv5band2630.fits      /tigress/hsc/astrometrynetdata/sdssdr9finkv5b/sdssdr9finkv5band2631.fits      /tigress/hsc/astrometrynetdata/sdssdr9finkv5b/sdssdr9finkv5band263_2.fits    field: 258 stars  quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  got a new best match: logodds 787.099.    logodds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    ra,dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    hit/miss:   hit/miss:   pixel scale: 0.167612 arcsec/pix.  parity: pos.  processccd.calibrate.astrometry.solver: got astrometric solution from astrometry.net  loadreferenceobjects: getting reference objects using center (1023.5, 2084.5) pix = fk5coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  loadreferenceobjects: search for objects at fk5coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  loadreferenceobjects: found 495 objects  loadreferenceobjects: trimmed 257 outofbbox objects, leaving 238  processccd.calibrate.astrometry.solver: fit wcs: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processccd.calibrate.astrometry: 186 astrometric matches  processccd.calibrate.astrometry: refitting wcs  processccd.calibrate.astrometry: astrometric scatter: 0.048271 arcsec (with nonlinear terms, 174 matches, 12 rejected)  processccd.calibrate.photocal: not applying color terms because config.applycolorterms is false  processccd.calibrate.photocal: magnitude zero point: 30.685281 / 0.058711 from 173 stars  processccd.calibrate: photometric zeropoint: 30.685281  processccd.detection: detected 1194 positive sources to 5 sigma.  processccd.detection: resubtracting the background after object detection  processccd.deblend: deblending 1194 sources  processccd.deblend: deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processccd.measurement: measuring 1552 sources (1194 parents, 358 children)   processccd warning: persisting background models  processccd: matching icsource and source catalogs to propagate flags.  processccd: matching src to reference catalogue  loadreferenceobjects: getting reference objects using center (1023.5, 2087.5) pix = fk5coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  loadreferenceobjects: search for objects at fk5coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  loadreferenceobjects: found 499 objects  loadreferenceobjects: trimmed 261 outofbbox objects, leaving 238  processccd.calibrate.astrometry.solver: fit wcs: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels      the verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    this is a pull request for fixes i have made.",1,train
DM-3142,Port HSC optimisations for reading astrometry.net catalog,"some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component.  this can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an hsc image, using an sdss dr9 catalog).  we have hsc code that caches the required information, making the catalog load much faster.  the code is from the following hsc issues:     https:/hscjira.astro.princeton.edu/jira/browse/hsc1087   https:/hscjira.astro.princeton.edu/jira/browse/hsc1143   https:/hscjira.astro.princeton.edu/jira/browse/hsc1178   https:/hscjira.astro.princeton.edu/jira/browse/hsc1178    while there have been some changes to the lsst astrometry code that will mean we can't directly cherry pick the hsc code, yet i think the main structure remains, so the approach can be copied without much effort.",3,train
DM-3144,Audit and improve warm-start configuration options,"many of our commandline tasks  particularly the highlevel mpibased drivers we're moving over from the hsc side  typically reuse intermediate data products they find on disk rather than regenerate them by default, and have a suite of configuration options to control this behavior.    while this aids in faster reprocessing of aborted or failed jobs, it can produce results users would consider surprising (""i reran with a new version of the pipeline and nothing changed""), and (imo) should not be the default behavior for any task.    we also need to guarantee that any warm start reprocessing always produces the exact same results as a single consistent run.  i do not believe this is currently the case for some of the options in processccdtask.    finally, we should put these configuration options somewhere other than the main task config, because they control how the processing is done, not what the results are, and hence should not be checked against existing config files in an output data repo before running.  this will probably require a new mechanism in pipe_base; perhaps a second config class associated with each task, containing only options that affect the ""how"" of processing without affecting the ""what"".    the first step of implementing this issue should be an rfc.",8,train
DM-3146,Purchase transceivers for use by AURA,cisco xenpack for extra long distance 10gbs,5,train
DM-3147,Install switches and transceivers on Pachon and Tololo,buy switches to deploy mpls over the fibers. ,8,train
DM-3148,Configure switches for AURA tenants,switch configuration for use by individual tenants of aura,8,train
DM-3151,CI validation of lsstsw's repos.yaml,"having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw.  it should be possible to cobble a sanity checker together that can be run from travis ci.",1,train
DM-3153,meas_base still uses eups in tests,tests/centroid.py uses eups to determine the location of the data file used by the test. this needs to be fixed to use a location relative to the test file.,1,train
DM-3154,meas_astrom still using eups in tests,"in dm2636 we modified the tests to be skipped if eups is not available. i've had a closer look and all the ones i have glanced at seem to be easily fixable to run without eups. the tests seem to be using eups to locate the measastrom (effectively asking eups for the location of the test file), then a path to the astrometry.net test data within the tests/ directory is located and then eups is asked to setup astrometrynetdata using that path. since the table files are all empty this is the equivalent to simply assigning the astrometrynetdatadir environment variable directly to the path in the tests subdirectory.    making this change to one of the tests seems to work so i will change the rest.",2,train
DM-3155,W16 Qserv Release and Testing,"this epic captures stories related to building, testing and maintaining qserv releases, along with related documentation. note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 sps per month.",23,train
DM-3158,Data Distribution + Qserv,nan,75,train
DM-3160,Improve name and default value of MeasureApCorrConfig.refFluxAlg,the config name reffluxalg should be reffluxfield (since it is a flux field name prefix) and the default should be  basecircularapertureflux5 instead of basecircularapertureflux0 (thus giving a reasonable radius instead of one that is ridiculously too small).    i should have handled it on dm 436 but it slipped through.,1,train
DM-3161,Implement MySQL-based KVInterface,"this story covers adding mysql based implementation of kvinterface. the implementation will be done in c, and it will be exposed to the python layer.",12,train
DM-3162,Extend KVInterface - add support for updates,the css facade and kvinterface currently do not support updates. this story covers adding support for basic updates.,4,train
DM-3163,CSS/QMeta interaction in czar,"css currently does not have any notion of locks. the snapshots of css should be taken per query, for each query and they should be done in coordination with query metadata. this will ensure tables used by a running query never gets altered or deleted while the query is running.",10,train
DM-3164,S17 Fine-tune Data Access Interfaces,nan,15,train
DM-3167,Install the LSST Stack on loaned laptop ,nan,5,train
DM-3170,Qserv Release and Testing,"this epic captures stories related to building, testing and maintaining qserv releases, along with related documentation. note that testing involve running larger scale tests more or less monthly to ensure we haven't broken anything. 3 sps per month.",9,train
DM-3171,X16 Qserv Refactoring,nan,29,train
DM-3172,Accessing the current obs_decam package ,installing the non official obs_decam package from simon krughoff's github   and processing some decam data blindly     ,14,train
DM-3173,In CalibrateTask if one disables psf determination then aperture correction will fail,"in pipetasks calibratetask, by default aperture correction uses source flag ""calibpsfused"" to decide if a source is acceptable to use for measuring aperture correction. if psf determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources.  ",1,train
DM-3174,"CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema","calibratetask instantiates measureapcorr, applyapcorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option.    in any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1).",1,train
DM-3175,Build 2015_08 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-3176,Build and Test 2015_09 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,3,train
DM-3177,Build and Test 2015_10 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,3,train
DM-3178,Build and Test 2015_11 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,3,train
DM-3179,Build and Test 2015_12 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,3,train
DM-3180,Build and Test 2016_01 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,3,train
DM-3181,Build and Test 2016_02 Qserv Release,see https:/confluence.lsstcorp.org/display/dm/qservreleaseprocedure for recipe.,1,train
DM-3182,Aperture correction not applied for some measurements,"aperture correction needs to be applied every time a measurement is run after it is first measured in calibratetask. as of dm 436 aperture correction is only being applied in calibratetask, which for example means the information is overwritten during the final measurement of processimagetask.run.    this is probably best done by adding code to apply aperture correction to basemeasurementtask, so it is inherited by singleframemeasurementtask and forcedmeasurementtask.",5,train
DM-3183,CalibrateTask instantiates some subtasks with the wrong schema,"calibratetask instantiates some subtasks with the wrong schema, in particular:   astrometry is instantiated with the final schema but run on schema1   measureapcorr, applyapcorr and photocal are instantiated with schema1 but run on the final schema    one way this can cause problems is that schema1 may not have the data needed to measure aperture correction (e.g. it may contain only one tiny radius of aperture flux), as came to light when running the lsst stack demo.",1,train
DM-3185,Investigate workflow for OpenStack via Python scripts,"we investigate the use of python scripts that work against openstack apis to start up vms and configure them for use, for example, in processing, build & test scenarios, etc.   we are initially working against  the isl openstack, and intend to test against the ""nebula"" system when it becomes available.  (this type if work was initiated in issues dm1787, dm1788 in a previous epic, and we continue within the context of dm 1273, )    ",12,train
DM-3186,Add PT.12 Filter/Science_Ccd_Exposure tables to extend test query coverage,"filter table is missing from case02, case05 data, so next query can't be tested:     datasets/case02/queries/3023joinobjectsourcefilter.sql.fixme   join on source and filter and select specific filter in region   https:/dev.lsstcorp.org/trac/wiki/db/qserv/in2p3/benchmarkmarch2013   https:/dev.lsstcorp.org/trac/wiki/db/queries/007    select objectid, taimidpoint, fluxtoabmag(psfmag)  from   source  join   object using(objectid)  join   filter using(filterid)  where   raps between 1 and 2  noqserv  and declps between 3 and 4  noqserv   withqserv  qservareaspecbox(1,3,2,4)  and  filtername = 'u'  and  variability between 0 and 2        same thing for case02:1011objectsforexposure and case02:1030_timeseries.sql",4,train
DM-3188,Fix UDF for case01 query: 3005_orderByRA.sql,"query      mysql host=127.0.0.1 port=4040 user=qsmaster batch qservtestcase01qserv e ""select  from object where qservareaspecbox(0.,1.,0.,1.)""    returns nothing whereas     select    from object  where raps between 0. and 1.     noqserv  and declps between 0. and 1.   does (but doesn't use geom index)",6,train
DM-3189,"Remove _chunkId, _subChunkId column from case02:Object table","this columns are qserv internal and shouldn't be in input data. for example, this prevents case02:3021_selectobjectsortedbyra to work.    check also that these columns aren't in other test data set and remove fixme suffix from related broken query.",4,train
DM-3190,Document deprecation of DecoratedImage,according to discussion on hipchat (20 july 2015)      jim bosch: https:/lsst web.ncsa.illinois.edu/doxygen/xmasterdoxydoc/afwsec_image.html. it should be.,1,train
DM-3192,Re-implement watcher based on new CSS implementation,"current watcher implementation (in admin/bin/watcher.py) is based on direct watching of zookeeper updates via kazoo. if we are to re implement css based on mysql then watcher needs to be updated to support it. mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed.",8,train
DM-3193,Audit existing test and development system,document in the wiki how the test and development systems are connected and configured.,3,train
DM-3194,Fix cluster install procedure and improve docker support,document howto update cluster from qserv release:    see  http:/ deployment.html,1,train
DM-3196,makeWcs() chokes on decam images in 10.1,"in 10.0, processccddecam.py could process decam images to completion (whether the wcs was read correctly is a different question). now it fails on makewcs() (see traceback below), and i suspect this change in behavior is related to dm2883 and dm2967.    repository with both data and code to reproduce:  http:/  (apologies for the size)    the attachment is a document describing the wcs representation in the images from the community pipeline, courtesy of francisco forster.    please advise. this ticket captures any changes made to afw.       d108179166118:decam yusra$ processccddecam.py newtestrepo/ id visit=0232847 ccdnum=10 config calibrate.dophotocal=false calibrate.doastrometry=false calibrate.measurepsf.starselector.name=""secondmoment"" dowritecalibratematches=false clobberconfig  : loading config overrride file '/users/yusra/lsstdevel/lsst/repos/obsdecamya/config/processccddecam.py'  : config override file does not exist: '/users/yusra/lsstdevel/lsst/repos/obsdecamya/config/decam/processccddecam.py'  : input=/users/yusra/decam/newtestrepo  : calib=none  : output=none  cameramapper: loading registry registry from /users/yusra/decam/newtestrepo/registry.sqlite3  processccddecam: processing   makewcs warning: stripping pvij keys from projection ratpv/dectpv  processccddecam fatal: failed on dataid=:     file ""src/image/wcs.cc"", line 130, in void lsst::afw::image::wcs::initwcs()      failed to setup wcs structure with wcsset. status 5: invalid parameter value   lsst::pex::exceptions::runtimeerror: 'failed to setup wcs structure with wcsset. status 5: invalid parameter value'    traceback (most recent call last):    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/pipebase/10.13g18c2ba749/python/lsst/pipe/base/cmdlinetask.py"", line 320, in call      result = task.run(dataref, kwargs)    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/pipebase/10.13g18c2ba749/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/users/yusra/lsstdevel/lsst/repos/obsdecamya/python/lsst/obs/decam/processccddecam.py"", line 77, in run      mi = exp.getmaskedimage()    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/dafpersistence/10.11g6edbc0028/python/lsst/daf/persistence/readproxy.py"", line 41, in getattribute      subject = oga(self, 'subject')    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/dafpersistence/10.11g6edbc0028/python/lsst/daf/persistence/readproxy.py"", line 136, in subject      setcache(self, getcallback(self)())    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/dafpersistence/10.11g6edbc0028/python/lsst/daf/persistence/butler.py"", line 242, in /      innercallback(), dataid)    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/dafpersistence/10.11g6edbc0028/python/lsst/daf/persistence/butler.py"", line 236, in /      location, dataid)    file ""/users/yusra/lsstdevel/lsst/repos/obsdecamya/python/lsst/obs/decam/decammapper.py"", line 118, in bypassinstcal      wcs         = afwimage.makewcs(md)    file ""/users/yusra/lsstdevel/lsst/dms5/darwinx86/afw/10.126g9124caf+1/python/lsst/afw/image/imagelib.py"", line 8706, in makewcs      return imagelib.makewcs(args)  runtimeerror:     file ""src/image/wcs.cc"", line 130, in void lsst::afw::image::wcs::initwcs()      failed to setup wcs structure with wcsset. status 5: invalid parameter value   lsst::pex::exceptions::runtimeerror: 'failed to setup wcs structure with wcsset. status 5: invalid parameter value'      ",2,train
DM-3199,Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version ," shmux could be used for parallel ssh (remove qserv builtin one)   look at ""serf and consul"" (see confluence pages)   improve doc: http:/    run multiple instances/versions of qserv using different run dir/ports and the same data",8,train
DM-3202,Include reference magnitude errors in PhotoCal,"photocal task currently ignores the uncertainties on reference sources, which can lead to problems when the reference and measured catalogs have relatively little overlap or otherwise disagree on how trustworthy a source is.",3,train
DM-3204,W16 Data Access and Db Release Documentation,write release documentation covering data access and database work.,5,train
DM-3205,Revisit cost of replicating non-partitioned tables on all nodes,"revisit size of all non partitioned tables, and cost of replicating them on all worker nodes.",4,train
DM-3206,Estimate I/O load for non-partitioned tables,"estimate realistic io load from user queries on non partitioned tables. consider whether there might be hot spots (eg., maybe a small subset of columns from exposure is used very often. if it is it, maybe it'd be worth replicating only these columns across all worker nodes and serve the rest from one shared file system).",6,train
DM-3207,Experiment with CONNECT engine for non-partitioned tables,"idea: store non partitioned tables in a dedicated mysql server, and bring them to the worker nodes using connect engine.    this story involves exploring if that would work, and uncovering potential pitfalls.",10,train
DM-3209,Add debugging for astrometry.net solver,"to be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions.  this is a pull request to add these.",1,train
DM-3212,Query Coverage,nan,28,train
DM-3213,Get ImageDifferenceTask running again,imagedifferencetask doesn't run. the issues i've seen so far are related to the measurement overhaul. this ticket will capture the one off updates needed to get this running again.     appropriate bugs and papercuts epic?,10,train
DM-3214,ChebyshevBoundedField should use _ not . as field separators for persistence,"chebyshevboundedfield uses ""."" instead of ""\"" as field separators in its afw table persistence. this is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to """" in that situation.    this shows up as a unit test failure in dm2981 (brought over from hsc) when an older version table is read in.    it is an open question whether to fix this as part of dm2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. in the former case i'm happy to do the work so i can finish dm 2981.    many thanks to jim bosch for diagnosing the problem.",1,train
DM-3215,Begin drafting specification document for the Level 1 System,don petravick (.5 fte)  jason alt (.8 fte)  paul wefel (.125 fte)  july 2015   august 2015,50,train
DM-3216,Initial work to process DECam data with LSST stack,hsinfang chiang (1 fte)  july 2015  august 2015,80,train
DM-3217,Extrapolate to the current document ,"discussed use  cases in the context of the towg, and also with the site and telescope group.  a picture of the  structure of it operations has (i believe consensus emerged) that is the  four layers itil cake    service design (cataloged, budget, availability, etc).  service transition (the work of inserting charge into the system).  service delivery  (the work of running the asis system of servinces)  itc   the work of providing the facility, hardware and networking.     i also obtained the ability to interact with ea. (but still working to master it and its concepts)",8,train
DM-3218,unable to create public images,errors are returned when attempting to upload an image marked as public.,1,train
DM-3223,Improve czar-worker communication debugging,"add features to make it easier to debug communication problems. particularly, record the source of a message, and remove extraneous messages.",2,train
DM-3224,Document setting up multi-node Qserv and running integration test,nan,4,train
DM-3227,openstack API endpoint is broken,"similar to what was observed in dm3226, the referral endspoint returned by     https:/nebulous.ncsa.illinois.edu:5000    are not fqdns.  this fundamentally breaks any attempt to use the api one step past authenticating with keystone.    this is an example http response:      http/1.1 200 ok  date: mon, 27 jul 2015 23:11:02 gmt  server: apache/2.4.10 (ubuntu)  vary: xauthtoken  xdistribution: ubuntu  xopenstackrequestid: reqac7bb61386ef43aba66375c2ed3fb124  contentlength: 1656  contenttype: application/json    , ""auditids"": [""fap8851vtqi1n5pymnoijw""]}, ""servicecatalog"": [], ""endpointslinks"": [], ""type"": ""image"", ""name"": ""glance""}, ], ""endpointslinks"": [], ""type"": ""compute"", ""name"": ""nova""}, ], ""endpointslinks"": [], ""type"": ""network"", ""name"": ""neutron""}, ], ""endpointslinks"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": ], ""name"": ""jhoblitt""}, ""metadata"": }}      ",1,train
DM-3228,evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1,see also https:/confluence.lsstcorp.org/pages/viewpage.action?spacekey=ldmdg&title=ncsanebulaopenstack+issues,2,train
DM-3230,"Refinement, restatement of DM facilites and functions ","began detailed refinement.  the initial version was a word document with omnigraffle figures.  began moving this to lsst confluence to make the model more accessible for scrutiny.    the top level  functional    physical  breakdown diagrams now exist and transferring the functional breakdown from word to confluence is in place.   i have request that the simple citation package be installed in  the lsst lira (not perfect, but helps).   ",10,train
DM-3232,Potential talk for All-Hands,nan,1,train
DM-3233,Potential talk for nsf cyber summit,nan,1,train
DM-3234,Collab. with Ron Lambert and Oliver W.,nan,2,train
DM-3237,Fix problems with no-result queries on multi-node setup,"for queries like:        select  from object where id = /    qserv can't map it to any chunk, and it ends up executing      select      from qservtestcase01qserv.object1234567890 as qst1_     where objectid=/    the chunk 1234567890 is a special chunk and it exists on all nodes.    and that fails with:    (build/qdisp/queryresource.cc:61)   error provisioning, msg=unable to  write  file; multiple files exist. code=2 ",1,train
DM-3239,Build a demo system for camera team to use the Firefly external task launcher,"we have the code to launch ab external task from firefly server. ( see dm 2991)  in order to facilitate the development by the uiuc group for camera team, we need to have a simple example to show how to connect the front extension to the external task at the server side.  ",10,train
DM-3240,Support to the camera team development,we need someone to attend the weekly meeting at uiuc  group for camera team. to discover issues and answer questions. this is an on going effort.   ,10,train
DM-3241,Create images for the mask bits at server side,"lsst fits images will have a extension that indicate the mask bits. in order to overlay the masks on the primary image, we need to turn the mask bits into a set of images. this task is to take the requested bits and fits as input, output a set of images for each requested bit. each bit will have different color. ",20,train
DM-3242,a simple demo version to use Firefly in iPython notebook,we want to build a simple demo version of firefly  that works in the ipython notebook.  it will make it easier for users to try out firefly visualization capabilities  with python apis. ,10,train
DM-3243,Include polygon bounds in CoaddPsf logic,this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc974. original description:    the coaddpsf class should use the polygon bounding areas that were added to exposure and exposurerecord in dm2981 (was: hsc 973) when determining which psf images to coadd.,1,train
DM-3245,Add support for passing query classification info from user to czar,we need to be able to pass information from user about query type (sync/async). this will require tweaking the parser.  ,8,train
DM-3246,Add support for async query results,"modify qserv to support async queries: send query results to the right place instead of to mysql proxy. in this story, we can simply use some reasonable default location and send the results there. later on we will extend qserv to make it configurable.",10,train
DM-3247,Add support for configuring async queries,"extend qserv configuration to allow a dba to specify (a) where results from async queries should be stored and (b) what rules to apply when purging old results.    note that we need to think about the purging rules, it is not immediately obvious what would make most sense.",5,train
DM-3249,Revisit and document user-facing aspects of async queries,"outline all aspects of async queries that are affecting users, discuss with the dm team, and document. this includes things like:    managing async queries (checking status, terminating)    retrieving results from async queries    managing query results (purging policies etc)    probably more, need to think about it...",8,train
DM-3253,Unify KVInterface python and c++ interfaces,swig the c mysql based kvinterface implementation.   ,8,train
DM-3254,FY18 Qserv Health Verification Tool,"need a tool for verifying if all the services are up and running, including things like whether udfs are loaded",79,train
DM-3255,Prepare and implement RFP for DWDM devices,prepare and implement rfp,10,train
DM-3256,Fiber path Gate to pachon,discussion on fiber path,5,train
DM-3257,Port flux.scaled from HSC,"https:/hscjira.astro.princeton.edu/jira/browse/hsc 1295 introduces flux.scaled, which measures the flux within a circular aperture that is set from the size of the psf, scaled by some factor.  stephen gwyn recommends using this as our fiducial calibration flux.",2,train
DM-3258,CoaddPsf.getAveragePosition() is not a valid position,this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1138 to lsst. that is an aggregate of two related minor fixes:     coaddinputrecorder should default to savevisitgoodpix=true so that average positions in the coaddpsf can be properly weighted;   computeaverageposition and docomputekernelimage should be consistent about the data included when determining whether a source is off image.,1,train
DM-3259,Define polygon bounds for CCDs based on vignetted regions,"this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc976 to lsst. the original issue description was:    we should set the polygon bounds (added in dm2981 [was hsc973]) for hsc ccd exposures to cover the nonvignetted regions. this should probably be done in isr or some other cameraspecific location.    note that, contrary to the description in dm2981, this functionality was not included there.",1,train
DM-3261,Fix problems in xrootd discovered in multi-node qserv tests,nan,15,train
DM-3262,Explore how to run multi-node tests,not testing qserv code often enough in multinode environment led to introducing many problems over the past two years since we last run large scale test. it should be simple for developer to run a multinode test. this story covers work related to understanding how to run integration test on multi node.,4,train
DM-3263,W16 Make Query Cancellation Robust,"it is clear that the code responsible for query cancellation needs some more thoughts and refactoring (it was prematurely rushed when daniel was about to leave slac). in particular, the code seems to have some subtle problems. we need to debug these problems and solve them.",49,train
DM-3264,Audit & cherry-pick HSC-1126 fixes,"https:/hscjira.astro.princeton.edu/jira/browse/hsc 1126 contains a number of unrelated bug fixes. given the nature of that ticket, it's not immediately clear which might already have been ported to lsst, which don't apply, and, of the others, what dependencies they have on code which might still be in the queue for merger.    we need to dig through that ticket and ensure that everything is properly merged.",2,train
DM-3265,add task to meas_astrom to fit an aribtrary WCS with a TAN-SIP WCS,"sometime in the past, russell owen wrote a method to take an arbitrary wcs and approximate it as a tanwcs (our implementation of fits' tansip wcs formalism).  that method is currently just a utility function in the unit test testfittansipwcshighorder.py.  this issue will promote that method to a fullfledge task in meas_astrom.",2,train
DM-3266,Reconfigure Openstack systems,1) configured the ipmi on 13 systems.,22,train
DM-3345,Administrative - 7-2015,nan,2,train
DM-3346,Networking support of Openstack efforts,nan,2,train
DM-3347,assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function,"assertwcsnearlyequaloverbbox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.testcase, so their first argument is a testcase. this is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testcase argument (though it need only be a trivial class with a fail(self, msgstr) method). worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe.    i have two proposals:   document the fact that testcase need only support fail(self, msgstr). this makes it clear how to safely use these functions as free functions.   allow testcase to be none, in which case runtimeerror is raised. that makes these functions even easier to use as free functions.  ",1,train
DM-3348,Include translation of aliases in measurement calibration,"tasks that calibrate measurement outputs should include transferring (and translating, as needed) the aliases in the original measurement catalog.  this should include transferring slots, which may involve ""renaming"" the original slots, as those names refer explicitly to raw measurements (i.e. ""slotpsfflux"" might become ""slotpsfmag"").",3,train
DM-3349,Add test case for ExposureRecord::contains,in dm 3243 we ported from hsc the ability to take account of the associated validpolygon when checking whether a point falls within an exposure. this functionality was not accompanied by an adequate unit test.,2,train
DM-3351,Reproducing errors of the current obs_decam package,"learning the stack and development worlflow by reproducing errors in obsdecam as dm3196.  changes from the stack need to be incorporated into obsdecam package to keep the package up to date, hence the errors. as a learning process i reproduced the errors and used her fix (afw branch u/yusra/dm3196) to move on.  ",2,train
DM-3352,Study basic afw ,"learn the basic operation of the aft package about handling images, tables, etc.  ",6,train
DM-3353,Management until end july 215,"finsh system development lead and on onboard jason alt.  deal with management of group providing ncsa openstack.  review and re review procurement contract prose and directions.  coordinate  towg input with ncsa management.    gain acume about exiting design documents, decide and prototype details refinement/ analysis /restatement needed to manage project at ncsa.  management of people.",10,train
DM-3354,Read in the FITS cube that Herschel project produced,irsa needs to be able to read in the fits cube generated by herschel project. we need to support and guide the effort so the code is generic enough for non herschel data. ,2,train
DM-3355,Support the FITS cube reader,rsa needs to be able to read in the fits cube generated by herschel project. we need to guide the effort so the code is generic enough for non herschel data.,1,train
DM-3356,Fix Firefly build script so it'll work with latest version of gradle,firefly build was failing when using gradle version 2.5.  minor changes to the dependencies declaration fixed it.,1,train
DM-3357,Margaret's mgmt. activities in July,nan,30,train
DM-3358,Add mysql-based test to multi-node integration test,"at the moment multinode integration test runs only on multinode using qserv, it does not run on plain mysql, and thus we can't validate results. the story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv.",5,train
DM-3359,Investigate jenkins creating a container per job,nan,2,train
DM-3362,Run large scale tests,nan,8,train
DM-3363,Debug problem with joins in multi-node tests,"we seen to have problems with joins:    select o.deepsourceid, s.objectid, s.id, o.ra, o.decl from object o, source s where o.deepsourceid=s.objectid;      cluster seems to have hung. i can send new queries to the czar, and they show up in the czar's log, but they don't get answered (they can be cancelled).  cancelling the join works(at least for the czar) but no further queries work.",4,train
DM-3364,Analyze qserv performance / KPIs,nan,5,train
DM-3365,Explore Qserv authentication and authorization,nan,8,train
DM-3366,Produce Data Access & DB team S15 Release docs,complete these documents:   https:/confluence.lsstcorp.org/display/dm/summer2015qservrelease   https:/confluence.lsstcorp.org/display/dm/summer2015webservrelease,4,train
DM-3367,Add multi-process python runner script for Galaxy Shear Experiments,"the current runner scripts are in tcsh and bash.  there is no good excuse for this, except that it was easy to implement.  since we need both multi threading and better parameter parsing, this will be replaced with a python script.",2,train
DM-3368,Port HSC MPI driver for single-visit processing,"transfer the reduceframes.py script and the processexposuretask it utilizes from hscpipe to a new package in the lsst stack (rfc 68 proposes calling this new package pool_tasks, but this isn't set in stone).    we should probably rename either the driver script or the task (or both), so they agree; the lack of consistency is a historical artifact on the hsc side, and i think it's time to change that.",6,train
DM-3369,Port HSC MPI driver for coaddition,"port the hsc driver for coaddition, stack.py from hscpipe to a new lsst package (the same as dm 3368).    in the process, we should remove the inclusion of processcoaddtask, and instead run detection and background subtraction only.    i think it might be time to consider renaming this task as well; i find it a little unfortunate we use ""coadd"" everywhere else but ""stack"" here.",4,train
DM-3370,Port HSC MPI driver for multi-band coadd processing,"port the hsc mpi driver of multiband coadd processing, multiband.py, from hscpipe to a new lsst package (the same as in dm3368).",4,train
DM-3371,Port HSC --rerun option for CmdLineTask,"port the hsc side's rerun option for specifying processing inputs and outputs.    this work should be preceded by an rfc; we've proposed implementing this option on the lsst side in the past, and it was met with some resistance as it isn't strictly necessary.  we've since found it extremely convenience on the hsc side, and i think it's very much worth porting.",4,train
DM-3372,"Port, replace, or defer HSC-side provenance of EUPS products","the hsc pipeline checks that setup eups products are identical between runs with the same output directory, in the same way configuration is checked in both the lsst and hsc pipelines.    the implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as is, or just wait for a better implementation to be provided by the process middleware team.  we should at least rfc this question now.",6,train
DM-3373,Port HSC code for generation of calibration products,"port hsc code for building calibration products (flats, bias frames, etc.).",6,train
DM-3374,add realistic Footprints to measurement code,the current measurement code for the galaxy shear simulations uses the full postage stamp bounding box for the footprint.  we need to use more realistic footprints for some of the tests we want to run.  that probably involves running sourcedetectiontask and somehow combining that with the input catalog based iterating already in the measurement code.,4,train
DM-3375,Test shear bias vs. CModel region.nGrowFootprint,"one piece of how the cmodel code chooses its fit region size is via ngrowfootprint, which is used to grow the original detection footprint.  we should test how changing this parameter affects the m and c shear biases between input and recovered shear.  they should decrease for larger ngrowfootprint values, and eventually plateau.  we want to find the point where this happens, and see how the parameter affects both the fit region area and the shear biases before this threshold.    it may be necessary to make a small modification to the cmodel code to output the fit region area to complete this test.",4,train
DM-3376,Test shear bias vs. CModel region.nInitialRadii,"like dm3375, but testing the region.ninitialradii parameter instead.  this parameter sets the fit region using a multiple of the halflight ellipse from an initial approximate fit.  the full fit region is formed as the union of this with the grown detection footprint, so it may be necessary to set ngrowfootprint to a negative number to see any affect from this parameter.",4,train
DM-3377,Initial issue investigation for the nebula openstack,"    the nebula openstack system at nsca first became available ~fri jul 24 and  the week of jul 27  31 was spent testing and debugging issues that the                 lsst team identified within, for example, dm3225, dm3219, dm 3227 and others.  ",20,train
DM-3378,NSF Cyber Summit talk,"nsf cyber security summit talk:  a case study of lsst cyber security.  talk goes over challenges and successes with lsst's security program.  talk is divided into four sections:  security plan, data security, user access, and security for the observation site.",7,train
DM-3380,Port HSC hooks for simulated source injection,port hsc hooks injecting simulated sources into real images to test processing.    this includes the code in fakes.py in pipe_tasks and its callers.  the pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by thirdparty plugins such as https:/github.com/clackner2007/fakesources.,4,train
DM-3381,Add test cases for thresholding,"in dm 3136 changes were made to the way thresholds are handled in detection (https:/github.com/lsst/measalgorithms/commit/a4b011dd0775908c925ad9f40f802f9ed8723ef9 and https:/github.com/lsst/measalgorithms/commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b). these were not accompanied by test cases, but they should be.",2,train
DM-3382,security playbook,practical document for handling and responding to incidents.,1,train
DM-3383,Meeting with on HTCondor,attended meeting with miron livny.,1,train
DM-3384,Port HSC improvements to HSM moments code,"the hsm shear estimation has received several improvements and important bugfixes on the hsc side that need to be ported to lsst.  this is complicated by the fact that much of the code has been entirely rewritten on the lsst side to work within the new measurement framework, but we've also synchronized this package with the hsc side much more frequently than with other packages.",4,train
DM-3385,Administrative - 8-2015,nan,1,train
DM-3386,Parallelism Framework migration,nan,2,train
DM-3387,Make use of good pixel count when building CoaddPsfs,"when building a coaddpsf we have the ability to take account of the number of pixels contributed by the inputs (see http:/ls.st/paj and dm 3258). however, the coaddpsf constructor fails to use this information. it should copy this field when copying the provided exposurecatalog, so that computeaverageposition can use it.",1,train
DM-3388,Edit end-to-end test plan to reflect current DM plans,nan,2,train
DM-3390,Re-generate data for large scale tests at in2p3,"sources were incorrectly duplicated, need to be redone",3,train
DM-3391,Refactor Zscale.java class ,"in early this year, the decision all data types would be converted to float in fitsread.  thus,the bitpixel is not relevant.  in zscale, it still uses bitpixel to test the data type.  it should be refactored in the same manner as fitsread etc. ",2,train
DM-3392,Fix precision related problem in UDFs,"scisql udfs seem to have a subtle precision problem. the following query that is not relying on scisql returns one row:      select ra, decl, deepsourceid   from object o   where decl between 0.992 and 0.993 and ra between 19.171 and 19.172;     decl                   0.992087616433663     1 row in set (2 min 9.49 sec)      but equivalent scisqlbased query:      select ra, decl, deepsourceid   from object o   where qservareaspecbox(0.992, 19.171, 0.993, 19.172);      will fail to find that row.    if we relax the search criteria just a little bit, it finds some other row, but still not the one with decl = 0.992087616433663      select ra, decl, deepsourceid from object o where qservareaspecbox(0.99, 19.171, 0.999, 19.172);     decl                 19.171425377618       ",4,train
DM-3393,Fix column names in query result,"the following shows the problem (see the column names in the results, they are not what user will expect). it happens for all aggregates: min, max, avg, count etc      select min(raps), min(declps), max(raps), max(declps), avg(raps) from object;     min(qs2min)   max(qs4max)       6.1011707745  3.89870649736      ",4,train
DM-3394,Cost Model Discovery,learn about the cost model (ldm 144) and related documents in preparation for updating it per the contract,9,train
DM-3396,Discourse evaluation (Part 2),work in support of evaluation discourse as a dm platform for internal and external interactions.,7,train
DM-3397,Find and evaluate multi-user password wallet for SQuaRE,work to find and evaluate an offtheshelf solution for sharing web services passwords between the square group.,1,train
DM-3398,Fix problem with default_engine,"fix the problem:      08/04/2015 05:39:47 werkzeug info: 141.142.237.30   [04/aug/2015 17:39:47] ""get /meta/v0/ http/1.1"" 200   08/04/2015 05:39:49 main error: exception on /meta/v0/db [get]  traceback (most recent call last):    file ""/home/becla/stack/linux64/flask/0.10.18/lib/python/flask0.10.1py2.7.egg/flask/app.py"", line 1817, in wsgiapp      response = self.fulldispatchrequest()    file ""/home/becla/stack/linux64/flask/0.10.18/lib/python/flask0.10.1py2.7.egg/flask/app.py"", line 1477, in fulldispatchrequest      rv = self.handleuserexception(e)    file ""/home/becla/stack/linux64/flask/0.10.18/lib/python/flask0.10.1py2.7.egg/flask/app.py"", line 1381, in handleuserexception      reraise(exctype, excvalue, tb)    file ""/home/becla/stack/linux64/flask/0.10.18/lib/python/flask0.10.1py2.7.egg/flask/app.py"", line 1475, in fulldispatchrequest      rv = self.dispatchrequest()    file ""/home/becla/stack/linux64/flask/0.10.1+8/lib/python/flask0.10.1 py2.7.egg/flask/app.py"", line 1461, in dispatchrequest      return self.viewfunctions[rule.endpoint](req.viewargs)    file ""/nfs/home/becla/stack/repos/daxmetaserv/python/lsst/dax/metaserv/metarestv0.py"", line 59, in getdb      return resultsof(text(query), scalar=true)    file ""/nfs/home/becla/stack/repos/daxmetaserv/python/lsst/dax/metaserv/metarestv0.py"", line 122, in resultsof      engine = currentapp.config[""defaultengine""]  keyerror: 'defaultengine'    ",1,train
DM-3399,Research and Documenting the L1 System,nan,4,train
DM-3400,Eliminate circular aliases in slot centroid definition," has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag.  fixing this should just involve an extra check in safecentroidextractor.",1,train
DM-3401,Explicitly disallow alias cycles in Schemas,"the current guard against cycles is lazy and incomplete, as it seemed unlikely we'd ever have them.  that's already been disproven (dm 3400), so it seems prudent to fix the guard code now.",6,train
DM-3404,Port HSC updates to ingestImages.py,ingestimages.py provides a camera agnostic manner of creating a data repository (including a registry).  the hsc fork contains multiple improvements not present on the lsst side.  we need these in order to ingest the hsc data.,2,train
DM-3409,organize workspace functions discussion ,nan,4,train
DM-3410,workspace functions discussion,nan,4,train
DM-3411,workspace functions specification document,the first version of the document is here https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=41783931,20,train
DM-3412,Review and edit import of Level 2 ICD milestones into DLP,nan,2,train
DM-3414,Clarify status of LSE-77,"work on lse77 per se appears not to have kept up with the status of the substance of the interface requirements, as represented in, for instance, lse239.  the action here is to see what change request actions may be appropriate for lse 77 at this point.",1,train
DM-3415,Review all DM ICDs for open issues and work with partner subsystems to clarify schedules for completion,nan,4,train
DM-3416, workspace functions discussion,nan,2,train
DM-3417,Fix overestimation of aperture correction error,"we're overestimating the aperture correction errors by including photon noise twice: both in the aperture correction errors and the original measurement errors.    this is a migration of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1277 to lsst, which is where it should be fixed.",8,train
DM-3418,Evaluate changes to LSE-209 and LSE-70,action item from    https:/confluence.lsstcorp.org/display/syseng/2015july0810ccsdaqocsdmworkshop+iv    send initial feedback on lse70 and lse209,4,train
DM-3419,obs_decam unit test for reading data ,the unit test wasn't working before and i edited the unit test of reading raw data. this got included with dm3462.   this unit test needs testdata_decam to be setup.      the test fails with the stack b1597 at makewcs (dm3196).   the afw branch u/yusra/dm3196 is a temporary fix before dm3196 is resolved.      ,2,train
DM-3421,Create testdata_decam ,create a new testdata_decam repo with public instrument calibrated data.        the 435mb file can be downloaded from http:/uofi.box.com/testdata decam,2,train
DM-3422,S15 Qserv Release and Testing,"this epic captures stories related to building, testing and maintaining qserv releases, along with related documentation.",10,train
DM-3425," CI, Deploy and Distribution Improvements Part II","[retitled epic to better capture current plan]    this epic is an umbrella for jenkins improvements, openstack and aws  automatic deployment, binary distribution, developer requests.    docker items in particular should go to a different epic. dm 2053    [jh 100%]     ",49,train
DM-3426,DLP/LDM-240 support chages - Part II,"service feature requests and bugfixes from jk, t/cams including format changes on sqre jirakit.",2,train
DM-3427,Meetings - Aug 2015 ,8/3: local group meeting  8/3: uiuc postdoc orientation  8/5: lsst2015 ncsa coordination meeting  8/10: local group meeting  8/17 22: lsst2015 bremerton all hands meeting  8/24: local group meeting  8/28: astro postdoc meeting,12,train
DM-3428,Set up new desktop and install the stack, set up software on the new imac   installed the lsst stack with eups and lsstsw   noticed a compiler requirement dm3405  ,2,train
DM-3429,Learn the development workflow and obs_decam status update , learn git workflow and branching of the stack.     had a long chat with yusra about obsdecam     reproduced obsdecam issues with different builds of the stack. there was a measurement failures that got solved with the newer builds of the stack. ,4,train
DM-3432,Debug problem with timeout,"if i run 4 simultaneous large queries: 3 object scans and 1 source scan, xrootd silently died on 2 machines. below i pasted the tail of the log files      ccqserv108    0808 00:22:33.824 [0x7fb739583700] warn  foreman (build/wdb/queryaction.cc:109)  queryaction overriding dbname with lsst  0808 00:22:33.824 [0x7fb74a00e700] info  root (build/xrdsvc/channelstream.cc:122)  returning buffer (256, (more))  0808 00:22:33.826 [0x7fb74a00e700] info  root (build/xrdsvc/channelstream.cc:122)  returning buffer (62, (last))  0808 00:22:33.826 [0x7fb74a00e700] info  root (build/xrdsvc/ssisession.cc:153)  requestfinished type=isstream  0808 00:22:34.468 [0x7fb739583700] info  root (build/wdb/queryaction.cc:261)  &&& fillrows size=8  0808 00:22:34.469 [0x7fb739583700] debug root (build/wdb/queryaction.cc:288)  transmit last=1  0808 00:22:34.469 [0x7fb739583700] debug root (build/wdb/queryaction.cc:307)  transmitheader  0808 00:22:34.469 [0x7fb739583700] info  root (build/proto/protoheaderwrap.cc:52)  msgbuf size=256 > [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:34.469 [0x7fb739583700] info  root (build/xrdsvc/ssisessionreplychannel.cc:85)  sendstream, checking stream 0 len=256 last=0        ccqserv124    0808 00:22:25.329 [0x7f25a67bf700] debug scansched (build/wsched/chunkdisk.cc:234)  chunkdisk registering for 2044 : select min(ra) as qs1min,max(ra) as qs2max,min(decl) as qs3min,max(decl) as qs4max from lsst.object2044 as qst1 p=0x7f257003d0b8  0808 00:22:25.329 [0x7f25a67bf700] info  foreman (build/wcontrol/foreman.cc:296)  runner running task: msg: session=7 chunk=2044 db=lsst entry time=fri aug  7 23:52:06 2015   frag: q=select min(ra) as qs1min,max(ra) as qs2max,min(decl) as qs3min,max(decl) as qs4max from lsst.object2044 as qst1, sc= rt=r7bff268d0e369dda8fa314132538a96ad20440  0808 00:22:25.329 [0x7f25a67bf700] info  foreman (build/wdb/queryaction.cc:177)  exec in flight for db = qb762c96f418726ae3457c74c0350d0c4  0808 00:22:25.329 [0x7f25a67bf700] warn  foreman (build/wdb/queryaction.cc:109)  queryaction overriding dbname with lsst  0808 00:22:25.330 [0x7f25a50b6700] info  root (build/xrdsvc/channelstream.cc:122)  returning buffer (44, (last))  0808 00:22:25.330 [0x7f25a50b6700] info  root (build/xrdsvc/ssisession.cc:153)  requestfinished type=isstream  0808 00:22:26.218 [0x7f25a67bf700] info  root (build/wdb/queryaction.cc:261)  &&& fillrows size=81  0808 00:22:26.218 [0x7f25a67bf700] debug root (build/wdb/queryaction.cc:288)  transmit last=1  0808 00:22:26.218 [0x7f25a67bf700] debug root (build/wdb/queryaction.cc:307)  transmitheader  0808 00:22:26.218 [0x7f25a67bf700] info  root (build/proto/protoheaderwrap.cc:52)  msgbuf size=256 > [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:26.218 [0x7f25a67bf700] info  root (build/xrdsvc/ssisessionreplychannel.cc:85)   sendstream, checking stream 0 len=256 last=0 ",4,train
DM-3433,Fix socket timeout problem in xrootd framework,"when we run a query that take long time, client times out, it closes the socket, which triggers cancellation on the server side.",4,train
DM-3435,W16 End-to-End Integration (Data Access portion),end to end system integration. this epic covers work related to  # loading data generated by pipelines into qserv  # fully integrating qserv with sui,22,train
DM-3436,Qserv - webserv integration,"setup qserv and configure webserv to talk to qserv. verify all works, and fix discovered problems.",2,train
DM-3437,Add column names metadata to db query results,"per discussion at data access meeting aug 10, it'd be good to send column names with the query results.",2,train
DM-3438,Revisit KPIs for Qserv,need to come up with kpis for qserv  ,2,train
DM-3439,Rename ingest.py to reduce confusion with database,"the ingestimages.py bin script provides a cameraagnostic manner of creating a data repository (including a registry).  the backend code resides in pipetasks under the name ingest.py, and the ingesttask.defaultname = ""ingest"", which means that configuration files in obs packages are also named ingest.py.  this choice of name was unfortunate, as it may be confused with ingest of sources into the database.  we should change the name to reduce this confusion, perhaps ingestimages.py like the bin script.",2,train
DM-3440,"add meas_extensions_photometryKron to lsstsw, lsst_distrib","measextensionsphotometrykron should be added to the ci system, since we are trying to keep it updated.    this is blocked by dm 2429 because that includes a fix for a unit test (which the ci system would have caught).",1,train
DM-3442,Processing y-band HSC data fails in loading reference sources,"  processccd.py /lsst3/hsc/data/ output /raid/price/test id visit=904400 ccd=50  [...]  processccd.calibrate.astrometry.solver.loadan: loading reference objects using center (1023.5, 2091) pix = fk5coord(319.8934727, 0.0006943, 2000.00) sky and radius 0.111920792477 deg  processccd fatal: failed on dataid=: could not find flux field(s) ycamflux, yflux  traceback (most recent call last):    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/cmdlinetask.py"", line 320, in call      result = task.run(dataref, kwargs)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/pipetasks/10.128gf9582e42/python/lsst/pipe/tasks/processccd.py"", line 85, in run      result = self.process(sensorref, postisrexposure)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/pipetasks/10.128gf9582e42/python/lsst/pipe/tasks/processimage.py"", line 160, in process      calib = self.calibrate.run(inputexposure, idfactory=idfactory)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/pipetasks/10.128gf9582e42/python/lsst/pipe/tasks/calibrate.py"", line 457, in run      astromret = self.astrometry.run(exposure, sources1)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b255/python/lsst/meas/astrom/anetastrometry.py"", line 177, in run      results = self.astrometry(sourcecat=sourcecat, exposure=exposure, bbox=bbox)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b255/python/lsst/meas/astrom/anetastrometry.py"", line 292, in astrometry      astrom = self.solver.determinewcs(sourcecat=sourcecat, exposure=exposure, bbox=bbox)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b255/python/lsst/meas/astrom/anetbasicastrometry.py"", line 409, in determinewcs      return self.determinewcs2(sourcecat=sourcecat, margs)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b255/python/lsst/meas/astrom/anetbasicastrometry.py"", line 437, in determinewcs2      astrom = self.useknownwcs(sourcecat, wcs=wcs, kw)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b255/python/lsst/meas/astrom/anetbasicastrometry.py"", line 308, in useknownwcs      calib = none,    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/lsstsw/stack/linux64/measalgorithms/10.115g0d3ecf6/python/lsst/meas/algorithms/loadreferenceobjects.py"", line 173, in loadpixelbox      loadres = self.loadskycircle(ctrcoord, maxradius, filtername)    file ""/home/lsstsw/stack/linux64/pipebase/10.14g6ba0cc715/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args,  keyargs)    file ""/home/lsstsw/stack/linux64/measastrom/10.119g6e01b25+5/python/lsst/meas/astrom/loadastrometrynetobjects.py"", line 141, in loadskycircle      fluxfield = getreffluxfield(schema=refcat.schema, filtername=filtername)    file ""/home/lsstsw/stack/linux64/measalgorithms/10.115g0d3ecf6/python/lsst/meas/algorithms/loadreferenceobjects.py"", line 40, in getreffluxfield      raise runtimeerror(""could not find flux field(s) %s"" % ("", "".join(fluxfieldlist)))  runtimeerror: could not find flux field(s) ycamflux, y_flux      we should be able to fix this by setting config parameters (e.g., calibrate.astrometry.solver.defaultfilter or calibrate.astrometry.solver.filtermap), but how do we keep that synched with the choice of reference catalog?  and once we get past astrometry, we also have the same problem in photocal.",2,train
DM-3444,Add support for parsing user log files,"in order to get timings for jobs executed, add support for non dagman generated log files.  these are the user log files that htcondor writes out for each individual job.",6,train
DM-3447,Improve packaging of shared libraries in scons,"as discovered through dm3161, our swiggenerated libraries are messy. specifically, we are dumping everything we might need into the czarlib library. that includes mysql and mysqlclient related things. csslib needs mysql functions too. given that czar imports both these libraries, we ended up with duplicate symbols. that is being patched in dm3161, but it needs a further look / cleanup. we need to break things into smaller libraries. difficulty: understanding dependencies and avoiding circular dependencies.",12,train
DM-3448,S17 Long-running Query Optimizations,"as discovered through dm3432 when a query runs for long time, czar does not get response from worker for a long time, and times out. a quick patch we did during summer 2015 tests was to increase the timeout to a large number.    the issue was discussed at https:/confluence.lsstcorp.org/display/dm/databasemeeting201508 12. if we knew how long a query was going to take, we could set the timeout to appropriate value, but we can't always estimate well how long the query is going to take. the best solution we came up with: periodically check with the worker asynchronously what the status of the query is. worker should respond with something like: queued, scheduled, working / started x sec ago. note, this might require changes to the xrdssi api.",26,train
DM-3449,Handle problems with connecting to mysql in czar,"we observed in s15 tests that if we run too many queries, czar is running out of connections to mysql and as a result through exception that is uncaught (and dies). we triggered this by starting 110 queries. to ""fix"" this problem we increased the maxconnections in etc/my.cnf from 256 to 512. so, most likely the easiest way to reproduce it would be to set the maxconnections to a very small number.     this story involves handling the ""uncaught exception"" gracefully.",3,train
DM-3450,Tweaks to configurations discovered during S15 tests,"apply tweaks we found useful when running large scale tests. this includes:  # etc/my.cnf: change maxconnections to 512  # add"":  export xrdrequesttimeout=64000  export xrdstreamtimeout=64000  export xrddataserverttl=64000  export xrd_timeoutresolution=64000  to init.d/qservczar  # add :  ulimit c unlimited  to all startup scripts in init.d. this will make sure core file is always dumped when we have problems.",1,train
DM-3451,Resolve problem with running many simultaneous queries,"when we run with 110 simultaneous queries, czar fails with ""uncaught exception""",2,train
DM-3452,Integrate pipelines with MySQL and Qserv,"load data produced by pipelines into mysql (on lsst10), and qserv",5,train
DM-3453,AstrometryTask.run return not consistent with ANetAstrometryTask,"anetastrometrytask.run returns matchmetadata but astrometrytask.run returns matchmeta. the two must agree. it turns out that matchmeta is more widely used, so i'll standardize on that.",1,train
DM-3455,ProcessImageTask.matchSources fails if using ANetAstrometryTask,"processimagetask.matchsources fails when using anetastrometrytask with the following error:    processccd.calibrate.astrometry: applying distortion correction  processccd fatal: failed on dataid=:     file ""src/table/schema.cc"", line 239, in lsst::afw::table::schemaitem/ lsst::afw::table::detail::schemaimpl::find(const string&) const [with t = double; std::string = std::basicstring/]      field or subfield withname 'astromdistortedx' not found with type 'd'.   lsst::pex::exceptions::notfounderror: 'field or subfield withname 'astromdistortedx' not found with type 'd'.'    traceback (most recent call last):    file ""/ssd/rowen/lsstsw/stack/linux64/pipebase/10.14g6ba0cc73/python/lsst/pipe/base/cmdlinetask.py"", line 320, in call      result = task.run(dataref, kwargs)    file ""/ssd/rowen/lsstsw/stack/linux64/pipebase/10.14g6ba0cc73/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/ssd/rowen/lsstsw/stack/linux64/pipetasks/tickets.dm3453g086c9ddd0a/python/lsst/pipe/tasks/processccd.py"", line 85, in run      result = self.process(sensorref, postisrexposure)    file ""/ssd/rowen/lsstsw/stack/linux64/pipebase/10.14g6ba0cc73/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/ssd/rowen/lsstsw/stack/linux64/pipetasks/tickets.dm3453g086c9ddd0a/python/lsst/pipe/tasks/processimage.py"", line 219, in process      srcmatches, srcmatchmeta = self.matchsources(calexposure, sources)    file ""/ssd/rowen/lsstsw/stack/linux64/pipetasks/tickets.dm3453g086c9ddd0a/python/lsst/pipe/tasks/processimage.py"", line 250, in matchsources      astromret = astrometry.loadandmatch(exposure=exposure, sourcecat=sources)    file ""/ssd/rowen/lsstsw/stack/linux64/pipebase/10.14g6ba0cc73/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/ssd/rowen/lsstsw/stack/linux64/measastrom/tickets.dm3453gbb2ad1f49c/python/lsst/meas/astrom/anetastrometry.py"", line 321, in loadandmatch      with self.distortioncontext(sourcecat=sourcecat, exposure=exposure) as bbox:    file ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in enter      return self.gen.next()    file ""/ssd/rowen/lsstsw/stack/linux64/measastrom/tickets.dm3453gbb2ad1f49c/python/lsst/meas/astrom/anetastrometry.py"", line 295, in distortioncontext      sourcecat.table.definecentroid(self.distortedname)    file ""/ssd/rowen/lsstsw/stack/linux64/afw/10.137gaedf466/python/lsst/afw/table/tablelib.py"", line 8887, in definecentroid      return tablelib.sourcetabledefinecentroid(self, args)  notfounderror:     file ""src/table/schema.cc"", line 239, in lsst::afw::table::schemaitem/ lsst::afw::table::detail::schemaimpl::find(const string&) const [with t = double; std::string = std::basicstring/]      field or subfield withname 'astromdistortedx' not found with type 'd'.   lsst::pex::exceptions::notfounderror: 'field or subfield withname 'astromdistorted_x' not found with type 'd'.'    this is probably a result of dm2939. the basic problem is that the distortion context in anetastrometrytask should not be run at that point in processing.  suggests that a simple clean fix is to make the distortion context a no op if the wcs already contains distortion, if that works. this is what i will try first.",1,train
DM-3456,Fix problems with talking from webserv to qserv,"flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. so basically, at the moment even the simplest query run via webserv that is directed to qserv fails.",2,train
DM-3457,Implement prototype stack documentation with Sphinx,implement a minimallyviable sphinx documentation repository for the lsst stack. the code is available at https:/github.com/lsstsqre/lsststackdocs,5,train
DM-3458,Research existing sphinx doc implementations,examine how python packages such as astropy structure and implement their sphinx docs.,2,train
DM-3459,make forced and SFM interfaces more consistent,"from :    simplemeasurementtask.run and forcedmeasurementtask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first)  ",1,train
DM-3460,applyApCorr mis-handles missing data,"in applyapcorrtask.run the following lines do not behave as expected because get returns none if the data is missing, rather than raising an exception:                try:                  apcorrmodel = apcorrmap.get(apcorrinfo.fluxname)                  apcorrsigmamodel = apcorrmap.get(apcorrinfo.fluxsigmaname)              except exception:  ",1,train
DM-3462,Make obs_decam handle raw data ,"the current obsdecam expects instrument calibrated data from the community pipeline, i.e. it  requires matching instcal (instrument calibrated), dqmask (the associated mask file), and wtmap (weight map) data from the same visit.  this issue is to add functionality so that raw decam images can be ingested into registry and retrieved by the data butler.     practically, this will create new or expand existing subclasses of cameramapper and ingesttask.        a brief summary of changes:   the unit test getraw.py is updated and should pass, with dm3196      working testdatadecam for the unit test is currently at lsstdev /lsst8/testdatadecam and https:/uofi.box.com/testdatadecam   decaminstcalmapper is renamed to decammapper, to reflect that butler can also get ""raw"" now besides ""instcal"". please update mapper in your data repositories.   to create a registry for raw data, run       ingestimagesdecam.py /path/to/repo mode=link filetype=""raw"" /path/ .fits.fz      the default filetype is ""instcal"" for ingestimagesdecam.py, so previous use for instcal stays.  ",13,train
DM-3463,psfex lapack symbols may collide with built in lapack,"on my mac measextensionspsfex fails to build due to the numpy config test failing. ""import numpy"" fails with:    dlopen(/users/rowen/lsst/lsstsw/anaconda/lib/python2.7/sitepackages/numpy/linalg/lapacklite.so, 2): can't resolve symbol _nsconcretestackblock in /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libvmisc.dylib because dependent dylib #1 could not be loaded in /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libvmisc.dylib    our best guess (see discussion in data management 20150814 at approx. 1:57 pm pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses.    in case it helps i see this on os x 10.9.5. i do not see it on lsstdev.",2,train
DM-3464,lsstswBuild.sh --print-fail should report config.log,nan,1,train
DM-3467,Make async cancellation more flexible,nan,8,train
DM-3468,drawing text to ds9 fails if size or the font family is set,"commands like    ds9.dot('xxxx', 100, 100, size=3)  ds9.dot('xxxx', 100, 120, fontfamily=""times"")    silently fail.  the problem is that commands like    xpaset  p ds9 regions command ''    fail; you need to say font=times 12 normal",1,train
DM-3469,Prepare for S15 end-to-end system and Firefly relesase,"for s15, dm will try to have an endtoend system in place, and work out all the issues during the process. sui will try to a web application to  access some of dada access apis (dax ) on a system at ncsa.  sui will have a simple firefly release to the community.",15,train
DM-3470,Install/deploy SUI web application at NCSA,"for summer 15 release,  we will deploya sui web app on ncsa accessible to dm team.     work with ncsa to have a server setup   install necessary software packages   install sui software   deploy the system and test ",5,train
DM-3471,A simple Firefly release for S15,"to install and deploy a firefly application from scratch will take a few hours, including getting all the necessary  software packages and install them all.  in order for users to get the system up and running in as little time as possible, we will provide a war file with embedded tomcat server. users don;t need any third party software except java1.8.  the time to get the system up and running after installing java1.8 will be a minute after the download of war file.",10,train
DM-3472,Implement Basic spatial lookups for the Butler ,"based on https:/confluence.lsstcorp.org/display/dm/dependencieson02c.06sciencedataarchiveandapplicationservices pipeline software will require spatial lookups via butler.    current plan is:  provide an ""ingest"" script that looks at a butler exposure dataset, pulls spatial information and data id out of it, then shoves all of that into an sqlite3 table that can be used for accelerated spatial lookups.  then provide a task that takes skymap parameters, and maps those to potentially overlapping exposures: the sqlite3 table is outside butler; and the will use a spatial location to get a dataid, and then pass that dataid into butler to get that file. ",10,train
DM-3474,Debug problems with near neighbor queries,"the following near neighbor query:      select o1.ra as ra1, o2.ra as ra2, o1.decl as decl1, o2.decl as decl2,   scisqlangsep(o1.ra, o1.decl,o2.ra, o2.decl) as thedistance   from object o1, object o2   where qservareaspecbox(90.299197, 66.468216, 98.762526, 56.412851)   and scisqlangsep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015      fails on the in2p3 cluster.",4,train
DM-3477,Design SQL API for getting query type,"when unsure, user should be able to check what type of query a given query is (for example, is the query ""select   from object where qservareaspec(1, 2, 10, 5)"" considered interactive or async? this story involves deciding how the sql api will look like.",2,train
DM-3478,Design API for passing query type,"user should be able to pass hint with a query indicating what query type it is. based on that the return result will either be the query result, or queryid. this story involves designing the api  (sql and restful).",3,train
DM-3479,Design RESTful APIs for async queries in WebServ,need restul api for:    retrieving partial results while query is running    killing async queries,2,train
DM-3480,Design SQL APIs for async queries,"need sql api for:    submitting async query, note that we should be able to specify where the results are going / what is the format of the results    retrieving status of async query    retrieving results of async query    retrieving partial results of async query while it is running  ",5,train
DM-3481,adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7,changes in the way  jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.,2,train
DM-3482,Attending Cyber Security Summit,attending nsf cyber security summit in my capacity as lsst iso.,3,train
DM-3483,Calibration transformation should not fail on negative flux,"before database ingest, measured source fluxes are converted to magnitudes as per dm 2305. the default behaviour of afw::image::calib is to throw when a negative flux is encountered, which derails the whole transformation procedure. better is to return a nan.",1,train
DM-3484,Design RESTful API for getting query type,"when unsure, user should be able to check what type of query a given query is (for example, is the query ""select   from object where qservareaspec(1, 2, 10, 5)"" considered interactive or async? this story involves deciding how the restful api will look like.",2,train
DM-3488,Debug problem with large results set,query returning 2 billion rows causes problems for czar   czar is using nearly 16 gb or memory. need to understand why ram usage in czar is correlated with result size.,1,train
DM-3489,replace Calib negative flux behavior methods with context manager,"dm 3483 adds a nice context manager for handling the behavior of calib objects when encountering negative fluxes.  this could be even nicer if we moved it into afw and integrated it with calib itself, replacing the existing static methods (which are bad because they use global variables).    this will be an api change, and will require an rfc.",2,train
DM-3490,Quick-and-dirty n-way spatial matching,"this issue will add limited nway spatial matching of multiple catalogs with identical schemas, sufficient for measuring fy15 kpms.  it will be a simple wrapper on our existing 2way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface).",2,train
DM-3491,Update ip_diffim to use the new NO_DATA flag instead of EDGE,"in ipdiffimm some uses of edge were converted to or supplemented with nodata, but others were not. this ticket handles the missing instances.",1,train
DM-3492,Correct for distortion in matchOptimisticB astrometry matcher,"matchoptimisticb does not correct for distortion, although an estimate of the distortion is available.  we suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for hsc.",5,train
DM-3493,Fix crosstalk following ds9 interface changes,"crosstalk.py in obssubaru uses ds9 without actually displaying anything, which causes trouble if displayds9 is not setup.",1,train
DM-3494,lsst.afw.display.setMaskTransparency doc doesn't match code,"the docstring for setmasktransparency says ""specify display's mask transparency (percent); or none to not set it when loading masks"", but (from inspection of the code), it doesn't do anything if transparency is none.  ",1,train
DM-3495,X16 Large Results,"as described in http:/ldm135.readthedocs.org/en/master/#queryaccessrelatedrequirements high volume queries can return large results (6 gb per query). current version of qserv poorly supports large results. we have observed that a single query that returns 40 million or twocolumn rows (~0.5 gb) requires nearly 16 gb of ram in czar, and czar uses 100% of cpu for extended period of time. this is most likely related to aggregating results   results are currently cached in memory before they are returned to the proxy. this epic involves redesigning this part of qserv. one reasonable approach would be to write to separate tables from each worker, and deliver the final result as union of these tables. or, perhaps we could use a shared files system and each worker would write results directly there, without burdening czar.    deliverable: demonstration involving 8 queries each returning 2 gb result handled through a single czar. czar should not use excessive amount of ram or cpu.",19,train
DM-3496,Add support for images produced by pipelines in end-to-end integration test,we need to add support for images that are produced by pipelines which are run as part of the endtoend integration tests   imgserv should be able to serve these images.,6,train
DM-3499,Add support for executing async queries through czar,nan,14,train
DM-3500,S17 Async Queries in WebServ,"add support for async queries in webserv. deliverable: webserv that allows to manage async queries (start, kill, retrieve results). partial results are not covered here.",12,train
DM-3501,Implement RESTful API for async queries in WebServ,nan,10,train
DM-3502,Research mysql proxy alternatives,"research alternatives to mysql proxy, including things like maxscale.",9,train
DM-3504,Improve spatial image search for butler,nan,11,train
DM-3505,Modify czar to use per query CSS metadata,"czar is currently caching css information, the snapshot is taken when czar starts. once we start dealing with more dynamic system where databases and tables can come and go anytime while the system is up (in particular, l3 databases and tables), the metadata in css would need to be refreshed per query to stay up to date",6,train
DM-3506,W16 Support Dynamic CSS Metadata in Czar,czar needs to support dynamic css metadata. this epic involves reworking facade and related code so that czar can have up to date css metadata (per query) instead of relying on static snapshots,40,train
DM-3508,Revisit Facade API,nan,6,train
DM-3511,Adopt Webserv to work with reworked db module,nan,4,train
DM-3512,Expose column metadata via metaserv,"we need to expose via restful apis information about columns such as units, ucds, column description etc. it will require querying information_schema with information from our ddt tables. this should include exposing information which columns are the ra/decl columns used for indexing.",6,train
DM-3513,Add support for row counts in Metaserv,"sui software would like to frequently check what the row counts for our tables. this story involves caching the information about the row count information in the metaserv.     note that for l3 tables, that might change, we will need to intercept queries that alter these tables, and update the cache.this is beyond the scope of this story.",4,train
DM-3514,Add flag to MetaServ showing if qserv_area_spec is available,sui software needs to know which tables support spatial queries. this story involves exposing this information via metaserv (the information is available via css) ,3,train
DM-3515,Refactor DipoleMeasurement: Dipole classification to plugin,"dipolemeasurementtask currently runs dipole measurement plugins and runs its own implemented dipole classification method. this ticket translates dipole classification to a plugin itself to simplify dipolemeasurementtask.  instead of inheriting from singleframemeasurementtask, dipolemeasurementtask will run singleframemeasurementtask setting the appropriate default slots and plugins (including dipole classification plugin).      ",6,train
DM-3516,Base DC network design,design proposal for base dc network,20,train
DM-3518,prune stale obs_subaru dependencies,"obssubaru has some eups dependencies that should be removed:    measextensionsmultishapelet (removed from the lsst stack, content moved to measmodelfit)    measmultifit (renamed to measmodelfit)    fitsthumb (need to cherrypick code from hsc to replace it)    pyfits (investigate what we use it for; it might not be needed)    psycopg2 (i don't think we'll ever use this on the lsst side; we should add it back to the hsc side after we re fork)",4,train
DM-3519,Create flux environment for input panels,nan,20,train
DM-3520,Organize React component to use new flux environment,nan,10,train
DM-3522,Releasing un-acquired resources bug,"running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with      terminate called after throwing an instance of 'lsst::qserv::bug'    what():  chunkresource chunkentry::release: error releasing unacquired resource      stack trace:      #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  missing separate debuginfos, use: debuginfoinstall expat2.1.08.el7.x8664 glibc2.1778.el7.x8664 keyutilslibs1.5.83.el7.x8664 krb5libs1.12.214.el7.x8664 libcomerr1.42.97.el7.x8664 libgcc4.8.39.el7.x8664 libicu50.1.211.el7.x8664 libselinux2.2.26.el7.x8664 libstdc4.8.39.el7.x8664 nsssoftoknfreebl3.16.2.39.el7.x8664 openssllibs1.0.1e42.el71.9.x8664 pcre8.3214.el7.x8664 xzlibs5.1.29alpha.el7.x8664 zlib1.2.713.el7.x8664  (gdb) where  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  #1  0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6  #2  0x00007fb6b15d29b5 in gnucxx::verboseterminatehandler() () from /lib64/libstdc.so.6  #3  0x00007fb6b15d0926 in  () from /lib64/libstdc.so.6  #4  0x00007fb6b15cf8e9 in  () from /lib64/libstdc.so.6  #5  0x00007fb6b15d0554 in gxxpersonalityv0 () from /lib64/libstdc.so.6  #6  0x00007fb6b1069913 in ?? () from /lib64/libgccs.so.1  #7  0x00007fb6b1069e47 in unwindresume () from /lib64/libgccs.so.1  #8  0x00007fb6ab554247 in lsst::qserv::wdb::chunkresourcemgr::impl::release (this=0x21d1cc0, i=...) at build/wdb/chunkresource.cc:398  #9  0x00007fb6ab552696 in lsst::qserv::wdb::chunkresource::~chunkresource (this=0x7fb68a5f9b70, inchrg=/)      at build/wdb/chunkresource.cc:131  #10 0x00007fb6ab560f0f in lsst::qserv::wdb::queryaction::impl::dispatchchannel (this=0x7fb65848c4d0) at build/wdb/queryaction.cc:392  #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::queryaction::impl::act (this=0x7fb65848c4d0) at build/wdb/queryaction.cc:187  #12 0x00007fb6ab562084 in lsst::qserv::wdb::queryaction::operator() (this=0x7fb658050548) at build/wdb/queryaction.cc:450  #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::foremanimpl::runner::operator() (this=0x7fb67400fa20) at build/wcontrol/foreman.cc:302  #14 0x00007fb6ab551cf0 in std::bindsimple/::minvoke/(std::indextuple/) (      this=0x7fb67400fa20) at /usr/include/c/4.8.2/functional:1732  #15 0x00007fb6ab551a8b in std::bindsimple/::operator()() (this=0x7fb67400fa20)      at /usr/include/c/4.8.2/functional:1720      tail of log file from xrootd log:      0821 19:08:58.530 [0x7fb68a6fb700] debug groupsched (build/wsched/groupscheduler.cc:139)  getnexttasks(1)>>>  0821 19:08:58.530 [0x7fb68a6fb700] debug groupsched (build/wsched/groupscheduler.cc:151)  returning 1 to launch  0821 19:08:58.530 [0x7fb68a6fb700] debug groupsched (build/wsched/groupscheduler.cc:154)  getnexttasks />>  0821 19:08:58.530 [0x7fb68a6fb700] debug scansched (build/wsched/chunkdisk.cc:199)  chunkdisk busyness: yes  0821 19:08:58.530 [0x7fb68a6fb700] debug scansched (build/wsched/chunkdisk.cc:171)  chunkdisk getnext: current= (scan=10436,  cached=8360,8259,) candidate=10301  0821 19:08:58.530 [0x7fb68a6fb700] debug scansched (build/wsched/chunkdisk.cc:184)  chunkdisk denying task  0821 19:08:58.530 [0x7fb68a6fb700] debug scansched (build/wsched/scanscheduler.cc:196)  getnexttasks <<<<<  0821 19:08:58.531 [0x7fb68a6fb700] info  root (build/xrdsvc/ssisession.cc:120)  enqueued taskmsg for resource(/chk/lsst/2732) in 0.001016 seconds  0821 19:08:58.531 [0x7fb6895f8700] debug foreman (build/wcontrol/foreman.cc:175)  registered runner 0x7fb66c141ab0  0821 19:08:58.531 [0x7fb6895f8700] debug foreman (build/wcontrol/foreman.cc:209)  started task task: msg: session=434445 chunk=2732 db=lsst entry time= frag: q=select o.deepsourceid,o.ra,o.decl,s.coordra,s.coorddecl,s.parent from lsst.object2732 as o,lsst.source2732 as s where scisqls2ptinbox(o.ra,o.decl,48.482655,54.274507,48.555903,54.196952)=1 and scisqls2ptinbox(s.coordra,s.coorddecl,48.482655,54.274507,48.555903,54.196952)=1 and o.deepsourceid=s.objectid, sc= rt=r4344458c9456ede5cbe0b5f42e1a1571d5dd7327320   0821 19:08:58.531 [0x7fb6895f8700] info  foreman (build/wcontrol/foreman.cc:296)  runner running task: msg: session=434445 chunk=2732 db=lsst entry time= frag: q=select o.deepsourceid,o.ra,o.decl,s.coordra,s.coorddecl,s.parent from lsst.object2732 as o,lsst.source2732 as s where scisqls2ptinbox(o.ra,o.decl,48.482655,54.274507,48.555903,54.196952)=1 and scisqls2ptinbox(s.coordra,s.coorddecl,48.482655,54.274507,48.555903,54.196952)=1 and o.deepsourceid=s.objectid, sc= rt=r4344458c9456ede5cbe0b5f42e1a1571d5dd7327320   0821 19:08:58.531 [0x7fb6895f8700] info  foreman (build/wdb/queryaction.cc:177)  exec in flight for db = qfd51ad249f62fb765e173d7b3cae5d94  0821 19:08:58.531 [0x7fb6895f8700] warn  foreman (build/wdb/queryaction.cc:109)  queryaction overriding dbname with lsst  0821 19:08:58.718 [0x7fb6a0d8e700] info  root (build/wdb/queryaction.cc:261)  &&& fillrows size=106  0821 19:08:58.718 [0x7fb6a0d8e700] info  root (build/wdb/queryaction.cc:261)  &&& fillrows size=210  0821 19:08:58.718 [0x7fb6a0d8e700] info  root (build/wdb/queryaction.cc:261)  &&& fillrows size=316    ...(thousands of fillrows lines)    terminate called after throwing an instance of 'lsst::qserv::bug'    what():  chunkresource chunkentry::release: error releasing unacquired resource    ",3,train
DM-3523,data center requirements,nan,2,train
DM-3524,Revise LDM-240,revising ldm 240.,10,train
DM-3525,Add ITIL model use cases to Enterprise Architect,nan,2,train
DM-3526,Prepare use case diagrams for LSST 2015 Operations Concepts breakout session,nan,2,train
DM-3527,FY19 Query Result Caching,"implement query caching. note that most likely we can't ""just"" rely on mysql caching, and we will need to do some custom tweaks, in particular for async queries.    i'd be useful to allow users to ""pin"" results from interesting queries. typical usecase: user runs a query (it can be interactive) and then decides to keep the results for longer time. ",79,train
DM-3530,LOE - Week ending 8/21/15,nan,3,train
DM-3531,LOE - Week ending 8/28/15,nan,4,train
DM-3533,Information categorization for docushare,project started to categorize documents on docushare as per lsst's information categorization policy.,3,train
DM-3534,Meet with Scott K for IdM project,scott k will come to ncsa to discuss initial idm requirements for lsst.,1,train
DM-3535,NIST SP 800.82 investigation,nist sp 800.82 might contain useful information for securing the scada enclave at the observatory site.  or it might even be the model we should use in full.,2,train
DM-3536,Czar Failover in xrdssi,in production we will need to recover from czar failures by automatically failing over to a different czar. this epic involves designing and implementing the interfaces in xrootd xrdssi that will be need to support czar fail over.,20,train
DM-3538,Translate more mask pixel bits from instcal data quality mask of DECam data ,"the mask pixel bits are defined differently in decam instcal data from different pipelines, see http:/community.lsst.org/t/decamdataqualitymasks/133  for a summary.      the current mapping in decaminstcalmapper is for the community pipeline prev3.5.0.  more mask bits were defined in desdm y1 products. this issue provides full mapping for desdm y1 products so all questionable pixels are translated into the maskedimage.       ",2,train
DM-3539,Fix bugs found in FitsRead ,"there was a bug found in writefitsfile(outputstream stream, fitsread[] fitsreadary, fits reffits) method where it saved the data from reffits into the output fits file instead of the data from the fitsread object.    there is another possible bug under investigation.  the output fits file created by fitsread.createnewfits() can not be opened as an image.   ",4,train
DM-3540,Refactor ChunkResource for testability,"while the wdb/testchunkresource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it.    we should refactor the classes in the chunkresource header and implementation files to make the sanity of the implementation checkable. in particular, i think that backend from chunkresource.cc should be a public interface (with a less generic name) that the unit test can implement, and that chunkresourcemgr should be a concrete class implemented in terms of a user specifiable backend.    this way the unit test can inject the dependency it wants (namely a mock backend that tracks subchunk tables as they are acquired and released), we don't pollute the actual implementation classes with ""i'm fake"" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests.    the first cut at this should include a check for the problem described in dm3522.",4,train
DM-3543,Emergent Uncategorized Work,epic to capture work that is not easy to categorize in other wbss.,30,train
DM-3544,Cleanup of initial astrometry improvements,"the astrometry improvements are working, but some cleanup would be good to remove dependencies on a.net and to provide default reference catalog loaders.",20,train
DM-3545,Improve aperture correction implemented in HSC,there is technical debt left over from the hsc lsst merge of the aperture corrections.  this epic will take care of the debt noted during the port.,15,train
DM-3546,Move LDM-151 to Sphinx/Read the Docs,move the ldm151 (dm applications design document) to restructuredtext (built with sphinx) and published automatically via readthedocs.org.    see discussion at http:/community.lsst.org/t/requestingcommentsfordesigndocumentationformatfordm/132?u=jsick    this is an experiment.,1,train
DM-3547,SsiService not being destroyed,ssiservice::~ssiservice is not being called. ,1,train
DM-3548,Learn about IPython / Jupyter internals,"in order to evaluate the suitability of ipython as a framework for level 3 work, and its ability to be integrated with the sui tools, the internal structure of ipython and its communication protocols need to be understood.    work on this story will include reading about ipython, experimenting with it, and reading the ipython code as needed.",14,train
DM-3549,Further improve the TAN-SIP WCS fitter,"lsst.meas.astrom.sip.makecreatewcswithsip, our c implementation of a tansip wcs fitter, fits for x and then y. thus it must be called several times (e.g. by fittansipwcstask) in order to converge. please make makecreatewcswithsip fit for x and y simultaneously. this would simplify its use and speed up fitting.    in addition, please investigate whether outlier rejection can be added to makecreatewcswithsip. as of dm3492 outlier rejection is implemented in fittansipwcstask, but it would make makecreatewcswithsip easier to use and speed up fitting if makecreatewcswithsip did the outlier rejection itself. on the other hand, the current solution may be sufficient.    once the above is implemented, please simplify fittansipwcstask by removing the unneeded extra iterations used to work around these problems.",12,train
DM-3550,Future measurement algorithm enhancements,nan,40,train
DM-3551,Attend SciPy 2015 tutorials,"attend the tutorials at scipy 2015 (july 67, 2015) in order to get handson experience with current scientific data analysis tools in the python environment.    planned attendance:    introduction to numpy   building python data applications with blaze and bokeh   efficient python for high performance parallel computing   jupyter advanced topics tutorial  ",4,train
DM-3552,Discuss the QA visualization needs,meet with sqaure lead to discuss the qa needs of visualization tools.     the whole suit team of 8 people met with square lead frossie for an all day discussion to outline the visualization needs that square team may need in support of the pipeline stack data processing verification. we agreed that the image visualization and xy 2d plot  components should be separately accessible through javascript api and python api. square team could build its own web portal using the javascript api and build its own analysis tool in ipython notebook using the python api. the resulting work work was captured in other epic and stories.   ,18,train
DM-3553,Access predefined catalogs via Data Access API,"as a part of endtoend exercise access predefined catalogs and their definitions from the new data access api.     we have been doing it via jdbc calls to qserv and queries to http:/lsst web.ncsa.illinois.edu/schema/index.php?sver=s12_sdss    even though we can not access qserv via data access api at the moment, it should be transparent to us in the future.    as for data definitions, for now we can only access column names and types. in future, more information (like units and field descriptions) should be available.",10,train
DM-3554,Binary FITS table catalog upload,"we need to be able to upload catalogs in binary fits table format.    we'll do it by converting the first table in the provided fits into an ipac table. our upload should be handling the conversion.    later, we should figure out how to handle multiple tables in one fits.",4,train
DM-3555,"Ignore ""SELECT @@tx_isolation"" queries",looks like one of the queries we registered in webserv is: cursor.execute('select @@tx_isolation') and that is bound to confuse qserv. need to suppress it at mysql proxy level.,1,train
DM-3556,Attend SciPy 2015 conference,"attend the scipy 2015 conference, july 8 10, 2015, to learn about current trends in this community.    notably, the conference covers a variety of interactive data analysis tools, including jupyter/ipython, interactive graphics packages such as vispy and bokeh, and astronomical data handling tools such as astropy.",6,train
DM-3557,Document SciPy 2015 takeaways,"write up what was learned, and recommendations, from scipy 2015.",4,train
DM-3558,Experiment with Jupyter widget technology and Firefly Tools,"based on dm 2047 work to date, investigate the feasibility of using the jupyter widget interface to wrap up firefly tools.",8,train
DM-3559,Implement improved Footprints,in s15/dm 1904 we began a redesign of the footprint api. we need to follow through and convert that to code on disk.,60,train
DM-3560,Complete HSC port: object characterization,merge all functionality from hsc to lsst that is required so that the lsst stack can be used for all standard hsc processing functionality and the hsc fork is removed.,100,train
DM-3561,Continued galaxy shear fitting experiments,build on the test framework delivered in dm 1108 to arrive at conclusions regarding the number of pixels which must be included in galaxy fitting and the number of shapelet terms needed in the psf.,60,train
DM-3562,Refactor executor code,nan,18,train
DM-3563,Add unit tests to exercise new scheduler,add tests (unit test and/or extend the integration test) to test the new scheduler.,10,train
DM-3564,Integrate Qserv code with cancellation-friendly xrdssi,nan,6,train
DM-3566,Add support for type aliases,nan,6,train
DM-3568,Complete HSC port: framework,"merge all framework functionality (afw, middleware, etc) from hsc to lsst that is required so that the lsst stack can be used for all standard hsc processing functionality and the hsc fork can be removed.",60,train
DM-3569,Research/reading for PAST prototype C++,nan,6,train
DM-3571,Make location of images more flexible,it'd be useful to be able to point imgserv to any location containing images (in particular for various random tests that we will be running over the next few years). right now this requires changing imgserv code. an idea tossed around: pass the location via uri (optional  parameter),4,train
DM-3572,Collect requirements for pipeline developer visualization tools,"this story covers a series of conversations with robert lupton, jim bosch, paul price, k t lim, and others going into details about how to make the visualization environment (based on firefly tools) more useful for developers.",4,train
DM-3573,Document AP simulator,"write up how the ap simulator current works, i.e. start the processes doing the ap simulator, sending messages to the base dmcs, how messages are sent from archive to jobs on the worker cluster, etc.",10,train
DM-3574,Replace qservAdmin.py use with CssAccess,we have new css interface which unifies c and python and it is time to replace qservadmin.py with cssacces in places like qserv admin.py and data loader.  ,8,train
DM-3578,Research requirements for chromaticity,nan,30,train
DM-3579,Redesign CalibrateTask,this covers design work only; an accompanying epic (perhaps in 02c.03?) will handle implementation.,10,train
DM-3580,Refactor sub-task interfaces,nan,50,train
DM-3581,"Audit, update & integrate top level tasks","as part of the hsc merge, we've pulled in a number of highlevel pipeline tasks, which may not be consistent with existing lsst tasks. audit the overall pipeline flow, ensuring consistency. to include:     rewrite toplevel pipeline tasks and butler datasets.   remove processcoaddtask.    ensure consistency between mpi drivers and individual cmdlinetasks.",25,train
DM-3582,Investigate options for physically motivated PSF models,"rather than developing psf models based on the optics and the atmosphere, this epic is focused on developing requirements and establishing what resources & expertise are available in other groups (e.g. desc) which we can make use of. if necessary, it will flow down to another epic in which we actually produce working code.",30,train
DM-3583,Develop improved galaxy model flux measurements,"hard thinking, cleaning up & optimizing the existing galaxy model flux measurement code.",50,train
DM-3585,DRP W16 emergent work: object characterization,catch all epic for emergent work in 02c.04.06 during w16.,20,train
DM-3586,DRP W16 emergent work: framework,catch all epic for emergent work in 02c.04.01 during w16.,30,train
DM-3587,Firefly infrastructure improvement to support new functions (W16),this epic will capture the necessary changes of firefly infrastructure to support new functions needed. it does not include the changes caused by the the conversion from gwt infrastructure to pure javascript based system using react and flux platform. ,40,train
DM-3588,Refactor the Firefly Java code (W16),this epic will capture all the refactoring work related to java code. we are converting portions of gwt code to pure javascript react based code. we will only refactor the java code if it is not going to be converted. ,30,train
DM-3590,Catalog transformation should pass through all non-measurement fields,"we need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs.  i believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation   they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic.",3,train
DM-3591,Add support for registry-free repository,existing butler unit tests should run without an sqlite database registry.,12,train
DM-3592,Refactor Backend to improve visibility,backend is a class that connects to mysql and can cause the worker to terminate. it is buried in the foreman and should be renamed and have the class defined in a header file so that it is more visible. the class should also be available to use for any other in memory tables. ,10,train
DM-3593,Firefly support for pipeline visualization needs  (W16),data products pipeline needs visualization capabilities for display. firefly needs to have new capabilities to support it. ,40,train
DM-3595,New functions for XY 2D plot (W16),firefly should support histogram plot,50,train
DM-3596,new algorithm and functions for 2D XY plot (F17),nan,30,train
DM-3597,use devtoolset-3 for Jenkins CI builds on EL6,nan,2,train
DM-3599,Firefly support for Camera team visualization needs (W16),camera team plans to use firefly visualization capabilities in camera test.  sui/t team will continue to support this effort. ,30,train
DM-3600,Workspace preliminary functional design  (W16),produce a preliminary functional design of user workspace. ,18,train
DM-3601,Authentication and authorization system API requirement,"produce a requirement document on the authentication and authorization  api needed for the full suit. work with other dm subsystems (db, square, ncsa) closely to achieve this. ",10,train
DM-3602,Prepare for system setup in NCSA for LSST web UI (X16),"we want to start preparing for the sui system setup in fall 2016 at ncsa to be able to do the following:    set up a web ui for dm or even general public to access, so we can have a constant endtoend system. it will be very useful for dm testing. suit will use it for simple test of accessing the dax apis.  pipeline team can use it to see the processed data, search source table etc. the goal is to be able to access sdss strip 82 data through suit web portal",10,train
DM-3603,Expose more Firefly visualization functions through JavaScript API (W16),expose more firefly visualization functions through javascript api to users so they can have more control in building their own web page. ,10,train
DM-3604,Expose more Firefly visualization functions through Python API (F16),expose more firefly visualization functions through python api as needed.,22,train
DM-3605,Data access (DAX) API design support (W16),continue to work with slac  in data access (dax) api design and test. participating the discussion and exercising the api as needed to help test and expose potential issues. ,10,train
DM-3606,"Submit change request for LSE-68, mid-phase-3 update","collect changes and submit a change request to lse68 for a midphase 3 update to the document, covering clarifications on guider data and image identification, among others.    includes preliminary work to prepare for change request.",4,train
DM-3607,Prepare document for CCB review of LCR-357,"lcr 357 was an outline of work to be done, based on discussions with the camera daq staff.  this task is to generate an actual proposed document change for the ccb.",6,train
DM-3608,provide detailed information needed to DAX meta API,"suit needs certain specific information through dax meta service when searching for meta data. for example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?",1,train
DM-3609,The Alert subscription system requirement gathering (F16),solidify the requirement for the alert subscription system. ,8,train
DM-3610,CCB review and posting of final updated document,"carry out the ccb review, respond to questions, support final implementation of updated document.",2,train
DM-3611,Prepare for Winter 2016 work on LSE-68,use a session at the lsst 2015 allhands meeting to prepare for lse68 work in the winter 2016 cycle.,3,train
DM-3612,option to plot error bars on XY plot,"when there are error or uncertainty for a data point, there should be option to plot the error bars in the xy plot. ",6,train
DM-3613,Time series plot,time series plot,10,train
DM-3614,capability to reverse the axis,"provide option to reverse the axis for xy plot, one example is the magnitude value convention. ",10,train
DM-3615,expose region overlay on image function through JavaScript API,expose region overlay on image function through javascript api,5,train
DM-3616,Expose image XY readout at cursor point function in JavaScript API,expose image xy readout at cursor point function in javascript api,2,train
DM-3617,Review risk register status,nan,3,train
DM-3618,Fix bug related to restarting xrootd in wmgr,changes from dm 2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.,1,train
DM-3619,Move env variables related to xrootd/czar and unlimit in etc/sysconfig/qserv,nan,1,train
DM-3620,OCS-DM-CCS-DAQ workshop,prepare for and attend the ocsdaqccsdm workshop in november 2014.  this is primarily intended to review the status of lse70 (the definition of the ocs interface to the subsystems) and therefore prepare for updates to lse 72 in time.,6,train
DM-3621,OCS-DM-CCS-DAQ workshop II,prepare for and attend the ocsdaqccsdm workshop in february 2015. this is primarily intended to continue to make progress on lse70 (the definition of the ocs interface to the subsystems) and therefore prepare for updates to lse 72 in time.,6,train
DM-3622,Overhaul deblender,rewrite the existing deblender code to use the refactored footprints (dm3559). take the opportunity to resolve known issues and add new functionality from sdss and elsewhere to make the deblender equal to the state of the art.,60,train
DM-3623,LDM-144 costing model update,nan,6,train
DM-3624,Investigate disk-only (no tape) data releases,"the question that is being raised is how much it would cost to keep more than the last two data releases on readilyaccessible storage (i.e. spinning disk). this will require changing several numbers and formulas in ldm141, the storage sizing  model, observing the results as they propagate through ldm 144, the cost model,  and checking to make sure that everything makes sense and nothing has been overlooked. it looks like an answer to this question will be needed somewhere between a month and three months from now.",2,train
DM-3625,Continue to refine the SUIT requirements (X16),we did lots of work in fy15 to redefine the suit requirements. the picture is getting clear as we talk to others in dm system and science community.  we would like to have a better definition and provide a document for this. ,10,train
DM-3626,a catch all epic for unexpected bug fixes  (W16),this is an epic for unexpected bugs found and need to be fixed in this cycle. ,10,train
DM-3627,implement jenkins support for running builds in docker containers,"dm3359 demonstrated the feasibility of running jenkins builds in oneoff docker containers but did not cover investigation of user creation of containers nor did it include a puppetized deployment.    per build containers are desirable for a number of reasons but the largest motivation is allowing dm developers to define their own jobs without disrupting the ""sanctity"" of the lsstsw build slaves.",7,train
DM-3628,HSC port: verification,"compare the lsst and (old) hsc stacks for compatibility; identify and resolve any differences. provide basic integration tests to square to ensure the reliable and consistent operation of the lsst stack for processing hsc data. cooperate with the square team to develop more elaborate qa testing.    this work is timeboxed to consume the available effort in w16, and will be continued in the subsequent cycle.",50,train
DM-3630,Change root to config in config override files,implement rfc 62 by using config rather than root in config override files for the root of the config.    note that i propose not modifying astrometrynetdata configs because those are numerous and hidden. they have their own special loader in loadastrometrynetobjectstask._readindexfiles which could easily be updated later. if desired. an obvious time to make such a transition would be when overhauling the way this data is unpersisted.,2,train
DM-3631,Management for Aug 23-29,"hiring  make offer to candidate, and also review candidates and strategy.  review proposal for fy 2015 spend  management meetings.  openstack meeting.",3,train
DM-3632,"Study JavaScript, read up on React ",spend about one hour per day to read up on javascript and react framework. get ready to work on some of the javascript programming in firefly,10,train
DM-3633,Add readMatches back to meas_astrom,"recent changes to measastrom accidentally removed a function readmatches (copied below). please restore it, preferably in its own module (though if someday we have more small python functions we may want a utils.py module).    also please include a unit test.      def readmatches(butler, dataid, sourcesname='icsrc', matchesname='icmatch', config=measastromconfig(), sourcesflags=afwtable.sourceionofootprints):      """"""read matches, sources and catalogue; combine.      @param butler data butler      @param dataid data identifier for butler      @param sourcesname name for sources from butler      @param matchesname name for matches from butler      @param sourcesflags flags to pass for source retrieval      @returns matches      """"""      sources = butler.get(sourcesname, dataid, flags=sourcesflags)      packedmatches = butler.get(matchesname, dataid)      astrom = astrometry(config)      return astrom.joinmatchlistwithcatalog(packedmatches, sources)    ",4,train
DM-3634,Preliminary design of Firefly core  using React and FLUX framework,propose a preliminary design for firefly core using react and flux framework. ,12,train
DM-3635,configDictField.py has code that relies on an undefined variable,"while taking a linter pass on pexconfig i found that configdict.setitem in configdictfield.py has some code that uses an undefined variable value. see the else clause in:              if oldvalue is none:                          if x == dtype:                  self.dict[k] = dtype(name=name, at=at, label=label)              else:                  self.dict[k] = dtype(name=name, at=at, label=label, x.storage)              if sethistory:                  self.history.append((""added item at key %s""%k, at, label))          else:              if value == dtype:                  value = dtype()              oldvalue.update(at=at, label=label, value._storage)              if sethistory:                  self.history.append((""modified item at key %s""%k, at, label))  ",2,train
DM-3636,continue L1 refined specifications ,"resynchronize with prior work after vacation.    write up page both engineering and facility database ingest  and use, incorporate suggestion and comments from kt and gdf.    clean up (better name entities, and move for better narrative flow) pages about the general thing that is called ""level 1"",   the context diagram and supporting prose for further descriptions,  review internally with jason, steve and margaret.    create a revised  diagram that would change ldm230 specifications. (all non crosstalk corrected data flow into a disk buffer, and and an new archive tasks empties it)  to make specifications consigned with the refined specifications on the page above.  yet to write the prose....    these pages are in my personal pages in the lsst confluence area.        ",4,train
DM-3637,preliminary detailed content required for Authorization and Authentication system for SUIT,provide the first draft  of detailed content required for authorization and authentication system from suit point of view to ncsa. ,6,train
DM-3638,RangeField mis-handles max < min,"rangefield contains the following bit of code to handle the case that max / max:              swap(min, max)      this is broken because there is no swap function and if there was it could not work in place like this. however, rather than replace this with the standard min, max = max, min i suggest we raise an exception. if max / max and thus that an exception will be fine.",1,train
DM-3639,"OCS-CCS-DAQ-DM teleconference, April 2015",prepare for and attend a half day teleconference on ocs issues.,2,train
DM-3640,"OCS-CCS-DAQ-DM workshop III, May 2015","prepare for and attend an ocssubsystems workshop at slac may 68, 2015.",6,train
DM-3641,Firefly server side extensions using DM stack (F16),design and implement a control system to extend firefly server side capabilities using task in dm stack.  this will make it easier to use dm stack for customized data processing. ,40,train
DM-3642,"Support OCS revision of LSE-70, LSE-209","support the ocs efforts to update lse70 and create a new associated document, lse209.  getting current versions of these under change control will allow us to complete a round of work on lse 72.",20,train
DM-3643,"install DM stack, get familiar with the current DM task concept ","install dm stack, get familiar with the current dm task concept.   this is for getting ready to use task with firefly server side extension capability.  here is the link to the tutorial.  https:/confluence.lsstcorp.org/display/dm/gettingstartedwithstackdevelopment",6,train
DM-3644,Support the design of Firefly core system using React and FLUX,working with loi on the design of firefly core system based react and flux frameowrk,4,train
DM-3645,Support the design of Firefly core system using React and FLUX ,working with loi on the design of firefly core system based react and flux frameowork  ,4,train
DM-3646,"LSE-72: OCS-CCS-DAQ-DM workshop, July 2015","work associated with workshop iv in the series, held at ncsa july 8 10, 2015.",2,train
DM-3647,Preliminary SUIT design ,working with the suit wg to produce a preliminary design of suit.,54,train
DM-3648,SUIT design document outline,sui/t design document outline.  ,2,train
DM-3650,on-going support to Camera team in UIUC,attend uiuc weekly meeting and give support as needed. ,2,train
DM-3651,MakeDiscreteSkyMapRunner.__call__ mis-handled returning results,"makediscreteskymaprunner.call will fail if self.doreturnresults is true due to trying to reference undefined variables. this is at least approximately a copy of a problem that was fixed in pipebase taskrunner.    makediscreteskymaprunner.call should be fixed in a similar way, and (like taskrunner) changed to return a pipebase struct.  ",2,train
DM-3652,SUIT design document outline,"work with gregory on the suit design document outline    1.  requirements flow down, making sure that we design the system satisfying the current requirements.  2.  use cases collection. at least one typical use case in each major science theme  3.  levels of different users   novice: treat the web portal as a archive to get some information, don't know much about lsst   novice expert: has some ideas of what special functions they would like, has some knowledge of lsst data   domain expert: knows lsst data very well and want some special functions ready to use   savvy expert: knows lsst data very well and like to use api to their own programming    4. functions for all different levels of users  5 system design    system diagram   details of the different parts   firefly server   firefly client   firefly server extension   firefly javascript api   firefly python api   firefly python api, jupyter notebook, and other python applications   workspace and level3 data   sui web portal sketch, workflow   dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                ",1,train
DM-3653,SUIT design document outline,work with john rector on suit design document outline,1,train
DM-3654,Summarize current LSE-75 status as intro for new T&S personnel,"with the arrival of new telescope & site personnel, especially the telescope scientist, , prepare a summary of the current state of lse 75 and its open issues.",4,train
DM-3656,Data loader doesn't work for match tables,qservdataloader.py fails to load match tables:    it does not invoke the correct partitioner executable for them    not all css parameters required for match tables are passed down to the css update code,1,train
DM-3657,Create change request for LSE-75,"create a change request for lse75, the tcs  to   dm icd.",2,train
DM-3658,Discussions on LSE-75 with Telescope & Site personnel,"pursue interactions with telescope and site personnel regarding lse 75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines.    covers work through the end of august 2015.",3,train
DM-3659,Initial discussions with Patrick Ingraham,"this story is a catchall for preliminary conversations about lse140 with the new calibration instrumentation scientist, patrick ingraham.",1,train
DM-3660,"Review pending work, clean up related JIRA DM- and LIT- issues",nan,2,train
DM-3661,Configure VMs to provide additional slots for task switching,"we have to set up a new set of slots that will be used to execute the overlapping thread of execution for alert production.   because of limited resources at this time, this means reconfiguring the worker nodes to provide additional slots, and to split the worker nodes into two sets.    ",3,train
DM-3662,Add support for clang and OS X to qserv scons,nan,2,train
DM-3663,Basis for HSC integration test,"we need to assemble the basis for an integration test using hsc data, to protect hsc processing and obs_subaru from upstream changes.    this includes the assembly of the required data, a basic mechanism to test the mechanics of data release production, sufficient for the square team to take it and incorporate it into the jenkins system.  addition of any scientific validation is deferred for now.",4,train
DM-3664,rename parameter vector methods in afw.geom.ellipses," notes that the writeparameters and readparameters methods on the ellipse classes are confusingly named, especially when compared to similar methods on meas.modelfit.model.",2,train
DM-3665,improve test coverage of CModel failure modes,"the cmodel has a large number of failure modes, largely dealing with different kinds of problems in the inputs, and a correspondingly large number of flags.  it also has some fairly complex logic determining which flags can be set simultaneously.  all of these combinations need to be tested.    dm 1574 may be useful in capturing these conditions from runs on real data.",6,train
DM-3666,Revisit Footprint design,nan,4,train
DM-3667,PSFEX does not build if PLplot is installed,"during the configure phase psfex checks for the presence of plplot. if plplot is found then the build fails (at least on a mac using homebrew):    /bin/sh ../libtool  tag=cc   mode=link clang  g o2 i/users/timj/work/lsstsw/src/psfex/lapackfunctions/include   o psfex check.o context.o cplot.o diagnostic.o fft.o field.o fieldutils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sampleutils.o vignet.o wcsutils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcsc.a l/users/timj/work/lsstsw/stack/darwinx86/fftw/3.3.31g8fdba61da39a3ee5e/lib lfftw3f lm  l/users/timj/work/lsstsw/src/psfex/lapackfunctions/lib llapackstub lf2c lm lplplotd  libtool: link: clang g o2 i/users/timj/work/lsstsw/src/psfex/lapackfunctions/include o psfex check.o context.o cplot.o diagnostic.o fft.o field.o fieldutils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sampleutils.o vignet.o wcsutils.o xml.o  ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcsc.a l/users/timj/work/lsstsw/stack/darwinx86/fftw/3.3.31g8fdba61da39a3ee5e/lib /users/timj/work/lsstsw/stack/darwinx86/fftw/3.3.31g8fdba61+da39a3ee5e/lib/libfftw3f.dylib l/users/timj/work/lsstsw/src/psfex/lapackfunctions/lib llapackstub lf2c lm lplplotd  undefined symbols for architecture x8664:    ""plwid"", referenced from:        cplotdrawloccoordgrid in cplot.o        cplotfwhm in cplot.o        cplotellipticity in cplot.o        cplotmoffatresi in cplot.o        cplotasymresi in cplot.o        cplotcounts in cplot.o        cplotcountfrac in cplot.o        ...  ld: symbol(s) not found for architecture x8664      this particular error is caused by psfex using a deprecated plplot api (plwid) that is not enabled by default and whose name is not translated to cplwid. this plplot change occurred in version 5.9.10 released in 2013. i assume upstream psfex has a fix for this.    given that lsst does not need the plplot functionality i think the simplest fix may well be to disable the test for plplot in our version.    it seems likely that there will be a reasonable number of systems ""in the wild"" who will have plplot installed so i'm inclined to think that this should be a blocker for the v11.0 release.    if we are lucky people will have all upgraded their plplot installs to v5.11.0 because in that version plplot change the name of the library from libplplotd to libplplot and psfex has hardwired the former rather than using pkgconfig. this results in configure not finding plplot. i don't think this eventuality is likely though.  ",1,train
DM-3670,obs_test needs to override map_camera and std_camera,"the butler can't get a camera unless the mapcamera and stdcamera are defined correctly.  in most cases the camera can be built by the mapcamera method.  in the case of obstest, the camera is built in the constructor of the mapper, so std_camera should just return the camera attribute.",1,train
DM-3672,W16 Webserv Unit Tests,"implement unit testing across the webserv components. use either a sqlite backend, or mock objects and mock results to emulate database interaction.",10,train
DM-3673,Gather candidates for Verification Datasets and identify collaborators,"  identify candidate verification sets (in the first instance, datasets with extant processed data used in one off reductions with lsst_apps to allow a preliminary assessment of current quality from a science qa perspective).     identify people within the project with effort, expertise and interest available to contribute to this effort. ",10,train
DM-3674,Present Verification approach at AHM,  present talk at ahm to seed verification effort and seek feedback. ,10,train
DM-3675,Resourcing Verification runs,  identify required resources for verification runs and communicate them to ncsa.   ,2,train
DM-3677,HSC backport: Cleanup interpolation tasks and implement useFallbackValueAtEdge,this is a port of the changesets from:  https:/hscjira.astro.princeton.edu/jira/browse/hsc 756  ,4,train
DM-3678,HSC backport: Standalone updates to star object selection,"this involves pulling over the following standalone (i.e. nonticket) hsc commits:  https:/github.com/hypersuprimecam/measalgorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead    https:/github.com/hypersuprimecam/measalgorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91    https:/github.com/hypersuprimecam/measalgorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873  (note, the above reverts some unwanted cameraspecific clauses added in the first commit.  may just squash them to only add the desired features)    https:/github.com/hypersuprimecam/measalgorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4    https:/github.com/hypersuprimecam/measalgorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030    https:/github.com/hypersuprimecam/measalgorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0    https:/github.com/hypersuprimecam/measalgorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909    https:/github.com/hypersuprime cam/measalgorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2",1,train
DM-3679,Allow building/publishing components off branches other than master,"support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream nonmaster branch).  since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork  we end up merging/rebasing from an upstream branch, then forcepushing the downstream master.  upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward.  the processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive.    it is proposed that we extend our tools to allow publishing components from branches other than master.  this would allow us to have xrootd for example be a ""pure"" fork of upstream  we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that.    this functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)",2,train
DM-3681,Jira PMCS EVM integration,this epic captures management support requests and non sped activities in fy15 covered by wbs 02c0102 that were not included in other epics. ,100,train
DM-3682,LDM-240 Long range planning,nan,100,train
DM-3683,Jira Data Management long range planning project,nan,100,train
DM-3684,Release engineering Part Two,"this epic covers testing and co ordination work associated with making  engineering and official releases, and code to support them.      [fe at 70%, jh at 20%, js at 10%]",40,train
DM-3685,Evaluate GitLFS,"we want to try out gitlfs and evaluate its suitability as a solution to storing binary data in repos for ci. we want to know how it compares in usability terms to gitannex, gitfat etc. ",17,train
DM-3686,Fix PATH and compiler version detection in qserv scons,"in recently merged dm3662 compiler version testing was done using os tools with regular $path. this is inconsistent with other scons tools which reset path when executing actions.   we want to do two things:   propagate path to the command execution   use scons tools to run ""$cxx  version"" instead of os tools to keep things consistent",1,train
DM-3687,Revisit KPIs for Image Access,need to come up with kpis for image query access,4,train
DM-3688,lsst_dm_stack_demo failure,viz:      $ ./bin/demo.sh  [...]  $ bin/compare detectedsources.txt.expected detectedsources.txt  failed (max difference 0.439326 over tolerance 0.004000) in column basegaussianfluxflux.  failed (max difference 0.439326 over tolerance 0.004000) in column basegaussianfluxfluxsigma.  failed (max difference 0.244707 over tolerance 0.004000) in column basepsffluxflux.  failed (max difference 0.244707 over tolerance 0.004000) in column basepsffluxfluxsigma.      this is on os x with lsstapps w2015_36.    ,2,train
DM-3690,Forward community.lsst.org (Discourse) notifications to existing mailman lists,setup a system to forward new post notifications from http:/community.lsst.org categories to their appropriate legacy mailman email list counterparts.    mailmain forward listdmstaffannouncementsdmdevel|    once this is implemented i will send a notice that all communications and replies should occur on discourse (these mailing lists should be readonly).    i will also send a notice that dm users is deprecated.  ,7,train
DM-3691,"CalibrateTask has outdated, incorrect code for handling aperture corrections","the cfht specific calibratetask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. the error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard calibratetask. the required changes are small. i plan to test by running 's cfht demo.",1,train
DM-3692,HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting,this is a port of the changesets from https:/hscjira.astro.princeton.edu/jira/browse/hsc 966.    it provides the ability to reserve some fraction of psf candidates from the psf fitting in order to check for overfitting and do cross validation.,1,train
DM-3693,HSC backport: allow photometric and astrometric calibrations to be required,"this is a port of the standalone changesets:  https:/github.com/hypersuprimecam/pipetasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352  https:/github.com/hypersuprimecam/pipetasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6  (the above is a fixup, so will likely be squashed)  https:/github.com/hypersuprimecam/pipetasks/commit/cf5724b852937cfcef1b71b7a372552011fda670  https:/github.com/hypersuprimecam/pipetasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610  https:/github.com/hypersuprimecam/pipetasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506  https:/github.com/hypersuprimecam/pipetasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039  https:/github.com/hypersuprimecam/obssubaru/commit/52733a7ab1731a15cbb93151851f57cec276f928  and hsc tickets:  https:/hscjira.astro.princeton.edu/jira/browse/hsc1085 and  https:/hscjira.astro.princeton.edu/jira/browse/hsc 1086",2,train
DM-3694,Decrease buildbot  frequency,"buildbot frequency is now down to two builds, one at 19:42 machine time (ncsa) and one at 1:42. this is to stop people needing buildbot runs to eups publish to have to wait before a ci build, since they are now done on[ https:/ci.lsst.codes ]/ jenkins.     ",1,train
DM-3695,Add unit tests for secondary index,"qproc::testindexmap.cc is very sketchy and doesn't perform any test for now (i.e. no boost_check). it should be improved to really cover code related to secondary index. a mock secondary index is required here, i.e. qproc::fakebacken should be strengthened.",8,train
DM-3697,Install squid proxy on cc-in2p3 build node,"can we add this one to current sprint? it is required to access docker hub on in2p3 cluster.    i also need to automate/document it, test it on build nodes, and be reviewed by in2p3 sysadmins.    cheers",4,train
DM-3698,Replace --trace with --loglevel in pipe_base ArgumentParser,"replace the trace argument with an enhanced version of loglevel that supports named values and numeric log levels (which are the negative of trace levels). this simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pexlogging trace and debug with log.    this work was already done as part of dm 3532; it just needs to be copied with minor changes (since there are no named trace levels in pexlogging).",1,train
DM-3700,Literature search for DCR -- Sullivan,go through the literature to find relevant seminal papers on dcr.    the outcome will be a bibliography and executive summary.  this should be posted on discourse.,15,train
DM-3701,Setup and conduct a conversation about DCR in the project,advertise and conduct a broadly advertised videocon on dcr in the context of diffim.  the result of this should be minutes.  ideally we would come out of this meeting with a list of possible techniques for dealing with dcr (preferentially sorted by priority).,4,train
DM-3702,Assemble the report on DCR,"everything learned through the literature search and project wide meeting should be synthesized into a single readable report that details the expected effects of dcr on difference imaging as well as possible mitigation techniques.    this may involve some preliminary analysis work to measure effectiveness of various techniques.    note that i expect this to be two weeks of work for two people, thus the 40 story points.  i don't know how to assign a story to two people.",40,train
DM-3703,Port suspect pixel flags to meas_base,"pull hsc pixelflags for suspect and suspect center over from measalgorithms to measbase. additionally there are a few places in measbase (& possibly in algorithms as well) which set flags that have comments such as ""set suspect flag if it was available"". each of these places should be updated to use the ported bit. the relevant commit can be found at https:/github.com/hypersuprime cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 and is discussed at https:/dev.lsstcorp.org/trac/ticket/2838    it may also be necessary to add or update a unit test to make sure the flag is set appropriately.",4,train
DM-3704,Refactor ImageDifferenceTask: Split into two tasks,"this issue is to modernize the diffim task.  once it is running it should be refactored to use the new measurement tasks and reference loading wherever possible.  also remove any one off code used in past reports.    measurement should be split into its own task.    the resulting task should be general, but not so general as to make it hard to run.    this issue is being split into three issues:   dm3704: split monolithic task into:   imagedifferencetask  creates image difference   processdiffim  perform detection and measurement on image differences (and calexps)   dm5294 refactor/clean up new imagedifferencetask    dm 5295 refactor/clean up new processdiffimtask",10,train
DM-3706,fix EventLog references in ctrl_orca,"there are a couple references to eventlog in ctrl_orca, which is an object that no longer exists.",4,train
DM-3707,qserv scons - do not copy files to variant_dir,"some people are not happy with our current scons setup which copies source files from source directories to variantdir, it makes it harder to trace errors using tools like eclipse or debug code. would be nice to get rid of the extra copy, but we still want to have separate build directory (variantdir). it should be simple enough, i think, but will need some testing of course.",2,train
DM-3749,Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set,"the scons build system is unaware of extra flags which may be set in sconsflags environment variable, which are used from scons utils. this will cause the build to fail. the package needs to behave properly and build in the presence of these flags",2,train
DM-3750,Prototype DRP sequence using DECam data,"learn the lsst stack and prototype a sequence for data release production using decam/des data as inputs.    assignees: hsinfang chiang, robert gruendl  duration: september 2015  february 2016",100,train
DM-3751,Refine design specification and requirements analysis of Level 1 and Level 2 systems,"review of existing design documentation and gathering of requirements of all level 1 & 2 (and 3) systems. specify both a functional and physical breakdown of the systems for longterm planning and for building the production infrastructure needed at ncsa.    assignees: don petravick, jason alt, paul wefel, steve pietrowicz, margaret gelman  duration: september  december 2015",75,train
DM-3753,Margaret's mgmt. activities in August,local coordination meetings  dmlt meetings  lsst2015 workshop and conference  hiring/interviewing  budget review and fy16 prep  invoice re breakouts  july tpr  etc.,28,train
DM-3755,Revisit shared scans design,nan,10,train
DM-3756,Implement feature sets requested by SUI and DRP processing in Process Execution Framework,extend the process execution framework with feature sets requested by sui and drp processing.    assignees: matias carrasco kind,38,train
DM-3757,Operations planning for Archive and US DAC in TOWG/TPWG,"develop use cases and plan for operations in the technical operations working group and technical proposal working group.    assignees: don petravick, margaret gelman, chris pond, robert gruendl  duration: september 2015   january 2015",58,train
DM-3758,Expose table metadata via metaserv,"sui team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. they'd also find it useful if we could send column description.",6,train
DM-3759,Analyze Qserv KPIs,"improve script for analyzing kpis, measure and document kpis.",24,train
DM-3760,Support operation of development infrastructure (lsst-dev and other),"administrative support to operate the lsst development cluster and lsst's use of the ncsa openstack nebula. includes planned maintenance activities and loe/emergent work. expect ~ 1fte day per week.    assignees: bill glick, matt elliot, bruce mather, paul wefel  duration: september 2015   february 2016",49,train
DM-3761,Sizing model storage costing update,"the question that is being raised is how much it would cost to keep more than the last two data releases on readilyaccessible storage (i.e. spinning disk). this will require changing several numbers and formulas in ldm141, the storage sizing model, observing the results as they propagate through ldm 144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked.    assignees: jason alt  duration: september 2015",6,train
DM-3762,Refine file system policies and services,"refine data management policies and services (e.g., data retention, backup policies). ideally we would have a draft of this by the november dmlt meeting.    assignees: jason alt, james parsons  duration: october   november 2015",13,train
DM-3764,Czar dies when parser throws exception,"running a query that mistakenly uses scisqls2ptinbox instead of qservs2ptinbox    select objectid, coordra, coorddec   from smmbremerton.deepcoaddforcedsrc   where scisqls2ptinbox(coordra, coorddec, 320.05, 0.457, 320.06, 0.46)    kills czar, the error message in czar log file is:    error ccontrol.userqueryfactory (build/ccontrol/userqueryfactory.cc:117)     invalid query: parseexception:parse error(antlr):unexpected token:  scisql_s2ptinbox:    we need to change the code so that a random query does not kill czar. this story involves fixing czar so that it does not die when parser chokes on the syntax.  ",6,train
DM-3765,"For registry-free butler, look up information in related data type.","butler reading information (in particular the observation time and length) out of an input dataset's file representation in order to provide rendezvous with calibration data in another repository (that does have a registry). today, that read is handled by geninputregistry.py so that the butler doesn't need to look into the dataset itself. if there's no registry, such a read will be necessary.    the ultimate test is to run processccd.py (a cmdlinetask that runs instrument signature removal) on a repository that contains raw frames that require more than one set of calibrations and have it pick up the correct ones.  that would have to be condensed down into a unit test.",12,train
DM-3766,Add Butler access to calibration data in obs_decam,"goals:   have something to ingest calibration data and create a calibration registry (calibregistry.sqlite3)   add mapping class calibration into decammapper and make butler able to get bias/flat/fringe.   include calib in testdatadecam and add to the unit test retrieving calib data.    summary:   a new task ingestcalibs.py is added to pipetasks for parsing through calibration data and creating a calibration registry.  the decam customized configuration is added in decamcalibsparsetask in obsdecam.   pipetasks/ingest.py is modified slightly for more general use while its original behaviors are kept by default.    the calibration data are decam communitypipeline products, including nightlymastercal bias/flat downloadable from noao science archive, and fringe of the latest version 56876 from  http:/ or also on /lsst8/decam/cal/decammastercal56876/  note that files from the two sources have different formats (mef one hdu for each detector, or one singlehdu fits for each detector).   new butler dataset types bias/flat/fringe are added along with functions to standardize them to exposures in decammapper.   tests of retrieving bias/flat/fringe by butler are added.     testdatadecam used by the unit tests is at lsstdev:/lsst8/testdata_decam/    known caveats for future users   the task could be a bit noisy when ingesting decam calibration data products.  but given the variety of data products on hand i might rather have those reminders than letting all pass silently.       i wouldn't be too surprised if future decam community pipeline products appear in different formats. depending on how different they become, decamcalibsparsetask might or might not need future modifications.   ",21,train
DM-3768,Resolve the issues found in the S15 end-to-end system exercise,there are a few items we need to take care to finish the endtoend system for s15. ,8,train
DM-3769,access the database created and populated for Bremerton end-to-end system,collect the information for the tables populated for bremerton endtoend exercise. use them in sui/t so we can access them using the dax api. ,2,train
DM-3770,build the SUI system on NCSA to use the right database and tables,"due to the changes of the database and tables, the system has to be rebuilt.",1,train
DM-3771,Resolve the issues accessing the newly populated tables,there are several issues need to be resolved for the system to work properly. ,5,train
DM-3772,Fix compiler detection for non-default gcc/g++ compiler,scons cxx=g\4.4 launches g4.4 version which returns g4.4 (debian 4.4.72) 4.4.7. nevertheless the  4.4 is not supported by qserv compiler detection tool. support will be added here,1,train
DM-3773,add RUNID option to EventAppender,a runid needs to be added as an option to eventappender to allow event logging selectors to receive only events for a particular run.,3,train
DM-3774,lsst_build's default ref from repos.yaml support is broken when building multiple packages,"a problem with the default ref in repos.yaml support implemented in dm 3679 was discovered last friday, shortly after deploying this feature to the production ci systems.    the default ref for xrootd was changed/overridden in repos.yaml to legacy/master.  this worked as expected (and as was tested) when setting xrootd as the sole lsstswbuild.sh product or when running rebuild by hand.  however, when building any package that pulled in xrootd as a recursive dependency, the master branch was being used (this case had not been manually tested).",1,train
DM-3775,HSC backport: updates to tract and patch finding,this is a port of the following hsc updates to how tracts and patches are found and listed given a set of coordinates.  these are all standalone commits (i.e. not associated with a ticket):  https:/github.com/hypersuprimecam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7  https:/github.com/hypersuprimecam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b  https:/github.com/hypersuprimecam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1  https:/github.com/hypersuprimecam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22  https:/github.com/hypersuprimecam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe  https:/github.com/hypersuprime cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3,1,train
DM-3776,LDM-144 Consistency Update,"due to the 2 year gap between the original authoring date of ldm 144 and the recent update,  the 'costing only' update caused the document to be less self consistent than desired. this work is to do more than 'costing' updates such that the document is useful and on target to be more than a costing umbrella.",3,train
DM-3777,server side preparation for  histogram plot (2),"1. on the initialization, server needs to return the following summary table for all numeric columns:       a. column name / description / unit      b. min and max values      c. number of points    2. for a given column and binning options, return the table of bins:     a. first column  numinbin  number of points in a bin     b. second column  binmin  bin's lower bound     c. third column  binmax  bin's upper bound     [101315] added by lz  here is the detailed requirement from tatiana:  the implementation of this ticket are two search processors (similar to ipactablefromsource search processor). both search processors should be located in edu.caltech.ipac.firefly.server.query package and produce an ipac table.     1.  getting table statistics    input:  the input to the first search processor should be tableserverrequest (treq) with ""searchrequest"" parameter set. the searchrequest parameter will be a serialized json string, which can represent a search request to any other search processor. this  parameter determines how the input ipac table is  obtained. (the input ipac table is produced similarly to how ipactablefromsource produces the result with ""processor"" parameter set.)    to parse searchrequest parameter into keyvalue pairs  the parameters for the search request  you can use json.simple library:    jsonobject searchrequestjson = (jsonobject)jsonvalue.parse(treq.get(""searchrequest""));  if (searchrequestjson != null)      }  one of the parameters should be the ""id"" (serverreuqest.idkey)  which tells which search processor to use to obtain input ipac table.    output:  the output ipac table produced by the first search processor should contain 5 columns: columnname, description, unit (unit or empty string), min, max, numpoints (the number of nonnull values).    2.  getting histogram data    input:  the input to the second processor should be tableserverrequest (treq) with ""searchrequest"" parameter set (exactly the same as ""searchrequest"" parameter in item 1) plus binningoptions. the binningoptions parameter will be serialized json object with the following keys (object properties):      notes:   the algorithm is simple fixed size binning for now, in future we'll need to add variable size bins algorithm described here: http:/   at the beginning, you can assume that column is column name, but it can be an expression based on several columns.    scale can be ""linear"" or ""log""    min and max define filter on the points included into histogram calculation, any of them or both can be absent. if both are absent, no filter needs to be applied.    output:  the output ipac table produced by the second search processor should contain 3 columns:  numinbin, binmin, binmax in that order. the table should be sorted by binmin. the binmax of row i should be always less or equal to binmin of row i+1. if number of points in the bin is 0, it's ok to skip this bin.    ",12,train
DM-3778,Fix compiler warns in protobuf clients,google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.,1,train
DM-3779,clean up gcc and eclipse code analyzer warns,"we've been ignoring some accumulating warns in the qserv build for some time now.  now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up.",1,train
DM-3780,Rationalize lsst/xrootd repo and maintenance procedures,"the procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and errorprone.    buildbot now has support for releasing packages from branches other than master.  given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsstspecific work forward on a dev branch.  this will make it much easier to track and contribute to the xrootd project moving forward.    existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken.",1,train
DM-3782,"Review, plan, procure development infrastructure (FY15)",nan,3,train
DM-3783,Refining file system policies,nan,2,train
DM-3784,W16 Operation of Joint Coordination Council,"activities associated with implementation of the moa establishing ccin2p3 as a satellite processing center during operations. includes preparation and execution of monthly jcc meetings, as well as a facetoface meeting for intense coordination scheduled for early november.    assignees: don petravick, margaret gelman, jason alt, robert gruendl  duration: september 2015  february 2016",56,train
DM-3785,W16 processing control emergent work,"bucket epic for bug fixes and minor work that falls outside planned epics.    assignees: steve pietrowicz, greg daues, matias carrasco kind, hsinfang chiang, james parsons  duration: september 2015  february 2016",30,train
DM-3786,Update sizing model for February 2016 refresh - technology and costing,"biannual refresh of sizing model (ldm144 et al.), including updates to both costing and technology. as it will be the first technology refresh in at least 2 years, we anticipate it will take a considerable amount of time. to prepare and gain insight on technology and costing trends, this activity includes attending sc2015 in midnov.    assignees: jason alt  duration: november 2015   january 2015",38,train
DM-3787,Liaise all groups to commission OpenStack for LSST,greg has been appointed the service manager of the ncsa nebula for lsst. work with lsst developers and ncsa system engineers to commission the openstack for lsst's use.    assignees: greg daues  duration: september 2015   february 2016,47,train
DM-3788,FY16 Hardware Purchasing Plan,"the annual acquisition strategy document describes the capabilities (hardware, compute cycles, software, licenses, etc.) planned for procurement during the fiscal year. we consider systems that will satisfy the needs of developers, systems for prototyping the production infrastructure, etc.    assignees: jason alt, bill glick  duration: september   october 2015",11,train
DM-3791,Evaluate PASTRY DHT implementation,the david keller kademlia implementation used in the earlier prototype has some bugs/limitations.  try to find a better offtheshelf dht and integrate with prototype framework.,6,train
DM-3792,obs_test data mis-assembled,obs_test images are mis assembled and need to be regenerated. this may affect some existing unit tests that rely on the data.,2,train
DM-3794,(FY16) Initial discussions and requirement consolidation,nan,2,train
DM-3796,remove install_name_tool fix to libpython2.7.dylib from anaconda package,"now that sim1314 has been merged, we should be able to remove the       if [[ $(uname s) = darwin ]]; then    #run installnametool on all of the libpythonx.x.dylib dynamic    #libraries in anaconda    for entry in $prefix/lib/libpython.dylib    do     installnametool  id $entry $entry    done   fi      from eupspkg.cfg.sh in the eups anaconda package, and still have galsim build correctly",1,train
DM-3797,Enable SSL to community.lsst.org,enable ssl (https) for the discourse site at community.lsst.org,1,train
DM-3798,Update flag names and config override files to current conventions,the deblend.masked and deblend.blendedness flag names in meas_deblender need to be updated to use underscores instead of periods.  various flag names in the examples scripts also need updating to the underscore and camelcase format.    a search for these flags throughout the database revealed a number of config files that need updating to current conventions.  these are also included here.,1,train
DM-3800,testProcessCcd.py computes values that are too different between MacOS and linux,"tests/testprocessccd.py runs processccd on visit 1 of obstest's data repository. the result on macos is surprisingly different than on linux in at least one case: psfshape.getixx() computes 2.71 on macos x and 2.65 on linux. iyy and ixy are likely different. it's worth checking all other computed values, as well. these differences likely indicate that something is wrong, e.g. in obstest, processccd, or the way the test runs processccd.    this showed up as part of fixing dm3792, but it is not clear if the changes on dm3792 actually caused or increased the difference between macos and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test.",2,train
DM-3801,The gains in obs_test's amplifier table appear to be incorrect,"as of dm3792 the gains in obstest's camera's amplifier table were set to the values reported in the headers of the lsstsim raw data used to generate obstest's raw data. (before that one nominal gain was used for all amplifiers).    however,  reports that these gains are incorrect. he measured the following gains by scaling the nominal gains by the median values in the biassubtracted data:    amp   meas      curr  name  gain      gain  00    1.7741    1.7741  01    1.8998    1.65881  10    1.8130    1.74151  11    1.8903    1.67073      we could simply adopt these values, but i would like to understand why the gains are so far off from those reported by phosim in the raw data.",4,train
DM-3802,"The obs_test's sensor is shown 90 degrees rotated from that desired, in camera coords","when plotting the obs_test sensor, e.g. using lsst.afw.camerageom.utils.plotfocalplane, the image is a short, wide rectangle. this suggests that the camera coordinate frame is rotated 90 degrees from the ccd coordinate frame (which has 1018 pixels in x and 2000 pixels in y).    we would prefer that the camera frame and ccd frame have the same orientation.",1,train
DM-3803,Fix Qserv compiler warnings with clang,"qserv triggers numerous warnings with clang on os x. full details are in the attached ticket, here we summarize the distinct warnings classes:    protobuf      /users/timj/work/lsstsw/stack/darwinx86/protobuf/2.6.1fbf04ba888/include/google/protobuf/unknownfieldset.h:214:13: warning: anonymous types declared in an anonymous union        are an extension [wnestedanontypes]      mutable union     qserv      in file included from core/modules/sql/statement.cc:32:  core/modules/sql/schema.h:74:1: warning: 'schema' defined as a struct here but previously declared as a class [wmismatchedtags]  struct schema       core/modules/proto/workerresponse.h:34:1: warning: 'workerresponse' defined as a struct here but previously declared as a class [wmismatchedtags]  struct workerresponse       in file included from core/modules/qana/querymapping.cc:46:  core/modules/qproc/chunkspec.h:51:1: warning: 'chunkspec' defined as a struct here but previously declared as a class [wmismatchedtags]  struct chunkspec       core/modules/qana/tableinfo.h:186:1: warning: 'dirtableinfo' defined as a struct here but previously declared as a class [wmismatchedtags]  struct dirtableinfo : tableinfo       core/modules/wbase/base.h:72:1: warning: 'scriptmeta' defined as a struct here but previously declared as a class [wmismatchedtags]  struct scriptmeta       in file included from core/modules/parser/booltermfactory.cc:46:  core/modules/query/predicate.h:86:27: warning: 'lsst::qserv::query::genericpredicate::putstream' hides overloaded virtual function [woverloadedvirtual]      virtual std::ostream& putstream(std::ostream& os) = 0;                              core/modules/query/predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::predicate::putstream' declared here: different qualifiers (const vs none)      virtual std::ostream& putstream(std::ostream& os) const = 0;                                    core/modules/parser/fromfactory.cc:62:15: warning: unused function 'walktosiblingbefore' [wunusedfunction]  inline refast walktosiblingbefore(refast node, int typeid)       in file included from core/modules/wsched/chunkdisk.cc:25:  core/modules/wsched/chunkdisk.h:130:10: warning: private field 'completed' is not used [wunusedprivatefield]      bool completed;                   in file included from core/modules/parser/predicatefactory.cc:45:  core/modules/query/predicate.h:86:27: warning: 'lsst::qserv::query::genericpredicate::putstream' hides overloaded virtual function [woverloadedvirtual]      virtual std::ostream& putstream(std::ostream& os) = 0;                              core/modules/query/predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::predicate::putstream' declared here: different qualifiers (const vs none)      virtual std::ostream& putstream(std::ostream& os) const = 0;                                    core/modules/parser/wherefactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c' [wdanglingfield]      printexcept(check c) : c(c) {}                                  core/modules/parser/wherefactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::printexcept/::printexcept' requested        here      printexcept/ p(mc);                               core/modules/parser/wherefactory.cc:269:12: note: reference member declared here      check& c;                     core/modules/rproc/protorowbuffer.cc:44:11: warning: unused variable 'largerowthreshold' [wunusedconstvariable]  int const largerowthreshold = 5001024;                    core/modules/util/testiterableformatter.cc:85:43: warning: suggest braces around initialization of subobject [wmissingbraces]      std::array/ iterable ;                                                                                                in file included from core/modules/qdisp/xrdssimocks.cc:37:  core/modules/qdisp/xrdssimocks.h:64:16: warning: private field 'executive' is not used [wunusedprivatefield]      executive executive;                         core/modules/xrdoss/qservoss.cc:77:1: warning: unused function 'print' [wunusedfunction]  print(std::ostream& os, lsst::qserv::xrdoss::qservoss::stringset const& h)     os x      core/modules/qdisp/queryrequest.h:54:25: warning: 'lsst::qserv::qdisp::badresponseerror::what' hides overloaded virtual function [woverloadedvirtual]      virtual char const what() throw()       core/modules/proto/taskmsgdigest.cc:55:5: warning: 'md5' is deprecated: first deprecated in os x 10.7 [wdeprecateddeclarations]      md5(reinterpretcast/(str.data()),        /usr/include/openssl/md5.h:116:16: note: 'md5' has been explicitly marked deprecated here  unsigned char md5(const unsigned char d, sizet n, unsigned char md) deprecatedinmacosxversion107andlater;                         core/modules/util/stringhash.cc:78:24: warning: 'sha1' is deprecated: first deprecated in os x 10.7 [wdeprecateddeclarations]      return wraphashhex/(buffer, buffersize);                           /usr/include/openssl/sha.h:124:16: note: 'sha1' has been explicitly marked deprecated here  unsigned char sha1(const unsigned char d, sizet n, unsigned char md) deprecatedinmacosxversion107andlater;                   core/modules/util/stringhash.cc:83:24: warning: 'sha256' is deprecated: first deprecated in os x 10.7 [wdeprecateddeclarations]      return wraphashhex/(buffer, buffersize);                           /usr/include/openssl/sha.h:150:16: note: 'sha256' has been explicitly marked deprecated here  unsigned char sha256(const unsigned char d, sizet n,unsigned char md) deprecatedinmacosxversion107andlater;                       xrootd      in file included from core/modules/qdisp/executive.cc:64:  in file included from core/modules/qdisp/xrdssimocks.h:33:  in file included from /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssirequest.hh:37:  /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssirespinfo.hh:43:1: warning: 'xrdssirespinfo' defined as a        struct here but previously declared as a class [wmismatchedtags]  struct  xrdssirespinfo    /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssisession.hh:45:1: note: did you mean struct here?  class xrdssirespinfo;    struct        core/modules/xrdoss/qservoss.h:64:17: warning: 'lsst::qserv::xrdoss::fakeossdf::opendir' hides overloaded virtual function [woverloadedvirtual]      virtual int opendir(const char )                     /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdoss/xrdoss.hh:63:17: note: hidden overloaded virtual function        'xrdossdf::opendir' declared here: different number of parameters (2 vs 1)  virtual int     opendir(const char , xrdoucenv &)                                     in file included from core/modules/xrdsvc/ssisession.h:32:  /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssiresponder.hh:177:27: warning: control may reach end of        nonvoid function [wreturntype]                            }                                  boost      /users/timj/work/lsstsw/stack/darwinx86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regexrawbuffer.hpp:132:7: warning: 'register' storage class specifier is        deprecated [wdeprecatedregister]        register pointer result = end;        ^  ",1,train
DM-3804,Fix order of arguments change in meas_base SingleFrameMeasurement,"in sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from measalgorithms to measbase is complete. this work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. see https:/github.com/lsst/meas_base/blob/tickets/dm 2915/python/lsst/meas/base/sfm.py#l271",2,train
DM-3806,convert newinstall.sh to use miniconda instead of anaconda,to match the conversion of lsstsw from anaconda  > miniconda to reduce the disk footprint and improve install times.,1,train
DM-3808,Setup lsst_sphinx_kit package structure,"setup the lsstsphinxkit package, including     setup.py   unit tests, tox and travis ci   readme stub   sphinx stub and readthedocs",1,train
DM-3811,HSC backport: Include documentation strings for config parameters when they are dumped,this is a port of the following hsc tickets:  https:/hscjira.astro.princeton.edu/jira/browse/hsc1072  and  https:/hscjira.astro.princeton.edu/jira/browse/hsc1175,4,train
DM-3814,Migrate LDM-152 to reST Design Doc Platform,nan,1,train
DM-3815,Intermittent build failures on v11 candidate with eups distrib,"we are seeing frequent intermittent failures on a variety of platforms when installing the v11 candidate with eups distrib install. repeating the command works. it doesn't seem to be evenly distributed between packages: cfitsio, measastrom(?), measalgorithms, and obslsstsim have been seen multiple times. it's been seen in the release verification ci (that uses the documented user facing process of eups distrib install instead of the factory ci which uses lsstsw) and in individual user ""manual"" builds on lsstdev.     test build key:   eups tag  comment b1688  tickets/dm3829 v110rc2    t20150914b1689  b1690  tickets/dm3829 v110rc2 master    t20150915b1692  ",6,train
DM-3816,levels in DecamMapper.paf is not quite right,"when ccdnum is not given as part of the dataid, instead of iterating over it, an error like this happens        runtimeerror: no unique lookup for ['ccdnum'] from : 61 matches      likely a problem in policy/decammapper.paf",1,train
DM-3817,Deploy docker images on ccqserv124/149,"creation of configured master and worker image will be improved here, and a deploymen tool (like swarm, of hand made) will be used to deploy images over in2p3 cluster.",6,train
DM-3818,Bi-weekly PO security meeting,bi weekly meeting with po on cyber security.,1,train
DM-3819,IdM work,work for lsst identity management and authentication.,2,train
DM-3820,NIST SP 800.82 investigation,nist sp 800.82 investigation for a more cohesive scada enclave security plan.,2,train
DM-3821,Recent CModel bugfixes from HSC,"i've just fixed two rather critical bugs in the cmodel code on the hsc side (they would have been introduced on the lsst side in the last transfer, dm2977):    the mininitialradius configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications.  this is hsc1306.    there was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude.  this is hsc 1313.    on the lsst side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the hsc issues was in debugging).",1,train
DM-3822,Orchestration work to support verification dataset processing,"upgrades, modifications, fixes, etc. to orchestration framework for use in square's verification dataset processing tests.    assignees: steve pietrowicz, greg daues  duration: september 2015   february 2016",27,train
DM-3823,W16 General Management Activities,nan,100,train
DM-3824,meas_astrom bugs exposed by new Eigen,"trying a newer eigen has exposed several issues in meas_astrom:   tests/createwcswithsip.py blindly uses sipwcs in the result returned by anetbasicastrometrytask.determinewcs2, but that attribute may be none   anetbasicastrometrytask.determinewcs2 terminates iteration early if the # of matches goes down, even though the result may be improved. in the case in question the first fit iteration results in significantly better rms error, but has one fewer matches, so the sip fit is rejected, triggering the first bug mentioned.",6,train
DM-3825,"write meeting agenda, for Sept 14 meeting ",nan,1,train
DM-3826,Further refinement ,added the image and engineering facility database.  and observatory operations server.,10,train
DM-3827,"email discussion w.r.t Service separation for L1, and also some work on ITIL roles","email to german about the all the l1 stuff and clarified that the l1 system were a  derive provided to the telescopes site (important for fitting this into the proper place in the who is worrying about what hierarchy.  also, revised sa to see fi the epo changes discusses affected the it roles in th model (not apparently)  lastly attitude the towc ",2,train
DM-3828,Management for week ending sept 11,"deal with hiring james parsons, and interfacing with the new ncsa organizations that will support lsst at ncsa, general group management issues",2,train
DM-3830,Deploy FY16 Storage Expansion,"install, and deploy a prototype production gpfs cluster.",20,train
DM-3831,Preparation work to process raw DECam data,try to run processccd.py with raw decam data and see what are yet to be solved for it to run. ,7,train
DM-3832,Deploy FY16 Nebula Expansion,"  install, test, and deploy additional capacity for the ncsa nebula.    (note: this work occurs outside of lsst so we can only track it at a less granular detail than other deployments. we can include the final story for blockers however.)",18,train
DM-3833,Decommission old development infrastructure,"decommission old hardware currently in the lsst development cluster when replacement equipment arrives and is provisioned.    assignees: bill glick, matt elliot, bruce mather  duration: november   december 2015",19,train
DM-3834,Migrate LDM-230 to new docs platform,convert ldm 230 from word to restructuredtext and deploy on readthedocs.org,2,train
DM-3835,Migrate LDM-135 to new design docs platform,convert ldm 135 from word to restructuredtext and deploy on readthedocs.org,5,train
DM-3836,Migrate LDM-129 to new design docs platform,convert ldm 129 from word to restructuredtext and deploy onto readthedocs.org,2,train
DM-3837,SuperTask Redesign,redesign pipe_base to allow the creation of supertasks which will be more flexible for  different execution applications,11,train
DM-3838,Evaluate authentication and authorization services for user workspace,"develop an identity and access management (iam) program.    deliverables (from sow):   lsst iam design document: describe lsst's current and expected iam needs and specify technical recommendations for the lsst iam system architecture, including interface standards (e.g., ldap, oauth) and system components (with functional descriptions and implementation recommendations).   lsst iam program plan: specify future and ongoing iam development and operational activities required to meet the lsst project mission.   lsst iam technical demonstration: provide an initial implementation of the lsst iam design according to the philosophy of ""rough consensus and running code."" the project will prioritize technical implementation work based on the immediate needs identified by lsst developers for functional implementations of iam system interfaces to ""fill the gaps"" needed for lsst development and integration work to proceed on schedule.    assignees: jim basney, terry fleurry, daniel thayer, alex withers (iso), don petravick, jason alt, margaret gelman  duration: october 2015  march 2016",88,train
DM-3839,Graphical representation Example demo,nan,2,train
DM-3840,LSE-72: Phase 3 in X16,advance phase 3 details as needed to eliminate obstacles to ocs and dm development during f16.,8,train
DM-3841,LSE-75: Refine WCS and PSF requirements in W16,clarify the data format and precision requirements of the tcs (or other telescope and site components) on the reporting of wcs and psf information by dm on a perimage basis.    depends on the ability of the t&s group to engage with this subject.    current pmcs deadline for phase 3 readiness of lse75 is 29sep2015.,8,train
DM-3842,LSE-68: ICD Details in X16,bring icd to phase 3 level of detail,6,train
DM-3843,LSE-74: ICD Details in W16,"""bring icd to phase 3 level of detail"" was the original specification, but actual work by the ocs group in the winter 2016 period didn't quite reach phase 3.  nonetheless, a very useful revision was submitted to and recommended for approval by the ccb at the march meeting.",6,train
DM-3844,Add tutorial-level documentation for ctrl_pool,"the new ctrl_pool package (port of hscpipebase) has no tutorial level documentation, making it hard to figure out how to start using the package.    unfortunately, i think only  is qualified to write it directly, though it may make sense to have someone unfamiliar write it while bugging paul a lot, both to transfer the knowledge and target the documentation better.",4,train
DM-3845,Add unit tests for ctrl_pool,"ctrl_pool (formerly hscpipebase) is being ported with no unit tests  the only testing is an example script that can be run by hand to demonstrate a piece of the functionality.    some functionality may simple not be amenable to tests (such as batch submission).  other parts may be tricky to run via scons because they're intrinsically parallel, and scons naively wants to be able to run each test in a separate process.  overcoming those problems is the reason this is challenging  there isn't really that much functionality to test.",8,train
DM-3846,Read over LSE-70 and LSE-209 and discuss for meeting,read over lse70 and lse209 for meeting on friday 9/11.,2,train
DM-3847,Grid overly bug,"when using the grid overlay in galatic coordinate over an image that is around longitude = 0, the overlay doesn't work properly. in bolocam, on fits image works but not the other one.    reproducible:   steps to reproduce:   go to atlas search, and select bolocam galactic plane survey (gps).  then enter single coordinate search on ""0 0 gal"" to find images taken around that.     see column ""fits filename"" and search for the sharcii instrument images such as  images/sharc2/l000.150.00.fits [bad]   and   images/sharc2/l000.000.00.fits [good]     then open it on irsaviewer by clicking on the icon link (first column).  on the image viewer, enable the grid overlay and click on the icon 'layer' on the toolbar.  change the coordinate system to galactic to see the problem.     on the overlay for the associated bolocam map there are three horizontal lines which exist only on the left hand side of the image. when the image coordinate system is set to ""gal"", the reported glon values range from 355.4 to 1.6 (from left to right), passing through 0.0 in the center.     same problem can be seen for   images/v1/innergalaxy/map/v1.0.2supergc13pcamap50crop.fits [bad]  ",4,train
DM-3849,evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 2,nan,1,train
DM-3850,Nebula metadata service is intermittent,"upon restarting one of my nebula instances (ktltest), i noticed a failure in the logs:    sep 14 18:07:29 ktltest cloudinit: 20150914 18:07:29,157  util.py[warning]: failed fetching metadata from url http:/169.254.169.254/latest/metadata      attempting to retrieve that url seems to randomly vary between succeeding, which returns:    amiid  amilaunchindex  amimanifestpath  [...]      and failing, which returns:    /   /    /500 internal server error/   /   /    /500 internal server error/    remote metadata server experienced an internal server error./         /  /    these failures may be contributing to observed sporadic ssh key injection failures.",2,train
DM-3851,Liaise with Long-Haul Network group on Base Site to NCSA network,coordinate with network group to establish base sitetoncsa lhn.    assignees: paul wefel  duration: september 2015   february 2016,4,train
DM-3852,Evaluate file management technologies,"evaluate technologies for data management (e.g., irods).",100,train
DM-3853,Consult on contracting and execution of Chilean Data Center contract,continue to consult and review design documents for the construction of the chilean dac.    assignees: tom durbin  duration: september 2015   february 2016,11,train
DM-3855,Calibration products preparation,develop the calibration products pipeline plan and begin initial implementation.,22,train
DM-3857,Stack documentation Part I,original documentation based on expressed team needs.     [js 100%]    ,30,train
DM-3858, Integration and test monitoring architecture Part II,work to be assessed on the basis of the outcome of part i   [dm 2050]    [100% jmp]        ,70,train
DM-3859,Supertask design and prototyping - Part I,"this epic covers design and prototyping work for an encapsulation of  tasks that allows for the chaining and interleaving of tasks with  qa/metrics tasks, intergration tests and/or kpm/analysis  afterburners. the emphasis at this point is on speed of development  and customisation; performance is assumed not to be an issue as it  will eventually be guaranteed by the process execution framework.     only square's effort is covered by this epic; additional effort on  this investigation is provided by the middleware wbs and not captured  here.     the outcome of this epic is a proposal for moving forward. in the  event that it is acceptable to the project engineer, further epics  might be define, hence part i.    [100% gpdf] ",42,train
DM-3860,Communication Toolchain support,"this epic covers support of communication tools primarily used by dm  and/or supported by dm on behalf of other parts of the project  jira,  discourse, hipchat, etc     the source of this work is primarily driven by shortterm user  requests, and so the outcome is timeboxed rather than planned.     [js 50% fe 50%]  ",20,train
DM-3861,QA Architecture Documents,design document outlining squash elements and implementation plan.    [fe 100%],10,train
DM-3862,"Stack Build, Packaging and Testing Improvements Part II",this epic is an umbrella for rfc 69 work.     [jh 90% js 10%]  ,60,train
DM-3863,Web design fixes DM Design Documents on Sphinx/Read The Docs,"solve fitandfinish issues with the stock readthedocs.org sphinx template when rendering dm design documents. issues include:     sections need to be numbered and those numbers need to appear in toc   rtd's toc does not properly collapse sub topics   appropriate styling for document title and author list   wrapping the changelog table   adapt section references so that just the section number can be referenced, independently of the section number and title in combination   section labels given explicitly in the rest markup are different from the anchors that sphinx gives to the /tags; the former are simply divs inserted in the html.    the solutions may involve    # reconfiguring the sphinx installation of individual documents  # forking the rtd html template, and/or  # developing extensions for sphinx in sphinxkit.",2,train
DM-3864, Integration Dataset for metrics and regression tests - Part I ,  this epic covers the defintion of a compact but rich dataset to  support regression testing and surrogate metric development for  regular automatic integration tests. it also involes a definition of  interim/surrogate metrics where they can aid testing and development  and/or where kmps cannot be calculated.    [dn 75% mwv 25%],50,train
DM-3865, Processing of DECAM and other Verification Datasets,"this epic covers work to lead and co ordinate the processing of  precursor datasets, decam in the first instance, through the stack and  produce an assessmment of progress and preliminary metrics. in w16  execution will be done using the orca/htcondor setup previously used  at ncsa for data challenges.    [dn 50% af 25% js 25%]      ",100,train
DM-3866,Do basic tests of CModel ellipticity measurements,"i have seen enough anomalies that i have had to go back a bit and do some basic tests of cmodel and its ellipticity bias.  this involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles.  these are zeroshear, zeroseeing tests which i probably should have run first thing.    it i understand everything i see in these tests, i will be confident that the results i am seeing for cmodel with varying footprint size, varying ninitialradii, and varying stamp size are correct.",4,train
DM-3869,Simultaneous astrometry requirements,the hsc group needs improvements to the simultaneous astrometry fitter delivered by astier et al.  in particular we'll need to do simultaneous photometry as well.  this epic is to determine the superset requirements for such a system.,9,train
DM-3870,Refactor Jointcal to use stack functionality,"jointcal currently has a lot of built in features that already exist elsewhere in the stack (e.g. gtransfo, starselector, points, etc.). these features should be removed and replaced with the equivalent stack functionality.",36,train
DM-3871,Implement simultaneous photometry,the simultaneous astrometry framework should be able to be extended to also fit the photometry at the same time.  this task is to do just that.  the task to create a pluggable framework should help with this.,56,train
DM-3872,Clean up Wcs classes,the wcs classes as they currently exist are not easy to extend and also contains overrides that are a bit ad hoc.  this task is to clean up the existing classes so that there is a single abstract base class.  it should also be a priority of this task to determine whether an upgrade in wcslib helps with the special cases (e.g. tan sip).,38,train
DM-3873,Gather requirements for improved Wcs classes,the new wcs class should be able to apply transforms in a stack so that many different distortions as different scales can be corrected for separately (rather than trying to correct the whole mess with a single 2d polynomial).  there is some work going on in the community around this.  this task should include conversations with community leaders in this area.,19,train
DM-3874,Produce a design for the new Wcs classes,this will require generating a design as well as getting it reviewed via rfc.  the hope is that other work can be ongoing while the rfc process is carried out.,50,train
DM-3875,Implement new Wcs classes,"once a design is accepted implement the new design. this epic covers the replacement of afw:image:wcs with afw:transform/afw:mapping (names currently in flux), but does not involve changes to xytransform.",39,train
DM-3876,Make Wcs persistable,"regardless of the mechanism used by the new wcs classes, they will need to be persistable.  this will probably require some significant work unless classes that are already persistable are used in the design.",56,train
DM-3877,Identify all corrections ISR needs to handle,"this is informed by decam etc.  this is just for the corrections that do not require detection.  for example, we will push out cr rejection until the question of snaps is decided.",9,train
DM-3878,Implement all ISR corrections for LSST,most corrections have some implementation.  perhaps the most difficult will be crosstalk since the implementation should allow for correction over multiple chips.,19,train
DM-3879,Make up a test for dipole measurement,"to facilitate work on the dipole measurement, a test will be helpful.  this could simply be an image with dipoles made with gaussian psfs with poisson noise on top.  the important thing is to be able to determine whether the algorithm is returning the right answer.",9,train
DM-3880,Improve the dipole measurement task,the current dipole measurement algorithm has some issues.  it also doesn't work in the current measurement framework.  this task is to improve the dipole measurement.  the result of this epic should be improved results when run on the test data.,38,train
DM-3881,Gather requirements to inform a redesign of the CalibrateTask,the current calibrate task is fairly brittle and hard to extend.   this task is to gather the necessary requirements for a redesigned calibrate task.,2,train
DM-3882,Implement the new CalibrateTask design,implement the redesigned calibratetask.,28,train
DM-3883,"Create initial cluster design, send internally for feedback and planning",gather feedback on initial designs for fy16 purchase plans.,2,train
DM-3884,Create data products description,addition of  to understand more of the requirements necessary for the functional design  ,3,train
DM-3885,"LSE-78: W16 revisions, harmonization with existing design",review lse78 for selfconsistency and consistency with the current dm and overall system design.,6,train
DM-3886,"Revise early integration milestones, LCR-323 and beyond","revise the list of early integration milestones with ocs, tcs, ccs, and daq to form a coherent plan.  coordinate with ncsa and other interested parties in dm.",6,train
DM-3887,Review ICD flowdown to DMSR and design documents,nan,8,train
DM-3888,Add missing space after if in Qserv code to conform to standard,"replace ""if("" with ""if ("" to follow standard.    find core/modules/ name "" .cc"" wc l  852    ",1,train
DM-3891,Review LCR-323 proposal for integration milestones,prepare for ccb action on lcr 323.  ensure that daq integration is included (it's not in the original lcr proposal).,4,train
DM-3892,"Review current version of LSE-78, prepare for LCR",do a comprehensive readthrough of the previous released version of lse78.  look for self consistency and for consistency with the rest of the dm and overall system design.  report issues to appropriate people.,3,train
DM-3893,Research existing DHT-based FS approaches ,the previous prototype provided confidence that a dht overlay could work for routing and placement at the scales and time constants needed for chunk distribution.  the next prototype will need actual data transport and storage management facilities layered on the node/key management provided by the dht layer.  explore existing works at this level to a greater depth pick from among proven approaches.,6,train
DM-3894,Provide values for relative astrometry KPMs in FY15,"should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",2,train
DM-3895,Provide values for PSF ellipticity KPMs in FY15,"should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",1,train
DM-3896,Provide values for photometric repeatability KPMs in FY15," should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",4,train
DM-3897,Provide value for DRP computational budget KPM in FY15,"should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",3,train
DM-3898,Fix xrootd compiler warnings with clang,"xrootd      in file included from core/modules/qdisp/executive.cc:64:  in file included from core/modules/qdisp/xrdssimocks.h:33:  in file included from /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssirequest.hh:37:  /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssirespinfo.hh:43:1: warning: 'xrdssirespinfo' defined as a        struct here but previously declared as a class [wmismatchedtags]  struct  xrdssirespinfo    /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdssi/xrdssisession.hh:45:1: note: did you mean struct here?  class xrdssirespinfo;    struct        core/modules/xrdoss/qservoss.h:64:17: warning: 'lsst::qserv::xrdoss::fakeossdf::opendir' hides overloaded virtual function [woverloadedvirtual]      virtual int opendir(const char )                     /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7fda39a3ee5e/include/xrootd/xrdoss/xrdoss.hh:63:17: note: hidden overloaded virtual function        'xrdossdf::opendir' declared here: different number of parameters (2 vs 1)  virtual int     opendir(const char , xrdoucenv &)                                     in file included from core/modules/xrdsvc/ssisession.h:32:  /users/timj/work/lsstsw/stack/darwinx86/xrootd/u.timj.dm3584ge22410fa7f+da39a3ee5e/include/xrootd/xrdssi/xrdssiresponder.hh:177:27: warning: control may reach end of        nonvoid function [wreturntype]                            }                            ^  ",1,train
DM-3899,support shared_ptr<Statistics>,"i would like to write some functions that return afw::math::statistics objects and wrap them with swig. unfortunately swig requires that any object returned by value must have a default constructor, and statistics does not. rather than try to add such an object i propose to make my functions return a sharedptr to statistics.    this ticket is a request to support that by adding the following to statistics.i:     %sharedptr(lsst::afw::math::statistics);    ",2,train
DM-3900,Review of [DM-2983],i was asked for a revision of [dm 2983] which is part of the backport hsc parallelization code,4,train
DM-3901,Update some tests to support nose and/or py.test,when sconsutils is migrated to use nose or py.test some test scripts will need to be modified because test discovery will be slightly different and the namespace of test execution will change.    two things to consider:   people would still like the option of running a test as python tests/testme.py.   we have to work out how to run the memory test case.  ,4,train
DM-3902,Fix protobuf compiler warning with clang,nan,1,train
DM-3903,Base Site Data Access Center Description Page,https:/confluence.lsstcorp.org/display/~petravick/dataaccesscenter,1,train
DM-3905,Gathering use cases for verification data sets,seeking out developer use cases of incoming data sets. need to determine if datasets will be accessed for verification only or by developers and qa in general. determine access methods. ,3,train
DM-3906,"Future infrastructure, common IT, and facility planning","includes consulting on developing use cases for base site commissioning cluster    jason alt, paul wefel, tom durbin",13,train
DM-3907,Specify FY15 Equipment Purchasing Plan,"the hardware contract was finalized at the end of july 2015, leaving 2 months of fy15 for spending the fiscal year hardware budget. in lieu of an annual acquisition strategy document, we draft a one off fy15 purchasing plan.    assignees: jason alt, bill glick, paul wefel  duration: september 2015",9,train
DM-3908,W16 Work on Alert Production Simulator,continued development of alert production simulator.    assignees: steve pietrowicz  duration: september 2015   february 2016,71,train
DM-3909,W16 Work on OCS Software Integration,"integrate the ocs software, delivered from the camera team, into ap.    assignees: steve pietrowicz  duration: november 2015   february 2016",38,train
DM-3910,Run and document multi-node test with docker,"in order to validate docker setup on ccin2p3 cluster, it is required to launch some test on consistent data. s15 largescaletest data doesn't seems to be compliant with latest qserv version so running multinode test would be interesting. nevertheless the multi node setup doesn't seems to be documented and, hence, is difficult to reproduce.",3,train
DM-3911,HSC backport: avoid I/O race conditions config write out,"this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1106    when running tasks that write out config settings files (processccd.py, for example), if multiple processes start simultaneously, an i/o race condition can occur in writing these files.  this is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  also, to avoid similar race conditions in the backup file creation (e.g. config.py1, config.py2, ...), a nobackupconfig option (to be used with clobberconfig) is added here to prevent the backup copies being made.  the outcome for this option is that the config that are still recorded are for the most recent run.",1,train
DM-3912,some ctrl_events tests execute outside of execution domain,"there are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be.  there's a check in place for tests to find this, but a couple of the tests do not have this check.",1,train
DM-3913,transfer and update orchestration documentation,"the self service orchestration documentation needs to be transferred from trac to confluence, and updated.",2,train
DM-3916,Misc work for this reporting week. ,"deal with comments,  coordinate with jason alt. deal with comments on existing work from kt and gdf",2,train
DM-3917,Begin thinking about governance aspects of DM operations,"thinking, (but not delivered use case)   what is the flow of tickets and the division of use cases between the science and data operations?  what kind of tooling needs can be assumed to exist to support service management what does that mean for the support of processes?  (specifically  looked at a summary of the state the market, and took specifics look at what itsm process supporting tools are open source.  as may be useful to prototype processes before committing to something..  looked at several, itop seemed to be mature/documented to a level that may be useable.    ",4,train
DM-3918,"Conduct, document, initial follow through on Sept JCC meeting ","developed agenda items, considered jcc meeting.  minutes are on the lsst confluence.  a major item of discussion was related to operations, since we are told this is a priority.  befall followup on ccin2p3 ism practices. ",1,train
DM-3919,General Management ,"hiring,  internal relationships within the ncsa organization. lsst meetings, general management",4,train
DM-3920,Replace boost::regex with std::regex,"boost 1.59 causes a ""keyword hidden by macro"" warn under clang in the regex package.  we should be using std::regex now anyway, so this is a good motivator to go ahead and convert.",1,train
DM-3921,Create and deploy a git-lfs prototype,"create and deploy a gitlfss3 server.    high level requirements:     the server should use github to authenticate users. any github user who is a member of the lsst organization has write access. this means they can push objects to the git lfs server.   the server should allow for anonymous read access. this means anyone can clone, pull, fetch, etc.   use an s3 compliant api to store objects.   simple, well defined method to redeploy.   uses https.   backs up to amazon glacier (or similar) periodically. the data should be ""slow"" so backing up approximately once a day is ok.",64,train
DM-3922,Update multi-node setup documentation,workers in multi node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.,1,train
DM-3923,W16 ISO Work,continuous work as lsst iso.    assignees: alex withers  duration: september 2015   february 2016,39,train
DM-3924,Centralize Sphinx configuration for Design Documents,centralize sphinx configuration for design documents in documenteer and provide a facility for design document authors to use yaml files to store document metadata rather than editing conf.py files.,2,train
DM-3926,Implement iostream-style formatting in log package,implement proposed in rfc 96 change to log macros. this ticket only covers defining new set of macros (logs() and friends) which use ostringstream for formatting messages. migration of all clients and removal of logf macros will be done in separate ticket.,1,train
DM-3928,port dax_*serv to python3,nan,3,train
DM-3929,Handle queries with no database,"sqlalchemy is generating some queries that are currently killing czar, the list is:      set autocommit=0  show variables like 'sqlmode'  select database()  select @@txisolation  show collation where `charset` = 'utf8' and `collation` = 'utf8bin'  select cast('test plain returns' as char(60)) as anon1  select cast('test unicode returns' as char(60)) as anon1  select cast('test collated returns' as char character set utf8) collate utf8bin as anon1  select 'x' as somelabel  select @@version_comment limit 1      czar should survive unfriendly syntax, and this will be addressed through dm3764.    in this story we will make sure that  sqlalchemygenerated queries are properly handled (not just these particular queries, but all queries that do not involve any database and any table). we should run such queries on a local mysql instance (alternatively, perhaps redirect to one of the workers?)",4,train
DM-3930,"AP, Co-add, Image cache definitions","added physical breakdown for alert postage stamp images file system, co add images file system and image cache file system",2,train
DM-3931,sandbox-stackbuild issues,"a number of issues with sandboxstackbuild, or rather its infrastructure.    1. problem with the librarianpuppet plugin and its mismatch with the puppet forge api ([jhoblitt] has a pr open apparently)    2. as a workaround to above, one needs to      gem install librarianpuppet  librarianpuppet install      but that runs into an issue with swapfile needing a downgrade to work with ubuntu 14.04. working state as of the time of this bug report for the puppetfile is:        forge 'https:/forgeapi.puppetlabs.com'    mod 'puppetlabs/stdlib'  mod 'camptocamp/augeas', '> 1.4'  mod 'stahnma/epel', '> 1.1'  mod 'petems/swapfile', '1.0.1'  mod 'jhoblitt/sysstat', '> 1.1'  mod 'maestrodev/wget', '~> 1.7'    mod 'jhoblitt/lsststack', :git => 'https:/github.com/lsstsqre/puppetlsststack.git'      3. which brings us to the fact that the vagrant puppet install plugin is broken with puppet 4, and new platforms are not supported under puppet 3. ergo, as is, can't bring up ubuntu 15.05 etc.     ticket is to get prs merged, fork and fix them ourselves, or find alternatives.   ",4,train
DM-3932,Histogram calculation for image stretch has infinite loop ,"when load the big.fits file, the image never came out.  it stopped at histogram.  there was an infinity in histogram.   ",4,train
DM-3935,Measurement plugin errors,"when doing measurements on coadds, several errors are thrown within the measurement plugins.      error in basegaussianflux.measure on record 283467884979: input shape is singular        error in basegaussianflux.measure on record 283467883979:     file ""src/sdssshape.cc"", line 842, in static lsst::meas::base::fluxresult lsst::meas::base::sdssshapealgorithm::computefixedmomentsflux(const imaget&, const lsst::afw::geom::ellipses::quadrupole&, const point2d&) [with imaget = lsst::afw::image::maskedimage/; lsst::afw::geom::point2d = lsst::afw::geom::point/]      error from calcmom   lsst::pex::exceptions::runtimeerror: 'error from calcmom'      the measurements were done on tiger, and the command used was:    measurecoaddsources.py /tigress/hsc/hsc/rerun/nate/oldclip/ output=/tigress/hsc/hsc/rerun/nate/oldclip/ c /home/nlust/optionstemp.py id tract=0 patch=2,2 filter=hscihscr     the data can be found within the rerun directory specified as the input to the command. the data was created using the commands:    assemblecoadd.py  legacycoadd /tigress/hsc/hsc/rerun/nate/ output=/tigress/hsc/hsc/rerun/nate/oldclip id tract=0 patch=2,2 filter=hscr selectid visit=1208 ccd=566472 selectid visit=1206 ccd=646572737980 selectid visit=1212 ccd=646572737980 selectid visit=23704 ccd=64657273 selectid visit=23706 ccd=566472 selectid visit=23694 ccd=646572737980 selectid visit=1204 ccd=646572737980 selectid visit=1220 ccd=636471727879 selectid visit=1218 ccd=565764657273 selectid visit=23718 ccd=646572737980 selectid visit=23692 ccd=646572737980 selectid visit=1210 ccd=636471727879 selectid visit=1216 ccd=565764657273 selectid visit=1214 ccd=646572737980 selectid visit=23716 ccd=636471727879 selectid visit=1202 ccd=646572737980    and     assemblecoadd.py legacycoadd /tigress/hsc/hsc/rerun/nate/ output=/tigress/hsc/hsc/rerun/nate/old_clip id tract=0 patch=2,2 filter=hsci selectid visit=19658 ccd=646572737980 selectid visit=1248 ccd=566472 selectid visit=19696 ccd=656673748081 selectid visit=19684 ccd=646572737980 selectid visit=1238 ccd=646572737980 selectid visit=19710 ccd=566472 selectid visit=19680 ccd=566472 selectid visit=1230 ccd=646572737980 selectid visit=1236 ccd=636471727879 selectid visit=19694 ccd=646572737980 selectid visit=1232 ccd=646572737980 selectid visit=19698 ccd=646572737980 selectid visit=1228 ccd=646572737980 selectid visit=1246 ccd=636471727879 selectid visit=19682 ccd=636471727879 selectid visit=19708 ccd=646572737980 selectid visit=19662 ccd=64657273 selectid visit=1240 ccd=646572737980 selectid visit=1244 ccd=565764657273 selectid visit=1242 ccd=565764657273 selectid visit=19660 ccd=646572737980  selectid visit=19712 ccd=565764657273  ",6,train
DM-3936,"Fix ""Executive error executing job"" on the cluster",nan,1,train
DM-3937,HSC backport: temporary file handling in butler,the hsc fork includes additional work to improve temporary file usage in the butler:   https:/hscjira.astro.princeton.edu/jira/browse/hsc1275: probable resource leakage by butler   https:/hscjira.astro.princeton.edu/jira/browse/hsc1285: eups.version files ignore umask    https:/hscjira.astro.princeton.edu/jira/browse/hsc 1292: prevent opening files that are already open,1,train
DM-3938,Pastry prototype C++,nan,22,train
DM-3939,move camera factory methods from obs_lsstSim to afw,"the methods defined in obslsstsim/bin/makelsstcamerarepository.py can be easily adapted for use in generating arbitrary, nonlsst cameras.  this is useful for the sims stack, both for testing purposes, and because members of other projects have begun asking us to use our code.    this ticket will take those methods, make them fully lsstagnostic, and place them in afw as utility functions.  the code in obslsstsim will refer back to these afw methods.",5,train
DM-3940,NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot,"the naivedipolecentroid and naivedipoleflux algorithms in ipdiffim have members which are instances of meas::base::safecentroidextractor. due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a centroid slot.    however, there is nothing in these algorithms which actually uses the safecentroidextractor or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the measbase framework. we should remove the  use of safecentroidextractor to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider).",1,train
DM-3941,Split FY16 plan into audience specific documents,"fy16 purchase planning needs to be split for specific audiences (ncsa planning, aura purchase approval)",2,train
DM-3942,Preliminaries for the LSST vs. HSC pipeline comparison through single-frame processing,"this ticket is in preparation for dm2984, which is to run both the hsc and lsst pipelines on 23 visits of hsc data, and do a detailed comparison of the science quality and robustness for the singleframe processing (processccdtask) stage only.  essentially, it is a detailed audit of all the functionality currently used in hsc singleframe processing and ensuring all relevant features have been pulled over to the lsst stack such that the comparisons to be done in dm 2984 will be meaningful and informative.",14,train
DM-3943,QMeta thread safety,"initial qmeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). need an urgent fix to avoid crashes due to concurrent queries in czar.",1,train
DM-3944,orchestration slide set for DM bootcamp,"create the ""orchestration and control"" slide set for dm bootcamp, which is being held from oct 5 7, 2015.  after review (and any revisions), the slide set will be uploaded to confluence, and a link to it will be put here.",5,train
DM-3945,Simplify task queuing / Runner code,nan,8,train
DM-3946,Cleanup cancellation-related worker code,nan,7,train
DM-3947,Remove dependency on mysqldb in wmgr,move remaining code that depends on mysqldb to db module,1,train
DM-3949,Remove dependency on mysqldb in qserv,remove remaining dependencies on mysqldb in qserv.:    ./core/modules/tests/mysqludf.py  ./core/modules/wmgr/python/config.py      and use the sqlalchemy from db module instead.,2,train
DM-3951,Remove qserv_objectId restrictor,"qservobjectid restrictor can be replaced by the in restrictor. this story involves checking if performance is acceptable if we use in restrictor instead of qservobjectid restictor, and if it is, doing the switch and removing the qserv_objectid restictor code.",3,train
DM-3952,Cleanup lua miniParser,"maybe some cleanup can also be performed in lua code. indeed ""objectid"" hint and parseobjectid() which seems useless.    indeed miniparser.parseit and miniparser.setandneeded seems useless.    removing this code will ease maintenance of objectid management.",1,train
DM-3954,E/I training and interview,interviewed and attended training for e/i concerns.,1,train
DM-3955,Investigate services for backups/data replication on Nebula openstack,files generated on instances of the nebula openstack  should be managed with some commensurate  level of data replication/backups.     we investigate services that might serve this task within the cloud context.,4,train
DM-3956,Package SQLAlchemy in eups,"db module is expected to be used by science pipelines, and (per k t, see qserv hipchat room) we have to package it through eups.",1,train
DM-3957,Enable CModel in CalibrateTask prior to PhotoCal,"cmodel needs to run in calibratetask before photocal in order to compute aperture corrections, but it also needs a calib objects as input, and that isn't available until after photocal is run.    on the hsc side, we dealt with this by adding preliminary photocal run before cmodel is run, but we could also deal with it by removing the need for a calib as input, at least in some situations.",1,train
DM-3958,Revisit provenance design / built proof-of-concept prototype,"discuss existing provenance design with the team, brainstorm and improve. primary focus on capturing provenance information. deliverable: proofofconcept prototype + description.",16,train
DM-3959,Revisit provenance sizing,revisit estimates of the size of provenance information.,6,train
DM-3960,First optimizations of provenance querying,think through the issues of querying provenance. deliverable: write up attached to this story. it will be transferred to more official provenance documentation through dm 3961,14,train
DM-3961,Document provenance design,nan,8,train
DM-3962,Build prototype of provenance,"building a proofofconcept prototype of provenance.    deliverable: a working, standalone (not connected to any data producer) prototype of the data provenance. not optimized / alpha version.",12,train
DM-3965,Meetings Sep 2015,verification datasets weekly meetings and tech talks,1,train
DM-3966,Add SQLite-based v01. unit tests for dbserv,nan,6,train
DM-3967,"Meetings, institute events, or other LOE, Sep 2015","ncsa or astronomy department activities.    ncsa allhands   local lsst group meetings    desillinois meetings   colloquia    other seminars, info sessions, or other local meetings",5,train
DM-3968,detailed out alert processing/ transmission to event brokers. ,"detailed out the interactions with the event broker, after consulting with john swinbank.  dealt with outputs of alert processing.  dealt with corrections and comments.   began considering annual processing",2,train
DM-3969,weekly management,"dealt with impending new hire.   good deal of intergroup coordination, and deal with ncsa re organization.",13,train
DM-3971,Package sqlalchemy in eups,"db module is expected to be used by science pipelines, and (per k t, see qserv hipchat room) we have to package it through eups.",1,train
DM-3973,FY pricing estimates,nan,1,train
DM-3974,Network / Storage reviews,initial (internal) review with networking and storage delegates,2,train
DM-3975,research items needed for wan simulator procurement,locate part numbers and pricing information,1,train
DM-3976,Consultation on system and network design,nan,4,train
DM-3977,Prepare pricing estimates for networking infrastructure,"provide switch quantities, weight, power and budgetary costs",4,train
DM-3978,Recommend network infrastructure for Openstack expansion,nan,1,train
DM-3979,Discussion of Base site network proposal,email based discussion with ron lambert on his network infrastructure proposal for the base site.,1,train
DM-3980,Post SQLAlchemy-migration tweaks,"implement some minor tweaks take came in late through pr comments, mostly related to sqlalchemy related migration",1,train
DM-3981,Improve the performance for making the image plot,make fitsread work better for multi threads,1,train
DM-3983,Larger Statistics needed for CModel Studies,"the stampsize and ninitalradius tests were not conclusive in the september sprint. and the error estimates appeared to be overly large. the ngrowfootprints test was barely significant.  this is a continuation of work started on dm1135 (dm1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (dm3984).    we started with a sample with 12 million galaxies (not all of which were used in dm3375 and 3376.  they appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample. ",6,train
DM-3984,Errors for shear bias fits,"dm 1135 mostly was inconclusive or at least not highly significant, due to the fact that the error bars were around the same size as the differences in most of the tests. this in spite of the fact that we ran 6x as many sample galaxies as great3sims.    investigate the reason for this, and see if we can estimate the errors more accurately.  investigate what the best way (or at least the way it is done in great3) to estimate the errors on the bias parameters.     if it is not from the covariance matrix of the regression parameters, there could be some work here.",6,train
DM-3985,Run multinode test using docker on one unique host,"qserv multinode test can be launched on n hosts using docker, but not on one unique host.  this ticket will allow developers to run multinode test on their workstation.",6,train
DM-3986,Deploy developer code on in2p3 cluster in Docker images,qserv latest release can be deployed easily on ccin2p3 cluster using docker. this ticket will allow developers to prepare worker and master containers using a specific qserv version and deploy it on ccin2p3 cluster.,6,train
DM-3987,remove unnecessary 'psf' arg to SourceDeblendTask.run(),"sourcedeblendtask.run takes both an exposure and a psf, even though it can get the latter from the former and always should.",1,train
DM-3988,Update FY2015 hardware budget plan,"update the fy2015 hardware purchasing plan with new budget, equipment specifications, and general costs. ",3,train
DM-3989,Week end 09/05/15,"support for lsst dev cluster, openstack, and accounts  for week ending september 5, 2015.",5,train
DM-3990,Provide detail specs to AURA,provided josh hobblitt details specs for procurement request,1,train
DM-3992,Week end 09/12/15,"support for lsst dev cluster, openstack, and accounts  for week ending september 12, 2015.",5,train
DM-3993,Display.dot origin swaps x and y,correcting for xy0 in dot currently does:    r = x0  c  = y0    which is backwards.,1,train
DM-3994,Backup Pugsley,created backup of pugsley in anticipation of new hardware and shutdown of temp mac os solution,1,train
DM-3996,Research using vSphere on Mac Pro,researched setting up vsphere on mac pro.,1,train
DM-3997,Week end 09/19/15,"support for lsst dev cluster, openstack, and accounts  for week ending september 19, 2015.",5,train
DM-3998,Learn about Lenovo storage and server options,went to lunch with lenovo to learn about systems that might be suitable for lsst deployment. learned about server and storage options.,1,train
DM-3999,Week end 09/26/15,"support for lsst dev cluster, openstack, and accounts  for week ending september 26, 2015.",5,train
DM-4000,Revamp FITS Viewer scrolling to stop using large div,the current scrolling system will not work as well with masking layer.  it is also suspected to use too much browser memory since is creates a very large div.  firefly will scroll images manually now.,12,train
DM-4001,Write next-generation stack doc writing guide,write a guide in the prototype lsst stack docs (https:/github.com/lsst sqre/lsststackdocs) covering how to document the lsst stack under the new doc infrastructure.    this exercise will implicitly involve designing how the new docs will work. content includes:    # how to write a user guide to a package (both content wise and in terms of organizing a package's doc files)  # how to write python doc strings  # coverage of restructuredtext and sphinx as implemented by the lsst stack docs.  ,7,train
DM-4003,Replace zookeeper CSS with mysql,to switch from qservadmin to cssaccess interface in our python tools we will need to replace zookeeper with mysql implementation because we do not have c kvinterface implementation for zookeeper.,2,train
DM-4004,Investigate what is missing to run ISR with DECam raw data and processCcd,"currently raw decam data can not be processed with the stack for instrumental signature removal (isr).  this issue includes efforts to understand how isr is done in the stack, learn about processccd, basic decam isr, and what is missing in the code stack to proceed.    ",13,train
DM-4007,"ctrl_execute templates still use ""root"" instead of ""config""","the templates that ctrlexecute fills in still use ""root"", instead of the new ""config"".  this causes extraneous warning messages to appear from pexconfig when executing ""runorca.py""",1,train
DM-4009,Allow FlagHandler to be used from Python,"the flaghandler utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the safecentroidextractor and safeshapeextractor classes.  unfortunately, its constructor requires arguments that can only be provided in c.  a little extra swig wrapper code should make it usable in python as well.",3,train
DM-4011,Rename forced photometry CmdLineTasks to match bin scripts,"we use names like ""forcedphotccd.py"" for bin scripts but ""processccdforcedtask"" for class names; these need to be made consistent, and it's the former convention that was selected in an old (non jira) rfc.",1,train
DM-4013,Initial discussion EFD with Dave Mills,review lts 210 and discuss design of efd cluster with dave mills.    are there opensource clustering solutions?,1,train
DM-4014,Replace boost::tuple with <tuple>,replace boost::tuple with /    this ticket will be completed as part of the dm bootcamp at uw.,1,train
DM-4015,Document Revision,"document cleanup. added pdus, networking estimates, power costs, vsphere licensing.",1,train
DM-4016,Write developer workflow documentation,write a developer workflow guide to walk through and document best practices for developing against the lsst stack.,6,train
DM-4019,Fix procedure for building docker image for 2010_09 release,this procedure should be straightforward but is currently failing due to gcc4.9/boost problem (dm4018),2,train
DM-4021,Replace boost::unordered_map with std::unordered_map,dm boot camp tutorial.    ,2,train
DM-4022,forcedPhotCoadd.py fails on HSC data,"when trying to run forcedphotcoadd.py on hsc data, i see the following error:      $ forcedphotcoadd.py /raid/swinbank/rerun/lsst/bootcamp id filter='hsci' tract=0 patch=7,7  : loading config overrride file '/nfs/home/swinbank/obssubaru/config/forcedphotcoadd.py'  cannot import lsst.meas.extensions.photometrykron: disabling kron measurements  traceback (most recent call last):    file ""/home/lsstsw/stack/linux64/measbase/11.02/bin/forcedphotcoadd.py"", line 24, in /      forcedphotcoaddtask.parseandrun()    file ""/home/lsstsw/stack/linux64/pipebase/11.02/python/lsst/pipe/base/cmdlinetask.py"", line 433, in parseandrun      parsedcmd = argumentparser.parseargs(config=config, args=args, log=log, override=cls.applyoverrides)    file ""/home/lsstsw/stack/linux64/pipebase/11.02/python/lsst/pipe/base/argumentparser.py"", line 360, in parseargs      self.applyinitialoverrides(namespace)    file ""/home/lsstsw/stack/linux64/pipebase/11.02/python/lsst/pipe/base/argumentparser.py"", line 475, in applyinitialoverrides      namespace.config.load(filepath)    file ""/home/lsstsw/stack/linux64/pexconfig/11.0/python/lsst/pex/config/config.py"", line 529, in load      self.loadfromstream(stream=code, root=root)    file ""/home/lsstsw/stack/linux64/pexconfig/11.0/python/lsst/pex/config/config.py"", line 549, in loadfromstream      exec stream in {}, local    file ""/nfs/home/swinbank/obssubaru/config/forcedphotcoadd.py"", line 10, in /      config.deblend.load(os.path.join(os.environ[""obssubarudir""], ""config"", ""deblend.py""))  attributeerror: 'forcedphotcoaddconfig' object has no attribute 'deblend'      this is with the stack version 11.0+3 and obs_subaru 5.0.0.1676 g4ae362c.",1,train
DM-4023,Local LSST IAM Meeting,meeting notes:     [10 minutes] review lsst identity management statement of work    on the confluence wiki. let me know if you have any followup questions/comments.     [10 minutes] plan initial project tasks    all: review materials at https:/confluence.lsstcorp.org/display/laaim. jim to add updates from last week's meeting.      [5 minutes] schedule followon project meetings    alex will schedule meeting with bill glick about ldap at ncsa.  terry and daniel will attend https:/community.lsst.org/t/dmbootcamp announcement/249 sessions of interest if available.,1,train
DM-4024,Local LSST IAM Meeting,"meeting attended by jim basney, tim fleury, dan thayer, and alex withers.  discussed upcoming dm bootcamp.  focused on suit diagram and determining what questions to ask at next biweekly meeting.   first draft recommendations by end of october.",1,train
DM-4025,First day activities,meeting with hr at 8:30am.  taken around for introductions through 10am.  remainder of day was proceeding through hr punch list for new hires such as:   ncsa intranet account setup   gain familiarity with ncsa intranet   kerberos and enterprise id setup   read and sign off  on security and ncsa policy.   outlook mail and calendar setup  2pm dm meeting,2,train
DM-4026,Second day start activities,requisitioned used macbook and monitor from it and set it up.  located lsst stack site and read build and install documentation  more hr account items.  requested confluence credentials from lsst.org  learned about sdss/stripe 82,2,train
DM-4027,groundwork for file management,installed correct xcode + misc. for stack version.  downloaded lsst stack from github repo and built/installed.  ,1,train
DM-4028,Incidental items and jobs for file mgmt. groundwork,"installed and built lsst tutorials package.  setup, fixed minor issues and ran first tutorial to check that initial stack  installation was successful.  learned about ccd operation and how the lsst ccd is laid out.  learned about raw ccd data from amplifiers, as well as other camera attributes  studied stack code for these things.  read description of astronomy associative relations as well as coadds.  pulled phosim from repo. built and installed.  had trouble getting phosim to run due to dynamic link library issues.  instrumented phosim code with pdb commands.  walked to bookstore and picked up new id badge.",2,train
DM-4029,Development for Developer activities!,"completed setup of confluence access.  set up lsst hipchat.  jumped into phosim a bit more and resolved errors by dropping in a few symlinks here and there;  not a solution for a proper execution environment, but a time save just to see phosim work and  have a platform for tinkering with camera attar's.  phosim ran successfully.  learned about ftts format and how the file is built up. learned about how key/val pairs   can proceed any data section throughout file by using data offset values.",2,train
DM-4030,integration of confluence data into learning curve,"started studying lsst coding policies and best practices via confluence.  colloquium.  small meetings with other lsst team members throughout day.  rebuilt phosim/lsststack to take advantage of multiple cores when rendering portion of sky to a file. built these commands into stack code. they would have to be custom #defined by ./configure at build time depending on computer arch. which is too much to do when just gaining familiarity so stuck to macbook  multicore specs, where i was working.  logged into jira and studied how tasks were proposed, realized, and checked as done.  extensive talk about bbq in group area of ncsa/lsst.  close reading and note taking of ldm 230 and related docs.",5,train
DM-4031,"Add doc directory, and fix doxygen warnings",add doc directory and fix doxygen warnings.,4,train
DM-4032,obs_sdss should use pydl.yanny instead of it's own copy thereof,"inside obssdss there is a yanny.py that looks like it was copied from either sdsspython_module or pydl. we should just depend on pydl (https:/github.com/weaverba137/pydl), so we can use whatever improvements it gets for free, and to prevent yet annother yanny reader floating around.",4,train
DM-4034,Display DECam focal plane mosaics using showCamera,try out some new functionalities of afw.camerageom.utils (from dm 2437) for decam raw data. raw data in testdata_decam are retrieved through butler and displayed as a focal plane mosaic.  ,2,train
DM-4035,Replace boost::array with std::array,"replace all use of boost::array with std::array in the dm stack    a quick search turned up use in 17 files spread over these packages:  ipdiffim, measbase, measextensionsphotometrykron and ndarray (which is presumably out of scope for this ticket)",4,train
DM-4036,Change from boost::math,"most boost::math contents (not including pi) are now available in standard c. please convert the code accordingly.    in addition to the packages listed above, boost/math is used in ""partition"" a package i don't recognize and not a component jira accepts.",5,train
DM-4039,New YAML config for community_mailbot,"per review comments to dm3690, the configuration should move to yaml with the following goals     have a configuration object that can be tested and passed around   redesign the configuration to allow for additional types of message handlers, such as twitter, hipchat/slack, etc.   move secret keys entirely into the configuration file   provide a configuration template   move any sort of hard coded configuration to the expanded yaml file (e.g., mandrill templates)",1,train
DM-4040,Refactor Scripts and Discourse interface in community_mailbot,"per review comments for dm3690, the community_mailbot can have slight code refactoring     refactor scripts into smaller testable units   refactor the discourse feed classes around an abc   more testing",1,train
DM-4042,Produce demo video for git lfs,produce a screencast tutorial of the dm git lfs implementation.,1,train
DM-4043,update memory management in jointcal,"jointcal currently uses a combination of raw pointers and a custom referencecounted smart pointer class, countedref (similar to boost::intrusiveptr).  the code needs to be modified to use a combination of sharedptr (most code), uniqueptr localscope variables and factory functions, and weakptr (at least some will be necessary to avoid cycles in some of the more complex data structures).  as part of this work, we'll also have to remove a lot of inheritance from refcounted, which is part of the countedref implementation.    this ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks.  nevertheless, i think it will be necessary to do this conversion before attempting any other major refactoring, as i'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous.",8,train
DM-4044,"integrate jointcal geometry primitives (Point, Frame, FatPoint) with afw","jointcal currently has three simple geometry classes that can be integrated relatively easily with existing classes in afw and measbase:    jointcal.point is equivalent to afw.geom.point2d.    jointcal.frame is equivalent to afw.geom.box2d.     jointcal.fatpoint is equivalent to meas.base.centroidresult.  we should probably move centroidresult to afw.geom, perhaps rename it (measuredpoint?), and reconsider its relationship with point.  this will require a bit of refactoring in measbase, but the usage in jointcal makes me think it's a sufficiently fundamental object to be included in afw.    we may find aspects of the interfaces in jointcal that we should add to afw.geom, but i think we'll mostly end up making trivial modifications to jointcal to use the afw interfaces.",8,train
DM-4045,integrate Gtransfo functionality with XYTransform,"meas_simastrom includes a gtransfo class hierarchy that is similar to afw.geom.xytransform, but with more functionality and some intentional differences, including:    xytransform objects are immutable; gtransfo objects are not.    gtransfo objects expose their parametrization, and can compute various derivatives with respect to those parameters.  xytransforms are essentially blackbox functions, and expose no parameterization.    unifying these classes is not entirely straightforward, and should include an rfc for the design prior to implementation.  overall, i think xytransform's simpler, lowerfunctionality interface and immutability is worth mostly preserving somehow; i think it's a more fundamental interface than gtransfo that can be used in more places.  but obviously we need to provide the more extensive gtransfo interface somehow as well.    my initial thought is that we should have two parallel class hierarchies (with a concrete class for each type of transform, such as polynomial distortion, in both), and an ultimate base class shared by both hierarchies.  that ultimate base class would contain most of the current xytransform interface but not require immutability, and one side of the tree would contain simple immutable objects while the other would contain the more extensive parameterized interface of gtransfo.",8,train
DM-4046,Redis cache for community_mailbot,switch from a json cache to a redis cache from the community_mailbot.,1,train
DM-4047,Add fake secondary index to testIndexMap.cc,"qproc/testindexmap.cc is sketchy and perform a very poor validation for now. it should at least use a minimal sql secondary index, embedded in a simple database like sqllite, inorder to validate secondary index lookup code.",8,train
DM-4048,Replace QsRestrictor::PtrVector With std::vector<QsRestrictor> and use move constructor,use of  qsrestrictor::ptrvector introduces a useless indirection. it maybe could be replace by std::vector/ and use of move constructor. this would simplify code (currently a confusion exists between empty vector and nullptr) and ease maintenance.,5,train
DM-4049,Security plan renewal,nan,3,train
DM-4050,IaM work,nan,2,train
DM-4052,bi-weekly IaM meeting,nan,1,train
DM-4054,Meetings Oct 2015,  verification datasets meetings  ,1,train
DM-4055,Final additions and review,"add remaining components: power estimates, vsphere annual licensing, networking, pdus, login nodes    review: misc expense fund, decommissioned services, financial targets",3,train
DM-4056,Chasing down Pan Starrs Requirements,pan starrs data release (ps1) will be used in the integration qserv environment purchased as part of fy16. catalog and file space requirements must be understood.,1,train
DM-4057,Prepare Plan for ICI Leadership Review,prepare plan and icigroupspecific points of interest for hardware/service deployment plans. ,1,train
DM-4058,Vendor Discussions,discussions with vendors on planned procurement. details of discussions will not be described here. this is for story point tracking only.,1,train
DM-4059,Begin Approval Process,"approval process for fy16 procurement plan. this requires approval from jeff k, victor and nsf (due to the cost increment being greater than $250k).    expected approval time frame: dec 2015.",1,train
DM-4061,Define policy based upon FY16 plans,re evaluate previously proposed storage policies: https:/wiki.ncsa.illinois.edu/display/lsst/storagepolicy?src=contextnavpagetreemode    plan new policies:  https:/wiki.ncsa.illinois.edu/display/lsst/changestostoragepolicyanddesign?src=contextnavpagetreemode,1,train
DM-4062,Data access rights and retention policies,"added data access center requirements, individual data access rights, data retention policies and other general cleanup.",1,train
DM-4063,Support new casting requirements in NumPy 1.10,the function imagesdiffer() in testutils attempts to or an array of unit16s (lhs) against an array of bools(rhs) valskipmaskarr |= skipmaskarr and errors with message    typeerror: ufunc 'bitwiseor' output (typecode 'h') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''samekind''    preventing afw from building correctly. ,1,train
DM-4064,Revisit database compression trade-offs,"as discussed at qserv meeting oct 7, it is not entire clear if it will be worth compressing data. need to revisit baseline.",5,train
DM-4065,Discuss with MySQL team,this story captures issues/topics that we want to bring up with mysql team.,2,train
DM-4067,SuperTask structure implementation ,starting to implement the structure of the super task framework for process execution,7,train
DM-4068,SuperTask framework extension,incorporate configuration and data reference to implementation,4,train
DM-4069,Bootcamp meeting,i've attended lsst dm bootcamp,3,train
DM-4070,SuperTask framework documentation and  refactorization,while still prototyping i need to fill documentation on new code as well as do some clean up as well,7,train
DM-4071,testPsfDetermination broken due to NumPy behaviour change,"old numpy behaviour (tested on 1.6.2):    in [1]: import numpy    in [2]: a = numpy.array([])    in [3]: numpy.median(a)  /usr/lib64/python2.6/sitepackages/numpy/core/fromnumeric.py:2374: runtimewarning: invalid value encountered in doublescalars    return mean(axis, dtype, out)    out[3]: nan      new numpy behaviour (1.10.0):    in [1]: import numpy    in [2]: a = numpy.array([])    in [3]: numpy.median(a)  [...]  indexerror: index 1 is out of bounds for axis 0 with size 0      this breaks testpsfdeterminer and testpsfdeterminersubimage, e.g.:    error: testpsfdeterminersubimage (main.spatialmodelpsftestcase)  test the (pca) psfdeterminer on subimages    traceback (most recent call last):    file ""./testpsfdetermination.py"", line 342, in testpsfdeterminersubimage      trimcatalogtoimage(subexp, self.catalog))    file ""/users/jds/projects/astronomy/lsst/src/measalgorithms/python/lsst/meas/algorithms/objectsizestarselector.py"", line 377, in selectstars      widthstdallowed=self.widthstdallowed)    file ""/users/jds/projects/astronomy/lsst/src/measalgorithms/python/lsst/meas/algorithms/objectsizestarselector.py"", line 195, in kcenters      centers[i] = func(yvec[clusterid == i])    file ""/opt/local/library/frameworks/python.framework/versions/2.7/lib/python2.7/sitepackages/numpy/lib/functionbase.py"", line 3084, in median      overwriteinput=overwriteinput)    file ""/opt/local/library/frameworks/python.framework/versions/2.7/lib/python2.7/sitepackages/numpy/lib/functionbase.py"", line 2997, in ureduce      r = func(a, kwargs)    file ""/opt/local/library/frameworks/python.framework/versions/2.7/lib/python2.7/sitepackages/numpy/lib/functionbase.py"", line 3138, in median      n = np.isnan(part[..., 1])  indexerror: index  1 is out of bounds for axis 0 with size 0  ",1,train
DM-4074,S18 Improve Webserv,nan,79,train
DM-4075,Assemble eslint rules for JavaScript code quality control,"review and assemble eslint rules, which enforce clean javascript and jsx code.    code cleanup to avoid too many rule violations.",8,train
DM-4076,JavaScript code cleanup - remove unused packages,"remove es6promise, reactmodal, other cleanup",2,train
DM-4078,convert underscore to lodash,"lodash has become a superset of underscore, providing more consistent api behavior, more features, and more thorough documentation. we'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions",2,train
DM-4079,Create a React component which manages tabs,"we need a react component, which manages tabs. ",6,train
DM-4080,Shutdown mechanism doesn't work when logging process is disabled.,"if the logging mechanism is turned off in ctrlexecute, the ctrlorca logger doesn't get launched.  the current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process.   if the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive.",8,train
DM-4081,Lead  the Firefly conversion from GWT to React/FLUX design meeting,a focused week long design meeting on firefly conversion from gwt to react/flux.   agenda and notes here https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=41786446  design document at https:/confluence.lsstcorp.org/display/dm/fireflyclientapplication+architecture  ,6,train
DM-4082,Firefly conversion from GWT to React/FLUX design meeting,a focused week long design meeting on firefly conversion from gwt to react/flux.    produce the first draft of design document   https:/confluence.lsstcorp.org/display/dm/fireflyclientapplication+architecture,10,train
DM-4083,Firefly conversion from GWT to React/FLUX design meeting,a focused week long design meeting on firefly conversion from gwt to react/flux. design document at  https:/confluence.lsstcorp.org/display/dm/fireflyclientapplication+architecture  ,6,train
DM-4084,Firefly conversion from GWT to React/FLUX design meeting,a focused week long design meeting on firefly conversion from gwt to react/flux.  design document at https:/confluence.lsstcorp.org/display/dm/fireflyclientapplication+architecture.  ,4,train
DM-4085,Attend DM boot camp,"attend dm boot camp to learn more about dm stack, butler, and task. ",2,train
DM-4086,Attend DM boot camp,"attend dm boot camp to learn more about dm stack, butler, and task.     most of the presentations are located at url https:/community.lsst.org/t/dmbootcamp announcement/249. presentations like afw, eups, tasks, and butler are necessary to participate in lsst, so everyone on lsst must understand these concepts. look at the list of presentations covering these topics and make sure your understand them. some of the remaining talks go into more detail or cover more specialized topics. those talks should be scanned to see if they are of interestß to you.",3,train
DM-4087,Attend DM boot camp ,"attend dm boot camp to learn more about dm stack, butler, and task. ",3,train
DM-4089,Consulting in September,nan,3,train
DM-4090,update cat logging information,"the ""cat"" package has a table which the logger in ctrl_orca uses to insert information from logging messages.  the format of the log messages has changed, and therefore the table in ""cat"" needs to be changed as well.",1,train
DM-4092,Update qserv for lastest xrootd,"small api change in latest xrootd, requires a parallel change to qserv.  paves the way for dm 2334",1,train
DM-4093,Update DECam camera geometry descriptions for raw data,"the overscan and prescan regions of instcal data have been trimmed, but they are included for raw data.  the amplifier information in the current camera descriptions were made for instcal data and do not include overscan and prescan regions.      while processing raw data with the current camera descriptions, the bounding boxes from the camera object seem incorrect for raw data.     code change summary:   use nonzero overscan and prescan regions   update the pixel array layout. my schematic of pixel array layout is attached in decamampinfo.png    screenshots are postisr images processed with bias and flat correction using the old or updated camgeom.  ",7,train
DM-4094,Educational Activities for In-Depth Reusable Background,bucket epic to capture effort spent in educational activities and meetings to gain in depth reusable background knowledge for the lsst project.,10,train
DM-4095,Please port showVisitSkyMap.py from HSC,the hsc documentation at http:/hsca.ipmu.jp/public/scripts/showvisitskymap.html includes a useful script for displaying the skymap and ccds from a set of visits. it would be convenient if a version of this script was available in the lsst stack.,1,train
DM-4098,Update Trust Level of all LSST DM Staff to Level 4 via the API,it seems safe to update the discourse trust level of all members of the lsstdm group on community.lsst.org to level 4 (full permissions). see https:/meta.discourse.org/t/consequencesofusingorbypassingtrustlevelsforcompanyorganizationstaff/34564?u=jsick    this should alleviate concerns that dm staff are being prevented from fully using the forum.    this ticket implements a small notebook to exercise the discourse api to make this trust level migration possible.,1,train
DM-4099,Provide upstream improvements to sphinx-prompt,provide prs to sphinxprompt (or decide to own a fork of sphinx prompt in documenteer) that includes     an actual package you can import    better error reporting when you forget to include a class with the prompt directive,1,train
DM-4100,Replace use of image <<= with [:] in python code,replace all use of the afw image pixel copy operator <<= with \[:] in python code.    see dm 4102 for the c version. these can be done independently.,2,train
DM-4102,Remove use of <<= from C++ code in our stack,"replace usage of deprecated image operator <<= in c code with assign(rhs, bbox=box2i(), origin=parent) as per rfc102    switch from [:] to assign pixels in python code where an image view is created for the sole purpose of assigning pixels (thus turning 24 lines of code to one and eliminating the need to make a view).",3,train
DM-4105,Update user documentation,"order by, objectid in and objectid between predicates support have been improved, this should be documented.    ",1,train
DM-4112,doc & demostrate ceph setup,nan,3,train
DM-4113,add lfs remote support to lsstsw/lsst_build,"support for cloning from lfs backed repos, when indicated via repos.yaml, is needed.  ",4,train
DM-4115,Update cfitsio to 3.37 (adding bz2 support),"per rfc 105, we should upgrade to cfitsio 3.37.",2,train
DM-4116,evaluate git lfs prototype and provide feedback,nan,3,train
DM-4117,Clean up lsst_stack_docs for preview,improve the presentation of the new docs overall:    # add a creative commons license  # remove stub documents from the presentation  # put readmes in all doc directories to explain what content will go in them  # clean up and update the source installation guide to reflect 11_0,1,train
DM-4118,Local LSST IAM meeting,"october 14, 2015 (local)     drawing identity access management architecture on whiteboard.     db access via kerberos        mariadb does pam but does it do kerberos tickets?             could then simply access with a ticket    if users are exporting vms/containers, do they need keytabs?  how do we support this?    what does the id linking?    do we need replication to base site so that base site can operate independent      can we get 2 lsst vms?      ncsa cyberinfrastructure standards?  mit kerberos?  openldap?      meeting with iain goodenow    ",2,train
DM-4119,Security Meeting with LSST PO,"october 8, 2015     refresh of security plan, subplan        talk about camera subsystem refusal to buyin to security plan     iam work moving along        continue biweekly meetings with developers        most likely going with kerberos        iain meeting with jim and co on authn/z work     joint technical meeting in february        when and where?  santa cruz, feb 2224th 2016     security plan refresh:        cover email with old document and instructions        dm: don p. and jeff kantor        epo        po: iain        camera         incidents: all reports are collected and acknowledged",1,train
DM-4120,Build instance and snapshot on Nebula for HTCondor worker with v11_0 LSST stack,build instance and snapshot on nebula for htcondor worker with v11_0 lsst stack.,12,train
DM-4121,Draft IaM Recommendations,"lsst iam draft recommendations from ncsa.  group includes: jim basney, terry fleury, and dan thayer.",23,train
DM-4122,ctrl_events test failures on CentOS7 VM on Nebula,i am seeing failure to build due to test  failures for ctrl_events on a centos7 instance on the nebula openstack.  details to follow.,1,train
DM-4123,Bootcamp meeting,slides and attendance at dm bootcamp,8,train
DM-4124,Bootcamp meeting,travel to princeton and attend dm boot camp ,8,train
DM-4125,pipe_tasks/examples/calibrateTask.py fails,"the self contained example calibratetask.py in pipetasks/examples/ fails when attempting to set field ""coord"" in refcat. exact error message        11:04:19vish~/lsst/pipetasks (u/lauren/dm3693)$ examples/calibratetask.py ds9  calibrate: installinitialpsf fwhm=5.40540540548 pixels; size=15 pixels  calibrate.repair: identified 7 cosmic rays.  calibrate.detection: detected 4 positive sources to 5 sigma.  calibrate.detection: resubtracting the background after object detection  calibrate.initialmeasurement: measuring 4 sources (4 parents, 0 children)   traceback (most recent call last):    file ""examples/calibratetask.py"", line 150, in /      run(display=args.ds9)    file ""examples/calibratetask.py"", line 119, in run      result = calibratetask.run(exposure)    file ""/home/vish/lsst/lsstsw/stack/linux64/pipebase/11.02g8218aaa5/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/vish/lsst/pipetasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run      astromret = self.astrometry.run(exposure, sources1)    file ""examples/calibratetask.py"", line 90, in run      m.set(""coord"", wcs.pixeltosky(s.getcentroid()))    file ""/home/vish/lsst/lsstsw/stack/linux64/afw/11.05g97168e01/python/lsst/afw/table/tablelib.py"", line 2372, in set      self.set(self.schema.find(key).key, value)    file ""/home/vish/lsst/lsstsw/stack/linux64/afw/11.05 g97168e0+1/python/lsst/afw/table/tablelib.py"", line 1064, in find      raise keyerror(""field '%s' not found in schema."" % k)  keyerror: ""field 'coord' not found in schema.""      note that wcs.pixeltosky(s.getcentroid()) is set to fk5coord(15.007663073114244  afwgeom.degrees, 1.0030133772819259   afwgeom.degrees, 2000.0)",1,train
DM-4127,Mask overlay does not work when the image is flipped,nan,2,train
DM-4129,Database connection problems in daf_ingest,"the dbauth connection fallback in ingestcatalogtask passes the ""password"" keyword argument to mysqldb.connect instead of ""passwd"", which fails. also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to mysqldb. this results in a type error. ",1,train
DM-4131,"Have a ""mark as current"" option in lsstsw","russell explained to me the advantage of having a specific eups tag associated with a given lsstsw installation. however, it would be very handy to have a way to get all installed packages automatically tagged as current as part of the installation process.    i suggest a ""c"" option to lsstsw which will tag everything as current after the full installation is complete. this way, partially installed packages won't get marked current, and people who do full installations can not have to deal with having to say ""t bblah"" every time they setup things.",4,train
DM-4133,Change type of LTV1/2 from int to float when writing afw images to FITS,"the ltv1/2 problem is originally my bug.  i used integer ltv1/2 in    afw/src/image/exposureinfo.cc:    data.imagemetadata>set(""ltv1"", xy0.getx());  afw/src/image/exposureinfo.cc:    data.imagemetadata>set(""ltv2"", xy0.gety());    whereas a more careful reading of the noao page  introducing them includes floating point examples.    the fix is to cast the xy0 values to float.  i'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial.  ",1,train
DM-4135,DHT prototype: HTTP server library refactor/cleanup,"this experimental library turned out to be quite useful.  next stage of prototyping will be making greater use of this, and i anticipate this library will also be used in production code.  spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team.",6,train
DM-4136,DHT prototype: test fixture rework,"the dht test fixture, developed during the preliminary work with kademlia, needs some updates for the next stage of the prototype:   adapt to reworked http library   adapt dht interface to be more generic  ",6,train
DM-4137,"Update DECam CCDs gain, read noise, and saturation values","the values of decam gain, read noise, and saturation value need to be updated.     this ticket is to update them in the detector amplifier information, which is used in isrtask.     talked to robert gruendl. these values should take precedence over the values in the fits header. they seem stable and do not seem to vary with time.   ",1,train
DM-4138,Re-implement packed keys in CSS,current implementation of jsonpacked keys in css has one complication  the name of the container for packed keys is the same as the key itself (plus .json suffix). this complicates handling of the subkeys because these keys need to be filtered out and these names are all different. it would be better to have easily identifiable packed key names.,4,train
DM-4142,Discover how to create Doxygen XML in build system,we need xml output from doxygen in order to import existing api documentation into sphinx. in this story i find out how to achieve this within the stack build system.,2,train
DM-4143,Demonstrate using Breathe for Python & C++ API reference in New Docs,demonstrate use of breathe for utilizing the existing doxygen api documentation in the new sphinx based doc platform.,3,train
DM-4145,Reduce scons output in qserv,"yesterday andyh expressed a valid concern that qserv prints too much info which makes it hard to find errors. by default scons prints whole command line for c compilation and linking which are quite long (~half screen depending on your screen size). most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""compiling something.cxx"" and have an option to print full command with verbose option.",1,train
DM-4146,Networking requirements for design,meeting with paul to discuss remaining network design.,1,train
DM-4147,Finance Contract Discussions,discussing updated contract hoops and game plan.,1,train
DM-4148,ICI agenda and mtg,nan,1,train
DM-4150,Investigating procurement of individual components,component breakdown and explanation of components as part of fy16 purchases in order to plan optimal purchasing through vehicles available to ncsa.,1,train
DM-4151,Search for uses of current afw.wcs in the stack,"search through the stack for all the uses of our wcs implementation (wcs, tanwcs, makewcs, and any other hidden objects) and make a list of all of those uses (on community for example). this list should note whether the usage is in c or python.",2,train
DM-4152,Document detailing usage of Wcs in the stack,"the information from dm 4151 should become a brief report on the kinds of usage of wcs in the stack. this could be posted to community or confluence, or it could be a brief latex document attached to the afw repo.    included in this report should be whether each current usage requires c, or whether it could be done with e.g. vectors returned from python.",4,train
DM-4153,RFD to collect current and future use cases of Wcs,"file an rfd requesting information about current and possible future use cases for a wcs system in the stack. this should get feedback from, at minimum, the alerts pipeline, drp, and level iii data producers. it should also get feedback from our resident wcs experts.    whether those use cases are currently implemented in our code, or could be generalized from it, isn't important to this rfd as this information will feed into a subsequent requirements document and rfc about what needs to be written/changed.    some use cases/buzzwords that will likely be included:    efficient x,y  > x',y'    stacking a sequence of transformations efficiently    easy extensibility    should color terms be included in the wcs or dealt with elsewhere?    pixel distortion effects (e.g. tree rings, edge roll off)    simultaneous astrometry (i.e. from image stacks)",4,train
DM-4154,Sever side Histogram for variable bin size,"for any given column or columns (expression of columns such as col1+log(col2) ) of a ipactable data, the variable bin histogram is needed.  the variable bin is based on ""bayesian blocks"" algorithm (http:/jakevdp.github.io/blog/2012/09/12/dynamicprogrammingin python/).  the output is a new ipactable (datagroup) with three columns, numpoints, binmin and binmax.",6,train
DM-4155,LSST Wcs requirements document,"based on the information compiled from dm4153 and dm4152, prepare a requirements document (latex, with references) describing all of the known wcs requirements for the various portions of the lsst stack. this document could live in afw or in its own repository, and could potentially become a published technical report/conference proceeding.",10,train
DM-4156,Evaluate existing Wcs libraries and report on our options,"as part of the requirements document in dm4155, we need a reportlikely section(s) of that same documenton the currently available wcs libraries (including, but not limited to ast starlink and astropy.coordinates) and summaries and references from the current literature about how other surveys and projects have managed their wcs.    for each of the currently existing options, the report should include at a minimum:    implementation language.    api and languages that can interface to that api.    how well supported (including number of contributors) is the project and how active is ongoing development.    its performance and optimization for both scalar and vector calculations, and its scalability to large data sets.    we also need to look for and/or request technical reports and other literature from existing and completed surveys, including des (robert gruendl), panstarrs (paul price), stsci (erik tollerud), and sdss (lupton and/or blanton?).",30,train
DM-4157,Provide a recommendation for how to manage Wcs in LSST,"the technical report from dm4155 and dm4156 should have as its conclusion a recommendation for what wcs system the lsst stack should adopt (either our own implementation, a third party library, or some combination thereof). beyond the conclusion section of that report, this will be provided as an rfc that includes:      executive summary    mock api    links to the relevant documentation (beyond just the above technical report)    a bullet list of the rationale for this decision    the conclusion of this rfc represents the end of this epic.",4,train
DM-4160,Unused variables in meas.algorithms.utils,"pyflakes 1.0.0 reports:    $ pyflakes 2.7 utils.py  utils.py:232: local variable 'chi2' is assigned to but never used  utils.py:481: local variable 'numcandidates' is assigned to but never used  utils.py:482: local variable 'numbasisfuncs' is assigned to but never used  utils.py:487: local variable 'ampgood' is assigned to but never used  utils.py:492: local variable 'ampbad' is assigned to but never used    in the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. please establish this for each one, then either remove them or fix the rest of the code.",1,train
DM-4162,Please implement a warper that works with a single XYTransform,"at present we only warp images based on a pair of wcs. this is needlessly restrictive. we should be able to define the transformation by function that computes f(x,y)  > x',y', e.g. an xytransform.    note that reversibility, while not strictly necessary, is very desirable. hence we might as well use xytransform.     i suggest we have only one underlying implementation in order to avoid code duplication. this could easily be done by implementing an xytransform that combines a pair of wcs.",6,train
DM-4164,Create a skeleton framework for Firefly using redux and react.,"as part of the gwt conversion to pure javascript, we've came up with a new design based on redux and react.  this task is to create all of the major components of the framework with minimal functionalities.  this will allow other developers to build upon this foundation core.",10,train
DM-4165,Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1,"under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https:/svn.boost.org/trac/boost/ticket/11622).    a patch for this is already committed upstream in boost.  it is proposed that we take this patch into the lsst t&p in interim until the next official boost release.",1,train
DM-4166,Internal documentation of procurement process,nan,1,train
DM-4168,"productize ""Data repository selection based on version""",finish & productize work from dm 5608,6,train
DM-4170,Butler: move configuration (.paf) file into repository,"we want to be able to keep the .paf file in the repository, with the data for which it provides configuration information.    at least for the time being it needs to be optional; it looks first in the repository and second in the ""old"" location for the paf.    in the case of a repository chain, the child should 'win'.",12,train
DM-4171,Butler: change configuration from .paf to something else,"for ""something else"" there are 2 major options: pex_config (dislike because the 'data' gets executed), and yaml (very well supported, is just data). 3rd option: we could use the sqlite registry, and define a schema for that.     (sqlite benefits: has support for write locks that will be necessary very soon. except, need writeoncecomparesame mechanism; so we'll need that 1. for normal files and 2. across n nodes. so maybe there's no benefit).    also, while we're there:  as noted in dm4170, registries.py needs to not use astropy and instead should use pyfits. if it's quick, that change should be made here (or turned into a separate ticket)",12,train
DM-4172,Build AL2S vlans from Miami to NCSA,to prototype the layer 2 circuit lsst will eventually have.,2,train
DM-4174,document proposal for Base site to NCSA data transfer,"with steve and james distill the actual data movement requirements, the mechanism to broker those transfers and the network technology to ensure the real time transfer of images",2,train
DM-4175,Provide network infrastructure support to deployment of new nodes,nan,3,train
DM-4176,Create baseline requirements for evaluating server hardware from different vendors,nan,1,train
DM-4177,Refine server evaluation specifications into a usable quantifiable form,nan,1,train
DM-4178,Discussions with Ron on Base site architecture,nan,1,train
DM-4179,update LSE-78 once current updates are applied,there are several sections with inaccurate information about the north american portion of the lhn and the implementation of the networking into ncsa as well as the base site commissioning cluster architecture.,2,train
DM-4180,Butler: provide API so that a task can define the output dataset type,"the task needs to be able to specify everything that needs to be in the policy file so that the butler can put and get data for a new dataset type.    consider that the policy data can be split between the cameraspecific and the taskspecific parts. (kt was thinking of calling the camera part the ""genre""), this potentially reduces the amount the task has to specify.   another option is to hard code some some of the policy in the butler itself:   the path template (it could be assembled out of the data id components)    if it's hard coded the task must pass the component dataid keys   if it's not hard coded the task must provide a template    python type    storage type    there could be user overrides too.",20,train
DM-4181,Butler: add support for skymap based dataIds,"load the skymap from the repository and use it a la metadata lookup to complete lookups. for example if the script wants all the patches that are in tract x, look in the skymap to get that information.    will be something like:  create a skymap object given a configuration (in the repository) (it has its own dataset type)  create a registry with that skymap object  use that registry to lookup skymap related parameters. might need to add to the policy file that a given configuration uses the skymap.    requires that data is already indexable by tract & patch, the work is adds iteration over tract/patch specified by the skymap.    example use case:          mybutler.querymetadata(datasettype, key, format, dataid)  }  ",15,train
DM-4183,Experiment with memcached for secondary index,"test whether memcached could be used to serve objectid > chunkid mapping, in particular from the performance perspective.",6,train
DM-4184,Experiment with xrootd for secondary index,"test whether xrootd could be used to serve objectid > chunkid mapping, in particular from the performance perspective.",6,train
DM-4185,Experiment with c-style arrays for secondary index,"use simple cstyle array (chunk[objid], allocated up to maximum number of objectids), or a singlelayer set of arrays (chunk[block][objid%blksize], where second index runs from 0 to size of each block) to store index compactly and provide minimum overhead for lookups.",5,train
DM-4188,Infrastructure Security at Site,nan,1,train
DM-4189,Outline of data flow,outline of data flow between chile>ncsa and ncsa>chile.,3,train
DM-4190,L1 system functions and responsibilities ,outline functions and responsibilities for all parts of the l1 system,35,train
DM-4191,Initial code change to run ISR with DECam raw data,"this ticket is for implementing changes in obsdecam in order to run processccd with raw decam data.    changes are mostly in decammapper and a new class decamisrtask is added. a test to retrieve defects with butler is also added. (testdatadecam is at lsstdev /lsst8/testdatadecam/ )    the pixels with bit 1 (bad, hot/dead pixel/column) from the community pipeline bad pixel masks are used as bad pixels.  the cp bpm fits files are directly used as defect files. due to their large size, they probably should not go into obsdecam repository so they are treated similarly as other calibration products.     with the changes of this ticket, the following ingest defects files into calibregistry:    ingestcalibs.py . calibtype defect pathtobpm/ fits      and the following should run past isr:    processccd.py .  config isr.dofringe=false isr.assembleccd.setgain=false calibrate.dophotocal=false calibrate.doastrometry=false calibrate.measurepsf.starselector.name=""secondmoment""      running fringe correction with decam raw data will be in future tickets (dm4223 and possibly more). also this ticket does not cover implementing or porting new isr functionalities that haven't yet been included in ip_isr (such as crosstalk).   ",14,train
DM-4192,Other LOE -- Oct 2015,"local lsst group meetings, ethics training, or other local meetings or tasks to comply with ncsa policies",2,train
DM-4193,Automate LSST Firefly standalone releases using Jenkins,"this task involves merging in feature branches, building firefly standalone, tagging, push changes to github, generating changlog, and using github api to publish the release.  release should be attached to the latest tag with downloading artifacts and changelog.",7,train
DM-4194,Python LogHandler does not pass logger name to log4cxx,"not sure how or why it happened, but presently python loghandler for lsst.log does not pass logger name to log4cxx layer and all messages from python logging end in root logger. ",1,train
DM-4195,Build proof-of-concept package documentation for lsst.afw,"build package documentation for lsst.afw under the new doc platform as a demonstration.     install a sphinx site in lsst.afw/doc   implement mvp documentation pages for lsst.afw packages (table, image, etc.)   c api reference from doxygen+breathe   python api reference from numpydoc    this ticket will not attempt to add new documentation content; only to show how existing content can be re organized.",2,train
DM-4196,Build ltd-mason for running a multi-package software documentation build,"lsst the docs (http:/sqr006.lsst.io) is a system for extending our existing jenkins build infrastructure to build sphinxbased software documentation for our eupsbased packages. this is a ticket to implement the ltdmason service, which runs on jenkins after the buildlsstsw.sh step and compiles software documentation.    specific outcomes of this ticket are     full specification of the yaml interface between ltdmason and buildlsstsw.sh (including creating a mock yaml file for local testing)   demonstration of a science pipelines documentation build on a local lsstsw environment (in conjunction with content from dm4195)   accommodations to the science pipelines documentation repo and documenteer for building sphinx packages are included in this ticket’s scope.    next steps are     standing up the service on jenkins and testing integration with buildlsstsw.sh   uploading to s3 (which involves building integration with ltd keeper.",17,train
DM-4197,"Improve Qserv master robustness for queries like ""select @@max_allowed_packet""","this king of query crashes qserv:      mysql host 127.0.0.1 port 4040 user qsmaster  e ""select @@maxallowedpacket""  ",1,train
DM-4201,Documentation and technical debt in meas_base/PixelFlags.cc,"the port from hsc of safeclipassemblecoaddtask has left some documentation and code quality changes to be made. specifics include:     in the process of porting, functionality was added which allowed users to specify additional mask planes to be converted to pixel flags. however this was fundamentally incompatible with the flag handler functionality that lsst was currently using. pixelflags was thus modified to allow safeclipassemble coadd to work with the user defined masks, but that made it fundamentally different than the other plugins. in the future this plugin should be brought more in line with all the other measurement framework. this will most likely involve rewriting sections of the measurement framework, to add the ability for users to more directly set runtime behaviors of the measurement plugins (such as specifying non default mask planes to work with).   because pixelflags could no longer use the the flag handler framework, sections of the safecentroidextractor had to be duplicated within pixelflags. this duplicated code is non ideal and should be rectified. possibly in the process of rewriting the measurement framework, the utils could be expanded to have convenience methods to access functionality when not using flag handlers    as with all of the measurement framework, pixelflags needs better documentation. this includes some line to line comments, but more importantly the over all functionality of the routine needs documentation. this includes: what the routine does; how it works; and why various design decisions were made",3,train
DM-4202,Revert temporary disabling of CModel in config override files,revert the temporary disabling of cmodel that relates to a bug noted in dm4033 that was causing too many failures to test that processccd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in dm2977 & dm 3821).     relevant files:    config/processccd.py   config/forcedphotccd.py   config/forcedphotcoadd.py   config/measurecoaddsources.py    ,1,train
DM-4206,wmgr should delete database from inventory when dropping it,when wmgr drops database it should also cleanup chunk inventory for that database.    ,2,train
DM-4208,Research web authentication and authorization and gather usage stories,"research sui, dax, butler, and qserv authentication and authorization requirements and schemes. document usage stories for all layers",6,train
DM-4209,Create unit tests for SafeClipAssembleCoadd,"porting safeclipassembecoadd from hsc to lsst left that functionality without a unit test. currently assemblecoadd is tested from within tests/testcoadds.py. this test does not call assemblecoadd directly however. the actual code pertaining to assembling a coadd exists within python/lsst/pipe/tasks/mocks/mockcoadd.py. this called from testcoadds, and is used to build and coadd synthetic images from known psfs amongst other things. this should be expanded to test both assemblecoadd and safeclipassemblecoadd, possibly with some sort of argument to the function call. it is important to keep testing both methods of generating a coadd.",3,train
DM-4210,Create documentation and examples for SafeClipAssembleCoadd,"safeclipassemblecoadd in hsc did not have adequate documentation, and thus neither does lsst post port. documentation which details the functionality and usage of this function should be created, and should be available either through the https:/lsst web.ncsa.illinois.edu/doxygen/xmasterdoxydoc/grouplssttask_documentation.html or its successor . examples of the usage and various options should also be included with the documentation.",4,train
DM-4212,Investigate shear bias errors from DM-1135,"dm 1135 mostly was inconclusive, due to the fact that the error bars were around the same size as the differences in many of the tests.  this in spite of the fact that we ran 6x as many sample galaxies as great3sims.    investigate the reason for this, and see if we can estimate the errors more accurately.    ",6,train
DM-4213,Rerun tests of DM-1135 with a larger number of galaxies,"as a result of the new error estimation, it became apparent that a larger number of sample galaxies were required in dm1135.  this is an expansion of that test to a larger number of galaxies.  since the original  great3 tests had about 2 million galaxies, we should be able to do these test reasonably well with the 6 million galaxy pool created in dm1135.  however, the measurements need to be rerun in some cases, and the error analysis done again.",6,train
DM-4214,Initial tests of ShapeletApprox,"this will be part of a wider set of test which i am hoping that jim will fill in as additional stories under dm1136    basically, we hope to see how extensive a shapelet approximation must be done for a psf in order to reach a stable result (stable meaning it would not markedly improve with increase in shapelet order).    for this intial test, i propose to compare singlegaussian, doublegaussian, full, and 2 higher order models.  i will also add a model with inner and outer defined to see if those have a significant effect.  it will tell use how the existing models line up and which parameters matter.    this test will be done using the great3sims subfield organization, and with a single, randomly chosen psf image from the 0.7 arcsec fwhm library (raw fwhm from phosim, the actual fwhm is closer to 0.8 arcsec)     this will probably be just the beginning of more extensive shapeletapprox tests, so this story should be a substory of a larger test project which i am hoping jim will define.",6,train
DM-4215,Butler: SafeFile and SafeFileName can overwrite good with bad in some cases,"a bad file can be written in butler, in the case where 2 temp files that use safefile or safefilename to the same location are started, one closes, and then the other fails   and then closes and writes the bad file. need to handle the exception in a way that does not write.    also, in the case of no exception (failure), when closing b, need to compare to a and throw if different.",4,train
DM-4219,Package capnproto for eups,"prototype is now getting to the point where a wireprotocol package like capnproto or protobuf is needed.  capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf>capnproto at some point.    this task is to go ahead and get capnproto packaged and published for use in the replication prototype.",2,train
DM-4220,Convert copyright/license statements to one-liners for RFC-45,"refactor how we manage copyright and license information in stack repositories    # identify a list of packages to process  # build and test an automated system of        adding a global copyright to each repo. content will be “copyright yyyyyyyy the lsst dm developers”. years will be determined by git history.      adding a gplv3 license file to each repo.      changing the boilerplate in all files to say ""see the copyright and license files in the toplevel directory of this package for notices and licensing terms.” use https:/gist.github.com/ktlim/fdaea18ab3d39afdfa8e      automatically branch, commit, merge and push    and deploy this automated system.",8,train
DM-4222,X16 Secondary Index - Implementation,implement / productize optimizations to secondary index proposed through dm 2119,27,train
DM-4223,IsrTask calls removeFringe in FringeTask but the method does not exist,the method removefringe of fringetask is called in isrtask but there is no removefringe.      not sure if removefringe was meant to be a place holder,1,train
DM-4224,getExposureId not implemented in obs_lsstSim,there is no implementation of the getexposureid method in processeimage.py.  this causes it to fail using a modern stack.,1,train
DM-4225,Collect single-host performance data for secondary index,"run production scale (billions of entries) tests on different index options, collect performance statistics for allocation (cpu, memory) and for queries.",3,train
DM-4226,Set up multi-host tests for secondary index technologies,"for clientserver technologies (memcached, xrootd, etc.), develop multihost test jobs to exercise productionscale indices (billions of entries, millions of queries).  index should be allocated on single ""master"" machine, with queries generated from one or more separate machines, possibly with multiple threads/jobs per machine.",5,train
DM-4227,Experiment with bulk updates to secondary index,"the nightly data loader job may need to add new objectids, or change the chunks of existing objectids, en masse.  develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data.  this is significant for client server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements.",7,train
DM-4228,Collect multi-host and bulk-update performance data for secondary index,"run secondary index tests across multiple hosts (server and clients), collecting performance data for production scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates.",5,train
DM-4229,Identify candidate technology for secondary index,"evaluate results of production scale performance tests, both single and multiple host.  identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements",3,train
DM-4230,Port HSC-1355: Improved fringe subtraction,"https:/hscjira.astro.princeton.edu/jira/browse/hsc 1355: ""with this fix, we get much  better fringe subtraction"".",2,train
DM-4231,Data Distrib proto (nov),nan,20,train
DM-4232,Variance is set after dark subtraction,"in the default isrtask, the variance is currently set after dark subtraction.  this means that photon noise from the dark is not included in the variance plane, which is incorrect.  the variance should be set after bias subtraction and before dark subtraction.     also points out (dm4191) that the assembleccdtask with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after fullframe bias subtraction.  i believe that assembleccdtask only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but i suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity).  i therefore suggest that this effective gain calculation be stripped out and that assembleccdtask not require variance planes.",5,train
DM-4234,HSC backport: Jacobian and focalplane algorithms,"add algorithms to compute the jacobian correction for each object (calculable from the wcs, but sometimes convenient)  and record the focal plane coordinates (instead of ccd coordinates) for sources (useful for plotting).    the standalone hsc commits to be cherrypicked are:    jacobian  measalgorithms  may 3, 2013  https:/github.com/hypersuprimecam/measalgorithms/commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454  may 24, 2013  https:/github.com/hypersuprimecam/measalgorithms/commit/ecad0d2559bb9815fc5560234f4502f35f50db73  may 28, 2013  https:/github.com/hypersuprimecam/measalgorithms/commit/7f3db53b56279929b9e416173ed09cf00dc81406    obssubaru  may 3, 2013  https:/github.com/hypersuprimecam/obssubaru/commit/d0969911ee1a655fd82998f0b936fa90f443d2fd  may 6, 2013  https:/github.com/hypersuprimecam/obssubaru/commit/e36bd1b4410812ca314f50c01f899d92acc0e7a5      focalplane  measalgorithms  may 24, 2013  https:/github.com/hypersuprimecam/measalgorithms/commit/dda3086f411d647e1a3e15451d7f093cd461873a  may 25, 2013  https:/github.com/hypersuprimecam/measalgorithms/commit/57d718bf51b255adf5789e389dfb776ecaa062d1  nov 21, 2014  https:/github.com/hypersuprimecam/measalgorithms/commit/95627d55cb7d64718a42027954474df5c3661a65    obssubaru  may 24, 2013  https:/github.com/hypersuprimecam/obs_subaru/commit/d999a32e7e10b25cceccc94b61890486f96c0bfd  ",4,train
DM-4235,HSC backport: countInputs and per object variance functions,"back port of the following two hsc tickets:    countinputs  https:/hscjira.astro.princeton.edu/jira/browse/hsc1276  measalgorithms  jul 1, 2015  https:/github.com/hypersuprimecam/measalgorithms/commit/51db0fd2624c7f9b641c93aa3cf6366539995d50    obssubaru  sep 24, 2015  https:/github.com/hypersuprimecam/obssubaru/commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda    variance  https:/hscjira.astro.princeton.edu/jira/browse/hsc1259  measalgorithms  jul 2, 2015  https:/github.com/hypersuprimecam/measalgorithms/commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff7ae7aea69b58dbf075350ccfd3802cfb    obssubaru  oct 19, 2015  https:/github.com/hypersuprime cam/obssubaru/commit/cf1e80958bb9164dacf42d2d35a94dd366c78892  ",6,train
DM-4236,Specify default output location for CmdLineTasks,"when neither \output or \rerun is specified as an argument to a cmdlinetask, any output from that task appears to be written back to the input repository. note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables.    the hsc stack behaves differently, using $input/rerun/$user as a default output location. a https:/community.lsst.org/t/newargumentparserbehaviorrerunflagintroduction discussion/345 suggests that this is the preferred behaviour.    please update the lsst stack to match the hsc behaviour.",2,train
DM-4237,unable to upload images to nebula,"i seem to be unable to upload an image to neblua from a url via either horizon or the nova cli client.  the request seems to queue briefly and then reports a status of killed. eg      glance imagecreate name ""centos7.1vagrant"" diskformat qcow2 containerformat bare progress copyfrom http:/sqrekvmimages.s3.amazonaws.com/centos7.1x86_64 ispublic false mindisk 8 min ram 1024  ",2,train
DM-4238,Fix integer casting error in numpy version 1.10 in obs subaru,fix type casting in obssubaru in lates numpy in obssubaru,1,train
DM-4239,Identify Qserv areas affected by secondary index,evaluate qserv software for the czars and workers to identify where an interface to the secondary index will be required for efficient operation.,5,train
DM-4240,Implement secondary index service in Qserv,"implement the selected seconary index technology (see dm2119) as a qserv service, providing a client api to be used elsewhere in the qserv system.  depending on the chosen technology, this may including configuration of a separate lightweight server for the secondary index, creation of database tables, etc.  the clientside api should be technology independent.",10,train
DM-4241,Implement bulk updating of secondary index in Qserv,"provide a service in qserv to support creation or modification of secondary index objectid chunk pairs in bulk, to support data loading.",4,train
DM-4242,Implement secondary index query in Qserv,"implement query of secondary index in qserv, using the technology selected in dm 2119.",4,train
DM-4243,Draft of Configuration Solicitation,draft of solicitation to be used for quote and configuration from multiple vendors for fy16 purchase. ,1,train
DM-4245,Image Viewer memory leak,"when reloading the same 500mb raft image into an image viewer (see the script below), it was discovered that single node firefy server with 3g memory runs out of memory after ~15 reloads    test case: keep reloading the html file with the following javascript, creating an image viewer with 500mb image:    function onfireflyloaded() );  }    follow up:    the bug was traced to java.awt.image.bufferedimage objects not being evicted from vissharedmem cache.    further search showed that java.awt.image.bufferedimage (along with java.io.bufferedinputstream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignoresizeof.txt, which lists the classes that have to be ignored when calculating the size of cache.    testing on single node server (vissharedmem cache is not replicated), using [host:port]/fftools/admin/status page:    before (java.awt.image.bufferedimage was commented out in ignoresizeof.txt)    after 14 reloads:  memory     used                      :      3.7g     max                       :     3.55g     max free                  :    488.0m     free active               :    488.0m     total active              :     3.55g     caches:    vissharedmem @327294449   statistics     : [  size:15  expired:0  evicted:0  hits:246  hitratio:nan  heapsize:1120mb  ]  out of memory on next reload    after the change (commented java.awt.image.bufferedimage in ignoresizeof.txt)    after 36 reloads:  memory     used                      :   1672.9m     max                       :     3.55g     max free                  :   1968.0m     free active               :   1468.0m     total active              :      3.6g    caches:    visshared_mem @201164543   statistics     : [  size:3  expired:0  evicted:34  hits:659  hitratio:nan  heap size:1398mb  ]    ",2,train
DM-4246,Local LSST Sec Meeting,local cyber security meeting at ncsa with dm group.,2,train
DM-4247,Image viewer: choosing pixel interpolation algorithm for scaled images,"pixel values are defined at integer coordinate locations. this means that when an image is rendered in a scaled, rotates, or otherwise transformed coord. system, an interpolation algorithm should be used to provide a pixel value at any continuous coordinate.    currently, firefly is using     renderinghints.keyinterpolation = renderinghints.valueinterpolationnearestneighbor,    which means that when an image is rendered in a transformed coord. system, the pixel value of the nearest neighboring integer coordinate sample in the image is used.     ""as the image is scaled up, it will look correspondingly blocky. as the image is scaled down, the colors for source pixels will be either used unmodified, or skipped entirely in the output representation.""    jon thaler's team would like to be able to choose a different interpolation algorithm, depending on the situation.      as an example see various resize algorithms in [thttp:/stackoverflow.com/questions/4756268/howtoresizethebufferedimagengraphics2dinjava].",4,train
DM-4248,LSST PO Security Meeting,bi weekly meetings with po to discuss cyber security issues in lsst.,1,train
DM-4251,Please include obs_subaru in CI,obs_subaru should be included in the ci system.,1,train
DM-4252,Create GitLFS Technical Note,create a square technical note describing the architecture of the gitlfs service implementation.,2,train
DM-4253,Literature search for DCR -- Reiss,go through the literature to find relevant seminal papers on dcr.    the outcome will be a bibliography and executive summary.  this should be posted on discourse.,15,train
DM-4254,Implement simple reference index files,"we would a very simple way to make small reference catalogs for astrometry and photometry. the use case is testing and small projects, where the overhead of making a full up a.net index file (or whatever we replace that with) is excessive.",5,train
DM-4255,Determine scope and requirements for CalibrateTask redesign,"there are two stories in dm 3579 that define two requirements for the redesigned calibratetask.  those along with the requirement that calibratetask be less brittle are a starting point for what the new task needs to do.  all other requirements should be gathered using the rfc process.    using the input from the rfc and other requirements, the scope for this particular redesign will be defined and stated in a discourse thread.",10,train
DM-4256,Sphinx support of sqr-001 technical note,support the distribution of a technical note sqr001     remove oxford comma in author list (documenteer)   solve issue where title is repeated if the title is included in the restructured text document   solve issue where name of the html document is readme.html,1,train
DM-4258,Investigate current dipole measurement examples and tests,there are tests in ipdiffim/tests/dipole.py for the dipole fitting.  there is also an example in the examples directory of ipdiffim.  this story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are.    an outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.,4,train
DM-4259,Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement,this will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values.  this may be simply refactoring the current tests.    this task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.,5,train
DM-4261,F17 Integrate L1 Database with Alert Production pipeline,integrate the prototype of the l1 database built through dm2036 with the alert production pipelines.  the design of the alert production l1 database is covered http:/ldm135.readthedocs.org/en/master/#alertproductionanduptodatecatalog.,79,train
DM-4262,Integrate qserv docs into the new doc system,"see frossie 11/04/2015 email to qservl list:    5 docs. so you guys have a sphinx site. fab, that’ll make it super easy to drop it into the new doc system that hosts sphinx on readthedocs  you can see a small example here http:/sqr001.lsst.codes/en/master/   the idea is to continuously deploy the docs so the release step should be very lightweight and doable via api calls. (my holy grail is to do a release with no local checkout involved).  ",2,train
DM-4263,SQuaRE design meeting 1,hold an in person design discussion with members of the square team.  ,4,train
DM-4264,SQuaRE supertask design meeting 2,hold a teleconference design discussion with members of the square team.  ,1,train
DM-4265,Create SuperTask design proposal,"create a design document covering the initial supertask concept and a scheme for how it would lead to solutions to longer term pipeline construction problems, support provenance recording, etc.",16,train
DM-4266,Should only read fringe data after checking the filter,the fringe subtraction is not necessarily performed if dofringe is true. it is only if the filter of the raw exposure is listed in config fringe.filters.      fringe data should not be read unless the filter is indicated. there are likely no such filter data and it would cause runtime errors.      seems related to changes from rfc26 and dm1299. ,8,train
DM-4267,Adapt an existing task to be usable as a SuperTask,"either by wrapping, or by converting, make an existing task usable as a supertask subclass so that it can be run under an activator.",8,train
DM-4268,FY20 Define Procedures and Build Tools for Schema Evolution,"as described in http:/ldm135.readthedocs.org/en/master/#dataproductionrelated requirements, the database system must allow for occasional schema changes for the level 1 data, and occasional changes that do not alter query results for the level 2 data after the data has been released. this epic involves preparing for dealing with schema changes in production syste.",79,train
DM-4269,Week end 10/03/15,"support for lsst dev cluster, openstack, and accounts  for week ending october 3, 2015.",1,train
DM-4270,Week end 10/10/15,"support for lsst dev cluster, openstack, and accounts  for week ending october 10, 2015.",4,train
DM-4271,Week end 10/17/15,"support for lsst dev cluster, openstack, and accounts  for week ending october 17, 2015.",3,train
DM-4272,Week end 10/24/15,"support for lsst dev cluster, openstack, and accounts  for week ending october 24, 2015.",7,train
DM-4273,Week end 10/30/15,"support for lsst dev cluster, openstack, and accounts  for week ending october 30, 2015.",3,train
DM-4274,Planning for new equipment setup (week end 10/03/15)," planning for hardware upgrades in racks   evaluate the setup of the new storage server installs lsststore101, lsststore141,lsststore143,lsststore144    set up and tested esxi server on new mac pro. worked on getting mac os x installed inside of esxi on the new mac pro hardware. found solution, need to find a better one if we are expecting to bring up and tear down instances on demand.",5,train
DM-4275,New equipment setup (week end 10/10/15)," planning on how to start setting up for new equipment     started to coordinate with josh hobblitt on what has been ordered and delivered    received 15 dell r730 servers (plan to set up in temporary rack before thursday’s maintenance)   installed spare rack at the east end of ncsa 3003row a. added three 125v 30a drops to spare rack   resolved problems with puppet on lsststor101   updated os on lsststor101, lsststor142144   installed zfs on lsststor101, lsststor142144   checking config of stor142 144 and stor101 for thursday outage    more esxi testing with mac pro",6,train
DM-4276,New equipment setup and regular maintenance (week end 10/17/15)  ," unpack and mount 6 dell r730’s   update the bios and firmware on 6 dell r730’s   install esxi 6.0 on 6 dell r730’s   rearranged hardware in ncsa 3003, move systems around in racks, reconnect power and networking, troubleshoot startup issues    applied kernel and other software updates to infrastructure as it was moved   setup vsphere vcenter server and compute nodes    discovered need to order drive caddies for lsst10 and updated idrac enterprise license for 15 new r730 servers",12,train
DM-4277,New equipment setup and configuration (week end 10/24/15)  ," unbox and mount six ups in racks, mount two new power panel pdu's for 30 amp service, connect power cabling to ups and to each of the supported systems. todo: complete the setup on each of the servers.   unbox and mount in the rack two r730 servers, connect network and power   itemized list of new dell servers received   debugging networking issues on new lsstesxi1 server  confirmed it's not an issue with switch ports or cables   setup vsphere virtual networking and moved 4 test vms to new setup   setup vsphere data protection (vmware backups via snapshots)",6,train
DM-4278,New equipment setup and configuration (week end 10/30/15), moved spare rack to row c   documented setup of new lsst vsphere setup (https:/wiki.ncsa.illinois.edu/display/lsst/lsst+vsphere)   debugging networking issues on new lsstesxi1 server  testing with new/alternate hardware   installed 6 new drives for historical log storage on lsst10 mysql server,5,train
DM-4279,bi-weekly IaM meeting,meeting oct 22nd  notes on lsst confluence,1,train
DM-4280,Bootcamp meeting,iam group attended dm bootcamp,4,train
DM-4281,"write meeting agenda, for Oct 19 meeting ",nan,1,train
DM-4282,"Conduct, document, initial follow through on Oct JCC meeting ",nan,1,train
DM-4283,Visit to Argonne - facility coordination,nan,2,train
DM-4284,Specify L1 system - design and WBS plan for construction,"spent 3 days in focused planning to knock out version 1 of the l1 system plan.  sps cover me, don.",12,train
DM-4286,Processing of DECAM COSMOS field - Part I,"this story covers work on the verification plan done in october.     the decam cosmos field was selected, however decam isr is not available so the starting point for now is community pipeline reduced data. have put the data through processccddecam and makecoaddtempexp but had a number of failures that we have so far not been able to pin down. a list of user experience issues is being collated and  will generate summer 16 cycle stories to address those that fall within square's defined scope of activities.     story closed to fit within month, but work is ongoing. ",20,train
DM-4287,Software Stack Introduction,installation of lsst stack for initial steps towards further understanding of the software components of the dm architecture. ,1,train
DM-4288,L1 Function and Design Mtgs,2 days of design discussions and refining functional diagrams of the alert productions and image ingest system.,4,train
DM-4289,Vendor Discussions re: specification documents,specification document sent. discussions with vendors covering document and schedule.,1,train
DM-4290,SC15 Scheduling and Prep,"vendor appt scheduling and conference workshop scheduling, logisitics, etc for supercomputing 15 in austin",1,train
DM-4291,LSST IaM Project,nan,2,train
DM-4294,investigate distributing automated lsst_apps builds via docker containers,nan,15,train
DM-4295,Run and document multinode integration tests on Openstack+Docker,"boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    fyi, lack of dns on openstack cloud cause problems, but a vagrant plugin seems to solve this.",8,train
DM-4296,Write up some introductory guides for Nebula usage,we write up some introductory guides for nebula usage to enable new users to get started.  these are located on confluence under :    https:/confluence.lsstcorp.org/display/ldmdg/ncsanebulaopenstackuserguide,4,train
DM-4298,want equiv of m1.xlarge flavor with smaller disk,"i'd like to be able to build images with vcpus & ram from the m1.xlarge flavor that can be run on a m1.medium with it's smaller disk image, this would require a new flavor with 16gib ram/8vcpus but only 40gib of disk.  something along the lines of:    openstack flavor create ram 16384 disk 40 vcpus 8 ...    is that possible?",2,train
DM-4299,Vagrant for Nebula OpenStack,create and document a vagrant configuration to use  lsstsw machine images on ncsa's nebula openstack cloud.,4,train
DM-4301,Convert banner and menu to react/flux,"add flux data model to capture menu and banner information.  convert banner and menu ui from gwt to react.  as part of this task, bring in fetch api to simplify client/server interactions.",8,train
DM-4302,update obs_lsstSim,"obslsstsim has seen some bitrot.  in particular, the ingest task and the addition of the getexposureid methods on processimagetask have not been propagated to obslsstsim.  this ticket will deal with those issues.",4,train
DM-4303,re-deploy lsstsw on Jenkins,pandas was added to the bin/deploy script in lsstsw to support  sims development.  this has already been merged to master in 4b1d1a0fa.  the ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.,3,train
DM-4304,Add unit testing into gradle build for Firefly's server-side code,add a test task to firefly's common build script.  this can be used by any sub project to run unit test.  added unit testing to jenkins continuous integration job to ensure new code does not break unit testing.,1,train
DM-4306,FY17 Implement Image Caching,"when a file is requested, a cache maintained by the service is checked. if the file exists in the cache, it is returned. if the file does not exist, configurable rules are consulted to remove one or more files to make room for it in the cache, if necessary. (if no room is currently available because all cached files are being used, the request is blocked.) the file is then regenerated by invoking application pipeline code based on provenance and metadata information stored in the repository. the regenerated file is placed in the cache.    this is documented in http:/ldm152.readthedocs.org/en/master/#imagefileservices baseline.",60,train
DM-4307,Please add HSC tests to CI,in dm3663 we (= ) provided an integration test for processing hsc data through the stack with the intention that it should be integrated with the ci system.    having this test available and regularly run would be enormously helpful with the hsc port  we've already run into problems which it could have helped us avoid (dm4305).,1,train
DM-4309,Try out nebula for stack developing and data processing,"follow instructions on:    https:/confluence.lsstcorp.org/display/ldmdg/ncsanebulaopenstackuserguide  https:/community.lsst.org/t/creatinganebulainstancea recipe/353    and create a nebula instance with the stack, update and install more packages in the stack, test by constructing a data repository and processing isr with decam raw images.    ",2,train
DM-4310,Missing Doxygen documentation,"as of https:/lsstweb.ncsa.illinois.edu/doxygen/xlinkmaster2015111002.53.27/ there were 19 ""mainpages in subpackages"" available through doxygen.    in the next build, https:/lsstweb.ncsa.illinois.edu/doxygen/xlinkmaster2015111021.16.19/, most of them have vanished and we only provide links for ndarray and lsst::skymap.    as of filing this issue, they were still missing from the https:/lsst web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/.    please bring them back!",1,train
DM-4311,Oct. on-going support to Camera team in UIUC,attend uiuc weekly meeting and give support as needed. ,2,train
DM-4312,Nov. on-going support to Camera team in UIUC,attend uiuc weekly meeting and give support as needed. ,2,train
DM-4313,Configure sshd with pam_krb5 and document,configure sshd in our test iam vm to use pam_krb5 so the user gets a kerberos ticket when logging in as discussed in our design doc. document the configuration steps.,4,train
DM-4314,Configure DAX web app with Kerberos authentication in IAM test VM,install the dax server (https:/github.com/lsst?query=dax) in our test iam vm at ncsa and configure it to use kerberos authentication using standard python flask kerberos support. document the process.,17,train
DM-4315,Set up test IAM MariaDB instance with Kerberos,install mariadb in our test iam vm at ncsa and configure it for kerberos password authentication. i understand that kerberos ticket authentication is not yet supported by mariadb. document the process and include references to the ongoing work to add kerberos ticket support.    create example tables that demonstrate access control for different logged in users.,2,train
DM-4316,Configure sssd with NCSA LDAP for accounts in test IAM VM,configure https:/fedorahosted.org/sssd/ with ncsa ldap for accounts in test iam vm using instructions from doug fein.,5,train
DM-4317,Configure httpd with SSL and mod_krb5 in IAM test VM,get ssl certificate and configure httpd for modauthkrb5 authentication in iam test vm. document the setup.,4,train
DM-4323,Replace fitsthumb in obs_subaru (port HSC-1196),"fitsthumb is now obsolete; all the functionality we need is available in afw. further, we want to drop it as a dependency to make the job of integrating obs_subaru with ci easier.",1,train
DM-4324,Bootstrap for new Sphinx/reST/RTD technical reports,"create a template repository for lsst (dm/square) technical reports that are written in restructuredtext, built with sphinx and published with rtd",2,train
DM-4325,investigate distributing automated lsst_apps builds,(aggregation ticket for [previous existing]] tightly related tasks in progress related to binary distribution),38,train
DM-4329,Coadd_utils tests should run and skip if afwdata is missing,"currently, the coadd_utils tests are completely skipped at the scons layer if afwdata can not be located. this is bad for two reasons:  1. are there any tests that can be run even if afwdata is missing?.  2. when we switch to a proper test harness (e.g. dm 3901) an important metric is the number of tests executed compared with the number of tests skipped.  each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. this should not be a global switch.",1,train
DM-4331,ActiveMQ Broker upgrade,"version 5.8.0 of the activemq broker, which is not part of the regular distribution, is quite out of date.   is working upgrading the activemqcpp library in dm 4330 to the current version, and this would be a good time to upgrade the broker to the latest release so we try and stay in sync.",1,train
DM-4332,Sizing model fixes," improve api with ldm144: need to separate spec of czar nodes from spec of worker nodes   revisit model for qserv czar nodes   fix chunk count for diaobject (dbl2 f139)   change low volume query result size, 0.5 gb > 1 mb (scireq g271).   revisit model for shared scans and multiple releases   add model for recovery from node failure for query access cluster   consider modeling intermediate results (concluded that intermediate results are relatively small, and given they are distributed across the cluster, it is not worth modelling them)   fix issue with transfer time of nightly products from archive to base     add dia to scans   consider modeling nodes for data loading   ldm139 and ldm141 don't document where mops scratch comes from or what it's supposed to hold.    revisit bulk data transfer from hq to base (shipping disks > over the network)",10,train
DM-4335,The astrometry task should print a warning or throw an exception when there is no reference star in the field,when there is no reference star in the field the exception raised by anetastrometry is identical to the one issued when the fit has not converged.    astrometry (matchoptimisticb) is throwing an exception with the message lsst::pex::exceptions::invalidparametererror: 'posrefbegind too big'.  i suggest a test to detect that there is not enough stars to fit the astrometry and to throw an exception accordingly.,1,train
DM-4336,Global Metadata,"revisit design of the global metadata for lsst image and file archive. this includes understanding interactions between butler, local per repository metadata, global metadata, and metaserv. sort out the dividing line between global metadata and dax (e.g., the schema https:/github.com/lsst/dax_metaserv/tree/master/sql should live in cat/sql, not in dax)    deliverable: document describing the architecture of the global metadata, including how pipelines, users and sui will interact with it. it should cover both writing/ingest and reading aspects.",60,train
DM-4337,S17 Implement Image and File Archive,implement fully working system capable of managing images and files. it will be used by the alert production system that we will be putting together in fy17,90,train
DM-4338,F17 Integrate Image and File Archive with Alert Production,nan,60,train
DM-4339,FY18 Integrate Image and File Archive with DRP,nan,79,train
DM-4340,S17 Butler,nan,100,train
DM-4342,Disentangle log messages from different processes,"it's difficult to disentangle interleaved log messages when running with multiprocessing.  two possible ways to do this would be:  1. have each process write to a different log file.  maybe we could allow the user to specify log filenames including %(pid)s and %(hostname)s.  more useful would be to allow the full range of keys from a dataid, but that might require some changes in how processing runs.  2. prepend each line of the log with context information (dataid, pid, hostname), allowing the user to use grep to do the disentangling.  to avoid overwhelming the log with the context information, a hash of the context information could be used instead, with the lookups published in the log before the processing starts.        brief summary on the changes:   chain logs of subtasks to their parent task logs   add prependedformatter as the new default formatter for log files (specified through logdest command line argument for cmdlinetask). the standard output to the terminal remains the same as before.   any string can be used to label a pex_logging log, and with prependedformatter this label prepends each log message. for cmdlinetask, the dataid is prepended.   the hsc's commit on prepending the timestamps is also ported to prependedformatter.   an example log message:    20160308t22:29:56.889933: []: mergecoadddetections: culled 1731 of 7751 peaks    ",12,train
DM-4343,Add args to s3s3bucket CLI,add sourcebucket and destbucket arguments to s3s3bucket the command line script. this is to allow for one off duplication of buckets.    increment version to 0.1.10.,2,train
DM-4346,tested upgraded activemqcpp package,"the activemqcpp package was upgraded as part of dm 4330, and i tested it to be sure the upgrade was backwards compatible with the code that exercises it in ctrl_events.   it is.",1,train
DM-4347,dax_imgserv 2015_10.0 build error,2015_10.0 has a build error under a current lsstsw/bin/deploy environment.  current speculation is that this is related to the conda version of numpy being upgraded to 1.10.1.,1,train
DM-4348,Replace XML-RPC with in-process communication,"with all recent development in css sector we should be able now to get rid of python in czar entirely. this is also a good opportunity to move from xmlrpc between proxy and czar with direct inprocess ""communication"". daniel said it's a good idea :)  ",10,train
DM-4349,Fix publishing script async issue and add additional release notes.,async command execution causes unpredictable and unreliable results.  switches to synchronous where possible.  also add additional description to the release notes.,2,train
DM-4351,Write technote on the new technical note platform,write a technote about the platform that github.com/lsstsqre/lssttechnote bootstrap lets dm members publish in. discuss current status and outline future plans.,6,train
DM-4352,"Design Mtg, Review and discussions of L1 processing","design/planning meeting for l1 system. materials read and discourse discussions on efd, mops, t&s docs and calibration production",3,train
DM-4353,"Reviewing quotes, power requirements, rack layouts","review of multiple quotes from multiple vendors across full year of purchases. derived power requirements and rack layouts for placement, networking, electrical work discussions to begin.",3,train
DM-4354,"SC15 Scheduling, Processor Futures Refresh","scheduling of several more meeting opportunities for next week. also, time spent reviewing nda materials on processor futures. ",1,train
DM-4355,S19 Run Large Scale Tests,nan,26,train
DM-4356,FY19 Implement Qserv Software Upgrading Tools,nan,53,train
DM-4357,FY19 Finalize Internal DRP Database,final changes to make drp database commissioning ready.,79,train
DM-4360,obs_subaru fails to compile after DM-3200,"due to atypical calls in obssubaru's hsc/sconscript to run scripts in the bin directory, obssubaru fails to compile after the changes made in dm 3200.",1,train
DM-4362,SuperTask phase 1 implementation,"this story represents the implementation of the first part of the supertask framework design,",8,train
DM-4363,Implement configuration for activator parsing,need to add configuration to the cmdlineactivator in new workflow tasks,2,train
DM-4364,First implementation demo,first stage demo of super task and workflowtask framework,4,train
DM-4365,write documentation for registry free repo,nan,1,train
DM-4366,Improve overscan correction for DECam raw data,"currently, the default overscan correction from isrtask is used for processing decam raw data. overscan subtraction is done one amplifier at a time.     however, a bias jump occurs due to the simultaneous readout of the smaller ancillary ccds on decam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot.     this ticket is to improve overscan correction for decam data so to mitigate this discontinuity in the isr processing.    arrangement of ccds on decam: http:/      more details:  there are 6 backplanes in the readout system, shown by the colors in decampixelorientation.png. in raw data files, the ccd's backplane is noted in the header keyword ""fpa"".  examination of some images suggests that science ccds on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. that is the y size of the focus ccds.     actions:  for ccds on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately.    ",5,train
DM-4367,Fix bug and add unit tests for PsfShapeletApprox ,"we discovered during this sprint that this plugin was giving us faulty values for all the models except for singlegaussian.  i will fix that bug on this issue.    obviously, a better unit test would have caught this.  i am adding a doublegaussian unit test, plus a test that the default models provide different results.  also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation.  ",3,train
DM-4368,Duration for various ShapeletPsfApprox Models,this is just a report of the amount of time it takes to run shapeletpsfapprox and cmodel over 10000 galaxies from galsim,2,train
DM-4369,Migrate lsst/ci_hsc repo to git-lfs.lsst.codes,"the github lfs backed repo https:/github.com/lsst/ci_hsc needs to be migrated to git lfs.lsst.codes.  a sanity check for any other ""live"" lfs repos under the lsst github org might also be a good idea.",4,train
DM-4370,Migrate testdata for DECam from disk to git-lfs,"there is a package full with test data for the obsdecam.  it is an eups package currently, but not a git repository.  i would like to migrate that into our hosted git lfs so the obsdecam package can be built by jenkins.   i'm hoping you would be willing to handle this for me.  if not i can find somebody else.  thanks!",1,train
DM-4373,HSC backport: Add tract conveniences,"this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc715.  here is the original description:      in regular hsc survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. measmosaic does not currently efficiently or conveniently iterate over tracts. for example:    mosaic.py /tigress/hsc/hsc rerun price/cosmos id field=sspudeepcosmos filter=hscr    note the lack of a tract in the id specifier — we want to iterate over all tracts. this is not currently possible. instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memoryhungry: currently consuming 11gb on tiger3 just for 12 exposures).  we need an efficient mechanism to iterate over all tracts by not specifying any tract on the commandline.      as this functionality was added specifically for measmosaic, it was going to be ported as part of dm 2674.  due to a recent desire to use this functionality, this ticket will be ported here.",1,train
DM-4375,A slimmer testdata_decam,"before this ticket, the files in testdatadecam are as they are downloaded from the archive.  some are mef, and the total is a bit big (1.2g).    i made a trimmed down version of testdatadecam, 109m and available here:    i trimmed it down by only saving the primary hdu and one data hdu.  however the unit tests in getraw.py become less meaningful and i am not sure if we really want to do this, because some complexities of decam files are about the mef. getraw.py tests butler retrieval of multiple dataset types, in particular tests if the correct hdu is retrieved.     nonetheless, getraw.py can pass  (with branch u/hfc/dm4375 of obsdecam)    note: the old testdatadecam still live on lsstdev:/lsst8/testdatadecam/",2,train
DM-4376,Learn and setup nebula as a development machine,personally establish nebula as a stack development platform.,2,train
DM-4377,Review VAO/IVOA protocols for use in LSST IAM,"following the good principals of re using prior work, review vao/ivoa protocols for use in lsst iam, in particular the http:/ and include a summary in our lsst iam design doc.",1,train
DM-4379,X16 Revisit Public Interfaces / ADQL,"we need to revisit the interfaces we will be exposing to public, wiht particular focus on adql vs sql92 (mysql flavor?).    deliverable: recommendation which public interfaces should be exposed to users from the data access services, with particular focus on adql vs mysql flavor.",11,train
DM-4381,"""SHUTOFF"" nebula instances consume core/ramIt  quota",it appears that halted/shutoff instances have no effect on resource quota usage.  eg:      $ openstack server list     name                   networks                                1956c6d08aec4f42a7818a68fd10179d  shutoff             ...  $ nova absolutelimits     used        141       0                  44                                   342016             1                           0          $ openstack server delete 1956c6d08aec4f42a7818a68fd10179d  $ nova absolutelimits     used        133       0                  43                                   325632             1                           0              ,2,train
DM-4382,Port registryInfo.py from obs_subaru into Butler,"in butler (probably butlerutils), users would like the ability to dump info from a repository's sqlite registry to text (console output). this is already implemented in obs_subaru/.../registryinfo.py, and basically just needs to be ported to butler in a sensible place. there are a few cases that assume certain columns are present and we need either to make the script more generic, or  suggests that maybe we need to standardize the registries.",8,train
DM-4383,Avoid restarting czar when empty chunk list changes,"currently czar caches empty chunk list after it reads the list from file. this complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. it could be done via special query (abusing flush for example) or via sending signal (problematic if czar runs remotely).    this can be potentially useful even after we replace empty chunk list file with some other mechanism as i expect that cache will stay around even for that.",2,train
DM-4386,Clean up ProcessCcdDecam,"processccddecam needs some cleanup:   run method simply delegates to the base class   propagatecalibflags is a no op (deliberately in cab69086, need to explore if the original problem still exists)   the config overrides (in config/processccddecam.py):   uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.    sets the background undersamplestype to reduceinterporder, which is the default.",1,train
DM-4387,Skymap fails tests on testFindTractPatchList,"when skymap is built and healpy is loaded, testfindtractpatchlist fails with:    ======================================================================  fail: test findtractpatchlist    traceback (most recent call last):    file ""/users/ctslater/lsstsw/build/skymap/tests/skymaptestcase.py"", line 245, in testfindtractpatchlist      self.assertclosesttractpatchlist(skymap, [tractinfo.getctrcoord()], tractid)    file ""/users/ctslater/lsstsw/build/skymap/tests/skymaptestcase.py"", line 284, in assertclosesttractpatchlist      tractpatchlist = skymap.findclosesttractpatchlist(coordlist)    file ""/users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseskymap.py"", line 146, in findclosesttractpatchlist      tractinfo = self.findtract(coord)    file ""/users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixskymap.py"", line 97, in findtract      index = healpy.ang2pix(self.nside, theta, phi, nest=self.config.nest)    file ""/users/ctslater/lsstsw/stack/darwinx86/healpy/1.8.112/lib/python/healpy1.8.1py2.7macosx10.5x8664.egg/healpy/pixelfunc.py"", line 367, in ang2pix      checkthetavalid(theta)    file ""/users/ctslater/lsstsw/stack/darwinx86/healpy/1.8.112/lib/python/healpy1.8.1py2.7macosx10.5x8664.egg/healpy/pixelfunc.py"", line 110, in checktheta_valid      assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e 5).all(), ""theta is defined between 0 and pi""  assertionerror: theta is defined between 0 and pi    this was missed during regular ci testing since healpy is not normally setup. ",1,train
DM-4391,Update testCoadds.py to accommodate changes in DM-2915,"as of dm2915, the config setting:  self.measurement.plugins['base_pixelflags'].masksfpanywhere = ['clipped']  is set as a default for measuremergedcoaddsourcestask.  however, this clipped mask plane only exists if a given coadd was created using the newly implemented safeclipassemblecoaddtask.  if a coadd was built using assemblecoaddtask, the clipped mask plane is not present, so the above default must be overridden to exclude it when using measuremergedcoaddsourcestask.  this is the case for the mock coadd that is assembled in the unittest code in testcoadds.py, so the config needs to be set for the test to run properly.    note that the associated tests for safeclipassemblecoaddtask will be added as part of dm4209.",1,train
DM-4393,Get analysis script working for HSC/LSST stack comparisons,"a script for performing pipeline output qa is under development for hsc.  the script provides many useful tools for plotting and analyzing pipeline outputs on single visits and coadds.  this is of general use for lsst and, in particular, will be adapted/expanded to include tools for the direct comparisons of identical data sets processed with both the hsc and lsst pipelines (i.e. dm2984).  appropriate adaptations for this script to run with the lsst stack will be made here (with the understanding that development is still ongoing on hsc and further adaptations will be accommodated as necessary/desired).    see https:/hscjira.astro.princeton.edu/jira/browse/hsc1320 and https:/hscjira.astro.princeton.edu/jira/browse/hsc 1359 for details and examples of the output of this script.",6,train
DM-4395,Update cmsd configuration for multi-node tests,"a particular cmsd configuration parameter prefixes a hardcoded path for queryresource, which needs to be removed. this seems to appear only during multi node tests.",1,train
DM-4396,ctrl_execute test fails to find test binary,"there's a test in ctrlexecute that exercises the bin/dagidinfo.py test program.   since the rewriteshebang rewrites happen after the tests are executed, the test that looks for the bin/dagidinfo.py binary fails, since it's not there before the tests execute.",1,train
DM-4397,Scale CommandLineTask multiprocessing timeout with workload,"the default timeout value for aborting a multiprocessing run in commandlinetask is too short. currently if no time length is supplied by the user, the default value gets set to 9999s. however if a processing task is quite large it is possible for the processing pool to take much longer to arrive at the result. currently if the processing pool does not complete it's run within that time limit, python multiprocessing will throw a timeout error. the timeout value should be scaled such that the supplied value is assumed to be the timeout length for one processing task, and should be scaled by the number of tasks divided by the number of cpus available. the command line task documentation should be updated to reflect this change.",2,train
DM-4398,Fix regexp for gcc48,"dm 2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes.",1,train
DM-4399,ctrl_execute test fails under El Capitan,"the test/testdagidinfo.py because it runs a script from bin.src, rather than bin.   this test needs to be rewritten.",1,train
DM-4400,SuperTask demo on other older tasks,"the examplecmdline task worked fine, need to show demo for other tasks from pipe_tasks",4,train
DM-4401,W16 Qserv Refactoring #2,additional refactoring of qserv as found necessary in w16,70,train
DM-4402,Experiment with light-weight SQL databases for secondary index,"evaluate the use of light weight sql, such as innodb, tokudb (now kyoto cabinet), or rocksdb to create and manage the secondary index.",8,train
DM-4406,Review of [DM-2983] part 2,second part and final of reviewing dm 2983,2,train
DM-4407,Debug Qserv on ccqserv125..ccqserv149,"it seems that some chunkquery doesn't return on long queries like ""select count() from object""  the query stall and print in czar log:    20151124t15:07:55.324z  egrep  ""executive::add\(job\(id=""  cut d'=' f2  sort > jobidadd.txt  cat  /qserv/run/var/log/qservczar.log  cut d'(' f 3  sort > jobiddone.txt    and then    qserv@ccqserv125:/qserv$ diff jobidadd.txt jobiddone.txt   1826d1825  /&) () from /usr/lib/x8664linuxgnu/libstdc.so.6  #2  0x00007ffbccbe8db9 in std::conditionvariable::wait/ >(std::uniquelock/ &, lsst::qserv::wsched::blendscheduler::/) (      this=0xfba620, lock=..., p=...) at /usr/include/c/4.9/conditionvariable:98  #3  0x00007ffbccbe88f9 in lsst::qserv::wsched::blendscheduler::getcmd (this=0xfba5c0, wait=true) at core/modules/wsched/blendscheduler.cc:156  #4  0x00007ffbccba3b07 in lsst::qserv::util::eventthread::handlecmds (this=0xfbe890) at core/modules/util/eventthread.cc:45  #5  0x00007ffbccbab137 in std::memfn/::operator()/(lsst::qserv::util::eventthread) const (this=0xfbe900, object=0xfbe890)      at /usr/include/c/4.9/functional:569  #6  0x00007ffbccbaafff in std::bindsimple/ (lsst::qserv::util::eventthread)>::minvoke/(std::indextuple/) (this=0xfbe8f8)      at /usr/include/c/4.9/functional:1700  #7  0x00007ffbccbaae45 in std::bindsimple/ (lsst::qserv::util::eventthread)>::operator()() (this=0xfbe8f8) at /usr/include/c/4.9/functional:1688  #8  0x00007ffbccbaacfa in std::thread::impl/ (lsst::qserv::util::eventthread )> >::mrun() (this=0xfbe8e0)      at /usr/include/c/4.9/thread:115  #9  0x00007ffbd4c49970 in ?? () from /usr/lib/x8664linux gnu/libstdc.so.6  #10 0x00007ffbd50ae0a4 in startthread (arg=0x7ffbad7fa700) at pthreadcreate.c:309  #11 0x00007ffbd43b904d in clone () at ../sysdeps/unix/sysv/linux/x8664/clone.s:111      something seems to prevent return of chunk query result...          ",5,train
DM-4408,HSC backport: fix memory leak in afw:geom:polygon,"this is a backport of a bug fix that got included as part of https:/hscjira.astro.princeton.edu/jira/browse/hsc1311.  it is not related to that issue in particular, so is being ported here as an isolated bug fix.      original commit message:  pprice@tigersumire:/tigress/pprice/hsc1311/afw (tickets/hsc1311=) $ git sub  commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b  author: paul price /  date:   wed oct 21 10:59:56 2015  0400         imagelib: import polygonlib to prevent memory leak             when doing ""exposure.getinfo().getvalidpolygon()"", was getting:             swig/python detected a memory leak of type 'boost::shared_ptr/  ', no destructor found.             this was due to the polygonlib not being imported in imagelib.      using polygonlib in imagelib then requires adding polygon.h to all      the swig interface files that use imagelib.i.      examples/testspatialcelllib.i               1    python/lsst/afw/detection/detectionlib.i    1    python/lsst/afw/geom/polygon/polygon.i      2    python/lsst/afw/math/detail/detaillib.i     1    8 files changed, 9 insertions()  ",1,train
DM-4410,Port detection task footprint growth changes from HSC,"in hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. this behavior needs to be ported to lsst, as some source records have footprints which are too small. when making this change, the new default needs to be overridden for the calibratetask, as it needs the original size.    the port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from measalgorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipetasks",5,train
DM-4412,Write a DM Collaborative Workflow document,"document our procedures for collaborative development with jira, git and github for the new docs.",11,train
DM-4414,Add git-lfs to packer-newinstall,gitlfs is not available in our deliverables. artifacts (binaries) such as vm images and docker data containers should provide a stable version of gitlfs.,2,train
DM-4415,Meetings - Nov 2015,"verification dataset meetings, illinois des meeting, single frame processing discussions, supertask meeting, openstack user meeting",2,train
DM-4416,Other LOE - Nov 2015,"local lsst group meetings, ncsa postdoc meetings, ncsa physics & astronomy theme meeting, or other local meetings, events, or tasks to comply with ncsa policies",5,train
DM-4417,Crash course on using git-lfs,learn to install and use gitlfs; help testing with migrating testdata_decam to gitlfs; verify tests pass with the new repository (dm 4370).,1,train
DM-4418,Learn about the task design in ISR processing,"learn the concept behind the previous api changes (rfc 26) in the tasks of isr processing, and data storage/retrieval involved. ",3,train
DM-4419,Explore basic middleware and orchestration tools,use runorca to launch jobs through lsst dev and do some single frame processing with it. also learn a little about process execution. ,3,train
DM-4420,Avoid bash usage in batch submission,ctrlpool currently creates bash submission scripts with an explicit /bin/bash bang line.   https:/github.com/lsst/ctrlpool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment14573759 on the review of dm2983 that we should using posix shell constructions instead.,1,train
DM-4421,faulty assumption about order dependency in ctrl_event unit tests,"a recent change to dafbase uncovered a couple of faulty tests in ctrlevents that incorrectly assumes the order in which assumed the order in which data in a propertyset would be received.   we can't assume which order these values will be put into the property set, and therefore into the list retrieved from the event object.",1,train
DM-4422,Image preparation time at server side measurement ,setup the mechanism to measure the time needed to prepare the image (generate the image in png or other suitable format) for client display. ,20,train
DM-4423,Image preparation time at server side measurement,measure the time needed to prepare the image on server side for client display.   reach the goal of less than one second.,20,train
DM-4424,Image preparation time at server side measurement,measure the time needed to prepare the image on server side for client display.   reach the goal of less than half second.,20,train
DM-4425,setup the measurement for Image rendering time to display at web UI,setup the mechanism to measure the time needed to render the image after the data received by the client for display.  ,30,train
DM-4426,measure Image rendering time to display at web UI,"measure the image rendering time, reach the goal of 1 second.",30,train
DM-4427,measure Image rendering time to display at web UI,"measure the image rendering time, reach the goal of 0.5 second.  ",30,train
DM-4428,Remove Task.display(),"as of dm 927 (included in release 9.2, end of s14), lsst.pipe.base.task.task.display() was marked as deprecated. it should now be removed.",3,train
DM-4429,Revisit mysql connections from czar,"need to revisit connections we maintain from czar to mysql. this include revisiting whether we need both sql/sqlconnection and mysql/mysqlconnection classes. (in some cases, like in infilemerger we have instances of both, which gets very confusing.)",12,train
DM-4431,setup mechanism to measure the query response time ,setup the method to measure the response time after query has been submitted from client.   1. query sent to the data provider from client  2. result returns from data provider  3. result displayed in the client    we can measure   1. the time from query submission to been sent to data provider  2. the time from data returned from provider to been displayed in the client,30,train
DM-4432,measure the query response time ,measure the time from query returned from data provider to been displayed in the client.,30,train
DM-4433, measure the query response time ,measure the time from query returned from data provider to been displayed in the client.,30,train
DM-4434,Setup the load test for Web UI portal ,setup the load test system to measure the performance of web ui portal,50,train
DM-4435,load test of Web UI portal: support 100 concurrent users,run the load test system with 100 concurrent users. measure the performance against other kpm epics in the same cycle. ,50,train
DM-4436,load test of Web UI portal: support 100 concurrent users,run the load test system with 100 concurrent users. measure the performance against other kpm epics in the same cycle.  ,50,train
DM-4437,Improve docker storage backend on RedHat-like distributions,"solve startup log message redhatlike distro: ""level=warning msg=""usage of loopback devices is strongly discouraged for production""?    this is due to use of devicemapper (default package option on redhatlike distros) without a dedicated hard disk, use of ""overlay"" backed storage seems better.  ",4,train
DM-4438,Replace sed with stronger template engine in docker scripts,"dockerfile are generated using templates and sed, this should be strengthened.",2,train
DM-4439,Remove useless xrootd client parameters,"this extract of etc/sysconfig/docker:      # used by qservczar  export qswresultdir=$/qservrun/tmp  # disabling buffering in python in order to enable ""realtime"" logging.  export pythonunbuffered=1    export xrdloglevel=debug  export xrdssidebug=1    # increase timeouts, without that, longrunning queries fail.  # the proper fix is coming, see dm3433  export xrdrequesttimeout=64000  export xrdstreamtimeout=64000  export xrddataserverttl=64000  export xrdtimeoutresolution=64000      has to be moved to:       # used by qservczar  export qswresultdir=$/qservrun/tmp  export xrdloglevel=debug  export xrdssidebug=1  # disabling buffering in python in order to enable ""realtime"" logging.  export pythonunbuffered=1      and then tested in multi node, and on in2p3 cluster.",1,train
DM-4440,Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless,these parameters may be useless (see dm 4395). if yes they can be removed to simplify configuration procedure.,2,train
DM-4442,IAM process for granting data access rights,document a process for granting of data access rights to lsst users according to the data access white paper (http:/ls.st/document5373).    on the wiki: https:/confluence.lsstcorp.org/display/laaim/grantingdataaccess+rights,1,train
DM-4443,Please document the --rerun option,"dm3371 adds the rerun option to command line tasks. the help for this option reads:    rerun name: sets output to root/rerun/output; optionally sets root to root/rerun/input    while essentially correct, that's not particularly helpful in understanding what's actually going on here. a motivation and description of this functionality is available in rfc95: please ensure that, or some variation of it, is included in the stack documentation.",1,train
DM-4444,ISR and calibration of a tiny set of DECam raw data,learn more about the beginning steps of single frame processing by processing a small subset of decam stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.,6,train
DM-4445,IaM work in November,work done in support of lsst's iam efforts.,1,train
DM-4446,Management for November,"centered around dmlt meeting, design process and hiring, in addition to general steering of activities at ncsa ",6,train
DM-4447,November TOWG/opeartions design  work,"towg attendance/ note + participating in beth's group.       detailed  wbs for dm,  condensed  wbs  to show high level,  effort estimates, point out problematic thinking in the estimates. ",8,train
DM-4448,JCC,"two day jcc activity at ncsa, including extended jcc meeting with hep centers likely to   host people exploiting lsst data.      writeup of extended meeting is here: https:/confluence.lsstcorp.org/display/jcc/extendedjccmeeting20151123   in the jcc section.     organize, coordinate, and write up meeting notes. ",5,train
DM-4449,Design refinement for the L1 system,"further specification of l1 design,     long list of questions for group but handled by gdf, began procession replies.    understood chilean buffer in l1 system  could be eliminated,  posed question about systems engineering process needed to support this.     genera work casing further developing the design into wbs  which is not 30+ lines of detail  began l1 con ops ,to guide group    learned of some (possible undocumented) ""fifth device"".",11,train
DM-4450,Data Distrib proto (dec),nan,15,train
DM-4451,S17 Implement Async Queries in Qserv," design and implement basic system for determining whether particular query is synchronous or asynchronous. the complete version will come through dm 1490. note that this work is related to shared scans (e.g., we need to know what scans we have running)   design sql api for starting and interacting with async queries.    modify qserv to support async queries (starting, getting status, retrieving results)    note, async queries are indirectly related to authentication (users should not see each other' async queries).    deliverable: qserv that accepts and executes queries asynchronously, and allows users to retrieve results.",50,train
DM-4452,HTML5 Sphinx theme for technotes,"build a sphinx theme for the technote platform. treat this work as exploratory, proof of concept work for customizing the html, css and js of the software docs.    objectives are    1. create a sphinx theme repo  2. show how gulp can be used to help develop web assets for the theme  3. establish a pattern for table contents columns that scroll independently of the main article  4. show how we can implement a html5 rst builder in documenteer to fix permalink issues and build true html5 output.",2,train
DM-4453,Finish documentation and comments on SuperTask ,"need to finish documentation, implementation and comments on the code",4,train
DM-4454,Fix multiple patch catalog sorting for forcedPhotCcd.py,"forcedphotccd.py is currently broken due to the requirement of the lsst.afw.table.getchildren() function that the sourcecatalog is sorted by the parent key (i.e. lsst.afw.table.sourcetable.getparentkey()).  this occurs naturally in the case of sourcecatalogs produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs.  this is indeed the case for forcedphotccd.py as a given ccd can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches.    there two places in the running of forcedphotccd.py where calls to getchildren() can cause a failure: one in the subset() function in references.py, and the other in the run function of singleframemeasurementtask in sfm.py.",2,train
DM-4455,Understand how the proposed interfaces fit with Qserv code,understand how the interfaces proposed by  in dm 3755 fit with the existing qserv code.,3,train
DM-4456,Re-locate LSST PS server and configure to reside on new layer2 circuits,nan,2,train
DM-4457,Investigate MemSQL,take a look at the memsql distributed database.,8,train
DM-4458,Week end 11/07/15,"support for lsst dev cluster, openstack, and accounts  for week ending november 7, 2015.",2,train
DM-4459,Week end 11/14/15,"support for lsst dev cluster, openstack, and accounts  for week ending november 14, 2015.",4,train
DM-4460,Week end 11/21/15,"support for lsst dev cluster, openstack, and accounts  for week ending november 21, 2015.",2,train
DM-4462,New equipment setup and configuration (week end 11/07/15), three dell r730's:   mount in a row racks   complete bios updates   moved ~25 vms over to new lsstvsphere infrastructure   setup lsstcondor\[16\] vms   setup lsstesxmac1 with networking    fixed networking issues on new lsst esx1 server (an undocumented host was squatting on the ip address),5,train
DM-4463,New equipment setup and configuration (week end 11/14/15), received dell idrac license upgrades for new dell r730 servers   received and configured vmware vsphere licenses from cdwg & aura   converted three physical systems to vm's:   lsstnagios   problems with software mirror raids.   vmware converter does not see software raided drives.   split the raid 1 into discrete drives.   chose sda to modify  failed as sad was faulty.   built new centos 6.6 vm   used crashplan to rebuild.   lsst7  converted after learning how to convert system to a fixed ip subnet   lsst8  converted   debugged lsst8 system migration to vmware.  partition table was invalid and was preventing move    finished moving all vms to new lsstvsphere infrastructure,6,train
DM-4464,"New equipment setup, configuration, and regular monthly maintenance (week end 11/21/15)"," virtualized physical system lsst xfer   worked with bmather or dell dirac licensing issue   cleaned up old and new hosts in ncsa dns, nagios monitoring, and qualys vulnerability scanner   completed connections for six ups",2,train
DM-4465,New equipment setup and configuration (week end 11/28/15), obtained dell idrac enterprise licenses & upgraded 4 of the 13 servers   installed base vm templates for os x 10.8 10.11,1,train
DM-4466,Decommissioning old equipment (week end 11/14/15), shutdown 17 (all) old esxi servers   shutdown 3 old condor servers,1,train
DM-4467,Decommissioning old equipment (week end 11/21/15)," shutdown last 3 old condor servers   shutdown lsstnetem, lsstps, & lsstpsbase servers   surplussed equipment:   ncsa servers ( 5 dell 1950's, 2 dell 2950) repurposed from a22 to c17   moved blade chassis to c20   move lsst test systems in a23",2,train
DM-4469,Write DM Git LFS Guide,refactor words from dm4412 into a toplevel doc page for using git lfs. this will leave dm 4412 more as a collaboration workflow document.,1,train
DM-4471,Consulting in November,"review of design documents, correspondence with the design team regarding data center details and floor space, and conferences via web links.",4,train
DM-4472,Shared scan implementation,fine tune the api proposed in dm 3755 and implement it.,10,train
DM-4473,Improvements to logging in xrootd,improve logging in xrootd to make it more compatible with qserv logging.,6,train
DM-4474,Fit Visualizer porting: Begin,"display fits image, server round trip, organize initial data structures, initial render",16,train
DM-4475,"FITS Visualizer porting: group, group scrolling",nan,6,train
DM-4476,"FITS Visualizer porting: zooming: group zooming, zoom fit, zoom fill, active plot selection",nan,6,train
DM-4477,Upgrade to react 0.14.3,nan,2,train
DM-4478,Fit Visualizer porting: create toolbar,nan,6,train
DM-4479,FITS Visualizer porting: Add canvas drawing infrastructure,nan,10,train
DM-4480,Fit Visualizer porting: Distance Tools,nan,4,train
DM-4481,Fit Visualizer porting: Drawing Target Center ,nan,3,train
DM-4482,FITS Visualizer porting: Marker tool,"improve the menu organization from the current one when doing the migration.   the implementation includes:   drawobj for marker  marker dropdown list under the marker icon to show the marker and footprint items (will be added later).  marker drawing layer rendering and operation including action creator and action dispatch functions.  marker ui component shown in drawing layers popup. the implementation set up some work which can be similarly expanded to footprints in the future.  title of marker layer on the layer control is like ""show: /"".   the title change as the text you enter for the label.  the mouse changes to a pointer once the cursor moves onto the marker, and the mouse becomes a resize sign when the cursor moves on the handler of the marker.  when the cursor becomes a pointer, the marker can be relocated by dragging the mouse, and when the cursor becomes a resize one, the marker can be resized by dragging the mouse.  the marker size changes as the image is zoomed.     maker drawing layer operation:   select 'add marker' from dropdown list to add a new drawing layer  click anywhere to locate the newly added marker  click and drag the mouse to relocate or resize the marker: for relocation: click and drag inside the circle, then drag the mouse. for resize: click inside the circle, then click and drag the handler to resize  label and its location are set from marker tool ui  ",10,train
DM-4483,FITS Visualizer porting: Grid drawing,nan,10,train
DM-4484,FITS Visualizer porting: Catalog drawing,nan,6,train
DM-4485,FITS Visualizer porting: Region Drawing,"region drawing for the following regions,   circle, ellipse, box, polygon, point (circle, box, diamond, cross, x, arrow, boxcircle), line, text, annulus, box annulus, ellipse annulus. (annulus is made for the case with at least two repeated regions of the same type).    add the following functions to file shapedataobj.js    makerectanglebycenter, makeellipse  drawellipse,  maketextlocationrectangle, maketextlocationellipse  update drawrectangle by adding the case which is given the rotating angle and rectangle center.",10,train
DM-4486,FITS Visualizer porting: Mouse Readout: part 1: projection,show the fits xy readout so that it update the position in the users selected coordinate system. the readout should also show the plot title.    write the dialog to change the coordinate system readout.    also show the pixel size and write the dialog that will change it between pixel size and screen pixel size.    [1/28/16]  move this ticket to the next sprint.  i cannot get it done in this sprint because  # spent time to work on other two tickets  # take off from work  # it takes more time than estimates since i am not familiar with reducer and store etc.  more study is needed.      ,15,train
DM-4487,FITS Save Dialog,nan,10,train
DM-4488,FITS Visualizer porting: Rotate,nan,2,train
DM-4489,FITS Visualizer porting: North/East Arrow,add the north and east arrow like the gwt system has.,10,train
DM-4490,FITS Visualizer porting: Layer Control Popup,nan,6,train
DM-4491,FITS Visualizer porting: Stretch Pulldown,nan,1,train
DM-4492,FITS Visualizer porting: Color bar pulldown,nan,1,train
DM-4493,FITS Visualizer porting: Restore to defaults & re-center,nan,1,train
DM-4494,FITS Visualizer porting: Show FITS Header,"task involves several steps:     server side: visservercommands.header needs to change to check to the json_deep parameter. in this case the return from  visserverops.getfitsheaderinfo should be converted into a format that the new javascript tables should understand (see loi how to get this format). look at visservercommands.areastat for an example.   client side: a call to the server: need to add getfitsheaderinfo into plotservicejson.js. for reference, look at other calls in  plotservicejson.js and the java version of the getfitsheaderinfo plotservicejson.java   client side: when header toolbar button pushed then make the call to the server.   client side: when server call promise is resolved then show a dialog with the table data. remember 3 color images should have a tab per color.",6,train
DM-4495,FITS Visualizer porting: Flip,nan,1,train
DM-4496,FITS Visualizer porting: Expanded View,nan,4,train
DM-4497,FITS Visualizer porting: Expanded Single,nan,4,train
DM-4498,FITS Visualizer porting: Expanded View : WCS Match,nan,6,train
DM-4499,FITS Visualizer porting: Expanded View: Grid,nan,6,train
DM-4500,FITS Visualizer porting: Crop,nan,4,train
DM-4501,Fit Visualizer porting: Select Area,nan,4,train
DM-4502,FITS Visualizer porting: Statistics - part 1,dialog only,4,train
DM-4503,"FITS Visualizer porting: selecting points of catalog from image view, showing selected points","able to draw a rectangle on the image, and select the catalog entries overlaid on the image",5,train
DM-4504,FITS Visualizer porting: Image Select Panel/Dialog,"converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: each ticket should reference this ticket as the base.    panel includes the following:   issa, 2mass, wise, dss, sdss tabs   file upload tab, upload widget might have to be written   url tab   blank image tab   target info reusable widget   3 color support  any panel should show for 3 times, for read, green, and blue in 3 color mode   must be able to appear in a panel or dialog   must add or modify a plot   allow to create version with most or less than the standard tabs. example  see existing wise 3 color or finder chart 3 color   a plot might need to be tied to specific type of image select dialog, we need a way to tie a plotid to and non standard image select panel.",1,train
DM-4506,Fit Visualizer porting: Thumbnail,nan,3,train
DM-4507,Fit Visualizer porting: Magnifier,nan,2,train
DM-4508,Experimentation and testing of new SuperTask infrastructure,wbs deliberately left unset as this is tracking loe work.,6,train
DM-4509,Migrate prototype SuperTask code to upstream repository,the prototype supertask code is now ready to be moved from the experimental repo to the upstream pipe_base repo. this requires the commits to be squashed and tidied.,6,train
DM-4510,makeDocs uses old style python,makedocs is written in python 2.4 style. this ticket is for updating it to python 2.7.,1,train
DM-4511,Improve reStructuredText documentation,"enhance docs by covering     images as links   table spans   abbreviations   :file: semantics, etc.",2,train
DM-4513,Quoting of paths in doxygen configuration files breaks makeDocs,in dm3200 i modified sconsutils such that all the paths used in doxygen.conf files were quoted so that spaces could be used. this change broke documentation building (dm4310) because makedocs did not expect double quotes to be relevant. this ticket is to fix makedocs and to re enable quoting of paths in config files.,4,train
DM-4514,Assess DECam ISR up to currently implemented,"not all known isr corrections are applied or implemented to decam data yet. for example, no crosstalk, edgebleed, nonlinearity, sky pattern removal, satellite trail masking, brighterfatter, or illumination correction.    but we have most of the basic isr already. with what we already have, identify issues that would severely affect the quality of post isr processing.",3,train
DM-4515,Flag out the glowing edges of DECam CCDs,"pixels near the edges of the decam ccds are bigger/brighter and correcting them is not trivial. one way to move forward is to mask them out.      desdm and cp mask 15 pixels on each edge.  the cut was later raised to 25 pixels, with the inner 10 pixels flagged as suspect.  ",5,val
DM-4523,Fix startup.py inside Docker container,qserv tag should be replace with qserv_latest,1,val
DM-4524,Margaret's mgmt. activities in November,dmlt @ princeton  weekly dmlt and standups  local meetings  tpr  etc.,18,val
DM-4525,Ops Planning and TOWG attendance - November,"towg meetings, review service catalog as input to lopt, review draft of operations wbs with don and athol",2,val
DM-4526,L1 design specification and planning - November,"with steve, jim, don, and jason, detailed design construction and planning for development of the image ingest and processing system. worked through ldm 230 and ocs design docs, input from discussions on confluence. made cleaner drawings (draft) and expanded project plan. the plan is currently a ""to do"" list, without schedule or resource loading yet. ",10,val
DM-4527,Facility Coordination Meeting and JCC meeting @ NCSA,"day 1 : facility coordination day with argonne, ccin2p3, nersc, and ncsa  day 2: extended jcc meeting    notes posted on confluence: https:/confluence.lsstcorp.org/pages/viewpage.action?spacekey=jcc&title=extendedjccmeeting201511 23 ",4,val
DM-4529,Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX,"compiling on macosx yosemite with xcode 7, a number of files fail compilation.    core/modules/util/eventthread.h,cc fails because uint is used as a data type.  this is nonstandard (though some compilers support it), and should be replaced with unsigned int.    core/modules/wbase/sendchannel.h,cc fails because #include / is missing.    core/modules/wsched/chunkstate.cc fails because #include / is missing.     build/qmeta/qmetalibwrap.cc (generated by swig) fails with many errors because the typedef unsigned long int uint64t included in qmetalib.i conflicts with macosx's typedef of it as unsigned long long.",1,val
DM-4534,shellcheck linting of lsstsw bash scripts,this issue is to recover a branch from dm 4113 that was not merged due to issues with installing shellcheck under travis.,1,val
DM-4536,Create release notes docs template,create a template for release notes/other release type documentation in the new docs.,2,val
DM-4538,Research simulation tools,"we need to do simulations of dcr and other effects when determining appropriate mitigation techniques.  this will require settling on a simulation tool for doing this.    the obvious choices are: phosim, galsim, and a roll your own solution.  look into which of these is the most reasonable choice and make a recommendation.",4,val
DM-4540,Roll Qserv into SQuaRE release - part I,improvements to codekit to support release process. ,4,val
DM-4544,Revisit short and long term plans for butler,revisit short and long term requirements and needs and capture it through stories.,5,val
DM-4545,Review of storage quotes,"reviewed 4 products from 3 vendors with storage group. prices and features comparison along with discussions on whether to integrate into condo or not. also, discussed security considerations.    preliminary vendor/product chosen. follow up questions sent to vendor. will review with storage before we are able to purchase.",2,val
DM-4546,Update to Sizing Model,"removed 'memory effectiveness' factor as it was already included in compute efficiency  updated capacity / tape to follow lto roadmap  updated bandwidth / tape drive to follow lto roadmap  changed ""tape!number of new tapes purchased"" to round up to an even number for the archive site  changed ""tape!number of new tapes needed"" to round up for both sites  changed ""tape!number of tape drives needed based on bandwidth"" to round up for both sites  fixed 'tape!tape bandwidth' colums to take into account mixed tape drive types  updated 'tape!hpss' to take into account retiring mover nodes    also began work with spectra logic to further improve tape predictions.",2,val
DM-4547,Power requirements and LSST footprint at NCSA,"finalized power requirements with the uofi engineer. i plan to distribute verification compute across multiple racks in order to reduce perrack power requirements and reduce perrack network port counts. this allows us to drop from 3x 60a for the verification cluster to 3x 30a which is the same for the other racks. this will result in some cost savings and simpler planning for future use of the racks.    also, provided a plan for lsst's footprint in npcf until 2032. we now have space reserved from the south side of roger to the north side of blue waters. this space should be very visible from the room camera i believe. ",2,val
DM-4553,Move butler from daf_persistence to daf_butler,nan,6,val
DM-4555,Add initial butler support for remote GET,"for get:  if the mapper returns a url:  retrieve the url contents into a file  return the path to the file.    this will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". this is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    need to solve the cleanup problem of when to delete the file that was downloaded.    for put:  serialize the object to a temporary file  transfer the file to the url",10,val
DM-4556,Fix docker workflow,"some issues where discovered while trying to package dm2699 in docker (for in2p3 cluster deployment), they're fixed here.     aptget update times out: why?   git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged   eupspkg er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged.",2,val
DM-4557,Base site additions to simulator,"started using login on the nebula cluster  set up some instances and used snapshot facility. detected an error with launching from volume.  met with chris lindsey about issue.      started coding replicator and foreman functionality.   began message taxonomy for comm between foreman and replicator.     installed log rotate, rabbitmq,  pip, pika and gcc on running instance. successfully tested  rabbitmq with another instance which had pika libs installed.  finished up prototyping of replicator and foreman code.    installed nova client on foreman instance so it can start and stop replicator instances  as needed at runtime. tested.    began a project to simulate realistic camera data from lsst to be used for various timings of dmcs prototype code. this was also a concentrated effort to learn about the nature of astronomical data, how it is represented, its various types of data, and practice with the c and python libraries for building fits files. two sample files were built: one additively built and converted 16bit des data into scaled 18bit data, then placed that binary representation in a 32bit integer, and the other was additively built up from phosim data. files were built to represent a single lsst camera raft with 9 hdus  one for each ccd. files were over 600mg. the code used in these files has been saved and parts turned into scripts for generating other sample files.      set up an environment for timing the imprint of these files as they are built and processed through the base dmcs pipeline code. initial timing was done on compression techniques.     wrote result paper on fits file generation and timing. errors due primarily to my own ignorance of this type of file format and how the data was truly represented were encountered several times. this project was personally extremely helpful in beginning to learn the vast amount of domain knowledge needed to complete future pieces of the l1 base site code package.      added coding for setting up raw data to be moved and assembled into fits files in dmcs , simulating part of the base dmcs data flow, into existing base replicator/foreman prototype.    ",70,val
DM-4558,"Functional drawings, specification writing, and info gathering ","initial meeting with don about spec work   began gathering architecture ideas for base dmcs 4    discussed requirements gathering for base site network operations with  steve p., paul, and don.   1    began specification draft for base site, and integrated drafts into project wiki 4    more spec ideas and posted them on dons pages under the ocs bridge page.  4    dmcs planning meetings begin in earnest.  16    planned dmcs diagrams for l1 base site and ncsa site as a group and drew them in support of margaret’s trip to dm meeting. these were refactored a couple of times and are in confluence.  8    camera meeting: about 1",38,val
DM-4559,Bug fix and improvement for DECam processing, bug fix in decammapper policy of fringe dataset   improve readme documentation about ingesting and processing raw data   bug fix on translating community pipeline's bad pixel mask (bpm)  previously in dm4191 i looked up the wrong table for the bpm bit definition.   flag the potentially problematic edge pixels as suspect (dm4515)   add data products for coaddition processing,6,val
DM-4560,Local LSST Sec Meeting,nan,1,val
DM-4561,Local LSST IaM Meeting,nan,1,val
DM-4562,Security Playbook,nan,2,val
DM-4563,Security Plan Renewal,nan,3,val
DM-4564,Convert basic table functionalities to JS.,"task includes server side json conversion, data modeling, and a simple react table for presentation.",20,val
DM-4565,Table (JS): selection feature.,this task is composed of:   converting java class selectioninfo.   reducing data into its table model state   rendering selectioninfo onto the tablepanel   creating action and action creator,6,val
DM-4566,Upgrade to the latest react-highcharts library,"we need to upgrade from the early version of react highcharts to the latest one, compatible with react 0.14.3. just switching to the new library does not work, need to resolve issues.",6,val
DM-4567,Table (JS): large table handling,this task is composed of:   creating and adding a paging toolbar to tablepanel   adding external data loading feature to tablepanel   creating prefetch and background data fetching mechanism   use websocket events for reporting background statuses     requires new serverside code.     depends on  dm4578   integrate websocket messaging into flux,14,val
DM-4568,Table (JS): sorting,this task is composed of:   introduce sorting feature into tablepanel   creating action and action creator    reducing data into its table model state,4,val
DM-4569,Histogram View of a Table,"combine histogramoptions and histogram widgets into a histogram viewer:   define histogram state tree, actions (getting/updating table statistics, getting/updating column data), and reducers   write a smart widget, which shows options and histogram sidebyside",10,val
DM-4570,Table (JS): filtering,"this task is composed of:   adding filter toolbar into tablepanel   filter validation syntax   creating action, action creator, and reducing data into its table model state   generating meta info for enumerated columns  not sure if we wanted this.    also, added acton feature to fieldinput.",6,val
DM-4571,Suggestion Box widget,"we need to find or implement a suggestion box widget in js. currently, it is used to suggest table column names in xy plot and in some forms.",6,val
DM-4572,Table (JS): table options,"this task is composed of:   adding table options panel to tablepanel.   providing features:     show/hide units in header     show/hide columns, reset to defaults, etc      page size",5,val
DM-4573,JS expression parsing library,since we are allowing column expressions we need a way to validate them on client side.,6,val
DM-4574,Table (JS): text view,this task is composed of:    adding text view option to tablepanel,2,val
DM-4575,XY Plot view of the table (JS): define state tree,"define state tree, actions, and reducers for xy plot view of the table.",4,val
DM-4576,XY Scatter Plot (JS) ,implement basic scatter plot widget using react highcharts library,8,val
DM-4577,XY Scatter Plot Options (JS),nan,10,val
DM-4578,Integrate websocket messaging into flux,this task is composed of:   design and implement messaging concept into flux    implementation thoughts:   convert inbound messages into actions   convert selected actions into outbound messages   add message action reducer with the concept of a message consumer.     consumer can be a predefined action creator or a function       allow consumers to be added/removed into/from the system after bootstrap,10,val
DM-4579,XY Plot view of a table (JS),"implement smart widget which shows toolbar, plot options, and xy plot.",10,val
DM-4580,XY Plot view of a table (JS) - Toolbar,"toolbar, which toggles plot options, selection and filter buttons    extra:    handling zoom from the toolbar rather than using builtin zoom    ability to switch between histogram and scatter plot view",8,val
DM-4581,XY Plot Viewer (JS) - density plot,implement density plot using reacthighchart library (highchart's canvasbased heat map).,12,val
DM-4582,XY Plot View of a table (JS) - selection support,"show/change selected/highlighted points. ideally, this should be done without redrawing the whole plot. ",8,val
DM-4583,SUIT: search returning images in a directory," create a sample search processor, which returns images in a given directory.   it should be using an external python task    update search form configuration to use this search processor to return image metadata",2,val
DM-4584,XY plot view of a table (JS) - density plot zoom support,density plot zooming requires server call.,6,val
DM-4585,XY Plot view of a table (JS) - density plot selection support (?),density plot   how do we support selection?    (in current version we turn off selection support when the plot is density plot),6,val
DM-4586,GWT Conversion: Login,"this task is composed of:   adding user info into banner     includes user name and links for login, logout, and profile.   convert serverside code to return json   use messaging to handle current user state.     depends on dm 4578 integrate websocket messaging into flux",4,val
DM-4587,GWT Conversion: Search Panel,"this task is composed of:   converting serverside code to return json   defining and loading search info data into application state     loading should be implemented so that it can be from a server fetch or a client declaration   creating action, action creator and reducing functions   rendering searchpanel from search info data      attach searchpanel to application:  depends on gwt conversion: layout",12,val
DM-4588,Create ctrl_platform_nebula package to exercise ctrl_orca orchestration on Nebula,we create a ctrlplatformnebula package to support processing with the lsst framework as orchestrated by the ctrlorca/ctrlexecute  packages within a htcondor pool that resides on the nebula openstack.,18,val
DM-4589,GWT Conversion: basic layout for results,"this task is composed of:   creating a results container that handle the layout of its components   define and load layout info into application state   creating action, action creator, and reducing functions   components include:     vis toolbar     last searched description     layout options: triview.  sidebyside, single and popout can be added at a later time.     tables, image plots, xy plots.   depends on dm 4590: gwt conversion: advance resizable layout panel",4,val
DM-4590,GWT Conversion: advance resizable layout panel,create an advance react component for layout.  features should include:   a set of predefined layouts   resize strategies    generic for reuse,6,val
DM-4591,GWT conversion: System notifications,"this task is composed of:   adding notification panel to the application   convert serverside code to use messaging for notifications   use messaging on clientside to handle notifications   creating action, action creator, and reducing functions   depends on dm4578 integrate websocket messaging into flux  ",3,val
DM-4595,GWT Conversion: History and routing,first pass at the implementation of history and routing.  define a framework in which the application can be:   bookmarked   record state in history    retore application from a url  ,4,val
DM-4596,Remove deprecated versions of warpExposure and warpImage,"afw.math supports two templated variants of warpexposure and warpimage, one that takes a warping control and the other which does not. the latter have been deprecated for a long time and are no longer used. i think it is time to remove them.",1,val
DM-4601,Build docs.lsst.io Doc Index Page,"create an html landing page for all dm documentation/documents     software documentation   developer guides   requirement and design documentation   technotes   papers   presentations    the page will be implemented as a static site. the page build will be template driven, with content scraped from the metadata.yaml resources of technotes (among other sources).    since this is the first square web project, this ticket will also involve effort in establishing a css+html pattern library and gulp based development workflows. long term, this investment will be returned with new dm.lsst.org, technote and sphinx documentation web designs.",2,val
DM-4603,sconsUtils tests should depend on shebang target,some tests rely on code in the bin directory. whilst these tests have been modified to use bin.src the general feeling is that the test code should be able to rely upon the shebang target having been executed before they are run.,1,val
DM-4604,make codekit repos.yaml aware,up to now codekit assumed the repo name is the same as the eups package name. fix it by using repos.yaml. ,1,val
DM-4609,Partition package should use the standard package layout,the partition package does not build on os x el capitan because the package is not laid out in the standard manner and whilst sconsutils is used most of the default behaviors are overridden. this means that fixes implemented for dm3200 do not migrate over to partition. i think the best approach would be to reorganize the package so that it does build in the normal way.,1,val
DM-4610,Research Kerberos and LDAP replication options,"iam components, including the kerberos kdc, need to be replicated between ncsa and chile machine rooms. this may impact whether lsst can use ncsa's production kerberos instance (if it supports selective replication) or needs a separate kerberos instance that can be replicated outside ncsa. this task is to research and document the options, in consultation with ncsa kerberos admins, and propose a kerberos replication approach.",1,val
DM-4611,"Receive, verify and test network equipment",nan,4,val
DM-4612,Install networking hardware into openstack and verify operation,nan,4,val
DM-4613,Verify Network Emulator operation,nan,4,val
DM-4614,Design network emulator integration into workflow,nan,2,val
DM-4615,Deploy heirarchical queuing to test image precedence,"once we have a working workflow ready to move data between the ""base site"" and ""archive site"" (both at ncsa), deploy a base site exit router with stacked queuing to test prioritization of image traffic over various network conditions.",8,val
DM-4616,Migrate Qserv code to stream-based logging,"migrate qserv code from logf to logs.     while doing it, we will also revisit logging levels: in particular we are abusing info, most of what is now in info should be on debug, in some places where info is used to cover unusual conditions, it should go to warning.     further, we will unify how we initialize logging structures. per discussions at 2015/12/09 qserv meeting, we like best loglogger log = log_get(""lsst.qserv././"") in anonymous namespace in cc files. logging from .hh files is strongly discouraged.    this involves changing ~600 places.",10,val
DM-4617,Send all chunk-queries to primary copy of the chunk,"we are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. while doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). this story involves tweaking xrootd to ensure we don't send chunkqueries to nodes hosting nonprimary copies.",5,val
DM-4625,Design for butler support of multiple repositories,"work on design for gregory/sui's request:    we need to understand how put()/writing works when multiple repositories are made visible through a single butler.  for get()/reading a single search order makes sense.  for put() it may be desirable to support alternative destinations (local disk, user workspace, level 3 db) or even multiple destinations for a single put().",6,val
DM-4628,Explore coadd processing with DECam data with default config,"starting with raw decam data, perform single frame processing and then try image coaddition with a few visits of images. ",6,val
DM-4630,Provide input to CalibrateTask design work,provide requirements and advice as input to the effort to redesign calibratetask (dm 3881).,10,val
DM-4631,Create IDL pipeline workflow for DRP processing - processCcdDecam,"for the verification datasets work we need the ability to run a dataset all the way through drp processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataids, creates individual log files, and creates helpful qa plots/metrics/webpages.  nidever will use his photred idl workflow and rewrite it for the stack.  the first step is processccddecam.",5,val
DM-4632,Create IDL pipeline workflow for DRP processing - makeCoaddTempExp,"for the verification datasets work we need the ability to run a dataset all the way through drp processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataids, creates individual log files, and creates helpful qa plots/metrics/webpages. nidever will use his photred idl workflow and rewrite it for the stack.  processccddecam is working.  the next step is makecoaddtempexp.  ",4,val
DM-4637,lsstsw should symlink afwdata or allow an envvar,"to reduce disk usage, it is very handy to be able to make build/afwdata and stack/afwdata/blah be symlinks. build/ is easy: just make the symlink and then never have touch it unless you rm your whole stack. stack/afwdata/blah is harder: each time you rebuild something that depends on afwdata, it will install a new copy of afwdata, which you'll have to manually remove and declare your symlink with a new tag.    a couple of ways to make this more automatic:      lsstsw checks whether build/afwdata is a symlink, and if so just makes the new stack ""install"" a symlink to the same directory.    check for some environment variable (e.g. afwdatabasedir or something) and if that exists, just make a symlink to it, or make a dummy eups table that points to that directory and don't put anything in stack at all.",5,val
DM-4639,modernize afw code and reduce doxygen errors,"i would like to make some simple modernizations afw code and reduce doxygen warnings as much as practical. the modernizations i had in mind were:   move doc strings from .cc files to .h files and standardize their format   use namespace lsst { namespace afw { ... in .cc files to make the code easier to read   eliminate all /::ptr and /::constptr typedefs (replacing with ptr(/) and const_ptr(/)).   make sure .py files import the appropriate packages from future and (where practical) pass the flake8 linter    regarding doxygen warnings: i think moving the documentation to .h files will help in many cases. some warnings may be impractical to fix, such as complaining about not documenting ""cls"" for python class methods.  ",6,val
DM-4641,Implement mouse interaction with the drawing infrastructure,nan,8,val
DM-4642,Migrate scisql and mysqlproxy to mariadb,"mysqlproxy and scisql relies on mysql, they should now move to mariadb",4,val
DM-4643,Add utility function to handle client-side download requests.,create utility function to handle client side download requests.  it needs to be done in a way that does not mess with history and current page state.,1,val
DM-4644,Add workflow code to lsst-dm github,"since we have split the code for super task, all the workflow code should  go in a different repository",2,val
DM-4645,L1-CONOPS,contribute to l1 conops document,10,val
DM-4647,investigate replicating EUPS published packages,"(this ticket is for work that has already been done, per internal discussion in sqre, but accidentally without an open ticket)    https:/github.com/lsstsqre/lsyncdeupspkg  https:/github.com/lsstsqre/sandboxpkg",4,val
DM-4648,Support sqlalchemy use with qserv,"when one tries to connect to qserv using sqlalchemy there is an exception generated currently:    $ python c 'import sqlalchemy; sqlalchemy.createengine(""mysql+mysqldb:/qsmaster@127.0.0.1:4040/test"").connect()'  /u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/default.py:298: sawarning: exception attempting to detect unicode returns: interfaceerror(""(mysqlexceptions.interfaceerror) (1, 'error totally whack')"",)    ""detect unicode returns: %r"" % de)  traceback (most recent call last):    file ""/"", line 1, in /    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 2018, in connect      return self.connectioncls(self, kwargs)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 72, in init      if connection is not none else engine.rawconnection()    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 2104, in rawconnection      self.pool.uniqueconnection, connection)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 2078, in wrappoolconnect      e, dialect, self)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 1405, in handledbapiexceptionnoconnection      excinfo    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/util/compat.py"", line 199, in raisefromcause      reraise(type(exception), exception, tb=exctb)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/base.py"", line 2074, in wrappoolconnect      return fn()    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 318, in uniqueconnection      return connectionfairy.checkout(self)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 713, in checkout      fairy = connectionrecord.checkout(pool)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 480, in checkout      rec = pool.doget()    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 1060, in doget      self.decoverflow()    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/util/langhelpers.py"", line 60, in exit      compat.reraise(exctype, excvalue, exctb)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 1057, in doget      return self.createconnection()    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 323, in createconnection      return connectionrecord(self)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/pool.py"", line 454, in init      execonce(self.connection, self)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/event/attr.py"", line 246, in execonce      self(args, kw)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/event/attr.py"", line 256, in call      fn(args, kw)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go      return oncefn(arg,  kw)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/strategies.py"", line 165, in firstconnect      dialect.initialize(c)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize      default.defaultdialect.initialize(self, connection)    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/default.py"", line 256, in initialize      self.checkunicodedescription(connection):    file ""/u2/salnikov/stack/linux64/sqlalchemy/201510.0/lib/python/sqlalchemy1.0.8py2.7linuxx8664.egg/sqlalchemy/engine/default.py"", line 343, in checkunicodedescription      ]).compile(dialect=self)    file ""/u2/salnikov/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/cursors.py"", line 174, in execute      self.errorhandler(self, exc, value)    file ""/u2/salnikov/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 36, in defaulterrorhandler      raise errorclass, errorvalue  sqlalchemy.exc.interfaceerror: (mysqlexceptions.interfaceerror) ( 1, 'error totally whack')      the reason for that is that sqlalchemy generate few select queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. here is the list of selects which appears in proxy log:    select cast('test plain returns' as char(60)) as anon1  select cast('test unicode returns' as char(60)) as anon1  select cast('test collated returns' as char character set utf8) collate utf8bin as anon1  select 'x' as some_label  ",3,val
DM-4649,Create and rename the sims_dustmaps repository to sims_maps,"create and rename the simsdustmaps repository to simsmaps.    this is my plan after talking to :    add simsmaps to lsstbuild/repos.yml. change related dependencies and create ticket branches, run ci to confirm the changes.",2,val
DM-4650,Update sims_dustmaps/sims_maps repository to use git-lfs,update sims_dustmaps to use git lfs.,1,val
DM-4651,Convert GWT code to pure JavaScript (X16),we plan to continue the gwt to js conversion in summer 16. the goal is to finish it.,100,val
DM-4652,CI debugging,diagnosing build failures and refreshing build slaves,1,val
DM-4653,Ci Deploy and Distribution Improvements part III,nan,60,val
DM-4656,Port code style guidelines to new DM Developer Guide,verbatim port of dm coding style guidelines to sphinx doc platform from confluence.     https:/confluence.lsstcorp.org/display/ldmdg/dmcodingstylepolicy   https:/confluence.lsstcorp.org/display/ldmdg/pythoncodingstandard   https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908666 and contents    i’m unclear whether these pages should be included:     https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20283399 (c ‘using’)   https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20284190 (how to use c templates)   https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20283399 (c+11/14; which should seem to belong in the code style guide)    any temptation to amend and update the style guideline content will be avoided.,5,val
DM-4657,Port RFC/RFD/Decision Making Page to new docs,port to new sphinx docs: https:/confluence.lsstcorp.org/display/ldmdg/discussionanddecisionmakingprocess?src=contextnavpagetreemode,3,val
DM-4665,Read and extend SuperTask technical note,read the current version of dmtn 002 and comment.    write new sections describing the overall architecture and the expected role of supertask in the system.,3,val
DM-4666,Review existing CmdLineTask instances' inputs and outputs,"review most or all existing dm cmdlinetask subclasses to understand their external inputs and outputs.  this will inform the design of the successor to the interim supertask.execute( dataref ) interface.    the issue is that the single dataref interface supports only 1:1 input:output relationships, or n:1 relationships where a list of inputs is derivable from an output dataid.  this is believed to be insufficiently general.",6,val
DM-4667,Improve sphgeom documentation,"per rfc117, the sphgeom package needs decent overview documentation, linked from the toplevel readme.md. the doxygen output should also be reviewed.",2,val
DM-4668,audit obs_subaru defaults and move them to lower-level code,"the obssubaru config overrides contain many useful settings that aren't actually specific to hsc or suprimecam.  these should be moved down to the low level defaults in the config classes themselves, so new obs packages don't have to copy these configurations explicitly.",2,val
DM-4671,configure WebDAV with Kerberos/LDAP on lsst-auth1,"configure webdav for kerberos authentication and ldap authorization. create example subdirectories where ldap groups determine access (using .htaccess files):   lsst: anyone in lsst group can read/write to this directory   ncsa: anyone in allncsaemploye can write, anyone in lsst can read",6,val
DM-4672,IAM process for managing L3 data access,document a process for managing access to l3 data products.    on the wiki: https:/confluence.lsstcorp.org/display/laaim/managingl3data+access,1,val
DM-4673,Prototype LSST User/Group Manager,nan,1,val
DM-4677,Design Interfaces for Memory Management for Shared Scans,part of the shared scans involve memory management   a system that will be used by qserv that will manage memory allocation / pin chunks in memory. this story involves designing the api between qserv and the memory management system.  ,8,val
DM-4678,"qserv/cfg has to be removed by ""scons -c""","qservmeta.cong was still pointing on mysql instead of mariadb, even after running ""scons c"". this error prone behaviour should be fixed.",2,val
DM-4679,work with database team to exercise all the APIs for data access (F16),sui will continue to work with database team to exercise all the apis for data access. all known issues should be worked out in s16 cycle.,40,val
DM-4680,Provide API for tabular data display using Firefly,we need to provide javascript api access to all the table displaying features to give user more control when using firefly api to displaying table data in their own web page or to build customized web ui ,10,val
DM-4681,Provide a prototype version of LSST web UI ,"suit deployment at ncsa to access sdss strip82 data processed by dm stack in 2013.   use the data access api, or tap api   light curve for time series data    connection between the light curve data point and the image that the data point coming from  ",60,val
DM-4682,Implementation of multiple repositories v1,nan,25,val
DM-4683,Implementation of multiple repositories v2,nan,15,val
DM-4688,Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor,"in histogram, when the data points fall on the bin edges,  the following rules are used:  #  for each bin, it contains the data points fall inside the bin and the data point fall on the left edge.  for example, if binsize=2, the bin[0] is in the range of [0,2].  the data value 0 is in bin[0] .  #  for each bin, the data point falls on the right edge is not included in the number point count. for example if binsize=2, the bin[0] is having the range of [0,2].  the data value 0 is in bin[0] but the data value 2 is not in the bin[0].  # for the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points.    the last rule is newly introduced.      ",2,val
DM-4689,Firefly visualization Java/JS code refactoring and bug fixes(F16) ,"this epic will capture all the java and js code refactoring in firefly, bug fixes, js code optimization and performance enhancement. ",80,val
DM-4690,Design worker scheduler for shared scans,nan,8,val
DM-4691,Data Distrib proto (Jan),nan,25,val
DM-4692,Refactor ProcessCcdTask and sub-tasks,"based on conversation spurred by dm3881 as discussed https:/community.lsst.org/t/requirementsforoverhauledcalibration task/370, this ticket will refactor processccdtask to be easier to extend and instrument, easier to understand, and more modular.    the main work will be to break up processccdtask into it's component modules and, and reconfigured to meet the requirements as outlined by the clo discussion.",16,val
DM-4694,distributed loader,nan,12,val
DM-4696,Implement memory mgmt for shared scans,nan,12,val
DM-4697,Implement worker scheduler for shared scans,nan,20,val
DM-4698,Add initial butler support for remote PUT,"for get:  if the mapper returns a url:  retrieve the url contents into a file  return the path to the file.    this will be optimized in subsequent stories by ""add read support for various transports to the afw object readers"". this is a degenerate case that will be used if the object does not have a reader for a given transport protocol.    need to solve the cleanup problem of when to delete the file that was downloaded.    for put:  serialize the object to a temporary file  transfer the file to the url",10,val
DM-4699,Create ALERT framework for qserv,"at the moment qserv code will throw exception when something wrong/unusual happens. that is not always the best idea to do in a server code that is needs to run 24x7. if we don't throw exception and just log the issue, it might get unnoticed in the log files. so, it'd be useful to have some sort of alert framework where we could send alerts when something strange / unexpected happens in qserv code and we are able to ""ignore it"" and continue running the server. it can be as naive as writing to a special place, or sending an email, or messaging the dba etc. this story involves designing and implementing such framework. the sooner we do it the better so that we don't accumulate new code that is throwing exceptions where it should not.",10,val
DM-4700,Revisit Qserv code that throws exceptions,we have ~500 places where we throw exceptions in qserv/core/modules/. revisit all of them and make sure we catch these exceptions properly.,15,val
DM-4701,Promote IsrTask to command line task.,"as pointed out by  in dm 4635, it would be quite useful to have the isrtask callable as a command line task without having to do all the other steps in processcoaddtask.",4,val
DM-4702,Promote CharacterizationTask to command line task,"in refactoring the processccd.py script, we'd like to make each component callable by command line as well.  this is to promote the image characterization task to a command line task.  a requirement will be that this task be able to run on data without isrtask having been run (command line tasks should be insulated as much as possible from knowing about previous processing).",4,val
DM-4703,Promote CalibrateTask to command line task,"the task that takes care of measurement and calibration on characterized images will be promoted to a command line task.  as with the other command line tasks, it should be possible to run the calibration and measurement command line task on data without necessarily running isrtask or characterizetask.    of course, this means the task will have to get a psf from somewhere, see https:/community.lsst.org/t/requirementsforoverhauledcalibrationtask/370 for suggestions.",4,val
DM-4704,Qserv integration tests fail on CentOS7 with gcc 4.8.5,"the version of gcc that ships with centos7, gcc (gcc) 4.8.5 20150623 (red hat 4.8.54), appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. installing devtoolset3toolchain and devtoolset3 perftools via yum to get gcc 4.9 resolves the issue.",2,val
DM-4705,qdisp/testQDisp fails with mariadb,"fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testqdisp with the message:    pure virtual method called  terminate called without an active exception      runnig it with gdb it' obvious that there is a problem with resource lifetime management in qdisp/testqdisp.cc. the problem is that xrdssisessionmock is destroyed sooner than other objects that use it.     one way to resolve this problem is to instantiate xrdssisessionmock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance.    big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier.",1,val
DM-4706,Rerun and create a repository for CFHT astrometry test.,"understand, re rerun, and recreate clean version of  's cfht astrometry test for the astrometry rms for two sample cfht observations.  this test is on the ncsa machines in  /lsst8/boutigny/valid_cfht    create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack.        ",1,val
DM-4707,Adapt CFHT astrometry test for DECam COSMOS field validation,adapt the cfht astrometry validation test to the decam reprocessing effort.      will focus on the repeat observations of the cosmos field.  goal is to just do a simple two observation comparison.  doing a full test of all of the observations will be a later story.  ,1,val
DM-4708,Integrate astrometry test into SDSS demo lsst_dm_stack_demo,incorporate the astrometry test as an optional component in lsstdmstackdemo.    this is chosen because lsstdmstackdemo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller cfht validation test of the dm stack.,1,val
DM-4709,Prototype a validation module of the stack using CFHT data.,"create a prototype standalone validation test of the astrometric performance of the stack on suitable cfht data.  module is called `validatedrp`     http:/github.com/lsst/validatedrpcfht    decide how those data should be provided (testdatacfht being one obvious possibility), and determine if obscfht tests and the tests for this validatedrp module should use the same test datasets.    this is prototyping for dm 2518.",1,val
DM-4710,host identification info needs to be part of log message,the eventappender needs to add host identification (host/process/id) information to the log message it transmits.   this was inadvertently left out.,3,val
DM-4711,Edit testdata_cfht to pass obs_cfht unit tests,this ticket covers the first half of the issues in dm 2917.     testdatacfht was left unedited while some past changes in obscfht megacammapper required coordinated changes.  the goal of this ticket is to simply pass the unit tests currently in obs_cfht.   ,1,val
DM-4712,Fix documentation and restructure workflowTask,nan,2,val
DM-4713,Improve documentation on pipe_base/supertask,nan,2,val
DM-4714,Bad OpenBlas setting in miniconda/numpy causes very poor performance for running multiple processes,"i have been running many processes of processccddecam.py on my new linux machine in tucson (bambam).  to my surprise, running 40 processes at once gets very poor performance (70 sec per process) compared to running a single process (16 sec). i expected some performance hit because of larger overheads but not a factor of 4!    i ran it both on a spinning hdd and pcie ssd but they both had the same problem.  i also tried running it on multiple visits versus multiple chips for a single visit (all accessing the same mef fits file) but this made no difference.  i tested it with various numbers of processes and found that the time per processes increases linearly with the number of processes running.      [jmatt] has been helping me track this down.  we used some performance tools (htop, iotop, and perf) to figure out what was going on.  it was clear that the issue was not a ram or i/o problem.  by watching htop while the 40 processes were running it became clear that once some of the processes hit ""deblending"" everything slowed down considerably and all cores were maxed out and showing lots of kernel traffic.  i also ran processccddecam.py with deblending turned off and the performance was much more reasonable (24 sec. per process).    after more digging (with perftop), we found that there was a lot of swapping going on during the deblending step by ""openblas"".  this is a package that numpy uses for speeding up certain computationally intensive tasks using multithreading (e.g. linear algebra).  by default each openblas instance takes advantage of all cores on a machine.  so all 40 processes were trying to use all available cores and most of the time was spent swapping between all of these threads.    openblas can be configured to use a more reasonable number of cores/threads, but the version that lsstsw uses is installed by miniconda via a dependency of numpy and, as far as we could tell, it's not possible to configured numthreads for openblas with miniconda.    we ended up compiling our own version of openblas with numthreads = 6 (the maximum threads that openblas uses) and the performance was great, 24 sec.    i'm not sure what the solution is for this but we probably don't want to go with miniconda for the default lsstsw installation (uses currently done by bin/deploy).    jmatt might have comments to add.   ",4,val
DM-4716,Track down reason for slow performance when running many jobs of processCcdDEcam on bambam,"during the processing of the cosmos data for the verification dataset work i ran many jobs of processccddecam.py on the new linux server, bambam.  the performance was very slow, 4x longer than running a single job at a time.  figure out what is going on.",2,val
DM-4717,Meetings Dec 2015,"verification dataset meetings, rfd meetings, des chicagoland meeting and preparation",6,val
DM-4718,Other LOE -- Dec 2015,"weekly lsst local grouop meetings, ncsa meetings (all hands, software, etc), code review, other local meetings, postdoc meetings and tasks",6,val
DM-4719,Vendor input on sizing predictions,discussions about tapepricing and diskpricing predictions from spectra and ddn respectively in order to improve our forecasting. this information needs to be incorporated into ldm 144.,2,val
DM-4720,More preparation for FY16 hardware,"more pricing iterations with several companies and incorporating that information into our final decision. more q&a with storage companies re: comparable features. compiled all storage option quotes into a spread sheet which now forms as a good comparison and helps ldm144  forecasting.     awaiting quotes for racks and pdus. now that rack size is known for the chilean dc, this will serve us well for ldm144 costs.     power issues for fy16 hardware are settled and we are ready to schedule installation as soon as the hardware purchase contract is complete.    tagging issues for fy15 hardware complete. looking into pre fy15 tagging. requested that lsst/aura perform an inventory request to complete the circle and prove the process.    working with spectra / netsource to create a sustainable tape condo that can serve lsst through 2030.",4,val
DM-4721,Plan DM’s communication / documentation / information architecture strategy,"plan and write a technote outlining communication and documentation platforms from a dm perspective. the technote will specify     how each platform is used   what developments need to be done   address integrations with lsstwide communications projects    address information architecture (generally, the ease of discovering the right information)",2,val
DM-4722,File tickets for list of stack deficiencies and suggested upgrades,"k t suggested that i take my list of ""stack deficiencies and suggested improvements""  on confluence and (with tim j.'s help) create tickets for each item (as much as possible) so that the work could be scheduled.  ",3,val
DM-4723,Continue learning about middleware,"learn more about orchestration, task execution, and logging.  ",3,val
DM-4724,Implement zenodio.harvest,harvest metadata about records in a zenodo community collection using the oai_datacite3 format. see https:/zenodo.org/dev    part of the https:/github.com/lsst sqre/zenodio python package. this tool will be used by our technote and the documentation indexing platforms.,3,val
DM-4728,Doxygen package fails to build with flex 2.6,to wit:      $ flex version  flex 2.6.0    $ bash newinstall.sh    lsst software stack builder  [...stuff...]  eups distrib: failed to build doxygen1.8.5.eupspkg: command:   source /users/jds/projects/astronomy/lsst/stack/eups/bin/setups.sh; export eupspath=/users/jds/projects/astronomy/lsst/stack; (/users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/doxygen1.8.5/build.sh) >> /users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/doxygen1.8.5/build.log 2>&1 4>/users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/doxygen1.8.5/build.msg  exited with code 252    $ grep error /users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/doxygen1.8.5/build.log  commentscan.l:1064:55: error: use of undeclared identifier 'yycurrentbuffer'  commentscan.l:1126:58: error: use of undeclared identifier 'yycurrent_buffer'      builds fine using flex 2.5.35 apple(flex31).,1,val
DM-4729,HSC backport: Add functions to generate 'unpacked matches' in a Catalog,"the qa analysis script under development (see dm4393) calls to hsc hscpipebase's https:/github.com/hypersuprimecam/hscpipebase/blob/master/python/hsc/pipe/base/matches.py which adds functions to generate ""unpacked matches"" in a catalog (and vice versa).  it will be ported into lsst.afw.table.    the port includes following hsc commits:  add functions to generate 'unpacked matches' in a catalog.  https:/github.com/hypersuprimecam/hscpipebase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186    adding check to prevent more obscure error.  https:/github.com/hypersuprimecam/hscpipebase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5    some little error handling helps.  https:/github.com/hypersuprimecam/hscpipebase/commit/61cc053b873d42802581adff8cbbdb52a348879e  (from branch: stagencsa3)    matches: add arrayi to list of field types that require a size  https:/github.com/hypersuprimecam/hscpipebase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5  (from branch tickets/hsc 1228)    please also include a unittest.",3,val
DM-4730,Adapt qa analysis script for LSST vs. HSC single visit processing comparison,"the qa analysis script ported from hsc and adapted to lsst on dm 4393 currently performs qa on single visit processing by comparing outputs of a single run from the different measurement algorithms and comparing those with the astrometry/photemetry reference catalog.  here we will add functionality to directly compare two different runs of the same dataset (requiring accommodations for two butlers).  since the goal is to compare outputs from runs on the lsst vs. hsc stacks, this will require a mapping of the different schemas of the persisted source catalogs of the two stacks.",10,val
DM-4731,Add labels to qa analysis plots for better interpretation,the plots output by the qa analysis script (see dm 4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics.  this includes magnitude and clipping thresholds.  this information should be added to each plot such that the figures can be interpreted properly.,2,val
DM-4733,lsst-build should support enabling Git LFS in an already-cloned repository,"a repository which does not use git lfs is created and described in repos.yaml. it runs through ci, and is cloned onto a jenkins build slave. subsequently, the repository configuration in repos.yaml is updated to enable lfs. the build system should notice this change and update the cloned repository on disk appropriately. currently, it doesn't.",1,val
DM-4734,afw fails to build on a machine with many cores,"the afw package does not build reliably (if at all) on a linux box at uw (""magneto"", which has 32 cores and 128 gb of ram). the failure is that some unit tests fail with the following error:        openblas: pthreadcreat error in blasthreadinit function. error code:11      for the record, /usr/include/bits/locallim.h contains this:    / the number of threads per process.  /  #define posixthreadthreadsmax 64  / we have no predefined limit on the number of threads.  /  #undef pthreadthreadsmax      it appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. according to [mjuric] the package responsible for this is eupspkg, and it tries to use all available cores.    a workaround suggested by [mjuric] is to set environment variable eupspkg_njobs to the max number of cores wanted. however, i suggest we fix our build system so that setting this variable is unnecessary. i suggest we hard code an upper limit for now, though fancier logic is certainly possible.    a related request is to document the environment variables that control our build system. i searched for njobs on confluence and found nothing.",1,val
DM-4735,Remove dead code from configuration procedure,  remove scratch db?   cleanup tmp/sql/.sql filesi   remove xrootd configuration script if useless?    cleanup configuration script style (i.e. tmp/.sh)  ,3,val
DM-4736,Study if mysqlproxy can be compatible with mariaDB client,mysqlproxy is not compliant with mariadb client: see https:/mariadb.com/kb/en/mariadb/mariadbvsmysqlcompatibility/#incompatibilitiesbetweenmariadbandmysqlproxy    nevertheless the trivial fix proposed (remove progressreport options) doesn't seems to work...      mysql host=127.0.0.1 port=4040 user=qsmaster  batch qservtestcase01qserv    error 1043 (08s01): bad handshake  ,4,val
DM-4737,Improve 'unit' tests using database, add mock database for it to work during unit tests or run it apart from unit tests?   fix testlocalinfile (read configuration file)    try all core/modules/sql/testsql            ,5,val
DM-4738, Improve  LOAD LOCAL INFILE management on czar side,"option ""mysqloptions( m, mysqloptlocalinfile, 0 );"" is added to all c sql client instance due to common sql interface, but is only required on master (for merging results) is it possible:    to remove local keyword (on czar virtfile is on the same machine that mariadb server)   or to set it in qserv czar/master configuration    or to set it in master mariadb instance only?",4,val
DM-4739,Update kernel on IN2P3 cluster,"the ""kernel panic"" issue is nonblocking rightnow for john due to machine automated reboot, but we have to solve it to target a stable production system.    with yvan, we're converging on next update for the cluster:     on my side i update qserv metadata on ccqserv100 w.r.t. new qserv metadata format, and then i test docker+qserv on ccqserv100>ccqserv124,   then ccin2p3 team launches a upgrade of the kernel to kernelml ( ""mainline stable"" branch of the linux kernel archives) on ccqserv100>ccqserv124, this could be done in january,   i control qserv behaviour is still the same than before,   then qserv developpers can use this cluster to see if ""kernel panic' issue is solved.    if it work we'll update to kernelml on ccqserv125>ccqserv149, if it doesn't, cc in2p3 and qserv developpers will have to find an other solution.    regards,    fabrice",5,val
DM-4740,Audit and document obs_subaru scripts,"obs_subaru has a bin.src directory containing a variety of miscellaneous scripts. some of these may be actively useful; others could be useful, but require modernizing to work with the latest version of the lsst codebase; others are obsolete or duplicate functionality available elsewhere. throughout, documentation is lacking.    please audit this directory: remove the scripts which are useless and ensure the others are working and properly documented.",5,val
DM-4741,Cyber security infrastructure document,this document details anticipated security infrastructure and roles needed for the operations of lsst at the base and summit observatory site.  ,1,val
DM-4742,Bi-weekly LSST IaM meetings for December,bi weekly iam meeting between ncsa and lsst for the month of december 2015.  local coordinating meetings also included.,1,val
DM-4743,Make deblender more robust against weird PSF dimensions," reports two problems with psf dimension calculations in the deblender that result in fatal errors, because earlier checks for bad dimensions intended to cause more graceful failures are incomplete.    the first appears to happen when the psf dimensions are highly nonsquare, and the image width is smaller than 1.5x fwhm while the image height is more than 1.5x fwhm (or the opposite).    measurecoaddsources fatal: failed on dataid=:     file ""src/image/image.cc"", line 92, in static typename lsst::afw::image::imagebase/::viewt lsst::afw:  :image::imagebase/::makesubview(const lsst::afw::geom::extent2i&, const lsst::afw::geom::extent2i&, cons  t typename lsst::afw::image::detail::typestraits/::viewt&) [with pixelt = double]      box2i(point2i(2,2),extent2i(11,11)) doesn't fit in image 7x15   lsst::pex::exceptions::lengtherror: 'box2i(point2i(2,2),extent2i(11,11)) doesn't fit in image 7x15'    traceback (most recent call last):    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.py"", line 321, in call      result = task.run(dataref, kwargs)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/pipetasks/python/lsst/pipe/tasks/multiband.py"", line 552, in run      self.deblend.run(exposure, sources, exposure.getpsf())    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 231, in run      self.deblend(exposure, sources, psf)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 308, in deblend      clipstrayfluxfraction=self.config.clipstrayfluxfraction,    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 354, in deblend      psf, pk, sigma1, patchedges)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 1073, in handlefluxatedge      psfim = psfim.factory(psfim, sbox, afwimage.parent, true)    file ""/sps/lsst/library/lsstsw/stack/linux64/afw/201510.08g4057726/python/lsst/afw/image/imagelib.py"", line 4630, in factory      return imaged(args)    file ""/sps/lsst/library/lsstsw/stack/linux64/afw/201510.08g4057726/python/lsst/afw/image/imagelib.py"", line 4472, in init      this = imagelib.newimaged(args)  lengtherror:     file ""src/image/image.cc"", line 92, in static typename lsst::afw::image::imagebase/::viewt lsst::afw::image::imagebase/::makesubview(const lsst::afw::geom::extent2i&, const lsst::afw::geom::extent2i&, const typename lsst::afw::image::detail::typestraits/::viewt&) [with pixelt = double]      box2i(point2i(2,2),extent2i(11,11)) doesn't fit in image 7x15   lsst::pex::exceptions::lengtherror: 'box2i(point2i(2,2),extent2i(11,11)) doesn't fit in image 7x15'      the second problem may occur when the overlap region between a psf image and the data image it corresponds to is only 1 pixel in either dimension.  in any case, there's a gap in the gracefulfailure logic that could let such a problem through, which would result in received error message:    measurecoaddsources fatal: failed on dataid=:   traceback (most recent call last):    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.  py"", line 321, in call      result = task.run(dataref, kwargs)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/pipetasks/python/lsst/pipe/tasks/multiband.py"", line 552, i  n run      self.deblend.run(exposure, sources, exposure.getpsf())    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, args, keyargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 231, in run      self.deblend(exposure, sources, psf)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, args, keyargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 308, in deblend      clipstrayfluxfraction=self.config.clipstrayfluxfraction,    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 312, in deblend      tinyfootprintsize=tinyfootprintsize)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 575, in fitpsfs      kwargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 752, in fitpsf      sx1, sx2, sx3, sx4 = overlap(xlo, xhi, px0+1, px11)    file ""/sps/lsst/library/lsstsw/stack/linux64/measdeblender/201510.03 ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 721, in overlap      (xlo <= xhi)  and (xmin <= xmax))  assertionerror  ",2,val
DM-4744,Demonstrate web authentication using CILogon and Globus,configure modauthoidc on lsst auth1 with cilogon and globus.,4,val
DM-4745,CONOPS to  support design activities.,"one foundational document  required for how ncsa astronomy core services working methods that is missing is a concept of operations (conops) for the l1 system.     better late than never,  i wrote a l1 cops illustrating the uses of the l1 system, allowing for further specifications and context for all staff involved in the project. the document currently exists a a draft in google docs.  it's been review by mario and kt.  this is more properly a systems engineering document, not at all sure where a final home for it belongs (or how it acquires status)",10,val
DM-4746,"Prepare  activity diagrams and backing conops  for LI provisioning and ARP, including satellite computing centers.","prepared two longer con accompanied by (hand drawn activity diagrams).      one conops /activity  diagram describe the work at the archive center to provide and support the l1 services used by telescope operations.    the second activity diagram and conops respdes to chuck clavers' request to have materials that explain the relationship of the stiletto computing center at ccin2p3 to archive center at ncsa.    both are draft; and coops seem to be systems engineering documents, and where to deliver a blessed version and who is responsible for this is unclear to me. ",5,val
DM-4747,Beth Willman visit,self explanatory,2,val
DM-4748,"Review gartner materials relating ITIL, Devops and related topic","read gartner materials related to itil, devops  it organization in preparation for more detailed thinking about data operations.    one major category of thought is ""mode 1 and mode 2"" type organizations. mode ! is the current typical controlled environment the strength is when something precious needs to be managed.  for lsst this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured.  mode 2 is ""doves"" which is best used for nimble, fall fast software.  an example e testing algorithms.",3,val
DM-4749,Management for December,"includes attendance at the nsf ci for facilities workshop (2d).  hiring,  hiring related presentation at u of i  aci.   management of group and effort distributed at ncsa.  oversight of some legacy projects (chileand data center, etc)",9,val
DM-4751,Connecting table with histogram viewer,"create a demo, which takes a url and shows a table with a histogram viewer connected to it.",6,val
DM-4753,Cleanup location of anonymous namespaces,"we place anonymous namespace in two ways: (a), inside lsst::qserv::/ namespace, or (b) before. this story involves cleaning it up   move them to before lsst::qserv::/",1,val
DM-4754,Add mysql connection to QueryContext ,"we need access to database schema for various reasons (analyzing queries, checking authorization, for queries like ""show create table"" and others).",3,val
DM-4755,Implement globally unique queryId,nan,7,val
DM-4756,Support human-friendly Thread ID in logging messages,"per discussion 1/6/2016, it'd be nice to have a function that generates user friendly threadid  on linux to simplify debugging.",4,val
DM-4758,create multi image viewer," port of multidataview.java   support grids, rows, finger chart type grid   support paging with table data sets   support datasetinfoconverter port   the components should be able to display any group of data   critical for firefly viewer",12,val
DM-4759,Port Data  set info converter achitechture,"defines various image data types, how to get them, groupings, artifacts.   i am not quite happy with how we did in in gwt so the design needs to be improved.  must be less complex.",8,val
DM-4760,L1 Concept of Operations (December work),"assist in developing a conops for the l1 system. this is the first step to making a detailed design and plan for construction.    revised/cleaned up/added to l1 conops    meeting to clarify calibration products production use case requirements     discussions about operational use cases, processes, functions of l1 system",10,val
DM-4761,Ops Planning - December," lopt and towg meetings   beth willman 2day visit; discussions about operations, proposal timeline and deliverables     prepared fte estimates for it roles    reviewed itil roles and clarified work descriptions    met with ncsa ici leads to get input on fte estimates for various roles   timing diagrams    worked on first draft of cycle diagram showing 24 hours of operations at ncsa",7,val
DM-4762,Margaret's mgmt. activities in December," meetings: security, idm, dmlt, supertask coordination, standups, etc.   staffing    ari meeting and preparation    reviewed resumes, discussed staffing plan   tpr, invoice breakouts, milestones   discussed ev process and mis tool design for internal management    etc.  ",13,val
DM-4763,Exploration of In-memory database packages used in time critical applications,"begin evaluation of potential inmemory data storage tools  selecting memcached and redis to start.  4    with the intent  to gain familiarity with these tools, procurred introductory volume on redis and began writing prototype python code to prototype lists, hashes, and lists of hashes. sketched out and implemented base python class with virtual save method, then wrote child classes for replicators, replicator health, replicator jobs etc. and tested this code and implemented the save methods.  4    installed the above code on a nebula instance that acts as a job manager, then ran real job messages through the system and simulated task assignment and completion, using redis to track jobs.  2    exploring how a logging and visualization harness might be included so job activity could 1) be observed in realtime, and 2) so a session could be played back as an after action review to investigate errors, bottlenecks, cold restart behavior, etc.  2    to finish out this story, redis replication must be included in the above prototype.",12,val
DM-4764,CONOPS-V1,evaluated first cut of conops document. formulated queries for clarifying specific questions regarding ldm230  2,2,val
DM-4765,Track and provide feedback on base site facitlity,nan,2,val
DM-4766,Build network testbed,nan,4,val
DM-4767,add CSS import and image import and clean up some existing jsx,nan,2,val
DM-4768,Port W16 CModel improvements from HSC,three significant changes were made to cmodel in https:/hscjira.astro.princeton.edu/jira/browse/hsc1339. they were described by http:/jeeves.astro.princeton.edu/pipermail/hscsoftware/all/4568.html. they include:     changing the method by which the initial approximation is determined;   changing the determination of the pixel region to use in fitting;    a new prior on ellipticity and radius.    please port these changes to lsst.    also include the results of fixing the bug described in https:/hscjira.astro.princeton.edu/jira/browse/hsc1384.,4,val
DM-4771,Week end 12/12/15,"support for lsst dev cluster, openstack, and accounts  for week ending december 12, 2015.",2,val
DM-4772,Week end 12/19/15,"support for lsst dev cluster, openstack, and accounts  for week ending december 19, 2015.",2,val
DM-4773,New equipment setup and configuration (week end 12/05/15)," setup ipmi on lsstesxi3, lsstesxi5, lsstesxi6   created ipmitools binary  ubuntu 12.04 has the correct libraries for esxi6   installed on lsstesxi3, lsstesxi5, lsstesxi6   configuration worked well on lsstesxi5, lsstesxi6.   lsstesxi3 is actually lsstesxi4.  system will not respond to ipmitool commands  suggest waiting for scheduled outage and manually setting up ipmi.   debugged networking issues with new equipment   cleanup of crashplan archives for new lsst system   mac vms   working on figuring out mac vm requirements and process with josh hobblitt   attempted to install and configure puppet on mac vms   running into configuration issues  ",3,val
DM-4774,New equipment setup and configuration (week end 12/12/15)," set up ipmi on lssttest1, lssttest2,…lsst test9   more research on using puppet on mac vms – little progress    cleanup of nfs space in its  ",2,val
DM-4775,New equipment setup and configuration (week end 12/19/15), research on using puppet on mac vms   considering using vagrant to manage virtualbox or fusion mac vms   tried to setup ldap auth for mac user auth   cleanup of nfs space in its,2,val
DM-4777,Updates to the Sizing Model,"updated processor projections based upon haswell and skylake expectations. added shipping rates, chilean and us power and cooling rates, updated memory pricing projections. working with spectra logic on updating and improving tape predictions, library space and power requirements, upgrade options and mapping of bandwidth and capacity requirements to hardware (need to figure in fudge factors for latency of mounting tapes, latency of seeks times, maybe space for tape migrations, replace replacement tapes with updating pricing that includes tape replacement). inclusion of that into the document will be in the next story.",3,val
DM-4778,"Contractual work, justifications, inventory for LSST hardware","reviewing hardware purchase contracts, reviewing internal hardware budget justifications (and attending related meetings), working with purchasing on 'vendor specific' purchasing options, incorporating updated vendor quoted pricing into expected hardware expenditures. ",2,val
DM-4780,meas_extensions_shapeHSM seems to be broken,"i have installed the measextensionsshapehsm package together with galsim and tmv (i documented it at : https:/github.com/darkenergysciencecollaboration/reprocessingtaskforce/wiki/installingthelsstdmstackandtherelatedpackages#installingmeasextensionsshapehsm) and tried to run it on cfht cluster data.     my config file is the following:      import lsst.meas.extensions.shapehsm  config.measurement.plugins.names |= [""extshapehsmhsmshaperegauss"", ""extshapehsmhsmmoments"",                                      ""extshapehsmhsmpsfmoments""]  config.measurement.plugins['extshapehsmhsmshaperegauss'].deblendnchild=''  config.measurement.slots.shape = ""extshapehsmhsmmoments""      when i run meascoaddsources.py, i get the following error :      traceback (most recent call last):    file ""/sps/lsst/library/lsstsw/stack/linux64/pipetasks/201510.010g1170fd0/bin/measurecoaddsources.py"", line 3, in /      measuremergedcoaddsourcestask.parseandrun()    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.py"", line 444, in parseandrun      resultlist = taskrunner.run(parsedcmd)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.py"", line 192, in run      if self.precall(parsedcmd):    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.py"", line 279, in precall      task = self.maketask(parsedcmd=parsedcmd)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/cmdlinetask.py"", line 363, in maketask      return self.taskclass(config=self.config, log=self.log, butler=butler)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipetasks/201510.010g1170fd0/python/lsst/pipe/tasks/multiband.py"", line 530, in init      self.makesubtask(""measurement"", schema=self.schema, algmetadata=self.algmetadata)    file ""/sps/lsst/library/lsstsw/stack/linux64/pipebase/201510.03g24e103a/python/lsst/pipe/base/task.py"", line 255, in makesubtask      subtask = configurablefield.apply(name=name, parenttask=self, keyargs)    file ""/sps/lsst/library/lsstsw/stack/linux64/pexconfig/201510.01gc006da1/python/lsst/pex/config/configurablefield.py"", line 77, in apply      return self.target(args, config=self.value, kw)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/measbase/python/lsst/meas/base/sfm.py"", line 247, in init      self.initializeplugins(schema=self.schema)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/measbase/python/lsst/meas/base/basemeasurement.py"", line 298, in initializeplugins      self.plugins[name] = pluginclass(config, name, metadata=self.algmetadata, kwds)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/measbase/python/lsst/meas/base/wrappers.py"", line 15, in init      self.cpp = self.factory(config, name, schema, metadata)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/measbase/python/lsst/meas/base/wrappers.py"", line 223, in factory      return algclass(config.makecontrol(), name, schema)    file ""/sps/lsst/dev/lsstprod/clusters/mypackages/measextensionsshapehsm/python/lsst/meas/extensions/shapehsm/hsmlib.py"", line 964, in init      def init_(self, args, kwargs): raise attributeerror(""no constructor defined  class is abstract"")  attributeerror: no constructor defined   class is abstract  ",1,val
DM-4781,MariaDB does not work together with mysql-proxy,we have switched to mariadb but there is one issue that complicates things  mysql client from mariadb fails to connect to mysqlproxy with an error:    error 1043 (08s01): bad handshake    so fabrice had to find a workaround for our setup to use client from mysqlclient package instead. this workaround is not perfect and it complicates other things. would be nice to make things work transparently for mariadb.    ,2,val
DM-4782,JIRA project for the publication board,the lsst publication board requests a jira project for managing its workload.       ,2,val
DM-4783,Rename temporarily mariadb client,"mariadb client isn't compliant with mysqlproxy and eups doesn't allow to override it with regular mysql client, so it will be rename, so that mysqlclientdir reference in qserv table file can be removed (indeed, it brokes qservdistrib setup, but not qserv setup, ...)",1,val
DM-4784,Consulting in December,nan,3,val
DM-4785,Update provenance in baseline schema,current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. this story involves bringing cat/sql up to data and replacing existing prv_  tables with tables we came up with in the epic.,2,val
DM-4786,Packge mysqlproxy 0.8.5,see https:/mariadb.atlassian.net/browse/mdev 9389,2,val
DM-4788,FITS Visualizer porting: Mouse Readout: part 2: flux value,"call the server when mouse pauses, include the flux value in the readout. the should also include support for 3 color.",4,val
DM-4789,FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support,add toggle button that make the mouse readout lock to last position click on.  it will not longer update on move but by click  include: 3 color support,8,val
DM-4790,S17 Refactor MySQL Connection in Qserv,nan,38,val
DM-4791,F17 Setup Qserv and ImgServ with PanSTARRS data,"once the panstarrs data becomes public, we should load it to qserv. this epic involves partitioning data, loading to qserv and making it ready for analysis by friendly scientists (and for our internal testing). we should also make the panstarrs images available through imgserv, the work involves setting up webserv instance and configuring imgserv for panstarrs.",24,val
DM-4793,Refactor prototype docs into “Developer Guide” and Science Pipelines doc projects,"refactor https:/github.com/lsstsqre/lsststackdocs into two doc projects     lsst dm developer guide that will be published to developer.lsst.io, and    lsst science pipelines that will be published to pipelines.lsst.io",3,val
DM-4794,Write Zoom Options Popup,write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds  the maximum size.      activate this popup from visualize/ui/zoombutton.jsx,2,val
DM-4798,DetectCoaddSourcesTask.scaleVariance gets wrong result,"detectcoaddsourcestask.scalevariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). the current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels.    getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1357 and https:/hscjira.astro.princeton.edu/jira/browse/hsc1383.",1,val
DM-4799,Rotate Popup,nan,4,val
DM-4801,Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending,"in dm4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. this breaks the lsstdmdemo, as now the results of processing are slightly different. the short term solution as part of dm4410 was to run the demo with the defaults overridden to be what they were prior to dm 4410. in the long term the values used in the compare script should be updated to reflect what would be generated with running processccd with the stack defaults. ",1,val
DM-4805,Some wcs keywords need to be removed from the metadata of raw DECam data,"header keys such as pvij left in the raw metadata confuse the making of wcs in later processing steps.   for example, when calexp is read in makediscreteskymap.py, makecoaddtempexp.py, and so on, this message appears:    makewcs: interpreting ratansip/dectansip + pvij as tpv  makewcs warning: stripping pvi_j keys from projection ratpvsip/dectpvsip    these calexp are created by running processccd.py on raw data, and are mis interpreted as tpv.  ",7,val
DM-4806,Test stack with mariadbclient,"now that we switched qserv to mariadb, it'd be good to switch the rest of the stack. this story involves trying out if things still work if we switch mysqlclient to mariadbclient.",2,val
DM-4807,Add Shared Scan Table Information to CSS ,some information should be added to css to indicate if a table should be locked in memory for shared scans and the effect the table is likely to have on the time it takes to complete a query.,4,val
DM-4808,Package mariadbclient,"there are some very low level modules that depend on mysqlclient (for example daf_persistence). it'd be too harsh to make them depend on mariadb, so we should package mariadb client.",1,val
DM-4809,X16 Fine-tune Shared Scans,"fine tune shared scans code, in particular take advantage of unique queryid.",29,val
DM-4810,Provenance Prototyping,"build a proofofconcept provenance prototype for a selected pipeline, perhaps hcs.",50,val
DM-4814,Create validation_data set for DECam validation test,create a `validationdatadecam` to provide a few images for decam validation tests.    use the cosmos field data as currently available on ncsa being processed by .    select just a few images for now.,2,val
DM-4815,Planning for Software Documentation Deployment Service,write initial draft of http:/sqr006.lsst.io that specifies how the documentation deployment service will work.,4,val
DM-4817,Read and understand `ci_hsc` and plan relationship with `validate_drp`,read through and run the `cihsc` tests and plan for how this module and efforts should relate to `validatedrp`.    a. add capabilities to `validatedrp` to run the tests in `cihsc`.  (/)  b. compare frameworks. (/)  c. plan for how such validation and continuous integration data sets should be constructed. (/)  ,2,val
DM-4820,Improvement of raw data handling in DecamMapper,two minor improvements with better coding practice:   be more specific copying fits header keywords. avoid potential problems if unwelcome keywords appear in the header in the future. suggested in the discussions in dm4133.    reuse isr.getdefectlistfrommask for converting defects. a more efficient method that uses the footprintset constructor with a mask and a threshold has just been adopted in dm4800.     processing is not changed effectively.  ,1,val
DM-4821,HSC backport: Remove interpolated background before detection to reduce junk sources,this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1353 and https:/hscjira.astro.princeton.edu/jira/browse/hsc1360.    descriptions from hsc:     we typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. we can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. we can then add this back in after detection of footprints and peaks.      i forgot to set the useapprox=true for the background subtraction that runs before footprint and peak detection. this will then use the chebyshev instead of the spline.  ,1,val
DM-4822,Code review,"dm4133, dm4800, dm4709, dm4707, dm 4814",4,val
DM-4823,Add Dropdowns to Vis toolbar,add the dropdown to the vis tool bar,2,val
DM-4824,Clean up div and css layout on FitsDownloadDialog,fitsdownload dialogs html and css is not quite right. needs some clean up.,1,val
DM-4825,makeDiscreteSkymap has a default dataset of 'raw',"the default dataset type for command line tasks is raw.  in the case makediscreteskymaptask is asking the butler for calexp images.  this shouldn't be a problem, but in my case i have calexp images, but no raw images.  this causes the task to think there is no data to work on, so it exits.",1,val
DM-4826,Understand async queries in Qserv,"try to understand, without doing actual implementation what is involved in  implementation of support for asynchronous queries in qserv and possibly web interface. should result in a roadmap for implementation at all levels.  ",10,val
DM-4827,Adapt `validate_drp` to standard python and bin subdir sturcture,move python files into python/lsst namespace convention.  decide on where validatecfht.py and validatedecam.py executables should live  add package requirements to ups/validate_drp.table,1,val
DM-4829,"Finish Fits View Decoration: context toolbar, title, expand button, etc",nan,5,val
DM-4830,Add error handling to PsfFitter in meas::modelfit,"the shapeletpsfapprox task uses a class called psffitter which is not a simplealgorithm and does not support error handling.  add error handling to this class, and modify the task definition in psf.py to call an errorhandler fail() function when the algorithm's optimizer.run() call fails.    also, add unit tests",6,val
DM-4831,Add bright object masks to pipeline outputs,"given perpatch inputs providing     id, b, v, r, ra, dec, radius      for each star to be masked, use this information to set:   a bit in the mask plane for each affected pixel   a flag in the source catalogues for each object that has a centroid lying within this mask area    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1342 and https:/hscjira.astro.princeton.edu/jira/browse/hsc 1381.",3,val
DM-4833,Update configuration for Suprime-Cam,the obs_subaru configuration for suprimecam needs updating to match recent changes in the stack.    port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1372.,1,val
DM-4834,Preliminaries for LSST vs HSC pipeline comparison through coadd processing,this is the equivalent of dm3942 but through coadd processing.    relevant hsc tickets include:      https:/hscjira.astro.princeton.edu/jira/browse/hsc1371,1,val
DM-4835,Allow slurm to request total CPUs rather than nodes*processors.,"on some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node.    it also makes sense to use the smp option this way.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1369.",2,val
DM-4836,Fix logic for applying aperture corrections,"with the current flow, the aperture corrections are being applied only after all the measurement plugins have run through, independent of their execution order.  this results in plugins whose measurements rely on aperture corrected fluxes (i.e. with execution order > apcorrorder) being applied prior to the aperture correction, leading to erroneous results.  the only plugin currently affected by this is baseclassificationextendedness.    this ticket involves applying a temporary fix to ensure proper application and order of aperture corrections.  however, the problem highlights the fact that the current logic of how and when aperture corrections are applied should be reworked (on another ticket) to be less error prone.",6,val
DM-4837,Implement brighter-fatter correction,"please port the prototype brighterfatter correction work by will coulton from hsc.    this covers https:/hscjira.astro.princeton.edu/jira/browse/hsc1189,  https:/hscjira.astro.princeton.edu/jira/browse/hsc1368, https:/hscjira.astro.princeton.edu/jira/browse/hsc1368. note also the stand alone commits https:/github.com/hypersuprimecam/obssubaru/commit/783b124b6813f5745ce1e444f61fb0114d055907 and (if this work is performed after dm3373)  https:/github.com/hypersuprime cam/obssubaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d.",4,val
DM-4839,High-level overview of DRP processing,"create high level overview of data release production, probably as an annotated flowchart, for use in sizing model work and as a graphical table of contents for more detailed descriptions.",6,val
DM-4840,Add sky objects,"please add ""sources"" corresponding to empty sky (ie, at positions where nothing else has been detected) and include them in multiband processing.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1336 and https:/hscjira.astro.princeton.edu/jira/browse/hsc1358. ",4,val
DM-4841,Use high S/N band as reference for multiband forced photometry,we are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the s/n.  please change this to pick the highest s/n band as the reference band when the priority band s/n is sufficiently low.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1349.,1,val
DM-4842,Don't write HeavyFootprints in forced photometry,there's no need to persist heavyfootprints while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1345.,1,val
DM-4847,Add new blendedness metric,https:/hscjira.astro.princeton.edu/jira/browse/hsc 1316 shifts the calculation of blendedness from measdeblender to measalgorithms and defines a new blendedness metric in the process. please port it.,3,val
DM-4848,Measure photometric repeatability and correctness of reported errors,1. calculate and plot photometric variability across series of n images.  compare to reported photometric errors.  designed for n > 5.  2. calculate and plot delta flux / sigma_flux for multiple observations of stars in field.  this is related to 1. but is focused on n=2 to n=5.  3. fit uncertainty distribution vs. magnitude to identify any floor in the photometric uncertainty and to check performance vs. photon counts.,5,val
DM-4849,LDM-151 - comments from Jacek,"i am reading your https:/github.com/lsst/ldm 151/blob/draft/dmapplicationsdesign.tex, and i have some minor comments suggestions. i am going to add comments to this story to capture it. feel free to apply to ignore :)",1,val
DM-4850,Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask,measuremergedcoaddsourcestask.setisprimaryflag() and processcoaddtask.setisprimaryflag() are effectively the same code. please split this out into a separate task which both of the above can call.    this is a (partial) port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1112 and should include fixes from https:/hscjira.astro.princeton.edu/jira/browse/hsc1297.,2,val
DM-4851,XY Plot action and reducers,write action and reducers for xy plot,6,val
DM-4852,Implement zenodio.metadata to mediate Zenodo's API with local YAML metadata,"http:/zenodio.lsst.io is a python package we’re building to interact with zenodo. for our various doc/technote/publishing projects we want to use yaml files (embedded in a git repository, for example) to maintain deposition metadata so that the upload process itself can be automated.    the zenodio.metadata sub package provides a python representation of zenodo metadata (but not file or zenodo deposition metadata).    see dm 4725 for the upload api work, which consumes the metadata objects.",2,val
DM-4856,Add __setitem__ for columns in afw.table,"it's confusing to have to use an extra [:] to set a column in afw.table, and we can make that unnecessary if we override setitem as well as getitem.",2,val
DM-4857,Replace killproc and pidofproc with kill and pidof,running at ncsa on openstack revealed that our qservstop.sh and qservstatus.sh fail because of missing killproc and pidofproc. it looks like (see eg http:/stackoverflow.com/questions/3013866/killprocandpidofproconlinux) these are not very portable and it is better to use kill and pidof.,1,val
DM-4858,imagesDiffer doesn't handle overflow for unsigned integers,"i'm seeing a test failure in afw's testtestmethods.py, apparently due to my numpy (1.8.2) treating images that differ by  1 as differing by 65535 in both numpy.allclose and array subtraction (which doesn't promote to an unsigned type).    does this still cause problems in more recent versions of numpy?  if not, i imagine it's up to me to find a workaround for older versions if i want it fixed?    (assigning to  for now, just because i know he originally wrote this test and i hope he might know more)",1,val
DM-4861,"Please provide ""getting started"" documentation on writing meas_base algorithms","measbase provides a framework for writing measurement algorithms in a uniform way. however, documentation on exactly how this should be done is fragmentary:     there's some basic documentation in https:/lsstweb.ncsa.illinois.edu/doxygen/xmasterdoxydoc/measbase.html which provides a useful introduction, but doesn't discuss common idioms and helpers such as flaghandler, safecentroidextractor and transformations.   the https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20284390 are not intended as documentation and aren't kept uptodate as new features are added, but can still be a useful reference.    https:/github.com/lsstdm/oct15bootcamp/blob/measurement/measurement/measurement.pdf at the october 2015 bootcamp, but a set of slides is no substitute for proper documentation, and again there's no expectation that these will be kept uptodate.    please provide a centralized, maintained guide to writing meas_base plugins.",4,val
DM-4862,Add point selection,"click and highlight a point.  is on when mouse readout ""lock by click"" is on. however, can me turned on externally by adding toolbar context menu options.",2,val
DM-4864,Setup webserv for SUI,"this story involves setting up a webserv in a vm (ncsa openstack) with a small data set: images and corresponding database catalog. we need to    setup vm    build the stack for webserv and qserv    identify images to load    run qserv in the vm    run ingest to load the data to mysql (mysql will run on lsst10) and qserv (qserv will run directly in the vm)    run two webservers   one with mysql backend, one with qserv backend     open the port numbers for the ipac team",10,val
DM-4865,Port HSC background matching routines,hsc has its own implementation of background matching: see background.py in hscpipe. please port it to the lsst stack.,4,val
DM-4866,Filter mask planes propagated to coadds,"some mask planes  crosstalk, not_deblended  do not need to be propagated to coadds. add an option to remove them.    this is a port of work performed on https:/hscjira.astro.princeton.edu/jira/browse/hsc1174 and https:/hscjira.astro.princeton.edu/jira/browse/hsc1294.",2,val
DM-4867,scisql build scripts are buggy ,"the scisql build script logic for mysql/mariadb version checking is broken on all platforms. there are also assumptions about shared library naming that do not hold on os/x, which means that the deployment scripts are likely broken on all platforms other than linux.",2,val
DM-4868,Setup LSST stack for verification datasets work,created a script to setup required lsst stack packages for bulge survey processing.  ,10,val
DM-4869,Installing a reference catalog to use in bulge survey processing,install astrometry.net index files for 2mass all sky catalog,5,val
DM-4870,Setup orchestration environment at lsstdev for bulge survey processing,follow instructions at   https:/confluence.lsstcorp.org/display/dm/1.quickstart lsstclusterorchestration    to set up the required packages to run the lsst stack at lsst cluster.,10,val
DM-4871,Debugging lsst.astrometry task in bulge survey processing,the decam bulge survey is being processed as part of the verification data sets effort. during astrometry calibration task a large number of failures (affecting ~100 of 213 visits) have been found in calibrate.astrometry.matcher. we report here details of the investigation around this issue. part of the task is to learn how to use the task built in debug.     more info:    https:/confluence.lsstcorp.org/display/sqre/bulgesurveyprocessing#bulgesurveyprocessing results,10,val
DM-4872,Setup firefly example for image visualization ,start from example provided by the firefly team    https:/github.com/lsst/suit,10,val
DM-4873,Test the matchOptimisticB astrometric matcher,"the matchoptimisticb matcher fails on many visits of the bulge verification dataset.  this prompted a deeper investigation of the performance of the matcher.  angelo and david developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that.  this should be robust enough for nearly all datasets that the lsst stack will be used on.",3,val
DM-4874,Write a firefly search processor that retrieves image paths from the butler,nan,5,val
DM-4875,base has no readme,"the base package does not have a readme file, so it's unclear what it's for. the package name is also somewhat unfortunate, being so generic, but at least with a readme it would be clearer how important it is (if it is, in fact, important).",1,val
DM-4876,Compile list of DM simulation needs for Andy Connolly,compile list of dm simulation needs over the next ~6 months to give to andy connolly (simulations lead).,3,val
DM-4877,Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.,"diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.     use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd.     use afw.display.ds9 to display the image and overlay the sources",5,val
DM-4878,Propagate flags from individual visit measurements to coadd measurements,"it is useful to be able to identify suitable psf stars from a coadd catalogue. however, the psf is not determined on the coadd, but from all the inputs. add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the psf.    make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the psf model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some"").    make the task sufficiently general that it can be used for propagating arbitrary flags.    this is a port of work carried out on https:/hscjira.astro.princeton.edu/jira/browse/hsc1052 and (part of) https:/hscjira.astro.princeton.edu/jira/browse/hsc1293.",2,val
DM-4879,Make coadd input catalogs contiguous,"it's convenient if we can assume that coadd input catalogs are contiguous  it simplifies the implementation of propagatevisitflagstask (dm4878), for example. make it so.    this is port of work carried out on https:/hscjira.astro.princeton.edu/jira/browse/hsc1293.",1,val
DM-4880,Test capabilities of python bokeh plotting library for making interactive plots - I,"we are testing the bokeh library plot for interactive visualization in the web, the python api is atractive and allows rapid prototype which is good for square build up its qa system.    some examples are available in this repo, including a scatter plot with linked histograms in both axis which seems really useful.    https:/github.com/lsstsqre/bokehplots    a more complete demonstration of bokeh is available in this webminar:     https:/continuum analytics.wistia.com/medias/f6wp9dam91    ",20,val
DM-4882,base_Variance plugin generates errors in lsst_dm_stack_demo,"since dm4235 was merged, we see a bunch of messages along the lines of:    processccd.measurement warning: error in basevariance.measure on record 427969358631797076: the center is outside the footprint of the source record    in the output from lsstdmstackdemo. (see e.g. https:/ci.lsst.codes/job/stackosmatrix/label=centos6/7482/console#consolesection3). it's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem.",2,val
DM-4884,S17 Improve Qserv Integration Tests,"integration tests need improvements, in particular, we want to run multinode integration tests easily (possibly without docker), get rid of mononode test. we should catch errors from individual tests.",15,val
DM-4885,Improve/simplify multi-node tests,nan,10,val
DM-4886,Script launch of HTCondor pool on Nebula OpenStack,"the ability to automate the launch of htcondor pools of worker nodes  (e.g., with lsst software installed) on the nebula openstack can be useful  in several ways for lsst dm.  on one hand, users can start up their own customized  pool should the standard pool available on other resources such as lsst dev not  be suitable (e.g., not enough cores, not enough memory per core/slot,  customized software not installed on the systems, etc.)  also, scripted launch  of a pool can be part of the development of a solution for offering  ""batch"" scheduling to the nebula openstack, following an approach similar to e.g.,  canfar ( http:/ , http:/cloudscheduler.org)  whereby  a ""cloud scheduler"" is used in conjunction with an htcondor central manager  to provide batch access to the cloud (i.e., submitted jobs are placed into an htcondor queue,  that will execute on launched instances, when those instances join the working pool.)  ",24,val
DM-4887,Refactor measurement afterburners into a new plugin system,"some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms:    they must be run after at least some other highlevel plugins, and may be run after all of them.    they do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs.    they may require an aggregation stage of some sort to be run on the regular plugin output before they can be run.    some examples include:    star/galaxy classification (with training done after measurement and before classification).    applying aperture corrections (estimating the correction must be done first).     bfd's p, q, r statistics (requires a prior estimated from deep data).    we should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from singleframemeasurementtask.  this will simplify some of the currently contorted logic required to make s/g classification happen after aperture correction, while making room for hierarchical inference algorithms like bfd and bayesian s/g classification in the future.    (we will not be able to support bfd immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction).    this work should probably be delayed until after the hsc merge and 's rewrite of processccdtask are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result.",8,val
DM-4888,Update git-lfs documentation to work with git-lfs 1.1.0+,"the git lfs client (1.1.0+) does not support empty username and passwords. to work around this, users can store the appropriate credentials directly with the credential helper.",3,val
DM-4889,Update git-lfs repositories to point to the git-lfs documentation.,update gitlfs repositories to point to the gitlfs documentation.    all documentation should be generic and point to:    http:/developer.lsst.io/en/latest/tools/git_lfs.html  ,1,val
DM-4892,Take DECam data with collimated beam projector,[mfisherlevine] and [rhl] will travel to ctio to observe on the blanco 4m telescope.,30,val
DM-4893,Write tutorial describing remote IPython + ds9 on lsst-dev," recently figured out how to set up his system to run a remote ipython kernel on lsst dev and interact with it from his laptop, including streaming image display from the remote system to a local instance of ds9.    he will write all this up so that others in the community can easily do the same.",2,val
DM-4894,Ingest DECam/CBP data into LSST stack," will ingest the data taken in dm 4892 into the lsst stack. initial experiments indicate problems with:     bias subtraction   flat fielding    bad pixel masks    these may already be remedied by work on obs_decam; if not, he will file stories and fix them.",3,val
DM-4895,Prepare calibration products for analysing DECam data,"determine if existing bad pixel masks, flats, etc are adequate for analysing the dm 4893 decam data, and, if not, provide alternatives.",3,val
DM-4897,Qualitative exploration of the CBP/DECam data,"having got the cbp/decam data loaded into the stack, explore the parameter space and understand data.    this should result in a series of stories describing more detailed analysis with quantitative results.",8,val
DM-4898,Implement the simulation and testing framework for analyzing image differencing,"we need to be able to test all aspects of image differencing.  this includes template generation, astrometric registration, and differencing.  we know that dcr will be an effect that will need to be mitigated so we will have to be able to simulate it and show how well various techniques deal with it.",50,val
DM-4899,Implement simulations for testing image differencing.,"implement a suite of simulations tools for testing the image differencing techniques, specifically with an eye toward dealing with dcr.",15,val
DM-4901,Use yaml configuration files to store camera-specific data ID and ref image information for validation testing.,"currently there is validatecfht.py and validatedecam.py as code.  these differ in just having defaultdata functions that specify the dataids to consider and the dataids to use as a reference for comparison.    storing the information necessary to create these sets of dataids in separate data files, to be stored as yaml would  1. improve the separation of code and data  2. clarify the usage and necessary information to run on a new or different set of data  3. make it easier to run different subsets easily by specifying a different input file    the proposals is that validatecfht.py and validatedecam.py would disappear from bin and be replaced by just validate_drp.py.  the examples in examples/runcfhttest.sh and examples/rundecamtest.sh will be updated to show the new usage.  the readme will also be updated.",1,val
DM-4903,"Expand button hide/show, delete button hide/show, display title options,","expand button hide/show, delete button hide/show, display title options,  support pv.hidetitledetail to control showing zoom level and rotation info (used by planck)  support external title bar (planck as well)  support checkbox on title bar (planck)  ",4,val
DM-4904,Buffer overrun in wcslib causes stack corruption,"the buffer 'msg' in wcsfix.c is used to report attempts by wcslib to reformat units found in fits files. it is allocated on the stack (in function 'unitfix') using a preprocessor macro defined size of 160 chars (set in wcserr.h). when attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by hsc seem particularly prone to triggering this behavior) and results in the session being terminated on ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. we have reported the bug to the creators of wcslib. as a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing wcserrmsglength defined in wcserr.h      we are providing a small python example that demonstrates the problem. run it as  python test.py /raw/    we are also providing a simple c program to demonstrate the bug. compile it as  cc fsanitize=address g i$wcslibdir/include/wcslib o test test.c l$wcslibdir/lib lwcs (on linux)  cc fsanitize=address g l$wcslibdir/lib lwcs i$wcslib_dir/include/wcslib o test test.c (on mac os x)",2,val
DM-4906,Investigate astrometry warnings from processing raw DECam data,"this ticket includes efforts to troubleshoot and improve processing decam raw data in jan 2016.   investigations of the warnings from solving astrometry led to dm4805, dm4859. this ticket also includes partial efforts in dm4859.         for validation, i ran processccd.py with two visits of raw stripe 82 decam data with and without the changes in dm4859 (using the first camera geometry fix, which is different from the final fix).  the script validatedecam in validate_drp is used to check astrometric scatter of the sources between the two visits.  output plots are in the attachments.  by fixing dm4859, the median astrometric scatter (mag < 21) decreases from 31.3 mas to 26.1 mas.  the number of matches between two visits increases from 56768 to 71265.  also attached are the ra/dec patches plots, using the showvisitskymap.py script from dm4095. the two colors represent the two visits of calexp wcs.  southern ccds had bad astrometric solutions before dm 4859.  the validation results are consistent with the patches visualization.   ",12,val
DM-4907,Cyber security infrastructure requirements,"documenting cyber security operational requirements by lsst, particularly at the obs. site.",2,val
DM-4908,Security plan renewal,"continuing work on cyber sec. plan renewal.  dm moving along, po slated next.",1,val
DM-4909,DM security meeting,security meeting/planning with lsst dm team at ncsa.,1,val
DM-4911,LSST IaM meetings at NCSA,nan,1,val
DM-4912,LSST IaM bi-weekly coordinating meeting,"meeting between ncsa csd group, ncsa lsst dm group, and other lsst groups.",1,val
DM-4915,Add option for object name resolution,"for some object names resolved by ned, the position is not right. in this situation, it would be better to get the position from simbad. currently, firefly offers two options: first ned, then simbad; first simbad, then ned. the third option to be added would be ""the best position according to the object type"".     it should check the object type returned by ned, making a decision whether to get position from simbad instead; and vice versa. ",6,val
DM-4916,Test obs_decam with processed data,"sometimes decamspecific bugs only reveal in or affect the processed data. for example the bug of dm4859 reveals in the postisrccd products.  if the bugs are decamspecific, some changes in obsdecam are likely needed.  it would be useful to have a more convenient way to test those changes. in this ticket i modify testdatadecam so that those data can be processed, and then allow wider options in the obsdecam unit tests.    i add testprocessccd.py in obsdecam that runs processccd.py with raw and calibration data in testdata_decam.  besides a short sanity check, i add a test (testwcspostisr) that tests dm4859. testwcspostisr fails without the dm 4859 fix, and passes with it.  ",3,val
DM-4917,Porting encodeURL of the java FitsDownlaodDialog code to javascript ,"when download an image,  the proper name needs to be resolved based on the url and   the information about the image.  in java code, it has the following three methods:     encodeurl  makefilename  maketitlefilename      these method should be ported to javascript.  thus, the javascript version of the fitsdownloaddialog will save the file in the same manner. ",2,val
DM-4919,Test performance of vertical-partition joins in mysql,"we are planning to vertically partition some tables (for example object). we should make sure such joins across say 5, 10 or 20 tables are not a problem for mysql from performance standpoint. the testing involves creating a wide table (say 200 columns) and testing a speed of full scan, then slicing that table vertically into different number of columns and using join to assemble the pieces together.",4,val
DM-4920,Support Multi image fits and controls,"add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number.    make sure the store will support multi images, add next, prev actions, etc",4,val
DM-4921,Make obs_subaru build with OS X SIP,"because of os x sip, obs_subaru fails to build on os x 10.11. in the hsc/sconscript file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs. ",1,val
DM-4923,want to see locations in trace when butler raises because multiple locations were found,"daf_persistence 11.02g56eb0a1+1 gives the unhelpful error message:      > runtimeerror: unable to retrieve bias for : no unique lookup for ['calibdate', 'calibversion'] from : 2 matches      (the old butler did this too).  the user wants to know what the 2 matches were  it's user error, but the user needs help and  printing the first few options (nicely formatted) is very useful.  i think i did this on the hsc side.      the butler code in question is actually in butlerutils/python/lsst/daf/butlerutils/mapping.py and my post doc gave me the wrong package.    it's in need():    >         if len(lookups) != 1:  >             raise runtimeerror, ""no unique lookup for %s from %s: %d matches"" % (newprops, newid, len(lookups))  ",1,val
DM-4925,"Make FlagHandler, SafeCentroidExtractor usable from Python","the measbase framework includes safecentroidextractor, a convenience routine for extracting a centroid from a source record, setting a consistent set of flags if that's not possible or if the centroid is in some way compromised. this consistent flag handling is made possible by the use of the flaghandler class.    unfortunately, flaghandler is not meaningfully usable from python, not least because it's impossible to define flags:    >>> import lsst.meas.base as measbase  >>> measbase.flagdefinition(""flag"", ""doc"")  [...]  typeerror: init_() takes exactly 1 argument (3 given)  >>> fd = measbase.flagdefinition()  >>> fd.name = ""flag""  [...]  attributeerror: you cannot add attributes to / >      looking further, even were we able to create flagdefinitions, the flaghandler is initialized with pointers to the beginning/end of a container of them, which seems like a stretch for python code.    please add python support for these routines.",2,val
DM-4926,Centroids fall outside Footprints,"in dm4882, we observed a number of centroids measured while running the lsstdmstack_demo routines fall outside their associated footprints. this was seen with both the naivecentroid and the sdsscentroid centroiders.    for the purposes of dm4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small footprints entirely.",8,val
DM-4928,Fix intermittent testQdisp failure,"the mocks used in the executive class don't mock cancellation correctly and doing so would require significant effort. when executive::squash() is called, the mocks threads are already running but waiting on the go barrier. squash() calls jobquery::cancel() for each thread and cancel() calls markcomplete() for the job because a queryresource has not been aquirred from xrootd. once all the jobs are cancelled and go is set to true, the ex.join() command doesn't wait for the jobs to complete since markcomplete() has already been called for all of the jobs. if any of the jobs take longer to complete than the main thread, they call markcomplete for an executive that no longer exists and cause the test to fail.",1,val
DM-4929,Fix build of MariaDB on OS X El Capitan,the current mariadb eups package does not build on os x el capitan because os x no longer ships with openssl developer files. mariadb has a build option to use a bundled ssl library in preference to openssl but the logic for automatically switching to this version breaks when the anaconda openssl libraries are present.,1,val
DM-4930,Deploy 4 bare metal hosts for testing Base to Archive transfer implementation,"james needs to test network communication methodologies in an environment that mimics the expected real world conditions. in order to minimize the complications with debugging, using bare metal machines in the first phase is preferred.    we can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose.",4,val
DM-4931,Qserv build fails on El Capitan with missing OpenSSL,qserv does not build on os x el capitan due to the absence of openssl include files. apple now only ship the openssl library (for backwards compatibility reasons). qserv only uses ssl in two places to calculate digests (md5 and sha). this functionality is available in the apple commoncrypto library. qserv digest code needs to be taught how to use commoncrypto.,2,val
DM-4932,Track kernel panic issue,"the line that caused the kernel panic is in modules/mysql/mysqlconnection.cc line 151.  currently the line is fine and is:          std::string const killsql = ""kill query "" + std::to_string(threadid);    this version of the line will occasionally cause the kernel panic (note the missing %1% that should be after kill query).          std::string killsql = boost::str(bo  ost::format(""kill query "") % threadid);  ",3,val
DM-4933,Create a utility function do do spherical geometry averaging,"i would like to calculate a correct average and rms for a set of ra, dec positions.    neither [jbosch] nor [price] knew of an easy, simple function to do that that existed in the stack.   suggested:      mean = sum(afwgeom.extent3d(coord.tovector()) for coord in coordlist, afwgeom.point3d(0, 0, 0))  mean /= len(coordlist)  mean = afwcoord.icrscoord(mean)      that makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack).    pedantically it's also not the best way to do a mean while preserving precision, but i don't anticipate that to be an issue in practice.    creating a function that did this would provide clarity.  i don't know where that function should live.    note: i know how to do this in astropy.  i'm intentionally not using astropy here.  but part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the lsst stack"".",1,val
DM-4934,on-going support to Camera team in visualization at UIUC,attend the weekly meeting and answer questions as needed,2,val
DM-4936,Enable validateMatches in ci_hsc,"python/lsst/ci/hsc/validate.py in cihsc https:/github.com/lsst/cihsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#l78:        def validatematches(self, dataid):          # xxx lsst.meas.astrom.readmatches is gone!          return    readmatches (or its successor) should be back in place as of dm 3633. please enable this test.",2,val
DM-4937,multiple CVEs relevant to mariadb 10.1.9 and mysql,"multiple cves have been released this week for mysql & mariadb.  the current eups product for mariadb is bundling 10.1.9, which is affected.  several of the cves do not yet provide details, which typically means they are ""really bad"".    https:/github.com/lsst/mariadb/blob/master/upstream/mariadb10.1.9.tar.gz    https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160505  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160546  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160596  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160597  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160598  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160600  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160606  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160608  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160609  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20160616  https:/cve.mitre.org/cgibin/cvename.cgi?name=cve20162047",1,val
DM-4938,Update scisql to v0.3.5,"in order to update mariadb to v10.1.10 scisql needs to also be updated to deal with the hardcoded version checking. for the current version we get this error with the latest mariadb:    :::::  [20160128t16:51:40.539306z]     userfunction(self)  :::::  [20160128t16:51:40.539334z]   file ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure  :::::  [20160128t16:51:40.539346z]     ctx.checkmysql()  :::::  [20160128t16:51:40.539392z]   file ""/home/build0/lsstsw/build/scisql/.waf1.6.1130618c54883417962c38f5d395f83584/waflib/configure.py"", line 221, in fun  :::::  [20160128t16:51:40.539410z]     return f(k, kw)  :::::  [20160128t16:51:40.539432z]   file ""tools/mysqlwaf.py"", line 85, in checkmysql  :::::  [20160128t16:51:40.539451z]     (ok, msg) = mysqlversion.check(version)  :::::  [20160128t16:51:40.539473z]   file ""tools/mysqlversion.py"", line 74, in check  :::::  [20160128t16:51:40.539514z]     if not comparisonop(versionnums, constraintnums):  :::::  [201601 28t16:51:40.539547z] unboundlocalerror: local variable 'constraintnums' referenced before assignment  failed during rebuild of dm stack.  ",1,val
DM-4939,IRSA developer mentoring effort,irsa is contributing to the firefly package development.  we need to put in time to mentor the developers. ,2,val
DM-4940,IRSA developer mentoring effort,irsa is contributing to firefly development. we need to mentor the new developers.,2,val
DM-4942,Fix type inference and return types makeMaskedImage et al,"the makemaskedimage function and cousins like makeexposure don't do the type inference they're supposed to do in c, because they use the old typename image/::ptr approach instead of ptr(image/).    they also return raw pointers, which is dangerous.  they should be converted to return shared_ptrs.  note that this will have to include adjusting or removing swig code (probably %newobject statements) that deal with taking ownership of the raw pointers.",1,val
DM-4943,Switch to MemManReal in the worker scheduler,"first iteration of worker scheduler uses a skeleton of the memory manager that doesn't actually look at any of the tables, files, or memory. the scheduler needs to be switched to memmanreal. ",15,val
DM-4944,butler should transparently allow files to be compressed or not,"see the conversation on c.l.o. at .    the summary is, when the mapper returns e.g. a non compressed file name e.g. foo.fits, that file may be compressed and the filename may reflect this e.g. in reality it might be named foo.fits.gz. on a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer.    tbd if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally).",6,val
DM-4945,Schemas for QA information,"this ticket is to capture preliminary design work we are doing for storage of qa system information, which we are working with the database team on.     as well as prior experience, jacek has made us aware of the sdqa tables in the schema:    https:/lsst web.ncsa.illinois.edu/schema/index.php?sver=baseline    and also plan on mining pipeqa for quantities of interest.     once we have a draft, there will be an rfd for soliciting further input. ",10,val
DM-4946,afw Wcs object copying does not copy exactly,"a probable bug in wcslib is causing wcscopy to create copies of wcs objects which are not the same as the object that was copied. in some cases when this object is passed to wcsset it fails, as the wcs object contains impossible values.    this has behaviour is nondeterministic (failure is only seen occasionally). the error has only been observed on osx, but we do not believe it to be operating system dependent (except insofar as different systems and compilers produce different memory layouts and hence different failure modes). this reliably causes cihsc to fail when running on a mac.    relevant lines in afw are image/wcs.cc:140 and the wcs copy constructor in image/wcs.cc:468    additionally a bug has been found in image/wcs.cc on line 485 where the flag property should be set on an element, and not on the object itself, ie wcsinfo[i]>flag =  1;.",6,val
DM-4948,Please improve the documentation for TransformTask and derivatives,"while working on dm4629 (overhauling processccdtask)  stumbled over transformtask, which he wasn't previously familiar with. existing doxygen documentation covers what this task does, but lacks context as to why it's useful. please provide a highlevel overview of what the intention is here.",2,val
DM-4949,Improve MySQL proxy code and add unit tests,qserv's proxy needs some cleanup:    1. standardize passing of q and qu parameters to methods  2. comment removal may have never worked  3. whitespace translation is likely buggy  4. a few unit tests verifying routing of queries would be nice  ,4,val
DM-4950,Build MVP of ltd-keeper web app covering ltd-mason interface,this ticket is to create an mvp of the ltdkeeper web app (restful api) that tracks versions of lsst the docs’ published software documentation. specifically this ticket will implement the restful endpoints needed by ltdmason. see http:/sqr006.lsst.io for design information.    http:/sqr006.lsst.io will be updated in this ticket as the design is clarified in implementation.,14,val
DM-4951,Add S3/Route53 project provisioning capabilities to ltd-keeper,"an authenticated user should be able to provision (and likewise, delete) an entire published software documentation project via ltdkeeper’s restful api. this includes creating an s3 bucket in square’s aws account and setting up route 53 dns. the user should also be able to delete a project. this ticket will add aws affordances to ltdkeeper. dm 4950 will be responsible for hooking this functionality into the methods that service api calls.",3,val
DM-4952,delegate argument parsing to CmdLineTask instances,"commandline argument parsing of data ids for cmdlinetask s is currently defined at the class level, which means that we cannot make data id definitions dependent on task configuration.  that in turn requires custom processccd scripts for cameras that start processing at a level other than ""raw"" (sdss, decam with community pipeline isr, possibly cfht).    instead, we should let cmdlinetask instances setup commandline parsing; after a cmdlinetask is constructed, it will have access to its final configuration tree, and can better choose how to parse its id arguments.    i've assigned this to process middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [rowen], [price], or [jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly.  i'll leave that up to [swinbank], [krughoff], and [mgelman2] to decide.",2,val
DM-4955,Update pyfits,the final version of pyfits has just been released. this ticket covers updating to that version. this will be helpful in determining whether the migration to astropy.io.fits will be straightforward or complicated.,1,val
DM-4956,Adapt SRD-based measurements of astrometric performance for validate_drp,"adapt the srdbased specifications for calculation of astrometric performance.  follow the examples for am1, am2 as presented at    https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=41785659    and detailed in dm3057, dm 3064",4,val
DM-4957,Generate JSON output from validate_drp for inclusion in a test harness,generate json output from validatedrp for inclusion in a test harness.    generate a file that summarizes the key metrics calculated by `validatedrp`.      develop naming conventions that will make it easy to plug into the eventual harness being developed as part of dm 2050.,2,val
DM-4958,Hard copy support- saving regions,this ticket will only do the region saving.    the scope has change somewhat since region saving will talk a little longer and making the png requires some server side work. dm 6139,10,val
DM-4959,ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP,"the cihsc package executes a number of command line tasks directly from scons based on command directives in a sconstruct file. on an osx 10.11 system with sip enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:   the scons executable starts with a #!/usr/bin/env python. running through /usr/bin/env strips dyldlibrarypath from the environment. (duplicates dm4954)   scons executes command using the https:/bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/scons/platform/posix.py?at=default&fileviewer=fileviewdefault#posix.py105. by default, that means /bin/sh on a mac, which, again, will strip dyldlibrarypath.    please make it possible to run cihsc on such a system.",1,val
DM-4960,LSST vs. HSC stack comparison: PSF estimation,"in order to determine the cause of the output differences between single frame processing runs of the same data using the lsst vs. hsc stacks (see figures attached to dm4730), a detailed look at some of the image characterization steps is required.  this ticket involves a detailed investigation of the initial psf estimation including:     a comparison of the initial object detection (will likely involve looking at the initial background estimate as well as the specific assignment of footprints)   which objects are selected as psf candidates   the initial psf model (as a function of position)  ",5,val
DM-4961,Obs_Subaru camera mapper has wrong deep_assembleCoadd_config,"when lsst switched to using safeclipassemblecoaddtask, the camera mapper for hsc was not updated accordingly. this causes cihsc to fail when it attempts to verify the config class type for the deepcoadd. camera mapper should be updated accordingly",1,val
DM-4962,January Operation Support Related Tasks,"account cleanup process for existing infrastructure (identify accounts, assign sponsors)    reconcile inventory between ncsa and aura (on going). mock request was generated by aura for dry run audit. several machines have been found not included in inventory. task to be completed in february.",3,val
DM-4963,Investigate Roger as fallover for Nebula,"investigate roger openstack as fallback for nebula during outages. internally, this required technical and coordination meetings. externally, this required interfacing with square in order to facilitate a proper evaluation.     this task is ongoing. ",2,val
DM-4964,January AAA Tasks,attended local aaa meetings and reviewed documentation. ,1,val
DM-4965,January Tasks,"security meeting with paul, bill, eyrich to review goals and coordinate efforts.   initial draft of the procurement plan. waiting for hardware contract.   updates to internal fy16 cost estimate spreadsheet and planning (not ldm 144).  updating expected expenditures based on update quotes.   meetings and discussions with obfs with respect to new vendors within mhec and procurement approval processing for fy16 components.",4,val
DM-4966,January Tasks,technology and pricing updates to ldm144. explanation update in ldm143.    meetings with multiple vendors re: longer term technology forecasts.,15,val
DM-4967,January Tasks,mtg w/ in2p3 re: itil implementation experiences.     mtg w/ in2p3 re: tape recall ordering,1,val
DM-4968,Jason January Tasks,"activities this month include: it sys admin meetings, lsst internal project meetings, conducting, coordinating, discussing interviews. meeting with candidates. ici coordination meeting (randy). discussion of worktobedone with onboarded teammates. relaying task prioritization to it for lsstrelated activities. ",4,val
DM-4969,DM Power Requirements,further discussions about power requirements at the chilean dc.,2,val
DM-4970,"Investigate logging, monitoring and metrics technologies and architecture","investigate technologies and architectures to use with panopticon, our logging system. perform preliminary research and evaluations into elk (elasticsearch, logstash and kibana), extensions to elk and other alternatives.",24,val
DM-4971,"Meetings, Jan 2016","verfication dataset meetings, techtalk, rfd, local middleware related meetings, etc",2,val
DM-4972,"LOE, Jan 2016","lsst local group meetings, postdoc meeting, other local meetings, etc",2,val
DM-4973,Reconsider high detection threshold in CharacterizeImageTask,"https:/community.lsst.org/t/whywasdetectionincludethresholdmultiplier10foroldprocessccdtask/500/6 that we consider providing psf estimation with the n brightest sources in the image, rather than only detecting bright sources.    [rowen] reasonably believes that this is beyond the scope of dm4692, hence this new issue.    there should be very little new code needed here, but it may involve quite a bit of experimentation and validation.",8,val
DM-4983,upstream patches/deps from conda-lsst,"where ever possible, missing dep information and patches from conda lsst should be upstreamed.  the patches have already been observed to cause builds to fail due to upstream changes.",3,val
DM-4985,Finish data distribution prototype (March),nan,3,val
DM-4990,Prepare for auth session at JTM,"prepare for jtm session with a working title of “how authentication/authorization technology can be used to implement and enforce data access rights and operational processes for lsst"".  prepare a final title and agenda for the session. tuesday from 3:30pm   5:00pm.",5,val
DM-4991,Save algorithm metadata in multiband.py,"the various tasks in multiband.py do not attach the self.algmetadata instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures.    we should also make sure this feature is maintained in the processccd.py rewrite.",3,val
DM-4992,work flow of light curve visulizaiton,"generate a description document of work flow that a scientist would go through in order to do time series research, visualize the light curve.",4,val
DM-4993,review of dependency on the third party packages,"we need to periodically review the status of the third party software packages that firefly depends on. making a plan to do upgrade if needed.   package.json lists out the dependencies firefly has on the third party software. the attached file was last modified 20160209.    package.json_version lists the current version of the third party packages, major changes were indicated by (m). the attached file was created on 20160229.     bq.      ""babel""     : ""5.8.34"",                           6.5.2 (m)      ""history""   : ""1.17.0"",                           2.0.0 (m)      ""icepick""   : ""0.2.0"",                            1.1.0 (m)                ""reacthighcharts"": ""5.0.6"",                      7.0.0 (m)      ""reactredux"": ""3.1.2"",                           4.4.0 (m)      ""reactsplitpane"": ""0.1.22"",                     2.0.1 (m)      ""reduxthunk"": ""0.1.0"",                           1.0.3 (m)      ""reduxlogger"": ""1.0.9"",                          2.6.1 (m)      ""validator"" : ""4.5.0"",                            5.1.0 (m)      ""chai"": ""2.3.0"",                                 3.5.0 (m)      ""esprimafb"": ""14001.1.0devharmonyfb"",        15001.1001.0devharmonyfb (m)      ""babeleslint""      : ""4.1.3"",                   5.0.0 (m)      ""babelloader""      : ""5.3.2"",                   6.2.4 (m)      ""babelpluginreacttransform"": ""1.1.0"",         2.0.0 (m)      ""babelruntime""     : ""5.8.20"",                  6.6.0 (m)      ""eslint""            : ""1.10.3"",                  2.2.0 (m)      ""eslintconfigairbnb"": ""0.1.0"",                  6.0.2 (m) works with eslint 2.2.0      ""eslintpluginreact"": ""3.5.1"",                  4.1.0 (m)  works with eslint 2.2.0      ""extracttextwebpackplugin"": ""0.8.0"",          1.0.1 (m)      ""htmlwebpackplugin"": ""1.6.1"",                  2.9.0 (m)      ""karmasinonchai"": ""0.3.0"",                     1.2.0 (m)      ""redux devtools""    : ""2.1.2"",                   3.3.1 (m)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (m)          ",2,val
DM-4994,Design single-sign-on authentication system for webserv,"outline a design of the authentication system (based on components provided by ncsa) that will support single sign on. current thinking involves two tokens: application token to certify the app is legitimate and to determine which users it can represent, and user token",6,val
DM-4995,Extend webserv API to pass security tokens,extend the https:/confluence.lsstcorp.org/display/dm/ap to pass security tokens.,8,val
DM-4996,Update validate_drp for El Capitan,validatedrp does not work on el capitan due to sip (system integrity protection) stripping dyldlibrarypath from shell scripts. the simple fix is to add    export dyldlibrarypath=$    near the top of the scripts.,1,val
DM-4997,Benchmark dipole measurement (dipole fitting),"benchmark dipole measurement (dipole fitting), compare speed directly to psffit (and/or galaxy measurement) task. runtime should be comparable (~factor of two?)  if not understand why. evaluate new implementation vs. current impl. accuracy?",8,val
DM-4998,Fix rotation for isr in obs_subaru,"approximately half of the hsc ccds are rotated 180 deg with respect to the others.  two others have 90 deg rotations and another two have 270 deg rotations (see http:/ .  the raw images for the rotated ccds thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections.  this is accomplished by rotating the exposure using the rotated context manager function in obssubaru's isr.py and the nquarter specification in the policy file for each ccd.  currently, rotated uses afw's rotateimageby90 (which apparently rotates in a counterclockwise direction) to rotated the exposure by 4  nquarter turns.  this turns out to be the wrong rotation for the odd nquarter ccds as shown here:     top left = raw exposure as read in  top right = flatfield exposure as read in  bottom left = incorrectly_ rotated raw exposure prior to flatfield correction",2,val
DM-4999,Implement new dipole fitting algorithm as SimpleAlgorithm,"implement new dipole fitting algorithm as simplealgorithm  implement measure, fail methods, define flags",14,val
DM-5002,Make ci_hsc resumable,"if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. this is because of the use of functools.partial to generate dynamic function. these differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. using lambda would suffer from the same problem. this ticket should change how the function signature is calculated such that scons can be resumed.    this work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole sconstruct file to run again.",2,val
DM-5005,Please trim config overrides in validate_drp,"validatedrp will test more of our code if it uses default config parameters wherever possible. to that effect i would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    for decam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    for cfht there are many overrides that are different, and an important question is whether the overrides in this package are better for cfht data than the overrides in obscfht; if so, please move them to obscfht.    as a heads up: the default star selector is changing from ""secondmoment"" to ""objectsize"" in dm4692 and i hope to allow that in validatedrp, since it works better and is better supported.    sorry for the incorrect component, but validate_drp is not yet a supported component in jira (see dm5004)",1,val
DM-5006,remove REUSE_DATAREPO in testCoadds in pipe_tasks,"when the test fails and the output directory is written but not populated, subsequent test executions fail every time until the output directory is deleted or reusedatarepo is set to false. this is misleading for users who don't know about this hidden feature.    furthermore, the reusedatarepo=false feature is broken; setting it false causes nameerror: global name 'datarepo_root' is not defined.    it would be better if the test cleaned up after itself (deleted all outputs) every time. if it's really important to reuse the outputs then the dir should be cleaned up in the case of failed writes and/or corruption.    ",1,val
DM-5008,F16 Data Access Model Refresh,"a refresh of the storage / io model (ldm 141). the work involves understanding cost impact, and discussing the impact on science. ",33,val
DM-5011,Resolve development issues by testing using WAN Emulator,a test plan draft was written and some short meetings were held regarding the use of the wan emulator. the manuals for the apposite netropy 40g emulator were retrieved and read. the test plan draft for three test projects is attached.,5,val
DM-5012,Continued WBS planning,"finished out all necessary fields in the first cut at the wbs.  split project into work phases,   began drilling down into the milestones for each phase, with accurate estimation the goal.  provided miscellaneous diagrams to capture expected functionality for various state transitions throughout the system.",10,val
DM-5013,Convert Confluence DM Developer Guide to Sphinx (hack day) ,"this is a hack day sprint to convert all remaining content on https:/confluence.lsstcorp.org/display/ldmdg to restructuredtext content in the sphinx project at https:/github.com/lsstsqre/dmdevguide and published at http:/developer.lsst.io.    the top priority for this sprint is to port all content into rest and have it tracked by git.    sprint ground rules    # before the sprint, clone https:/github.com/lsstsqre/dmdevguide.git and pip install r requirements.txt in a python 2.7 environment so that you can locally build the docs (make html).  # claim a page from the list below by putting your name on it. put a checkmark on the page when you’ve merged it to the ticket branch (see below).  # see http:/developer.lsst.io/en/latest/docs/rststyleguide.html for guidance on writing our style of restructuredtext. pay attention to the http:/developer.lsst.io/en/latest/docs/rststyleguide.html#sections and http:/developer.lsst.io/en/latest/docs/rststyleguide.html#internallinkstolabels.  # if you use pandoc to do an initial content conversion, you still need to go through the content linebyline to standardize the restructuredtext. i personally recommend copyandpastingandformatting instead of using pandoc.  # your git commit messages should include the url of the original content from confluence.  # merge your work onto the tickets/dm5013 ticket branch. rebase your personal work branch before merging. jsick is responsible for merging this ticket branch to master.  # put a note at the top of the confluence page with the new url; root is http:/developer.lsst.io/en/latest/.    planned developer guide table of contents    we’re improving the organization of dm’s developer guide; there isn’t a 1:1 mapping of confluence pages to developer.lsst.io pages. below is a proposed section organization and page structure. these sections can still be refactored based on discussion during the hack day.    getting started — /gettingstarted/     ✅ onboarding checklist (confluence: https:/confluence.lsstcorp.org/display/ldmdg/gettingstartedindm). i’d like this to eventually be a quick checklist of things a new developer should do. it should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. the ncsaspecific material should be spun out. https:/confluence.lsstcorp.org/display/dm/communicationandlinks). i see this as being an overview of what methods dm uses to communicate, and what method should be chosen for any circumstance.   finding code on github (new). this should point out all of the github organizations that a developer might come across (dm and lsstwide), and point out important repositories within each organization. replaces the confluence page https:/confluence.lsstcorp.org/display/ldmdg/lsstcoderepositories    processes — /processes/     ✅ team culture and conduct standards (confluence)   ✅ dm development workflow with git, github, jira and jenkins (new & confluence: https:/confluence.lsstcorp.org/display/ldmdg/gitdevelopmentguidelinesforlsst  https:/confluence.lsstcorp.org/display/ldmdg/gitcommitbestpractices  https:/confluence.lsstcorp.org/display/ldmdg/dmbranchingpolicy)   ✅ discussion and decision making process (new & https:/confluence.lsstcorp.org/display/ldmdg/discussionanddecisionmakingprocess)   ✅ dm wiki use (https:/confluence.lsstcorp.org/display/ldmdg/dmwikiuse) https:/confluence.lsstcorp.org/display/ldmdg/policyonupdatingdoxygen); needs to be addressed with tct. interlink with the developer workflow page. https:/confluence.lsstcorp.org/display/ldmdg/transferringcodebetweenpackages) https:/confluence.lsstcorp.org/display/ldmdg/policyonchangingabaselinerequirement)   ✅ project planning for software development (https:/confluence.lsstcorp.org/display/ldmdg/projectplanningforsoftwaredevelopment) https:/confluence.lsstcorp.org/display/ldmdg/jiraagileusage) https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=21397653) (do not port; see discussion below.)   licensing (new) need a centralized page to discuss license and copyright policies; include boilerplate statements.    coding guides — /coding/     ✅ introduction and note on stringency language (confluence: https:/confluence.lsstcorp.org/display/ldmdg/dmcodingstylepolicy)   ✅ dm python style guide (confluence: https:/confluence.lsstcorp.org/display/ldmdg/pythoncodingstandard)   ✅ dm c style guide (confluence pages: https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908666  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908756  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908685  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908674  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908706  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=16908737  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20283399  https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20283856)   coding style linters (new; draft from confluence https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20283861 and https:/confluence.lsstcorp.org/display/ldmdg/pythoncodingstandardscompliance   ✅ using c templates (https:/confluence.lsstcorp.org/pages/viewpage.action?pageid=20284190); this page needs to severely edited or rewritten, however.   ✅ profiling (https:/confluence.lsstcorp.org/display/ldmdg/profiling|). also add a section ‘using valgrind with python' (new) https:/dev.lsstcorp.org/trac/wiki/tct/boostusageproposal) https:/confluence.lsstcorp.org/display/ldmdg/softwareunittestpolicy) https:/confluence.lsstcorp.org/display/ldmdg/coverageanalysis) https:/dev.lsstcorp.org/trac/wiki/unittestingprivatefunctions) https:/ and provide english style guidance specific to dm. capitalization of different heading levels; use of chicago manual of style; a ‘this, not that’ table of spelling and word choices.   ✅ restructuredtext style guide (new)   ✅ documenting stack packages (new)   ✅ documenting python code (new)   ✅ documenting c code (confluence, adapted from https:/confluence.lsstcorp.org/display/ldmdg/documentationstandards); needs improvement   ✅ writing technotes (new; port readme from https:/github.com/lsstsqre/lssttechnotebootstrap/blob/master/readme.rst)    developer tools — /tools/     ✅ git setup and best practices (new)   ✅ using git large file storage (lfs) for data repositories (new)   ✅ jira work management recipes (new)   ✅ emacs configuration (https:/confluence.lsstcorp.org/display/ldmdg/emacssupportforlsstdevelopment). see dm5045 for issue with emacs config repo  https:/confluence.lsstcorp.org/display/ldmdg/configforvim)  https:/confluence.lsstcorp.org/display/ldmdg/ncsanebulaopenstackuserguide  https:/confluence.lsstcorp.org/display/ldmdg/introductiontostartinganebulainstance  https:/confluence.lsstcorp.org/display/ldmdg/startaninstanceusingabasesnapshotwiththelsststack. add the http:/sqr002.lsst.io? https:/confluence.lsstcorp.org/display/ldmdg/gettingstartedindm  https:/confluence.lsstcorp.org/display/ldmdg/developertoolsatncsa   ✅ using the bulk transfer server at ncsa (https:/confluence.lsstcorp.org/display/ldmdg/usingthebulktransferserveratncsa) https:/confluence.lsstcorp.org/display/ldmdg/thelsstsoftwarebuildtool); lsstsw and lsstbuild documentation. https:/confluence.lsstcorp.org/display/ldmdg/addinganewpackagetothebuild) https:/confluence.lsstcorp.org/display/ldmdg/distributingthirdpartypackageswitheups) https:/confluence.lsstcorp.org/display/ldmdg/triggeringabuildbotbuild) https:/confluence.lsstcorp.org/display/ldmdg/buildbotfaqonerrors) https:/confluence.lsstcorp.org/display/ldmdg/buildbotconfigurationandsetup https:/confluence.lsstcorp.org/display/ldmdg/creatinganewdmstackrelease); though this page or a modern equivalent should probably belong with the software docs? https:/confluence.lsstcorp.org/display/ldmdg/ncsanebulaopenstackissues   https:/confluence.lsstcorp.org/display/ldmdg/dmsystemannouncements   https:/confluence.lsstcorp.org/display/ldmdg/dmdevelopmentservers    the following pages are either not relevant, generally misplaced, or need to be updated/recalibrated:     https:/confluence.lsstcorp.org/display/ldmdg/gitcrashcourse   https:/confluence.lsstcorp.org/display/ldmdg/basicgitoperations   https:/confluence.lsstcorp.org/display/ldmdg/handlinggitpushproblems   https:/confluence.lsstcorp.org/display/ldmdg/lsstcoderepositories; see the proposed “finding code on github” page for a replacement.   https:/confluence.lsstcorp.org/display/ldmdg/standardsandpolicies: this is a good toc for the confluence docs; but not longer needed for the new docs.   https:/confluence.lsstcorp.org/display/ldmdg/documentationguidelines. some of this could be re purposed into an intro to the ‘writing documentation’ section; some of this should go in a ‘processes' page.    https:/confluence.lsstcorp.org/display/ldmdg/dmacknowledgementsofuse: this probably belongs in documentation for the software projects that actually used this work.",5,val
DM-5014,Set doRenorm default to False in AssembleCcdTask,change the default value of assembleccdconfig.dorenorm to false for the reasons given in rfc 157 and to implement that rfc.,1,val
DM-5015,Optionally report do-nothing config overrides,"as discussion on dm4692 and in various hipchat rooms, it's too easy for cameralevel config override files to contain many options that don't actually change anything, because they simply override the defaults with the same default values.  to aid in tracking these down and removing them, we should have an option in which cmdlinetask s (delegating to pex_config) refuse or warn about overrides that have no effect.    we should probably not make failing on do nothing overrides the default behavior, but we could consider making warning the default behavior.  mostly, i think it's important just to be able to find such options when wanted.",4,val
DM-5018,Modernize version check scripts in matplotlib and numpy packages,the version check scripts in the stub matplotlib and numpy eups packages use old python conventions. they should be updated to work with 2.7+.,1,val
DM-5019,FITS Visualizer porting: Expanded mode single - part 2,i split dm4497 into two part so i can demonstrate code reviews. this part has paging controller & layout cleaned up.  this tickets is messy because it involves a lot of refactoring of the reducers.  therefore i am going to end it and move the rest of the ui work to dm5088.,4,val
DM-5022,Modernize python code in Qserv scons package,"the site_scons python code is not using current project standards. for example, print is not a function, exceptions are not caught as e, map is called without storing the result and map/filter/lambda are used where list comprehensions would be clearer.    most of these fixes are trivial with futurize.",1,val
DM-5023,"Adds, Moves, Change support for DNS, network, IP addressing, etc",nan,6,val
DM-5024,"define rack, pdu specifications and obtain pricing quotes",nan,3,val
DM-5025,Base site and summit RFP,working with ron to create a rfp for the acquisition of network equipment for the summit and base. ,10,val
DM-5026,Fix dependencies for eups-packaged sqlalchemy,"eups packaged sqlalchemy lists mysqlclient as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and mysqlclient does not actually provides python module so this dependency does not even make anything useful). so dependency on specific external package should be declared on client side and not in sqlalchemy, mysqlclient should be removed from sqlalchemy.table.",1,val
DM-5027,eval NCSA vSphere/OSX support -- first attempt,nan,1,val
DM-5028,Design interconnect for GPFS cluster prototype,nan,4,val
DM-5029,Create physical and logical network diagrams for first phase of purchases,nan,6,val
DM-5030,Tests fail on Qserv on OS X El Capitan because of SIP,os x el capitan introduced system integrity protection which leads to dangerous environment variables being stripped when executing trusted binaries. since scons is launched using /usr/bin/env the tests that run do not get to see dyldlibrarypath. this causes them to fail.    the same fix that was applied to sconsutils (copying the path information from lsstlibrarypath) needs to be applied to the test execution code used by qserv's private site_scons utility code.,2,val
DM-5033,X16 Data Access and Database Documentation,"update the documentation for data access and database  bring it up to date with the design. this includes ldm135 (database design), and creating a new ldm document or dax design).",73,val
DM-5035,"DAX & DB Docs (Fritz, March)"," document data distribution   create structure for dax doc   bring over provenance documentation from prov_prototype   update ldm135 to reflect the updates to the storage/io model   update ldm152   fix ldm 135: 3.3.6.4 and 3.3.6.5 should be 3rd level, so 3.3.7 and 3.3.8  ",3,val
DM-5036,DAX & DB Docs (John), refresh shared scans design documentation (in ldm135)   add info about query cancellation (in ldm135),10,val
DM-5037,DAX & DB Docs (AndyS), document db and table metadata   document async queries    document data loader,5,val
DM-5038,DAX & DB Docs (Nate),  improve butler documentation,8,val
DM-5039,DAX & DB Docs (Brian),  document webserv/imgserv/metaserv/dbserv,10,val
DM-5040,DAX & DB Docs (Mike),document secondary index,5,val
DM-5041,DAX & DB Docs (Serge)," document spatial indexing   document database ingest    refresh ""stored procedures and function"" in ldm 135",6,val
DM-5042,Load panstarrs data to qserv,nan,19,val
DM-5043,Setup webserv with panstarrs data,nan,5,val
DM-5049,"update ""newinstall.sh"" nebula images & docker containers",[hchiang2] is looking for nebula images newer than w201545 (from the exploratory work in dm 4326) and [gdaues] is interested in images with a complete lsst_distrib install for orchestration testing.  new builds should incorporate the pending change to newinstall.sh that converts from anaconda to miniconda.,6,val
DM-5050,SingleFrameVariancePlugin takes variance of entire image,"singleframevarianceplugin takes the median variance of the entire image, rather than within an aperture around the source of interest.  a footprint is constructed with the aperture, but it is unused.    this means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processccd on hsc visit=1248 ccd=49 with dm 4692).",1,val
DM-5052,Design replacement for A.net index files,we need a simple way to hold index files that will be easy to use and simple to set up.,2,val
DM-5053,Some small things slipped through in winter 2016,fix up things that slipped through or were delayed in winter 2016.  the individual things are small parts of larger epics and typically are the result of emergent work or increased scope.,30,val
DM-5054,Implement Approx/Interp improvements,"we are making due with the current approximation and interpolation scheme, but the two should be merged.  this must really be done after the hsc merge because of the difficulty of doing large refactoring before then.",30,val
DM-5055,Assess priority of Aprox/Interp upgrades.,this is to assess the priority of a major approximation and interpolation refactor.,4,val
DM-5056,RFC corrections for ISR.,create a list of isr requirements and have it rfcd.,9,val
DM-5057,Assess the corrections that need to be imlemented,the stack can do many of the corrections needed.  assess the status of the current algorithms and identify any deficiencies.,4,val
DM-5058,Improve and implement crosstalk in ISR,"isr needs the following correction algorithms: fringe, crosstalk, overscan (with discontinuity), and nonlinearity.  several of these are implemented in hsc and have been ported, but may need some work.     as noted below this will now only be the crosstalk portion of the isr upgrades.",10,val
DM-5059,Design the refactoring for ProcessCcd,there is a significant design issue when refactoring a piece of this importance.  carry out a design study to implement in dm 4692.,12,val
DM-5068,Week end 1/09/16,"support for lsst dev cluster, openstack, and accounts  for week ending january 9, 2016.",1,val
DM-5069,Week end 1/16/16,"support for lsst dev cluster, openstack, and accounts  for week ending january 16, 2016.",2,val
DM-5070,Week end 1/23/16,"support for lsst dev cluster, openstack, and accounts  for week ending january 23, 2016.",2,val
DM-5071,Week end 1/30/16,"support for lsst dev cluster, openstack, and accounts  for week ending january 23, 2016.",2,val
DM-5072,New equipment setup and configuration (week end 1/23/16),  finished setting up mac vsphere infrastructure with paul,2,val
DM-5073,New equipment setup and configuration (week end 1/30/16), set up new lsst dev7 as centos 7 server   continuing to set up ipmi on new test servers (working with dell on issue with idrac license upgrade),3,val
DM-5074,Decommissioning old equipment (week end 1/16/16), recovery of old lsst used equipment   moved remaining surplussed last servers to wiping bench   started wiping drives    re purposed 10 dell 1950  ,2,val
DM-5075,Decommissioning old equipment (week end 1/23/16),  complete the cleanup of last used ncsa systems,1,val
DM-5077,Lenovo test server, mount lenovo test server in lsst1 rack. install fiber card and networking. test pxe boot to 10g nic.   work on getting lenovo to pxe boot to 10g card   booted satisfactorily to 1gb interface – loaded centos 7    abruptly ends after menu with 10gb card  ,3,val
DM-5078,PcaPsf can hit an assertion failure,"this is bad for multiple reasons:  1. when multiprocessing, the assertion failure kills a single process, which prevents the final join of the multiple processes, so the job hangs forever.  2. the failure is not logged.  3. hard assertions like this should only occur when we break the system integrity, which this does not (i.e., it's too big a hammer for the problem).      pprice@tigersumire:/tigress/pprice/dm4692 $ eups list s  afw                   tickets.dm4692gd8ad35cd961     b1901 setup  afwdata               201601.0         b1901 b1902 setup  astrometrynet        0.50.lsst25      b1901 b1902 setup  astrometrynetdata   sdssdr9finkv5b         setup  base                  201601.0         b1901 b1902 setup  boost                 1.59.lsst5        b1901 b1902 setup  cfitsio               3360.lsst4        b1901 b1902 setup  coaddchisquared      201601.06       b1901 setup  coaddutils           201601.06       b1901 setup  dafbase              201601.0         b1901 b1902 setup  dafbutlerutils       tickets.dm4692g048b33c50e3     b1901 setup  dafpersistence       201601.01gf47bb691    b1901 b1902 setup  displayds9           201510.043      b1901 setup  doxygen               1.8.5.lsst1       b1901 b1902 setup  eigen                 3.2.5             b1901 b1902 setup  fftw                  3.3.4.lsst2       b1901 b1902 setup  geom                  10.050           b1901 b1902 setup  gsl                   1.16.lsst3        b1901 b1902 setup  ipdiffim             tickets.dm4692g543ea8fde53     b1901 setup  ipisr                201601.06       b1901 setup  lsstbuild            local:/tigress/pprice/lsstsw/lsstbuild   setup  mariadbclient         mastergf2dee38289        b1901 b1902 setup  matplotlib            0.0.15           b1901 b1902 setup  measalgorithms       tickets.dm4692g3d073a93d71     b1901 setup  measastrom           tickets.dm4692gbbf15418e61     b1901 setup  measbase             local:/tigress/pprice/dm4692/measbase   setup  measdeblender        201601.06       b1901 setup  minuit2               5.28.00.lsst2     b1901 b1902 setup  ndarray               10.158           b1901 b1902 setup  numpy                 0.0.15           b1901 b1902 setup  obssubaru            local:/tigress/pprice/dm4692/obssubaru  setup  obstest              tickets.dm4692g1533aee20f1     b1901 setup  pexconfig            201601.0         b1901 b1902 setup  pexexceptions        201601.0         b1901 b1902 setup  pexlogging           201601.0         b1901 b1902 setup  pexpolicy            201601.0         b1901 b1902 setup  pipebase             201601.06       b1901 setup  pipetasks            local:/tigress/pprice/dm4692/pipetasks  setup  psfex                 201601.0         b1901 b1902 setup  pyfits                3.4.0             b1901 b1902 setup  python                0.0.3             b1901 b1902 setup  pythond2to1          0.2.12            b1901 b1902 setup  pyyaml                3.11.lsst1        b1901 b1902 setup  scons                 2.3.5             b1901 b1902 setup  sconsutils            201601.0         b1901 b1902 setup  skymap                201601.06       b1901 setup  skypix                10.0347          b1901 setup  stscidistutils       0.3.71gb22a065  b1901 b1902 setup  swig                  3.0.2.lsst1       b1901 b1902 setup  utils                 201601.0         b1901 b1902 setup  wcslib                5.13.lsst1        b1901 b1902 setup  xpa                   2.1.15.lsst3      b1901 b1902 setup    pprice@tigersumire:/tigress/pprice/dm4692 $ processccd.py /tigress/hsc/hsc rerun price/dm4692 rerun price/dm4692 id visit=1248 ccd=100 clobberconfig  /tigress/pprice/lsstsw/miniconda/lib/python2.7/sitepackages/matplotlib/fontmanager.py:273: userwarning: matplotlib is building the font cache using fclist. this may take a moment.    warnings.warn('matplotlib is building the font cache using fclist. this may take a moment.')  : loading config overrride file '/tigress/pprice/dm4692/obssubaru/config/processccd.py'  warning: unable to use psfex: no module named extensions.psfex.psfexpsfdeterminer  cannot import lsst.meas.extensions.photometrykron: disabling kron measurements  cannot enable shapehsm (    file ""src/utils.cc"", line 42, in std::string lsst::utils::getpackagedir(const std::string&)      package measextensionsshapehsm not found   lsst::pex::exceptions::notfounderror: 'package measextensionsshapehsm not found'  ): disabling hsm shape measurements  : loading config overrride file '/tigress/pprice/dm4692/obssubaru/config/hsc/processccd.py'  : input=/tigress/hsc/hsc  : calib=none  : output=/tigress/hsc/hsc/rerun/price/dm4692  cameramapper: loading registry registry from /tigress/hsc/hsc/rerun/price/dm4692/parent/registry.sqlite3  cameramapper: loading calibregistry registry from /tigress/hsc/hsc/calib/calibregistry.sqlite3  processccd: processing   processccd.isr: performing isr on sensor   processccd.isr: applying linearity corrections to ccd 100  processccd.isr.crosstalk: applying crosstalk correction  processccd.isr: set 0 bad pixels to 3147.74  processccd.isr: flattened sky level: 3847.800781 / 2114.507723  processccd.isr: measuring sky levels in 8x16 grids: 3884.324645  processccd.isr: sky flatness in 8x16 grids  pp: 15293.248379 rms: 1173.423587  processccd.isr: setting rough magnitude zero point: 34.678409  processccd.charimage: processing   processccd.charimage.repair: identified 6044 cosmic rays.  processccd.charimage.detectandmeasure.detection: detected 127 positive sources to 5 sigma.  processccd.charimage.detectandmeasure.detection: resubtracting the background after object detection  processccd.charimage.detectandmeasure.measurement: measuring 127 sources (127 parents, 0 children)   processccd.charimage.measurepsf: measuring psf  /tigress/pprice/lsstsw/stack/linux64/measalgorithms/tickets.dm4692g3d073a93d71/python/lsst/meas/algorithms/objectsizestarselector.py:354: runtimewarning: invalid value encountered in less    bad = numpy.logicalor(bad, width / self.widthmax)  processccd.charimage.measurepsf: psf star selector found 6 candidates  meas.algorithms.psfdeterminer warning: you only have 3 eigen images (you asked for 4): reducing number of eigen components  meas.algorithms.psfdeterminer warning: you only have 1 eigen images (you asked for 3): reducing number of eigen components  meas.algorithms.psfdeterminer warning: you only have 1 eigen images (you asked for 2): reducing number of eigen components  python: /tigress/pprice/lsstsw/stack/linux64/eigen/3.2.5/include/eigen/src/core/redux.h:202: static eigen::internal::reduximpl/::scalar eigen::internal::reduximpl/::run(const derived&, const func&) [with func = eigen::internal::scalarmaxop/; derived = eigen::cwiseunaryop/, const eigen::matrix/ >; eigen::internal::reduximpl/::scalar = double]: assertion `size && ""you are using an empty matrix""' failed.  aborted      note:   this occurred while testing dm4692.  the local pipetasks and obssubaru are on that ticket branch.  the local measbase is for the fix from dm 5050.   one root cause of the bad psf modeling may be bad rotations in the application of the calibs ( is looking into that; don't know if there's a ticket number), but this should never happen regardless.",1,val
DM-5079,Operations planning ,"draft operations planning w.b.s.  create additional activity diagrams,  draft dppd operations processing talk.",15,val
DM-5080,January Management ,develop design and project management methods in conjunction with l1 design.  deal with ari labor component  general management of staff,15,val
DM-5084,PropagateVisitFlags doesn't work with other pipeline components,"propagatevisitflags, which was recently ported over from hsc on dm4878, doesn't work due to some inconsistencies with earlier packages/tasks:    the default fields to transfer have new names: ""calibpsfcandidate"" and ""calibpsfused""    we're not currently transferring these fields from icsrc to src, so those fields aren't present in src anyway.  i propose we just match against icsrc for now, since it has all of the fields we're concerned with.    it makes a call to afw.table.exposurecatalog.subsetcontaining(point, wcs, bool), which apparently exists in c but not in python; i'll look into seeing which hsc commits may have been missed in that port.",1,val
DM-5085,"Please add a package that includes obs_decam, obs_cfht and all validation_data datasets","it would be very helpful to have an lsstsw package that added all supported obs packages (certainly including obscfht and obsdecam, and i hope obssubaru) and all validationdata packages. this could be something other than lsst_apps, but i'm not sure what to call it.",1,val
DM-5086,Enable aperture correction on coadd processing,"aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds.",1,val
DM-5088,"Add auto play,select which dialog, close button working,  to expanded mode",add the auto play to expanded mode.  add the choose which dialog to expanded mode. make close button work.    i am breaking this up the expanded mode ticketa because the task is getting so big and ticket dm 5019 involved reducer refactoring.  also the refactoring needs to get into the dev branch.,4,val
DM-5089,Add task discovery on command line activator,i'll add a way to specify on the command line the path or the package to discover for cmdlinetask or supertasks,4,val
DM-5090,Investigate alternative for networkx before RFC,i'll make sure i explored other alternatives before creating a rfc for adding networkx which by itself require other packages. this is needed for the pipeflowx work. i tried one stand alone package before pygraphviz but then decided to migrate to networkx as it is more complete and allow other possible future features,4,val
DM-5091,LDM documentation of butler basics & multiple repositories,nan,2,val
DM-5092,Security plan renewal,renewal of the lsst security plan.  starts with dm.,3,val
DM-5094,HSC backport: Set BAD mask for dead amps instead of SAT,this is a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc1095 and a leftover commit from https:/hscjira.astro.princeton.edu/jira/browse/hsc1231: https:/github.com/hypersuprimecam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4 (noted in dm3942).,3,val
DM-5095,Redirect confluence based pages to new developer guide.,delete and apply redirects to all migrated pages in old confluence based developer guide,1,val
DM-5096,Make validateDrp a Task.,"make validatedrp a task so   1. it can easily be run from the command line or programmatically.  2. it can import the standard command line arguments  3. it can be logged in the same way.    this eventually should fit into dm2050, and dm3859.",2,val
DM-5097,Update validate_drp to use TransformTask to store calibrated measurements,"currently validatedrp uses some manual crude addition of calibration information and constructs new schemas to store this information.  this is essentially what transformtask is meant for.  using this would simplify the code, make it less fragile, and ideally eventually integrate more transparently with future calibration improvements or redefinitions of how zeropoints are tracked..    1. learn how to use transformtask.  note dm 4948 is the doc task for this.  2. adapt the code.  3. verify unchanged results on existing validationdatadecam and validationdata_cfht.",2,val
DM-5098,Add tests to validate_drp to verify SRD calculations and utility function behavior,the current validate_drp is woefully lacking in tests.    1. the key srd metrics definitely need to have test cases that verify the calculation of these important metrics.  2. overall the utility functions would benefit from testing.,4,val
DM-5099,Polish IN2P3 cluster upgrade to CentOS7,"what remains:     problem with docker 1.9.1overlayxfs => switch to docker 1.10.1? then switch back from devicemapper to overlay?   problem with qserv uid: go back to 1000, instead of 1008?",4,val
DM-5100,Docs for ltd-keeper,create a documentation project within ltdkeeper that documents the restful api while it is being developed. this will allow the http:/sqr 006.lsst.io technote to have a place to link to for detailed information.,1,val
DM-5101,"Fix --id examples in processCcd.py and friends to correctly show ""ccd=1^2"".","the required '' convention for lists of things, e.g. ccd, filter, visit and such is surprising.  but, worse, the documentation is currently wrong in its examples and presents several ccd=1,2, patch=1,2 examples.     fix the id examples in pipetasks and other uses of processccd.py in obs packages to correctly match the required syntax.    here's the current list in pipetasks, but check other packages as well.      [serenity tasks] grep '[09],[09]'  .py | grep '""'  assemblecoadd.py:                               help=""data id, e.g. id tract=12345 patch=1,2"",  coaddbase.py:        parser.addidargument(""id"", ""deepcoadd"", help=""data id, e.g. id tract=12345 patch=1,2"",  imagedifference.py:        parser.addidargument(""id"", ""calexp"", help=""data id, e.g. id visit=12345 ccd=1,2"")  makediscreteskymap.py:            boxi = afwgeom.box2i(afwgeom.point2i(0,0), afwgeom.extent2i(md.get(""naxis1""), md.get(""naxis2"")))  multiband.py:        parser.addidargument(""id"", ""deepcoadd"", help=""data id, e.g. id tract=12345 patch=1,2 filter=r"",  multiband.py:                               help=""data id, e.g. id tract=12345 patch=1,2 filter=gr^i"")  multiband.py:        parser.addidargument(""id"", ""deepcoadd"", help=""data id, e.g. id tract=12345 patch=1,2 filter=r"",  processcoadd.py:        parser.addid_argument(""id"", ""deepcoadd"", help=""data id, e.g. id tract=12345 patch=1,2"",  ",1,val
DM-5102,Rewrite integration test queries with spatial constraint returning empty results,"some queries in the integration test suite return empty results, here's how to catch them:    # this should be done for alll tests cases  egrep ""^0$"" ~/qserv run/201602/tmp/qservtestcase02/outputs/mysql/  # empty results files have also to be tracked      there parameters should be fixed to query a region containing data (use select  on object).",5,val
DM-5103,Add scans for DRx-1 to the model,"per rfc134 we need to support scans for drx1. this story involves building this into the model, costing it, and changing the baseline.",6,val
DM-5104,Add scans for DRP-produced Dia* tables to the model,"per rfc 133, we need to support scans on diaobject table, possibly dia source tables as well. this story involves adding it to the model, costing it and adding it to the baseline.",6,val
DM-5105,new conda 'mkl' dependent packages break meas_base tests,"continuum release/rebuilt a number of packages last friday to depend on the the intel mkl library.     https:/    there are [new feature named] versions that continue to use openblas but the mkl versions appear to be installed by default.  this causes at least multiple measbase tests to fail.after extensive testing, i have confirmed that the measbase tests do not fail with the equivalent 'nomkl' package.  in addition, mkl is closed source software that requires you to accept and download a license file or it is timebombed to stop working after a trial period.            dockercentos7: [ 36/36 ]  measbase 201510.09g6daf04b7 ...      dockercentos7:      dockercentos7:   error: from /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/build.log:      dockercentos7: tests/sincphotsums.py      dockercentos7:      dockercentos7: tests/measuresources.py      dockercentos7:      dockercentos7: tests/testapertureflux.py      dockercentos7:      dockercentos7: tests/testjacobian.py      dockercentos7:      dockercentos7: tests/testscaledapertureflux.py      dockercentos7:      dockercentos7: the following tests failed:      dockercentos7: /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/measbase201510.09g6daf04b7/tests/.tests/sincphotsums.py.failed      dockercentos7: /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/measbase201510.09g6daf04b7/tests/.tests/measuresources.py.failed      dockercentos7: /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/measbase201510.09g6daf04b7/tests/.tests/testapertureflux.py.failed      dockercentos7: /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/measbase201510.09g6daf04b7/tests/.tests/testjacobian.py.failed      dockercentos7: /opt/lsst/software/stack/eupsbuilddir/linux64/measbase201510.09g6daf04b7/measbase201510.09g6daf04b7/tests/.tests/testscaledapertureflux.py.failed      dockercentos7: 5 tests failed    (the exact cause of the test failures was not investigated as this should not have happened)    this change has also broken the ability to import an existing conda env from 20160205 or earlier that uses scipy due to some sort of package version resolution problem.  explicit declaring it as the scipy package without mkl fixes the resolution problem.    there is a new 'nomkl' package, when installed, any subsequent package installations will default to versions without mkl.  however, this does not fix any already installed packages.    i am traumatized by the lack of reproducible  build envs even within a few days of each other.  after discussion in the tucson office, i'm going to pin the lsstsw and newinstall.sh conda package versions with a commitment from square to update them on a monthly basis.  i already have a test version of lsstsw/bin/deploy that defaults to a bundled package but with a option flag to use bleeding edge.  ",4,val
DM-5106,"newinstall.sh fails with ""eups: command not found"""," has reported the following output when running newinstall.sh on el6.      installing eups (v2.0.1)... done.  setup: no module named utils  installing miniconda2 python distribution ...   newinstall.sh: line 277: eups: command not found      clearly, a command failure which should have been fatal was ignored.  ",2,val
DM-5107,Fix effective coordinates for defects in obs_subaru,"the defects as defined in obssubaru (in the hsc/defects/20nnnnnn/defects.dat files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel.  however, the lsst stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  as such, the defect positions are being misinterpreted for the rotated ccds in hsc (see http:/  this needs to be remedied.",2,val
DM-5109,Offset in gaussian-psf in ci_hsc,"i'm seeing what looks like an aperture correction problem in psf gaussian on ci_hsc coadds.  this gets in the way of our ability to do star/galaxy classification, and suggests potentially more serious problems elsewhere.  ",2,val
DM-5110,S17 Data Access and Database Documentation,update the documentation for data access and database,65,val
DM-5111,S17 Data Access and Database Documentation,update the documentation for data access and database,65,val
DM-5112,FY17 Data Access Model Refresh,"a refresh of the storage / io model (ldm 141). the work involves understanding cost impact, and discussing the impact on science.",26,val
DM-5113,S18 Data Access and Database Documentation,update the documentation for data access and database.,65,val
DM-5114,S18 Data Access and Database Documentation,update the documentation for data access and database,65,val
DM-5115,FY20 Data Access and Database Documentation,update the documentation for data access and database.,100,val
DM-5116,FY18 Data Access Model Refresh,"a refresh of the storage / io model (ldm 141). the work involves understanding cost impact, and discussing the impact on science.",26,val
DM-5117,FY19 Data Access Model Refresh,"a refresh of the storage / io model (ldm 141). the work involves understanding cost impact, and discussing the impact on science.",26,val
DM-5118,FY18 Data Access Model Refresh,"a refresh of the storage / io model (ldm 141). the work involves understanding cost impact, and discussing the impact on science.",26,val
DM-5119,FY19 Data Access and Database Documentation,update the documentation for data access and database.,100,val
DM-5120,"Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo","validate_drp current takes as input both a repository and a configuration file.  the configuration file contains information to construct the list of dataids to analyze.    however, these dataids could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  add a function that loads the set of dataids from the repo. (/)  2.  select reasonable defaults for the additional parameters specified in the config file. (/)  3.  design how to handle multiple filters. (/)",5,val
DM-5121,Add multiple-filter capabilities to `validate_drp`,"design and refactor `validate_drp` to produce results for multiple filters.    1. decide on the syntax for the yaml configuration file that denotes the multiple filters.  e.g., which visit goes with what filter? (/)  2. organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/)  3. add a filter designation to the default output prefix. (/)    note: matching objects across filters is outofscope for this ticket.",1,val
DM-5122,LOAD DATA LOCAL does not work with mariadb,"after we unmessed mariadbmysqlclient we see errors now when trying to run integration tests:      file ""/usr/local/home/salnikov/dmyyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in request      raise servererror(exc.response.statuscode, exc.response.text)  servererror: server returned error: 500 (body: ""]""}"")  20160210 14:17:40,836  lsst.qserv.admin.commons  critical  error code returned by command : qservdataloader.py v config=/usr/local/home/salnikov/testdatarepo/datasets/case01/data/common.cfg host=127.0.0.1 port=5012 secret=/home/salnikov/qservrun/201602/etc/wmgr.secret deletetables chunksdir=/home/salnikov/qservrun/201602/tmp/qservdataloader/leapseconds nocss skippartition onetable qservtestcase01_mysql leapseconds /usr/local/home/salnikov/testdatarepo/datasets/case01/data/leapseconds.schema /usr/local/home/salnikov/testdatarepo/datasets/case01/data/leapseconds.tsv.gz      it looks like mariadb client by default disables local option for data loading and it needs to be explicitly enabled.  ",1,val
DM-5124,Adapt all HSC calibration data to LSST camera geometry,"in the http:/ approximately half of the hsc ccds are rotated 180 deg with respect to the others, two others have 90 deg rotations and another two have 270 deg rotations.  the hsc camera geometry defined a coordinate system where pixel (0, 0) is always the lowerleft corner.  however, the new camera geometry in the lsst stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  as such, accommodations have had to be made for the rotated ccds on obssubaru.  see dm4998 and dm5107 in particular for details.  the need for these accommodations, and the accommodations themselves, should be removed.  this entails a reingestion of the hsc calibration data files (bias, dark, flat, etc.) as well as a redefinition of the defects files in obs_subaru.",4,val
DM-5125,qserv fails when it mixes mariadb and mariadbclient directories,"when i tried to run qservconfigure after installing qserv 2016017gbd0349f i got this error:    20160210 16:03:16,915  lsst.qserv.admin.commons  critical  error code returned by command : /home/salnikov/qservrun/201602/tmp/configure/mysql.sh      running script configure/mysql.sh:    $ sh x /home/salnikov/qservrun/201602/tmp/configure/mysql.sh     echo ' installing mysql database files.'   installing mysql database files.   /u2/salnikov/stack/linux64/mariadbclient/10.1.11/scripts/mysqlinstalldb basedir=/u2/salnikov/stack/linux64/mariadbclient/10.1.11 defaultsfile=/home/salnikov/qservrun/201602/etc/my.cnf user=salnikov   echo 'error : mysqlinstalldb failed, exiting'  error : mysqlinstalldb failed, exiting   exit 1      and       $ /u2/salnikov/stack/linux64/mariadbclient/10.1.11/scripts/mysqlinstalldb basedir=/u2/salnikov/stack/linux64/mariadbclient/10.1.11 defaultsfile=/home/salnikov/qservrun/201602/etc/my.cnf  user=salnikov    fatal error: could not find mysqld    the following directories were searched:        /u2/salnikov/stack/linux64/mariadbclient/10.1.11/libexec      /u2/salnikov/stack/linux64/mariadbclient/10.1.11/sbin      /u2/salnikov/stack/linux64/mariadbclient/10.1.11/bin      so it looks for mysqld in mariadbclient, the same directory as mysqlinstalldb script, mysqlinstall_db should be actually running from mariadb.  ",1,val
DM-5127,FY18 Centralize access to database servers,"we will have multiple services: l1 live database, multiple dr databases, calibration databases, efd etc. it'd be nice if users would not have to know which server / which port / which dialect (plain mysql or qserv etc) to use. instead, it'd be good to have a single entry point that redirects to the right place.",60,val
DM-5128,Cost adding the support for Object / DiaObject joins in Qserv,"per rfc133, we should support object / diaobject joins. that requires changes to query analyzer (and possibly elsewhere), currently we only support selfjoins on objectid for director table. we'd need to either make diaobject a director table and allow directordirector joins, or allow directorchild joins. this story involves costing how much effort it will be to implement it (and making a straw man proposal how to implement it)",2,val
DM-5129,Create InputField for generic use cases.,"create a composable, validating inputfield so it can use outside of the form/submit use case.",2,val
DM-5130,"B-F correction breaks non-HSC custom ISR, ci_hsc","the addition of brighterfatter correction on dm4837 breaks obscfht's custom isr, since it slightly changes an internal isr api by addding an argument that isn't expected by the obscfht version.  it also breaks ci_hsc, since the b f kernel file isn't included in the calibrations packaged there.  ",1,val
DM-5131,make the fits statistics call work with JSON,nan,1,val
DM-5132,obs_subaru install with eups distrib fails,"thus:    $ eups distrib install t w201606 obssubaru  ...    [ 52/52 ]  obssubaru 5.0.0.160ge4efae72 ...     error: from /users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/obssubaru5.0.0.160ge4efae72/build.log:    traceback (most recent call last):    file ""tests/hscrepository.py"", line 91, in setup      self.repopath = createdatarepository(""lsst.obs.hsc.hscmapper"", rawpath)    file ""tests/hscrepository.py"", line 63, in createdatarepository      checkcall([ingestcmd, repopath]  glob(os.path.join(inputpath, "".fits.gz"")))    file ""/opt/local/library/frameworks/python.framework/versions/2.7/lib/python2.7/subprocess.py"", line 540, in checkcall      raise calledprocesserror(retcode, cmd)  calledprocesserror: command '['/users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/obssubaru5.0.0.160ge4efae72/obssubaru5.0.0.160ge4efae72/bin/hscingestimages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/t/tmptuskuf', '/users/jds/projects/astronomy/lsst/stack/darwinx86/testdatasubaru/mastergf9ba9abdbe/hsc/raw/hsca90402512.fits.gz']' returned nonzero exit status 1      ran 8 tests in 9.928s    failed (errors=7)  the following tests failed:  /users/jds/projects/astronomy/lsst/stack/eupsbuilddir/darwinx86/obssubaru5.0.0.160ge4efae72/obs_subaru5.0.0.160ge4efae72/tests/.tests/hscrepository.py.failed  1 tests failed  scons:   [checkteststatus] error 1  scons: building terminated because of errors.   exit  4    please fix it.",1,val
DM-5133,Make meas_simastrom a stack package,"currently the simastrom code is sitting outside lsst which makes it not very visible and does not get built regularly making it sensitive to bitrot.  before we can really continue to gather requirements and develop the system, we need to bring it inside the fence.    this should make the package located https:/github.com/lsst france/meas_simastrom buildable and usable in the lsst system with a minimum of external dependencies.  by usable, i mean that it should be callable as a task.  this does not need to solve the problem of persistence.",10,val
DM-5135,Make ci_hsc buildable by Jenkins,"1. make sure cihsc is buildable by lsstsw / lsstbuild  (/)  2. add cihsc to lsstsw/etc/repos.yaml so that one can request that jenkins builds it.  (/)  3. verify that the test in cihsc fails on known broken tags and passes on known successful tags. (/)    no dependencies will be added to lsstsims or lsstdistib.  this is meant to provide the ability to request that jenkins do these builds and to fail if something has broken them.    this will later be expanded to new packages cicfht, cidecam, and cisim.    the key goal is to make sure one hasn't broken obs packages in their butler interface or in their processccd    additional notes and thoughts from hipchat discussion  [ktl]  sounds good to me; we might have an ""lsstci"" toplevel metapackage depending on all of them which is what jenkins would run regularly.     if the goal is to test obs packages, then my first instinct would be to put that in the obs package.  longer term goal to test the stack with different precursor datasets.  if this is testing obs packages on a slower cadence than the builtin tests, it's ok for that to be a separate package.    [jbosch]  eventually, i think we need to run a ci dataset for each camera, then run some camera generic tests on each of those, then run some cameraspecific tests on each of those.  so we don't want to go too far down a road in which all tests are cameraspecific, but maybe we don't have a choice until we have some better unifying framework for them.  i've certainly been putting some checks in ci_hsc that would be valid for all other cameras, if we had a ci package for them that went through to coadd processing.",2,val
DM-5136,Increase key_buffer_size,"i just looked at my qserv run/etc/my.cnf and i don't see us setting keybuffersize there. looking at mysqld run as part of qserv i can see it is set to 128 mb. that is pretty low given we are planning to do lots of joins. please add an entry in my.cnf that sets it to something higher with a comment that ""~20% of available ram is recommended"".",1,val
DM-5137,on-going support to Camera team in visualization (Feb. 2016) ,attend the weekly meeting and answer questions as needed.  help with the python and js debug ,4,val
DM-5138,document adding git-lfs repos to CI,nan,1,val
DM-5139,Update apr and apr_util,apr and aprutil are outdated and lagging behind the versions on rhel6. they should be updated as agreed in rfc76.,1,val
DM-5140,Move luaxmlrpc to lsst-dm/legacy-,"we no longer need luaxmlrpc because we run czar inside proxy. we should move it to lsstdm/legacy, and remove mentioning it in readme.",1,val
DM-5142,DM Power Requirements Justification,"the power requirements for the base site appeared to have increased greatly since lse239 or ldm144 v140. significant effort was spent digging through ldm144 for precise rack counts, rack weights, rack power. further time was spent on the analysis of why the power requirement is greater then expected. this involved analyzing swing space power requirements, max swing space needed, investigation into what lse239 refers to 'expansion' (turns out to be alert processing), attributing alert processing power requirements to the base (ldm 144 contributes to archive site but contingency is still in place for base site operations), comparing peak and steady state power needs. also discussions around reinforcing the floors for greater rack weights.  ",6,val
DM-5143,Jason Feb Tasks,"weeks 1&2  interviews, team mtgs, uptime institute tier discussions: 1.5 pts  weeks 3&4  team mtgs, ici meetings, set/prioritize it goals 4 pts",6,val
DM-5144,Jason Feb Educational Activities,"learning dm stack deployment and layout, reading on redesign of butler 1.5",2,val
DM-5145,Avoid merge table (i.e. result_m) creation on czar side,"when launching a query which require an aggregation/merge, qserv first creates a resultm table to collect chunk query results and then a result table. on the other hand, for a query which doesn't require a merge, only result table is created.    if merge query was send to mysqlproxy right after query parsing (like it is currently done with ""order by"" clause only, this would be then generalized to all merge queries), creation of resultm table could be avoided. this would lead to simpler c code, and aggregation would be performed at the same time that returning result, which may lead to better performance. queries wich requires or not aggregation step would be processed exactly the same way on the c side (store results of chunk queries), and mysqlproxy would release lock on result table when running aggregation/merge query (here, one can consider that simply concatening results of query would also be a kind of aggregation).    please not that removal of result_m table would also free some space on master, which is a bottleneck.    , i propose you to plan this interesting feature for next sprint, feel free to postpone it. i think that intersection with ""query coverage"" story might not be empty.",10,val
DM-5147,Provide usable repos in {{validation_data_*}} packages.,"reinterpreted ticket:  1. provide alreadyinitialized repositories in the `validationdatacfht`, `validationdatadecam`, and `validationdatahsc` packages alongside the raw data.  the goal is to allow both easy quickstart analyses as well as comparisons of output steps from processccd.py and friends at each step of the processing. (/)  2. add (cfht,decam,hsc).list files to provide for easy processing of the available dataids in the example data. (/)  3. update readme files to explain available data.  (/)    [original request:]  in validationdrp when i run examples/runxtest.sh i find that any data i had saved in cfht or decam is lost, even if i have carefully renamed it. this is very dangerous and i lost a lot of work due to it. at a bare minimum please do not touch any directories not named ""input"" or ""output"".    lower priority requests that i hope you will consider:   have the the input repo be entirely contained in the validationdatax packages, ready to use ""as is"". that would simplify the use of those packages by other code. it would also simplify validatedrp, and it would just leave the output repo to generate (which already has a link back to the input repo).    have runxtest.sh accept a single argument: the path to the output. (the path to the input is not necessary if you implement the first suggestion).",3,val
DM-5148,IN2P3 cluster worker nodes failed to start due to Innodb error,"next error happens when starting mariadb on worker (with existing data from 35tb dataset, which were generated by mysql):    20160213 22:02:36 139632684558144 [note] innodb: completed initialization of buffer pool  20160213 22:02:36 139632684558144 [error] innodb: autoextending data file ./ibdata1 is of a different size 640 pages (rounded down to mb) than specified in the .cnf file: initial 768 pages, max 0 (relevant if nonzero) pages!  20160213 22:02:36 139632684558144 [error] innodb: could not open or create the system tablespace. if you tried to add new data files to the system tablespace, and it failed here, you should now edit innodbdatafile_path in my.cnf back to what it was, and remove the new ibdata files innodb created in this failed attempt. innodb only wrote those files full of zeros, but did not yet use them in any way. but be careful: do not remove old data files which contain your precious data!  20160213 22:02:36 139632684558144 [error] plugin 'innodb' init function returned error.  20160213 22:02:36 139632684558144 [error] plugin 'innodb' registration as a storage engine failed.  20160213 22:02:36 139632684558144 [note] plugin 'feedback' is disabled.  20160213 22:02:36 139632684558144 [error] unknown/unsupported storage engine: innodb  20160213 22:02:36 139632684558144 [error] aborting  ",4,val
DM-5150,Create an easy place to add tests to ci_hsc,create a single file where tests for validating source can be added. the tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.,1,val
DM-5151,"Code review, Feb 2016","dm3733,dm4825",2,val
DM-5152,"Meetings, Feb 2016",verification dataset meetings,1,val
DM-5153,Process a tiny set of raw DECam Stripe 82 data,process some decam data to gain familiarity with process execution and learn to debug issues,8,val
DM-5154,Continue learning middleware,"ramp up with the middleware status and development. look into packages pipebase, pexconfig, pex_logging. ",16,val
DM-5155,"LOE, Feb 2016","local lsst meetings, postdoc meetings, ncsa all hand meetings, rfds, ncsa software meeting, astronomy events, workshops, travel to jtm, other local meetings. ",10,val
DM-5156,Please document MemoryTestCase,"lsst.utils.tests.memorytestcase is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. please add appropriate documentation to the http:/developer.lsst.io/en/latest/coding/unittestpolicy.html.    see also https:/community.lsst.org/t/whatisthepolicyforusinglsstutilstests memorytestcase.",1,val
DM-5160,"Record CCD, visit of input catalog in `validate_drp`",1. record the ccd and `visit` of the individual source in the catalog so that it is available for later analysis.  3. update `analyzedata` to use these newly available ccd and `visit` information in the catalog.  ,1,val
DM-5161,HSC backport: Support a full background model when detecting cosmic rays,"this is a port of the following two standalone hsc commits:    https:/github.com/hypersuprimecam/pipetasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb    if there are strong gradients (e.g. m31's nucleus) we need to do more than  treat the background as a constant.  however, this requires making a copy  of the data so the backgroundisaconstant model is preserved as a special  case    https:/github.com/hypersuprime cam/pipetasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b    when the background is subtracted with finer binsize, new exposure  will be created and cosmic rays will be detected on that exposure.  but the image of that exposure was not properly returned back.    ",1,val
DM-5162,Audit the LSST and HSC codebases for differences,"we've already merged a lot of code from hsc to lsst, and are optimistic that we've captured most of the big ticket items. however, we need to perform a thorough comparison of the codebases to check there's nothing we're missing. please do that, and file tickets in the dm3560 and dm3568 epics to describe outstanding work.",4,val
DM-5163,Modify System layout to support expanded views,each of the visualizers needs to expand to full screen.  we need to modify our current layout system so each and expand and collapse so that the old view is restored. the system needs to be flexible enough so an 'expanded version' of the component can be used.,4,val
DM-5164,Tests in daf_persistence should skip properly,some of the tests in daf_persistence have a couple of problems that cause difficulties with modern test frameworks:  # unittest is not being used at all in some cases  # skipping is done with a print and a sys.exit    they need to be modernized.,2,val
DM-5165,Mouse Readout: part 1.5 - update flux server call to work in JS,nan,1,val
DM-5166,Analyze catalog-comparison CmdLineTasks,analyze the qa cmdlinetask collection being generated by  sufficiently well to determine the interface requirements needed to represent them as supertasks.    does not include actually designing that interface.,6,val
DM-5167,Standup Fastly infrastructure for LSST the Docs,lsst the docs will use fastly to serve docs out of an s3 bucket with wellformatted urls thanks to routing at the varnish layer. see https:/ for an overview of the desired setup and http:/sqr006.lsst.io for an overview of lsst the docs. specific outcomes are:     create s3 bucket for ltd   create fastly account (may be a demo account pending negotiations with fastly)   basic configurations for fastly account   research pricing/configure a tls certificate for .lsst.io domains   set up base vcl configuration on fastly.  ,3,val
DM-5169,Fastly API interactions for LSST the Docs,"using fastly’s api, have ltdkeeper setup new builds and editions:     add surrogatekey to headers of objects uploaded to s3 (happens on ltdmason side)   configure varnish to serve specific bucket directories as specific domains (dm4951 has added route 53 interactions to ltdkeeper)   purge content when editions switch or content is deleted.    dm5167 is covering nonapi driven work to configure fastly.    see https:/ for a writeup on serving static site via fastly. see also http:/sqr006.lsst.io for an overview of lsst the docs.",8,val
DM-5171,New XY  functions to be developed (F16),there are several new functions requested by users,45,val
DM-5174,Manipulating masks is confusing,"a possible bug exists in afwimage.exposure.getmaskedimage(). this function returns a copy of the exposure's masked image, and not the actual maskedimage owned by the exposure. this means that any changes made to the mask are done only on the copy, and are not reflected in the exposure's maskimage. the intended behavior seems to be that a shallow copy be returned with pointers to all the original objects (such as the mask). this however does not seem to be the case, a deep copy is always made instead. verify that the intended behavior is indeed happening. steps to reproduce      coaddexposure = afwimage.exposuref()  coaddexposure.getmaskedimage().getmask().addmaskplane('test')  print(coaddexposure.getmaskedimage().getmask().getmaskplanedict().asdict())  m = coaddexposure.getmaskedimage().getmask()  print(m.getmaskplanedict().asdict())  m.removeandclearmaskplane('test')  print(m.getmaskplanedict().asdict())  print(coaddexposure.getmaskedimage().getmask().getmaskplanedict().asdict())       a second concern, though not necessarily a bug, is that adding and removing mask planes is confusing due to inconsistent manipulation of global state. for example:    in [1]: import lsst.afw.image as afwimage    # create two separate masks  in [2]: mask1 = afwimage.masku()  in [3]: mask2 = afwimage.masku()    # neither mask contains a ""test"" plane  in [4]: 'test' in mask1.getmaskplanedict()  out[4]: false  in [5]: 'test' in mask2.getmaskplanedict()  out[5]: false    # adding a plane to one updates a shared list of planes, so it appears in the other  in [6]: mask1.addmaskplane('test')  out[6]: 9  in [7]: 'test' in mask1.getmaskplanedict()  out[7]: true  in [8]: 'test' in mask2.getmaskplanedict()  out[8]: true    # but deleting a plane from one affects only that particular mask and not the global state  in [9]: mask1.removeandclearmaskplane('test')  in [10]: 'test' in mask1.getmaskplanedict()  out[10]: false  in [11]: 'test' in mask2.getmaskplanedict()  out[11]: true  ",4,val
DM-5175,Add CSS information for shared scans to integration test data.,some tables int the integration tests need to be flagged as needing to be locked in memory and given a scan rating.,1,val
DM-5178,lsstsw deploy on OS X fails in miniconda install,testing the fixes for the deploy script in dm4359 it seems that the part of the script installing miniconda no longer works on os x because the list of packages to be installed has been derived from a linux system and not all the linux packages have os x equivalents. there needs to be a peros list of packages. the default os x list seems to be:    # this file may be used to create an environment using:  # $ conda create name / file /  # platform: osx64  astropy=1.1.1=np110py270  conda=3.19.1=py270  condaenv=2.4.5=py270  cycler=0.9.0=py270  cython=0.23.4=py271  freetype=2.5.5=0  libpng=1.6.17=0  matplotlib=1.5.1=np110py270  nomkl=1.0=0  numpy=1.10.4=py27nomkl0  openssl=1.0.2f=0  pandas=0.17.1=np110py270  pip=8.0.2=py270  pycosat=0.6.1=py270  pycrypto=2.6.1=py270  pyparsing=2.0.3=py270  pyqt=4.11.4=py271  python=2.7.11=0  python dateutil=2.4.2=py270  pytz=2015.7=py270  pyyaml=3.11=py271  qt=4.8.7=1  readline=6.2=2  requests=2.9.1=py270  scipy=0.17.0=np110py27nomkl0  setuptools=19.6.2=py270  sip=4.16.9=py270  six=1.10.0=py270  sqlalchemy=1.0.11=py270  sqlite=3.9.2=0  tk=8.5.18=0  wheel=0.29.0=py27_0  yaml=0.1.6=0  zlib=1.2.8=0  ,1,val
DM-5179,miniconda2 eups package fails to install on OS X,the miniconda2 eups package attempts to install the relevant conda packages by downloading a list from the lsstsw repository. this fails for the same reason that lsstsw fails in dm5178 in that the list of packages is not osspecific. this means that newinstall.sh does not work any more on os x.,1,val
DM-5181,"update ""newinstall.sh"" nebula images & docker containers - w_2016_08",nan,1,val
DM-5182,Hook up help system,we need to help system like we have in gwt.,8,val
DM-5185,Implement Lock plot button on toolbar, write a button on the toolbar that monitors the active plot view's group and shows the locked or unlocked icons   add an action and reducer functions the will toggle the lock state of the group.,6,val
DM-5186,Add Xrdssi plugin configuration file,"xrdssi plugin configuration file could be useful for sharedscan.  to pass plugin configuration file path to xrootd  http:/xrootd.org/doc/dev42/xrdconfig.htm#passingplugincommand (use xrdssi)  to get this argument from c+  http:/xrootd.org/doc/dev42/ssireference.htm#toc431242001    then an add hoc config file parser needs to be used (not to be xrootd dependant), json/yaml could be used.",6,val
DM-5187,Set Qserv master in env variable for Docker containers,"this would allow use of pre configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn.  see http:/xrootd.org/doc/dev42/syntax_config.htm  and    if defined ?~exportpath    set exportpath = $exportpath    else    set exportpath = /tmp    fi    all.export $exportpath    ",3,val
DM-5188,Add fftools API: Image Viewer plus foundational work,nan,10,val
DM-5189,Add fftools API: Table,nan,6,val
DM-5190,Add fftools API: XYPlots and Histgram,nan,8,val
DM-5191,"Coverage, Coverage API, ImageMetaData API",nan,8,val
DM-5192,Add remote (python) API support ,"the python interface needs to be ported.  this involves the following:     modify fireflyclient.py   change all the api to work by dispatching remote actions. there is currently a dispatchremoteaction method in  fireflyclient.py   on the server side clean up file pushcommands.java, pushjob.java and serverparams to remove the old api.   move the fftools/python to firefly/python   clean up the test notebooks.  there are currently several, some don't work and should be removed.  others should be clean test cases.   make sure the python support for rangevalues is consistent with the java and javascript. i suspect it is not.    make sure events can be received into the python.  ",16,val
DM-5193,attend the bi-weekly meeting authentication and authorization discussion,attend the bi weekly meeting authentication and authorization discussion. provide input and feedback to iam. ,4,val
DM-5194,Deploy ltd-keeper as a Docker Container,ltdkeeper should be deployed as a docker container as a best practice for maintainable cloud microservices.    this involves writing a dockerfile committed to the ltdkeeper repo and demonstrating that the container can be stood up on google container engine.    i plan on use datacontainers attached to the service’s container to maintain the sqlite db. this ticket should document how to operate ltdkeeper and apply updates to both the ltd keeper app and db migrations..    this ticket also involves initial overhead in researching docker/kubernetes.,12,val
DM-5196,swift API availability?,"the downtime announcement email for nebula unavailable feb 9 10 mentioned a ""roadmap"" for swift.  i have checked and post maintenance, there is not a swift endpoint available in the catalog.  is there a time line for availability?",1,val
DM-5197,Test and robustify shapelet PSF approximations,"the cmodel code ported from hsc only works as well as the shapeletpsfapproximation algorithm that runs before it, but we've switched on the lsst side to a more flexible algorithm that isn't as nearly as battle tested as what's been running on the hsc side, and there are some concerning indications from 's work that it can be catastrophically slow on some reasonable psfs.  on this issue, i'll run it on some real hsc data and try to improve it, even if that means reducing the flexibility back to what was on the hsc side in some ways.",8,val
DM-5198,FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support,drawing overlay 3 color support,8,val
DM-5200,instance I/O errors,"the kernel dmesg for instance bbfd74586dd64412a8ba8d417c3df56b has started reporting thousands of block i/o errors and these are starting to trickle up as a filesystem i/o errors.  i suspect this is likely a hypervisor i/o issue.       field                                     manual                                                                 osextaz:availabilityzone             1                                                                      osextsts:taskstate                   active                                                                 ossrvusg:launchedat                  none                                                                   accessipv4                                                                                                     addresses                                                                                                      created                                 m1.xlarge (5)                                                          hostid                                  bbfd74586dd64412a8ba8d417c3df56b                                   image                                   vagrantgeneratedcomshorc                                             name                                    [                                                                     progress                                8c1ba1e0b84d486fbe7a665c30030113                                       properties                              [, ]                          status                                  20160211t23:36:25z                                                   userid                                 +  ",1,val
DM-5201,sph-partition does not support BLOB fields,"next command fails with blob field in input file:    dev@clrinfoport09:~/src/qservtestdata/datasets/case01/data$ sphpartition out.dir /home/dev/qservrun/git/tmp/qservdataloader/object part.prefix chunk configfile /home/dev/src/qservtestdata/datasets/case01/data/common.cfg configfile /home/dev/src/qservtestdata/datasets/case01/data/object.cfg in object.tsv  csv record contains an embedded line terminator, a trailing escape character, or a quoted field without a trailing quote character.    note that the command works in input file is reduced to its first line.    note that mysql import works fine:    mariadb [qservtestcase01_mysql]> load data infile ""/tmp/object.tsv"" into table object fields terminated by '\t' optionally enclosed by '""' lines terminated by '\n';  ",15,val
DM-5202,Remove LOGF macros from log package,we have removed all uses of logf macros from qserv and as far as i know no other clients use those macros. it's time to clean up log package itself from those macros.,1,val
DM-5203,Add support for 3 Color,most of this is done.  i just need to plot a few 3 color images and work out the bugs.,4,val
DM-5204,Remove remaining LOGF macros from qserv,"there are still few cases of logf macros in qserv, have to replace them all.",1,val
DM-5205,replace Associations::CollectRefStars with LoadReferenceObjectsTask,"astroutils.cc has code for loading usno catalog which is used by associations:collectrefstars to build a reference list. we should replace this with  loadreferenceobjectstask from meas_algorithms to both make it more generic, and to remove problematic endian handling in astroutils.",10,val
DM-5206,Please do not write garbage to the FITS EQUINOX,"the equinox is not relevant when dealing with icrs coordinates.    when afw manipulates wcs objects, it simply doesn't bother initializing the equinox field of its wcsinfo struct when dealing with an icrs system.    when afw persists the wcs to fits, it blindly writes whatever happens to be in that uninitialized field to the fits header. thus, we end up with something like:    equinox =      9.87654321e+107 / equinox of coordinates    this should be no problem, since, per the http:/fits.gsfc.nasa.gov/standard30/fitsstandard30aa.pdf (page 30), the equinox is ""not applicable"" if they radesys is icrs. the reader should thus ignore this value.    however, http:/ds9.si.edu version 7.4.1 (the latest release at time of writing) does not ignore the equinox. rather, it refuses to handle the wcs for the image. note that version 7.3 of ds9 does not seem to have the same issue.    while this does seem to be a bug in ds9, it's easy enough to work around by simply not writing equinox.",1,val
DM-5208,Evaluate MariaDB GSSAPI Authentication Plugin,"as a followon to dm4315, deploy the new https:/mariadb.com/kb/en/mariadb/gssapiauthenticationplugin/ in the iam testbed for kerberos ticketbased authentication, to provide single signon access.",2,val
DM-5209,Improve worker configuration files.,"configuration on the worker is done by setting environment variables in a script, which is lacking in flexibility, but there is a question of if it is worth changing to some form of text configuration file. the code that reads the configuration could be improved in either case.",10,val
DM-5218,Run Qserv multinodes integration tests inside Travis,this aims at preparing integration of this procedure inside jenkins ci,4,val
DM-5219,Add configured requirements parameters.  Pass/Fail test.,1. add pass/fail routine to report success/fail against metrics.  do this for     srd  (/)     configured metrics  (/)    2. add pass/fail reporting to running of `validate.drp.run`  (/),4,val
DM-5222,Add a ci_hsc daily build,"please add a daily build of `cihsc` to the jenkins system.    this does not need to explicitly build `lsstdistrib` or `lsstsims`.  the only product to list is `cihsc`.    in the slightly near future, i anticipate that this build will be replaced by a daily build of a metapackage `lsst_ci`.",5,val
DM-5233,implement cycle change in DLP,"summer > fall, winter > sprint, add x16",1,val
DM-5246,X16 Object Characterization Bucket,catch all epic for essential bugs and improvements in object characterization emerging during x16.,20,val
DM-5247,Segfault in shapeHSM centroid extractor," reports a segfault in measextenstionsshapehsm. he provides the following backtrace:    program received signal sigsegv, segmentation fault.  0x00007fffe7043156 in lsst::afw::table::baserecord::getelement/ (this=0x21c8d60, key=...)  at include/lsst/afw/table/baserecord.h:61  61 typename field/::element   getelement(key/ const & key)     see the discussion at dm4780.",2,val
DM-5248,Implement background gradient fit in pre-sub. images for dipole fit,add a linear background gradient fit to the integrated pre subtraction and dipole fitter (for testing).  this will eventually be implemented in the measurement plugin.,2,val
DM-5249,Implement a way to pass more than one exposure to a SingleFrameMeasurement (DipoleMeasurementTask),nan,10,val
DM-5250,Add background gradient fit to new dipole measurement task,nan,2,val
DM-5251,lsstsw breakage with spaces in paths,there are still some issues relating to using lsstsw to build the stack when spaces are in the path to the $lsstsw location. this is a fine thing to sort out on rodeo day...,1,val
DM-5252,"Base ""bright star"" cut on S/N instead of magnitudes","the astrometry histogram generated by validatedrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration.  suggests (and i agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration.  ",2,val
DM-5262,X16 Butler,work on improving butler:   refactor butler multiple repository support based on user feedback.    formalize butler configuration mechanism and define configuration persistence.    rfc and implement support for repo selection based on version.    design and rfc mechanism for butler to define output dataset type.    r&d & stub implementation of butler storage factorization (python type  file type  storage location)   implement spatial lookups in butler    minor bug fixing,49,val
DM-5264,Modernize python in lsst_build,the python in lsst_build uses old style print and exception handling. these should be updated to the current standard.,1,val
DM-5265,Turn on bias-jump fix for all CCDs ,"the overscan fix to handle bias jump in an amplifier done in dm 4366 introduced a new config parameter overscanbiasjumpbkp, and the fix is applied for ccds on the backplanes specified in overscanbiasjumpbkp.  previously, the default is to only fix ccds on backplanes next to the focus chips. but  also see the bias jump features in other ccds.  it would make more sense to turn it on for all ccds by default. ",1,val
DM-5267,Prepare narrative description of Level 3 operations from the perspective of the SUIT,"also known as the ""level 3 conops"" needed by ncsa.",10,val
DM-5270,Provide comparison routines for comparing two repos of the same data,adapt the hsc capabilities from dm4730 as represented on pipetasks u/laurenm/dm4730 (as prepared by [lauren] and [price])  into generally available pipetasks routines for comparison of two different repositories of the same data.  the intended use case is comparing two different algorithms or configurations on the same data and providing individual sourcemeasurement to sourcemeasurement comparisons for debugging new algorithms and understanding the behavior.,3,val
DM-5271,Audit SuprimeCam policy and update to current standards,obssubaru's policy/suprimecammapper.paf contains a number of entries that look wrong (e.g. deepforcedphotcoaddmetadata should be deepcoaddforcedconfig) or do not apply to lsst (e.g. stackconfig) and doesn't contain a number of entries that might be expected (e.g. transformed_src).    please ensure that this file is updated to comply with current expectations.,1,val
DM-5273,rename meas_simastrom to jointcal and flatten namespace,"moving meassimastrom from lsstfrance/ to lsst/ also resulted in a name change per rfc 123, and a namespace flattening: it's not derived from meas; it's a task. this is the necessary first step in getting it integrated into the stack.",1,val
DM-5274,Filtering from XY Plot table view (JS),allow to filter in a selected area from xy plot.,4,val
DM-5275,make floating point exception handling cross-platform (or remove it),"jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on osx. were these an important part of error handling in meas_simastrom, or can i just remove them?",2,val
DM-5276,plan to upgrade the third party packages,"the following packages need to be reviewed and maybe upgraded.      ""babel""     : ""5.8.34"",                           6.5.2 (m)      ""history""   : ""1.17.0"",                           2.0.0 (m)      ""icepick""   : ""0.2.0"",                            1.1.0 (m)                ""reacthighcharts"": ""5.0.6"",                      7.0.0 (m)      ""reactredux"": ""3.1.2"",                           4.4.0 (m)      ""reactsplitpane"": ""0.1.22"",                     2.0.1 (m)      ""reduxthunk"": ""0.1.0"",                           1.0.3 (m)      ""reduxlogger"": ""1.0.9"",                          2.6.1 (m)      ""validator"" : ""4.5.0"",                            5.1.0 (m)      ""chai"": ""2.3.0"",                                 3.5.0 (m)      ""esprimafb"": ""14001.1.0devharmonyfb"",        15001.1001.0devharmonyfb (m)      ""babeleslint""      : ""4.1.3"",                   5.0.0 (m)      ""babelloader""      : ""5.3.2"",                   6.2.4 (m)      ""babelpluginreacttransform"": ""1.1.0"",         2.0.0 (m)      ""babelruntime""     : ""5.8.20"",                  6.6.0 (m)      ""eslint""            : ""1.10.3"",                  2.2.0 (m)      ""eslintconfigairbnb"": ""0.1.0"",                  6.0.2 (m) works with eslint 2.2.0      ""eslintpluginreact"": ""3.5.1"",                  4.1.0 (m)  works with eslint 2.2.0      ""extracttextwebpackplugin"": ""0.8.0"",          1.0.1 (m)      ""htmlwebpackplugin"": ""1.6.1"",                  2.9.0 (m)      ""karmasinonchai"": ""0.3.0"",                     1.2.0 (m)      ""redux devtools""    : ""2.1.2"",                   3.3.1 (m)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (m)  ",2,val
DM-5277,replace buildbot with jenkins job(s),removing buildbot and replacing it with jenkins would provide a number of benefits     one less dashboard for developers to know about / interact with   one less system for sqre to maintain    lessening the cost of refactoring the ci drivers scripts as synchronized updates to two ci system configurations would no longer be necessary    it should also be easy to go one step further and try to eliminate the need for developers to manually log into the lsstsw account on lsst dev to publish eups distrib packages. ,3,val
DM-5278,Attend JTM,"joint technical meeting 2/22 24, santa cruz",6,val
DM-5279,arrays not properly transmitted,sending a property set with an array as one of the entries only passes the last element of the array.,1,val
DM-5280,Port HSC afw changesets to LSST,"we identified in dm5162 several changesets that still need to be ported from hsc to lsst:     2c12255372bde846ba0429b5b542960e57d169f0, 0aec617e0ea604cde85105de3dade279a4fe10df: footprint::overlapsmask   76f3706f6688b23d5b0c71e66af3e94095a9f821: copywithinfootprint: respect image size   f49676d7f1348f9de8ca21ee633e0c25473251ae: implemented linear and zscale transformations, hsc1206, 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04   c369a8ad53962aba950f7710210be4b23f45a523: utils: make mosaic.nimage a property   maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb: add display.utils function to plot the bboxes of inputs to a coadd in ds9   386a4b71d974d9e5672fe8690d0db3e56c9fb40f: box2?.getcorners   f3d3029c561b957069cbf280b62ea8e37447c068: calib: add operator= for scaling zeropoint   6c1845474f28f528a95190eeb88f095b11999078: check in #3092 (iterable coord) directly on master   f3d3029c561b957069cbf280b62ea8e37447c068: calib: add operator= for scaling zeropoint   5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e, 0d1ab12db604d5e42a5d72f028411a64294283ce: footprint merging   b8578746d69920bc1e1089cca4b4acb230f0e8d5: interpolate: add support for ndarray   3de3339aa075f869d73a5bc66fc65dbee8ae3d16: fix unit test fallout from interpolate std::vector change   08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04, f73544e15abd2760bf84794798cf4b84e97e938d: added xsize, ysize, and rescalefactor arguments to rgb; hsc 1207   88d838b74898d9572bbc8c46121da029958c1c72: rgb: fix makergb so it can replace saturated pixels and produce an image   254d7248ea20d98de481d968f6503d1610b16ae7: remove tests of writing rgb images to jpeg and tiff   3252a40ad55222b882acf14d2f7cf0f3fe80f585: added matchcontrol and implemented it for matchxy and matchradec   81c6063a32883b748f3770b7124d74ced7b480f5: implement and test includemismatches option in matchcontrol object   fd4c0baec617155fac0816607a5acba88e8970f5: add support for renaming without replacing the full field in schemamapper    79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56: handle empty footprints when normalising  ",4,val
DM-5281,"Port HSC skymap, shapelet changesets to LSST",we identified in dm 5162 some changesets that still need to be ported from hsc to lsst:     skymap:   f83f71718eac5307d575d3113ee3757a63a16de2: set vertex list in explicittractinfo.     shapelet:   bb928df3fc2fafe5183e0d075da19994f0af4fc7: let the value to normalize to be specified in [multi]shapeletfunction  ,1,val
DM-5282,Port HSC meas_algorithms changesets to LSST,"we identified in dm5162 several changesets that still need to be ported from hsc to lsst:     1293a31c19c238ba2c2acd8f67ec1be742764b66: binnedwcs   9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background   ad74fe8595ec523d6269e36ec2db051534bf3e9a: add initial.flags.pixel.interpolated.any to objectsizestarselectorconfig.badflags   69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: check for bad psf measurements (hsc1153)   a54b1ac52678025d3317e8a379c2849d3eb567ba: pcapsfdeterminer: catch case of no good psf candidates in debugging   c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: indent seems to be wrong for debug display   2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: make idspancompar more deterministic   f99eb46f484609673b45290eaaba47688d7b4a24: cr code has to take care of 'no_data' mask   6f6b786bce8ca34bf4c67f75f965130dea027147: handle small numbers of psfcandidates (hsc1176)   d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for measureapcorrtask   65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: test case for measurement of negative objects    there are also a couple of issues that were identified in the dm5162 review:   delete tests/config/measuresources.py  mere configuration, oldstyle measurement    testpsfdetermination has method 'xtestrejectblends'",4,val
DM-5283,Port HSC daf_butlerUtils changesets to LSST,"we identified in dm5162 several changesets that still need to be ported from hsc to lsst:     daee24edba01b01a0412df7f9b4cf70be5b10860: cameramapper: allow a default filter name to be provided   e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: cameramapper: normalize path names, and remove leading double slash   476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameramapper.backup()   b2491ef60e5e23afa7d9f0297f257e694aa1af35: only attempt to update wcs if it's available   9f62bcce588fa9abc8e1e44ff2f0275e5230f629: registry: hold registry cache for a single thread only (hsc1035)   412f03b95b7a5e82003ab33a61bd43adbf465188: registry: use a pool of registries to avoid having too many open files",2,val
DM-5284,Port HSC meas_extensions_simpleShape package to LSST,"hsc uses a package, measextensionssimpleshape, which needs to be ported to lsst.  the package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins.",3,val
DM-5285,Port HSC meas_extensions_multiShapelet changesets to LSST,we identified in dm 5162 a few changesets that still need to be ported from hsc to lsst:     bf5f753133ae4b41357f9789ff4763949ebb6ffb: fitpsf: reduce outerorder to 1   a54d6cbd41baf916fac2a1bb235a8502af14edfd: provide explicit instantiations for the sake of clang 6.0 on os/x 10.9    a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: fitprofilealgorithm: bail if the psf flux failed  ,1,val
DM-5286,Port HSC meas_deblender changesets to LSST,"we identified in dm 5162 a few changesets that still need to be ported from hsc to lsst:     a8cf6c22df14494d6dcf2d7354c695cba9506301: clarify tiny footprint limit   624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayfluxrule   db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning    we also identified a few differences that should be resolved:   clipstrayfluxfraction defaults to 0.01 for lsst, 0.001 for hsc    stray file, src/baseline.cc.orig, on lsst side  ",1,val
DM-5287,Port HSC ip_isr changesets to LSST,we identified in dm 5162 some changesets that still need to be ported from hsc to lsst:     f1cee734998f1faf86c02af42ea599b077847eeb: isrtask: allow fallback to a different filter when loading calibrations   89cd629bb8e1a72a545176311b1ef659358d95af: saturationdetection: apply to overscan as well as image  ,1,val
DM-5288,Port HSC pipe_tasks changesets to LSST,"we identified in dm5162 some changesets that still need to be ported from hsc to lsst:     31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiband: save backgroundsubtracted coadd as deepcoadd_calexp   e99e140feafe28e6f034143e8ee2ae58e9a9358d: rejig interface for detectcoaddsourcestask to provide nondatarefcentric api   829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects   measuremergedcoaddsources.domatchsources defaults to false   processimageconfig.dowriteheavyfootprintsinsources defaults to false ?   56666e8feba6893ac95fd4982d3e0daf6baf2d34: wcsselectimagestask: catch imagepoly is none    we also noticed some differences:      calibrateconfig.setdefaults doesn't call parent   calibratetask.run isn't returning apcorrmap   reservefraction= 1 instead of 0.2  ",3,val
DM-5289,Port HSC obs_subaru changesets to LSST,"we identified in dm5162 several changesets that still need to be ported from hsc to lsst:     8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for hsc narrowband filters   69d35a890234e37c1142ddbeff43e62fe36e6c45: set radius for flux.naive, adjust comment for flux.sinc   8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly   8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionsize   e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelscale for jacobian correction   remove processccdonsitedb.py, processstack.py   rename stacker.py to coadddriver.py or whatever nate chooses in dm3369   49e9f5dcf16490f6be6438b89b17911a0cd35fb2: fixed obvious errors caused by introducing vignetteconfig   8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for hsc narrowband filters   daa43eeac46e8708de6f37feeb5d5d16a3caca11: hscmapper: set unit exposure time for dark   77ff7c89d56bed94bca4f320f839dbd20fbab641: set bad mask for dead amps instead of sat    we also noticed the following need to be done:     forced photometry configuration (ccds and coadds)   sanitize config of obssubarudir (use getpackagedir)   multiband config files need ""root"" > ""config""   no astrometry in measurecoaddsources   narrow bands missing from priority list   detectcoaddsources removed from multiband   move filtermap from config/processccd.py into own file",5,val
DM-5290,Add z-index for dialogs components,some of the outside modules that we have brought in have a z index.  we need to make sure that our dialog components stay on top of them.,2,val
DM-5291,Docker-ready configuration system for LTD Keeper,"to deploy ltd keeper in a docker container (dm5194), it’s best practice to handle all configurations through environment variables. in dm4950, ltd keeper was configured through files for test and dev/deployment profiles. what we should do is continue to allow hard coded configurations for test and dev environments, but have a third fully fledged configuration environment that’s driven entirely by environment variables.    the environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing).    this should also resolve how to deal with google container engine/kubernetes auth flow works with environment variables, config files, and profiles.",1,val
DM-5292,Add queue support,"the ctrl_events package currently only supports activemq topics.  this change will add support for queues.    additionally, there will be some minor code and assertion clean up as noted in dm 5279.",12,val
DM-5294,ImageDifferenceTask: Refactor Image DifferenceTask,"the original dm 3704 was to refactor all imagedifference task. this issue was split into 3 tasks:  1) split image difference task into two tasks (1) to generate an image difference, and (2) to run detection and measurement on it: processdiffim.py  2) refactor the image difference portion  3) refactor the processdiffim portion    this ticket refactors the new task that just generates and image difference. ",8,val
DM-5296,Rewrite unit tests for new dipole measurement task,nan,8,val
DM-5297,Make jointcal buildable under CI,"once jointcal is part of the stack, we need to get it under continuous integration, buildable by jenkins, etc. there is only one unittest in the package currently, but at least getting that test built and run will catch a number of basic problems.    this requires having its dependencies (cholmod from suitesparse) under ci as well.    as part of this, it would be good to have at least one ""integration test"" that runs jointcal as part of processccd, to catch problems that appear when that interface changes.",8,val
DM-5298,Document simple simulator,document the simple simulator produced in dm 4899.  this will also involve some refactoring and adding unit tests to make it usable by others in the group.,8,val
DM-5300,X16 Design Discussions,design discussions within the team and with other teams to ensure the plan is complete and properly scoped,64,val
DM-5301,Compare LSST and HSC pipelines through through single-frame processing,"following dm2984, we are confident that the isr performed by the lsst and hsc stacks is equivalent. extend that work to cover the whole of singleframe processing (processccdtask).    there are two deliverables for this issue:   a collections of plots and/or comparison scripts that run on the pipeline outputs that can be used to compare their quality in all the ways we think matter. this should probably be in a new repo. some of these tests should compare the results of the two pipelines against each other (as that's easier to do), but others should be useful for tracking the quality of the pipeline even after we abandon the hsc fork.   a set of new jira issues that capture the code transfers we think we will need to get the lsst pipeline up to the same level of quality as the hsc one.",10,val
DM-5302,manage jenkins core + plugin versions,there have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions.  this would be avoided by putting all plugin versions under change control.    including:   the versions of all jenkins components need to be explicitly specified   the stored job config.xml's should be updated to reflect plugin version changes    the hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week  ,5,val
DM-5307,Get high volume test script working again at IN2P3 cluster,"currently runquerys.py script falls over when running high volume tests:     ""lv"": 75,   ""ftsobj"": 3,   ""ftssrc"": 1,   ""ftsfsrc"": 1,   ""joinobjsrc"": 1,   ""joinobjfsrc"": 1,    ""nearn"": 1    we need this to be working again to validate recent work on schedulers and to support upcoming work on large results, etc.",10,val
DM-5308,Test removal of response queuing on czar to see if this provides useful flow control,nan,3,val
DM-5311,Enable automated publication of qserv-dev release,this would allow integration tests in ci not to break when some qserv dependencies change. indeed ci uses a docker container which include qserv dev to build current qserv version.,4,val
DM-5312,Additional vertical partitioning tests,"test potential improvements in manyverticalshards test (20,50) run times with query optimizer settings.",5,val
DM-5313,Refine MemManReal implementation per design discussion w/ John,nan,10,val
DM-5314,Implement unique query-id generation,"there are currently two separate query ids defined for queries in czar code:   ""user query id""  defined in czar::submitquery(), used for constructing table names for result table and message table   ""qmeta query id""  id obtained from qmeta after registering the query (by userqueryselect::_qmetaregister())    currently user query id is used by the rest of the czar code to track the processing of this query, qmeta id is not used yet for anything except qmeta registration and updates.     qmeta id will be used for async query identification and there is no actual reason to keep two ids around, so we should replace user query id with the qmeta generated one everywhere. one minor issue is that currently message table name is built and table is locked before we register query in qmeta. need to understand it and see if we can reverse that logic.",4,val
DM-5315,DM Verificational Plan Document for CoDR,"this epic captures work resulting from a systems engineering request for a document on the dm verification plan.     in summary, this document must describe how are dm deliverables are going to be verified against the requirements?    george agrees that structuring this dm verification plan around the already drawn kpms is the right thing to do.     square will draft a document for internal dm circulation and eventually leading to a projectled conceptual design review of the dm verification plan.     the skeleton plan is:   go through the kpms   list the method (== tools) by which is kpm will be measured      => describe the requirements for that method      => criteria of success     state when it can be measured     describe the data necessary or planned for doing the measurements    additionally, a process (most likely an end2end run) will be described that can verify that external to dm interfaces are being correctly met.     there is no requirement from the point of the codr for a resourceloaded plan leading to this work. that is expected to follow from a successful codr.           ",28,val
DM-5316,Research alert production database design,nan,4,val
DM-5317,Identify specs within VO stack which should be implemented by database team,nan,6,val
DM-5318,Begin exploratory TAP implementation within dbserv,"this is a quick coding foray, to try to shake loose unforseen implementation dependencies or speedbumps with tap integration.    timeboxed at 4 points to fit into a single sprint with brian's current resource loading  this is intended to be only a clarifying start.",4,val
DM-5319,Fix mariadb CI,patch package is missing in docker container used by travis ci.,1,val
DM-5320,Make Bright Object Masks compatible with all cameras,"currently all of the logic that goes into using bright object masks falls into obssubaru and pipetasks. this ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. the work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. bright object masks were originally introduced in dm 4831",2,val
DM-5321,MeasureApCorrTask should use slot_CalibFlux as default ref flux,"measureapcorrtask uses ""basecircularapertureflux170"" as its default reference flux. it should use ""slotcalibflux"" instead.    also check obssdss packages for overrides that can be removed; obssdss certainly has one in config/processccdtask.py",1,val
DM-5322,Remove any redundant or unused datasets,please remove any redundant or unused dataset names from policy files throughout the stack.,1,val
DM-5323,estimateBackground should not make a deep copy of the exposure,implement rfc155: change estimatebackground as follows:   always subtract the background   modify the exposure in place   replace estimatebackground with the run method of a new task subtractbackgroundtask    replace getbackground (which fits a background) with subtractbackgroundtask.fitbackground,4,val
DM-5324,"Convert GWT code to pure JavaScript (X16, part2, basic)",continue to work on the gwt code conversion to javascript.,100,val
DM-5327,Create network monitoring dashboard for nebula sys admins,nan,4,val
DM-5328,JTM meeting in Santa Cruz,"met with many individuals, had lots of good conversations. helped bring me closer to the working of the project and provided a level set for where activities are currently at. attended lhn session for most of wednesday. ",10,val
DM-5329,Assist in OSX VM environment deployment,nan,2,val
DM-5330,Add ExposureIdInfo class,"implement rfc146: add exposureidinfo class to dafbutlerutils    this will be implemented in dafbutlerutils as part of dm4692, with a unit test in obstest because dafbutlerutils has no camera mapper or camera repo in its test directory.",4,val
DM-5331,Add usesMatches to star selectors,implement rfc126 add usesmatches to star selectors    this will be implemented as part of dm4692,4,val
DM-5333,a Catch all bug fix epic (X16),a epic for reported bugs in this scycle,10,val
DM-5334,GWT Conversion: Table results container,"create a result container for table data.  this task is composed of:   create actions, action creators and reducing functions   dynamically add/remove table from view   support expanded mode   tabpanel support for deleting tabs.",6,val
DM-5336,Fix minor issues in docker procedure," params.sh was missing at configuration   startup.py wasn't importing correctly module ""utils""    remove unused parameters in params.sh",1,val
DM-5337,"Planning for GPFS, etc.", gathered filesize statistics from existing nfs for planning gpfs   assisted with gpfs client setup on test servers    reviewed infrastructure changes for jason,2,val
DM-5338,Week end 2/07/16,"support for lsst dev cluster, openstack, and accounts  for week ending february 7, 2016.",5,val
DM-5339,Week end 2/14/16,"support for lsst dev cluster, openstack, and accounts  for week ending february 14, 2016.",6,val
DM-5340,Week end 2/21/16,"support for lsst dev cluster, openstack, and accounts  for week ending february 21, 2016.",2,val
DM-5341,Week end 2/28/16,"support for lsst dev cluster, openstack, and accounts  for week ending february 28, 2016.  ",4,val
DM-5342,Jason Feb Tasks,"procurement activities to prepare ""procurement plan activity 1"". pdu, rack, network selection and review. refresh quotes for compute, storage, rack, pdu, electrical. refresh, finalize, present (internally) and review design for fy16 infrastructure.    draft and review of procurement plan activity 1 document.",6,val
DM-5343,New equipment setup and configuration (week end 2/07/16)," updated lsstdev7 with few missing pieces after initial user testing   setup 3 of 8 lssttest servers   confirmed ipmi setup on new test servers (working with dell on issue with 1 idrac license upgrade)   completed and verified ipmi setups   installed licenses for lssttest1  lssttest6   reassociated ipmi lssttest1m lasttsst6m with the correct systems.   installed centos7 on lssttest1  lssttest6. (in progress)   ups setup   setup table with location of systems in 3003 racks    setup apcusbd on lsststor141, lsststor142, lsststor143, lsst stor144, lsst20, lsst13",3,val
DM-5344,Jason Mar Tasks,"week 1: admin mtg, group mtg, ici mtg, interview. 1pt  week 2: meetings, ici task planning and prioritization 2pt  weeks 3&4: interviews, admin and group meetings, ari sow",6,val
DM-5345,New equipment setup and configuration (week end 2/14/16)," still pushing at dell to fix broken idrac license   added 5 systems to rsa otp system   completed the setup of lssttest1, lssttest4, lssttest5, lssttest6   reinstalled lssttest1 to correct error in puppet install, completed centos7 install, installed puppet and ran puppet   corrected network error on lssttest4, completed centos7 install, installed puppet and ran puppet   completed the setup of installed lssttest5, completed centos7 install, installed puppet and ran puppet    installed lssttest6, completed centos7 install, installed puppet and ran puppet",1,val
DM-5346,Recover from accumulated technical debt,"through the s15 and w16 cycles the drp group focused on merging functionality from hsc. to expedite this process, we accepted lower quality documentation and poorer test quality than would normally be required. we need to recover from this, and other, accumulated technical debt.",50,val
DM-5347,Add tests for recent improvements to CModel,"in dm 4768 we ported a number of improvements to cmodel from hsc. however, these were not accompanied by test cases. please add them.",3,val
DM-5348,Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask,"update processccdtask so that it can be used with different datasete types as appropriate for the isr task. this will allow us to get rid of obsspecific variants processccdsdsstask and processccddecamtask    the plan is to change processccdtask as follows:   set domakedatareflist=false in the call to addidargument   get the dataset type from the isr task (default to ""raw"") and set it in data container   make the dataref list by calling makedatareflist on the data container    question for decam folks: do you want two executable scripts for decam (one that processes data from the community pipeline and one that performs isr)? or do you prefer one exectutable (in which case you switch between performing isr and reading the output of the community pipeline output by retargeting the isr task)? if you prefer one binary, then which should be the default: perform isr or read the output of the community pipeline?",2,val
DM-5349,Revise LSE-140 to account for recent changes to calibration instrumentation,"produce a revision of lse140, the dm  to   auxiliary instrumentation icd, taking into account recent changes to the calibration instrumentation.",5,val
DM-5350,Establish goals and create EA framework for LSE-140 update,"deliverable: together with , identify the changes needed and develop initial content in ea.",2,val
DM-5351,Create change request for LSE-140,deliverable: change request and document diffs for lse 140,1,val
DM-5352,Add Vis toolbar to expanded mode,nan,1,val
DM-5353,Upgrade minuit2,minuit2 5.34.14 came out in 2014. the current version in the stack is 5.28 from 2010. minuit2 is annotated on the dm third party software page as being approved for 6monthly uprev. minuit2 is only used by afw.    release notes for 5.34.14:     several fixes and improvements have been put between this verion and the previous standalone one (5.28). main new features is now the support for using the root::math::minimizer interface via the class root::math::minuit2minimizer also in the standalone version. a new test has been added (test/mnsim/demominimizer.cxx) to show this new functionality   other major improvements is in the control of the error messages. one can now use the class mnprint::setlevel(int value) to control the output level. the same can be achieved by calling root::math::minuit2minimizer::setprintlevel.,1,val
DM-5355,meas_algorithms uses packages that are not listed in table file,measalgorithms directly uses the following packages not expressed in the table file:   minuit2   dafpersistence   dafbase   pexconfig   pexexceptions   pexpolicy  ,1,val
DM-5356,Test consistency of Shear Measurements with different Psfs,"dm 1136 was done with a single psf, partly to avoid some of the problems we found with psfshapeletapprox.  in this issue, i will look at consistency of the measurement for different psfs.",8,val
DM-5357,Test error estimation with bootstrap resampling,test the error estimation code using bootstrap resampling.,6,val
DM-5358,Move supertask code our from pipe_base,create a new package pipe_supertask and move all supertask code and activator there. will soon create a poll to pick a better name.,3,val
DM-5359,Update DMTN-002 to reflect last changes,"need to update documentation with latest changes on pipebase, pipesupertask and pipe_flow",1,val
DM-5360,Update {{pipe_flow}},update pipeflow to change dependencies and examples to reflect migration to pipesupertask,1,val
DM-5361,Begin Image Select Panel/Dialog,"this is the first stop in image select panel:     design basic panel   implement tabs: issa, 2mass, wise    implement ability to replace a image",10,val
DM-5362,Image Select panel: finish tabs,"implement      dss, sdss, blank image tabs   a reusable radius input field     implement upload file tab  ( the reusable upload file component is done in dm 5584)  ",10,val
DM-5363,Image Select Panel: 3 color support,"add 3 color support.  basically take the panel an be able to repeat of red,green, and blue.  we might want to use a disclosure component.    what has been done: (copied from github commit message)  add 3 colors support to image selection dropdown, disclosure component is used for r, g, b field group tabs.    for sizeinpufiield component, fixed validation, add props 'showfeedback' for feedback display and add valid range in error message popup.  (this update is based on the following issues as sizeinputfield is used in other panel      popup message should have the range are part of the message      the feedback at the bottom should be optional, turned on by a property.      when a value such as ‘111d’ is entered,  it does not validate correctly. )    some issues:    should 'disclosure' component's status be kept?     image creation doesn't work if any of r, g, b is disabled.",10,val
DM-5364,Image Select Panel: Support add or modify of plot,previously the image select panel would only modify a plot.  now give it the ability to add a plot.,8,val
DM-5365,Enable CC-IN2P3/Qserv team communication in order to prepare for Pan-STARRS large scale tests,"the goal of this ticket is to enable communication between ccin2p3 and qserv team in order to prepare for panstarrs data ingestion into qserv. this data ingestion step is necessary for the large scale tests of qserv foreseen for summer 2016.    specifically, we need to understand:     what is the size of the data set to be imported to ccin2p3?   where the panstarrs data set to be imported is currently located?   what mechanisms will the host of panstarrs data make available to ccin2p3 for downloading the data set?   does the envisaged ingestion mechanism into qserv requires that the data transit through the qserv master server or will each qserv worker be able to ingest its own chunk of data?   after the ingestion process is finished, do we need to keep a copy of the ingested data out of qserv?      given the size of the dataset likely involved in this process, this project will probably require that we (both qserv and ccin2p3 experts) set up specific mechanisms and equipment for efficient transport, storage and ingestion of these data. timely planning and several testing campaigns seem necessary for this project to make progress.      ",8,val
DM-5367,Change default value of MeasApCorrConfig.refFluxName to slot_CalibFlux,"the default value of measapcorrconfig.reffluxname is presently ""basecircularapertureflux170"". this should be changed to ""slotcalibflux"". that is what the slot is intended for. the slot usually points to ""basecircularapertureflux170"", but obssdss, at least, overrides this.    additional jobs:   update obssdss config/processccd.py to remove the override for this value, since it will no longer be needed.   check for and remove unnecessary overrides in other obs packages",1,val
DM-5368,Use modern TAP package declarations for all EUPS third party packages,in dm4670 the tapness of the packages was declared using a .tappackage file. the modern fix is to use a $tappackage environment variable in the eupspkg.cfg.sh file. this is how pyyaml was implemented.,1,val
DM-5370,Create {{lsst_ci}} package as a continuous integration build target,"create an lsstci package to be built for the continuous integration testing.    plan:  1. create empty package that has dependencies on obscfht, obsdecam, obssubaru, testdatacfht, testdatadecam, testdatasubaru. (/)  2.  ensure above builds. (/)  3.  add obslsstsim and ensure that it builds. (/)    the following were moved to dm5381:  [  : how can i get strikethrough to work in the following list?]  3. add dependencies on validationdatacfht and validationdatadecam, and validatedrp.  4. run cfht, decam quick examples in validatedrp.  5. test for successful running of the above examples.  fail and trigger jenkins failure message if these examples fail.  6. check performance of cfht, decam runs against reference numbers.  fail if there is a significant regression.  7. decide how to include ci_hsc, which currently can take at least 30 minutes to process the image data. ",1,val
DM-5371,butler planning for X16,nan,4,val
DM-5372,Fix obs_* packages and ci tests broken by DM-4683,"the butler changes in dm4683, in particular the removal of .mapper from the interface exposed by a butler object, broken obscfht, obsdecam, and cihsc.    this issue will fix those changes, and search for additional broken things.    this work is proceeding in conjunction with dm5370 to test that the ci system, e.g. lsstci, is sensitive to these breakages and fixes.",1,val
DM-5374,Add to baseline a dedicated replica of L1 database just for scans,"per rfc133, users will sometimes need to do full table scan through l1 catalogs, and our baseline does not allow for full scans on the l1 catalog. it'd be good to maintain a replica of l1 for such scans. this story involves changing ldm141 and adding hardware for the replica. ",4,val
DM-5379,Remotely attend JTM 2016 sessions,ssia.  the final hours of the final day were very valuable.,6,val
DM-5380,L1 base messaging topology.,"make sequence diagram for base site messaging and enumerate each message, then provide a narrative description of each message (including logical flow control if applicable) and an example message payload for the message dictionary. 4  meetings about message exchange style.  meetings about using queue fanout for return messages from forwarders and distributors, or binding using routing keys. 4  rapid prototyping of some of these ideas for evaluation. 4",12,val
DM-5381,Create {{lsst_qa}} package as a daily build target for regression testing,"1. add dependencies on validationdatacfht and validationdatadecam, and validationdatahsc.  (/)  2. add dependency on validatedrp.  (/)  3. run cfht, decam quick examples in validatedrp.  (/)  4. test for successful running of the above examples.  (/)  5. implement in a testing framework.  6. check performance of cfht, decam runs against reference numbers. fail if there is a significant regression.  (/)  7. include ci_hsc, which currently takes 1000 seconds  to process the image data.  8. check performance of hsc runs against reference numbers.  fail if there is a significant regression.",4,val
DM-5384,Port SdssShape changes from HSC meas_algorithms to LSST meas_base,"in porting meas_algorithm changes from hsc to lsst, modifications to the sdssshape algorithm were discovered. these changes should be transferred to lsst.",3,val
DM-5385,calib_psfReserved is only defined when candidate reservation is activated,"the schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns.  however, measurepsftask only creates the calib_psfreserved column when reservefraction > 0.  this causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs.",1,val
DM-5387,Filter  editor,"a dialog to edit all the filters on the data for table and xy plot.     and, or conditions?    implemented:   display column's units and descriptions   add single column filter with autocorrection   add freehand filters field with validation and auto correction   reset, clear filters as well.    'column' is sticky... scrolling left/right will not affect it.    ",8,val
DM-5388,catalog search panel,the search panel to do the catalog search,10,val
DM-5389,GWT Conversion: Dropdown Container,"create drop down container to display search panel, catalog search panel, image search panel, etc.     ",4,val
DM-5390,JavaScript loading/caching plan,"we need to ensure that the latest version of the application(javascript) is loaded. conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded.  to do this, we created a tiny fireflyloader.js script whose role is to load the main script and then its dependencies. fireflyloader.js is configured to never cache so that the latest main script is always picked up. the main script is appended with a unique hash on every build.  this ensures that the browser will pick up the new script the very first time, and then cache it for future use. ",2,val
DM-5392,Please stop leaving repoCfg.yaml files around,"after a recent change to daf_persistence and possibly other packages i'm finding that many packages leave repocfg.yaml files lying around after they run unit tests.    i'm not sure what is best to do about these files. if they are temporary, as i am guessing, then i think we need some way to clean them up when the tests that generated them have run. if they are intended to be permanent (which would be surprising for auto generated files) then they should probably be committed?    i hope we can do better than adding them to .gitignore.",1,val
DM-5394,Investigate boost compiler warnings and update boost to v1.60,"as reported in comments in dm1304 clang now triggers many warnings with boost v1.59:    /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'staticwarningline148' [wunusedlocaltypedef]      booststaticwarning(typex::value);        /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/serialization/staticwarning.hpp:100:33: note: expanded from macro 'booststaticwarning'  #define booststaticwarning(b) boostserializationbsw(b, line)                                    /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/serialization/staticwarning.hpp:99:7: note: expanded from macro 'boostserializationbsw'      > boostjoin(staticwarningline, l) booststaticassertunusedattribute;           /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'boostjoin'  #define boostjoin( x, y ) boostdojoin( x, y )                               /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'boostdojoin'  #define boostdojoin( x, y ) boostdojoin2(x,y)                                  /users/rowen/uw/lsst/lsstsw/stack/darwinx86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'boostdojoin2'  #define boostdojoin2( x, y ) x##y                                   /:25:1: note: expanded from here  staticwarning_line148  ^    v1.60 is the current version so we should see if these warnings have been fixed in that version.",2,val
DM-5395,Prototype afw/AstroPy integration,"investigate and prototype options for integrating afw with astropy. in particular, this epic focuses on establishing how tightly, if at all, astropy and afw::table should be coupled.",25,val
DM-5396,Transfer relevant HSC documentation to LSST,"audit the hsc http:/hsca.ipmu.jp:8080/questions/ and http:/hsca.ipmu.jp/hscsphinx/ documentation. identify the parts which are still relevant to lsst. transfer them to the lsst documentation.    first assumption is that questions go to http:/community.lsst.org in an appropriate category, while the howto documentation is incorporated into the developer or user guides as appropriate. confirm this with square before starting work.    the intention is to carry out most of this work as a group in a ""dockathon"" session.",20,val
DM-5397,X16 Framework bucket,catch all epic for emergent work in 02c.04.01.,20,val
DM-5398,Support for DM replanning process,throughout the x16 cycle we expect to have to assign effort to support the ongoing dm replanning process. this work takes two forms:   tasks assigned by the dmlt working groups;   facetoface discussions with other parts of the dm project.    both are captured in this epic.,25,val
DM-5399,HSC port: data release verification,"throughout s15 and w16 we have worked to merge changes from the hyper suprimecam stack to lsst. this work is now broadly complete, but requires acceptance testing by hsc. in support of that, the lsst stack must be brought to a level at which it is capable of reproducing the january 2016 hsc data release. this work is a continuation of the effort undertaken in dm3628. it will reach a successful conclusion when the hsc project undertakes future development based on the lsst stack.",50,val
DM-5400,Cleanup jointcal,"before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. this ticket is for that.",2,val
DM-5401,Calibration Products Pipeline development in X16,continued investigation and characterization of the decam cbp data.,25,val
DM-5402,Make cluster deployment scripts more generic and enable ccqserv100...124,these scripts will be improved (i.e. more genericity) and integrated inside qserv code. qserv will be deployed on ccqserv100 to ccqserv125,3,val
DM-5403,Developer Guide Content & Maintenance Backlog Epic,general maintenance and original content for the dm developer guide (http:/developer.lsst.io) based on needs during the cycle.,7,val
DM-5404,LSST the Docs Production Deployment,in dm1139 we developed lsst the docs. lsst the docs is described in http:/sqr 006.lsst.io. this epic will focus on the deployment of lsst the docs as a reliable production service for documentation builds and hosting.,16,val
DM-5405,Re-enable CModel forced measurement on CCDs,"recent changes from the hsc side (dm4768) were implemented in a hurry, and break cmodel forced measurement when the reference wcs is different from the measurement wcs (as is the case with forced measurement on ccds).  this was considered an acceptable temporarily, since forced ccd measurement is currently severely limited by our lack of deblending, but we'll need to fix it eventually.    the fix is trivial from an algorithmic standpoint but may require a bit of refactoring (at least changing some function signatures; maybe more).    this should include reenabling the different wcs complexity in testcmodelplugins.py,",2,val
DM-5406,Require fields listed in icSourceFieldsToCopy to be present,"calibratetask presently treats config field icsourcefieldstocopy as a list of fields to copy if present. this was required because one of the standard fields to copy was usually missing. however,  fixed that problem in dm 5385. now we can raise an exception if any field listed is missing (though i propose to continue ignoring icsourcefieldstocopy if issourcecatalog is not provided).",1,val
DM-5407,Rename datasets to utilize butler aliases,"now that the butler has alias features that can allow for some degree of dataset substitutability, we should consider renaming (or adding aliases) for our existing datasets to make the naming consistent and analysis code more generic.    this work should be proceeded by an rfc with a proposal for the new names and a migration plan.    it might make sense to defer this until the highlevel pipeline descriptions are more mature and we can choose relatively futureproof names, but hopefully the alias features will also make migration easy enough that this doesn't matter a lot.",4,val
DM-5408,Qserv do not return very same BLOB field than MySQL,"enabling query qservtestdata/datasets/case01/queries/0007.2fetchsourcebyobjidselectblob.sql.fixme will reveal this bug.    qserv chunk table contains next blob:    mysql socket /home/dev/qservrun/git/var/lib/mysql/mysql.sock user=root password=changeme qservtestcase01qserv e ""select blobfield from source6630 where sourceid=29809239313746172;"" > 29809239313746172.chunk6630    vi 29809239313746172.chunk6630    blobfield    db\0w\0\0\0b\0k\0b\0firstfieldsecondfield(�q.�      but qserv returns:    db\0w\0\0\0b\0k\0b\0firstfieldsecond_field(�q.�      see dm 991 for additional informations.  ",8,val
DM-5410,DecamIngestTask is mis-calling openRegistry,"`decamingesttask` is miscalling `lsst.pipe.tasks.registrytask`. line 59:      with self.register.openregistry(args.butler, create=args.create, dryrun=args.dryrun) as registry:    openregistry is expecting a directory name, not a butler object for the first argument    thanks to  for diagnosing this.",1,val
DM-5412,Test new dipole fitting task on real data,"test new task on real data (which data, tbd); inspect results by eye and compare with existing dipolemeasurementtask output. this is necessary prior to incorporation into the imagedifference commandline task.    this test may also indicate that further optimizations are necessary (dm5721).",6,val
DM-5413,Incorporate new DipoleFitTask into imageDifference command-line task alongside existing DipoleMeasurementTask,"incorporate the new task into the commandline task. the goal of this ticket is to implement dipolefittask alongside the existing dipolemeasurementtask, eventually to replace it.    this is likely to have additional stories added, including testing, possibly as part of dm 5412.",6,val
DM-5414,Create buildable SuiteSparse external package,"to get jointcal to build in the stack, we need to satisfy the suitesparse dependency by creating an external package for suitesparse.    assuming it builds cleanly, this should satisfy the remaining requirement of rfc 153, now that the licensing question has been answered there.",2,val
DM-5415,generation of conda binary packages for DM software products,"this epic covers work in generating binaries for stack releases. at this point we are persisting with the plan to produce conda binary packages for ease of use on the user side, though their reliable generation has so far resisted automation.    conda binaries will be produced for the 12.0 stack release. ",32,val
DM-5416,Ci Deploy and Distribution Improvements part IV,this is a bucket epic for ongoing improvements to the ci system,8,val
DM-5418,Release engineering Part Three,"this epic covers testing and co ordination work associated with making  engineering and official releases, and code to support them.      (fe:8, dn:6.5, js:8)",23,val
DM-5419,ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd,"since the first week of march 2016, cihsc fails its test that requires that >95% of the psf stars be identified as stars in the coadd.  i suspect this is related to the dm4692 merge.    here is a sample job that fails:  https:/ci.lsst.codes/job/stackosmatrix/9084/label=centos6/console    the relevant snippet of the failure is:      [20160310t17:12:06.667778z] : validating dataset measurecoaddsourcesconfig for   [20160310t17:12:06.697383z] cameramapper: loading registry registry from /home/build0/lsstsw/build/cihsc/data/registry.sqlite3  [20160310t17:12:06.697615z] cameramapper: loading calibregistry registry from /home/build0/lsstsw/build/cihsc/data/calib/calibregistry.sqlite3  [20160310t17:12:07.716310z] cameramapper: loading registry registry from /home/build0/lsstsw/build/cihsc/data/registry.sqlite3  [20160310t17:12:07.716443z] cameramapper: loading calibregistry registry from /home/build0/lsstsw/build/cihsc/data/calib/calibregistry.sqlite3  [20160310t17:12:08.663566z] : measurecoaddsourcesconfig exists: pass  [20160310t17:12:08.721051z] : measurecoaddsourcesconfig readable (/): pass  [20160310t17:12:08.721077z] : validating dataset measurecoaddsourcesmetadata for   [20160310t17:12:08.721249z] : measurecoaddsourcesmetadata exists: pass  [20160310t17:12:08.721663z] : measurecoaddsourcesmetadata readable (/): pass  [20160310t17:12:08.721715z] : validating dataset deepcoaddmeasschema for   [20160310t17:12:08.721878z] : deepcoaddmeasschema exists: pass  [20160310t17:12:08.726703z] : deepcoaddmeasschema readable (/): pass  [20160310t17:12:08.726834z] : validating source output for   [20160310t17:12:10.203469z] : number of sources (7595 > 100): pass  [20160310t17:12:10.204166z] : calibpsfcandidate field exists in deepcoaddmeas catalog: pass  [20160310t17:12:10.204772z] : calibpsfused field exists in deepcoaddmeas catalog: pass  [20160310t17:12:10.205468z] : aperture correction fields for basepsfflux are present.: pass  [20160310t17:12:10.206159z] : aperture correction fields for basegaussianflux are present.: pass  [20160310t17:12:10.207193z]  fatal: 95% of sources used to build the psf are classified as stars on the coadd (0 > 0): fail  [20160310t17:12:10.207455z] scons:  [.scons/measurehscr] assertionerror : failed test: 95% of sources used to build the psf are classified as stars on the coadd (0 > 0)  [20160310t17:12:10.207481z] traceback (most recent call last):  [20160310t17:12:10.207525z]   file ""/home/build0/lsstsw/stack/linux64/scons/2.3.5/lib/scons/scons/action.py"", line 1063, in execute  [20160310t17:12:10.207556z]     result = self.execfunction(target=target, source=rsources, env=env)  [20160310t17:12:10.207593z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 133, in scons  [20160310t17:12:10.207611z]     return self.run(args, kwargs)  [20160310t17:12:10.207646z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 122, in run  [20160310t17:12:10.207663z]     self.validatesources(dataid)  [20160310t17:12:10.207732z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 191, in validatesources  [20160310t17:12:10.207749z]     0.95psfstars.sum()  [20160310t17:12:10.207786z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 52, in assertgreater  [20160310t17:12:10.207816z]     self.asserttrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [20160310t17:12:10.207853z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 43, in asserttrue  [20160310t17:12:10.207877z]     raise assertionerror(""failed test: %s"" % description)  [20160310t17:12:10.207919z] assertionerror: failed test: 95% of sources used to build the psf are classified as stars on the coadd (0 > 0)  [20160310t17:12:10.209935z] scons: building terminated because of errors.      this is the test that fails    https:/github.com/lsst/cihsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#l169    class measurevalidation(validation):      datasets = [""measurecoaddsourcesconfig"", ""measurecoaddsourcesmetadata"", ""deepcoaddmeasschema""]      sourcedataset = ""deepcoaddmeas""      matchdataset = ""deepcoaddsrcmatch""        def validatesources(self, dataid):          catalog = validation.validatesources(self, dataid)          self.asserttrue(""calibpsfcandidate field exists in deepcoaddmeas catalog"",                          ""calibpsfcandidate"" in catalog.schema)          self.asserttrue(""calibpsfused field exists in deepcoaddmeas catalog"",                          ""calibpsfused"" in catalog.schema)          self.checkaperturecorrections(catalog)          # check that at least 95% of the stars we used to model the psf end up classified as stars          # on the coadd.  we certainly need much more purity than that to build good psf models, but          # this should verify that flag propagation, aperture correction, and extendendess are all          # running and configured reasonably (but it may not be sensitive enough to detect subtle          # bugs).          psfstars = catalog.get(""calibpsfused"")          extstars = catalog.get(""baseclassificationextendednessvalue"") < 0.5          self.assertgreater(              ""95% of sources used to build the psf are classified as stars on the coadd"",              numpy.logicaland(extstars, psfstars).sum(),              0.95psfstars.sum()          )      note that the assertion failure messages is a bit confusing.  it should say  ""fewer than 95% of the sources used to build the psf are classified as stars on the coadd.""",1,val
DM-5420,Integration and test monitoring implementation Part I,"configure, develop and deploy an elk system.    high level requirements:   es.lsst.codes  an current version elasticsearch cluster.   collect.lsst.codes  a server with multiple services to collect and aggregate logging and messages.   logging.lsst.codes  a server with kibana hooked up between.   packer and ansible deploys to create artifacts and deploys on nebula and aws. optionally docker.    use elk to monitor gitlfs and our ci system.        ",52,val
DM-5422,Understand and test real space extension for ZOGY,the zogy algorithm (http:/arxiv.org/abs/1601.02655) can be implemented as a real space extension of a&l.,43,val
DM-5423,Documentation,we are reserving time this cycle for people to contribute to architecture and documentation efforts.,38,val
DM-5424,Switch PropagateVisitFlags to use src instead of icSrc,"on dm5084  switched propagatevisitflags to match against icsrc instead of src because we weren't yet matching `icsrc` to `src` in processccdtask.  that's now been done on dm4692, so we can revert this.    after doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives.",2,val
DM-5425,Provide an easy way to set Coord fields of a source catalog,"we sometimes need to set the coord fields of a source catalog, e.g. when fitting a new wcs or when studying an `icsrc` catalog (whose coord field is not set). it would be nice to have a central, easily found way to do this. right now we have the following as a static method of `tansipwcstask`, which works fine but is in a poor location:          def updatesourcecoords(wcs, sourcelist):          """"""update coords in a collection of sources, given a wcs          """"""          if len(sourcelist) < 1:              return          schema = sourcelist[1].schema          srccoordkey = afwtable.coordkey(schema[""coord""])          for src in sourcelist:              src.set(srccoordkey, wcs.pixeltosky(src.getcentroid()))      the other direction is also useful for reference catalogs, though from a practical standpoint the only user is probably `meas_astrom`. even so, i suggest that this be made publicly available in the same way. again, this is presently a static method of `fittansipwcstask`:          def updaterefcentroids(wcs, reflist):          """"""update centroids in a collection of reference objects, given a wcs          """"""          if len(reflist) < 1:              return          schema = reflist[0].schema          coordkey = afwtable.coordkey(schema[""coord""])          centroidkey = afwtable.point2dkey(schema[""centroid""])          for refobj in reflist:              refobj.set(centroidkey, wcs.skytopixel(refobj.get(coordkey)))      i hope this can remain python code, but admit that the extra speed of c might come in handy in some cases. in any case, once the function is in a central location we can implement it in c if we find the need.",2,val
DM-5426,Participate in X16 DMLT Working Groups,"bosch, lupton & swinbank are members of https:/confluence.lsstcorp.org/display/dm/dmltworkinggroups during x16. this epic captures work related to the activities of those groups.",45,val
DM-5427,SingleFrameVariancePlugin can give numpy warnings,"singleframevarianceplugin can produce the following numpy warning, with no hint as to where the problem is coming from:    /users/rowen/uw/lsst/lsstsw/miniconda/lib/python2.7/site packages/numpy/core/methods.py:59: runtimewarning: mean of empty slice.    warnings.warn(""mean of empty slice."", runtimewarning)    i tracked it down by adding the following code to the calling code:    import warnings  with warnings.catchwarnings():      warnings.filterwarnings('error')      it would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it.    one way to reproduce this problem is to run tests/testprocessccd.py in pipe_tasks. however, it is commonly seen when running processccd on other data, as well.",2,val
DM-5428,ObjectSizeStarSelector can produce numpy warnings,"`objectsizestarselector` can produce the following numpy warning:     runtimewarning: invalid value encountered in less    this occurs at the following point in the code:            for i in range(ncluster):              # only compute func if some points are available; otherwise, default to nan.              pointsincluster = (clusterid == i)              if numpy.any(pointsincluster):                  centers[i] = func(yvec[pointsincluster])    where `func` has been assigned to `numpy.mean`. when i have seen this occur i have found that `dist` is an array of `nan`    i suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. if logging a message would be helpful, then please do that (and if rfc 154 is adopted, a log will be available).    one way to reproduce this is to run `tests/testprocessccd.py` in `pipe_tasks`. however, i often see it when running `processccd.py` on other data, as well.",2,val
DM-5430,Tune and improve ngmix MCMC sampling,improve the ngmix mcmc sampling plugin to get it working well on most sources.  this may require actually contacting erin sheldon and getting his help (but he's quite eager to help).,10,val
DM-5431,Changes to galaxy_shear_experiments Python code,"this ticket describes changes which were made to the test runner and analysis scripts during the dec 2015  feb 2016 period.  most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    the large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    since these changes do not pertain to any single test (though many were done during dm1136), i have put them on a separate ticket.",5,val
DM-5432,Add SFM plugin for ngmix fitting,add an sfm pluggin for ngmix fitting using one of the simple fitters in ngmix/fitting.py.    this should depend on dm 5429 (or a suitably configured modelfit_shapeletpsfapprox) for approximating the psf as a sum of gaussians.    testing and tuning this algorithm to get it working well should be deferred to another issue.,10,val
DM-5434,Ensure that new object loads are added to secondary index,"l3 users might be generating new objectids that are not in the dr object table. if that is the case, adhoc l3 analysis would be triggering updates to secondary index.  it is possible that l3 users will be bringing data from other surveys and might partition it drway and cross match.  we need some mechanism to mark the secondary index dirty when something new gets added to the object table, and trigger refresh.",10,val
DM-5435,Provide a shared stack on lsst-dev & other relevant systems,"following the discussion in rfc156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include lsstdev.",3,val
DM-5436,Create unit test for ip_isr fallbackfilter,dm 5287 introduced a configuration option that allows specifying a fallback filter in the event that getting a specific butler product fails. currently there is no test for this functionality. one should be created which tests all the logical paths. this may involve just adapting or mimicking another test that already exists.,2,val
DM-5437,Move tests/negative.py from meas_algorithms to meas_base,"porting code from hsc to lsst brought over a unit test into measalgorithms for functionality that exists in measbase in lsst. this is due to the refactoring of code into measbase on the lsst some while ago. this unit test currently runs with code from measalgorithms, which means it can not simply be moved, as measbase comes before measalgorithms in the build order. this work may involve rewriting the unit test to use different code, or evaluating if it is worth bringing that functionality to meas_base along with the test. the code in question is the detection task.",2,val
DM-5438,Data Backbone ConOps,data backbone first edit : 1pt (week 1)  data backbone second edit : 1 pt (week 2)  third edition : 1 pt week 3&4,3,val
DM-5441,Astropy integration with LSST DM Software,work covering the investigation of how to integrate astropy into the lsst dm software stack.,30,val
DM-5443,Compare Astropy and LSST functionality,this story will examine the overlap between astropy and afw and examine different approaches that could be taken to integrate astropy into the dm software.,10,val
DM-5444,Write Report on Astropy integration proposals,a report is to be written on the astropy integration plan. this report will be in the form of an spie paper.,20,val
DM-5445,"Convert GWT code to pure JavaScript (X16, part3 visualization)",this epic is for the remaining effort in extra 2016 cycle related to firefly visualization coed conversion from gwt to pure javascript. ,100,val
DM-5446,Add scipy as a stack dependency,adding scipy as a stack dependency is still a nebulous term to me.  david is going to follow up on how to do this exactly (it's already in conda_packages.txt).,2,val
DM-5447,Write technical note describing galaxy shear fitting experiments,"through s15 (dm1108) and w16 (dm3561), http:/sqr000.lsst.io/en/master/.",8,val
DM-5448,Familiarization with ngmix codebase,download the ngmix codebase from https:/github.com/esheldon/ngmix. install it and its dependencies in the same environment as the lsst stack. experiment with using it and understanding how it works,3,val
DM-5449,Convert GWT code to pure JavaScript (F16),the remaining work for converting gwt code to pure javascript,100,val
DM-5450,Visualization algorithm related research (S17),we have some algorithm related issues that need some research time. ,40,val
DM-5451,inter team discussion (X16),this epic is reserved for inter team discussion and supply/collect input to/from other teams.,6,val
DM-5452,create support in Butler for multiple repositories,"we need to be able to find repositories based on criteria such as version, validity date, etc.  this story is to provide support & proof of concept that demonstrates this.",12,val
DM-5455,Implement experimental DCR correction,"after the discussion about dcr, a few avenues for dealing with dcr were enumerated.  it was found that using imaging to model the dcr could be a very fruitful approach.  nate lust has suggested an algorithm that performs well in simplified, onedimensional systems.    this epic is to extend this algorithm to 2dimensions and add realistic seds, bandpasses, etc.  the result will be an implementation of the algorithm applied to the simulated data.  with measurements of how well it corrects for dcr in the context of image differencing.",54,val
DM-5457,Adapt LTD Mason for Single-package doc builds on Travis CI,"ltd mason was originally intended to build docs for dm’s eupsbased packages from our jenkins ci/cd servers. there is tremendous value in consolidating all of dm’s sphinxbased documentation deployments to use lsst the docs rather than read the docs. this ticket will design and implement adaptations to ltd mason to build single repo doc projects (technotes, design documents, the developer guide, and even generic software projects) from a travis ci environment. also includes a template .travis.yml and associated documentation to allow others to enable travis builds for their documentation.    we name travis specifically because it is the easiest platform for implementing ci for generic open source projects.",4,val
DM-5458,Update SQR-006 LSST the Docs technote to reflect deployment in DM-5404,this ticket will ensure that http:/sqr006.lsst.io reflects the lsst the docs continuous delivery platform as it is deployed in the dm5404 epic. (sqr006 was initially written as a planning/design document).    this story should be closed only once the dm 5404 epic is ready to be closed.,4,val
DM-5462,Add non-linearity correction to ISR task,"implement rfc 164    at the moment some preliminary code is on ticket branches, but this need to be redone once the rfc is finished.",6,val
DM-5463,Don't restore the mask in CharacterizeImageTask.characterize,characterizeimagetask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute psf. this is unnecessary because repair and detection both clear the relevant mask planes before setting new values.,1,val
DM-5465,Finish getting obs_decam ISR working with CBP data,"success criteria:     flats should be totally flat, i.e. bias jump problem fixed everywhere, amplifier levels fixed (both of these are currently hit & miss at the moment).   crs should be properly interpolated over for nonsky images (as this means no psf estimate).     use unbinned images to confirm that bad pixel masks are correct everywhere.  ",10,val
DM-5468,S17 Qserv Refactoring,"refactoring of qserv as found necessary in f16. specific tasks will be added during f16, and will include bug fixes, fixing major deficiencies discovered during f16, and keeping qserv code uptodate (latest compilers, supported oses, security and alike). the scope of the work is limited by the number of story points assigned to this epic. ",23,val
DM-5470,Develop C++ code for experimenting with Python binding,produce a small c codebase that can be used for experimenting with the various technologies we can be used for exposing c to python. it should enable us to experiment with as many of the potential pain points with these technologies as possible,3,val
DM-5471,Wrap example C++ code with Cython,"take the example c codebase developed in dm5470, and expose it to python in the most idiomatic possible way using cython. produce a http:/sqr000.lsst.io describing how this was carried out and discussing any particular pain points either in implementation or results.",10,val
DM-5472,Update meas_mosaic for compatibility with new single frame processing,"following https:/community.lsst.org/t/backwardincompatiblechangestoprocessccdtaskandsubtasks/581, icsrc no longer includes celestial coordinates and icmatch is no longer being written. meas_mosaic requires this information. provide a work around.",3,val
DM-5473,Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema,"since 15 march, the cihsc build in jenkins has been failing as follows:      [20160316t14:23:13.548928z] traceback (most recent call last):  [20160316t14:23:13.548956z]   file ""/home/build0/lsstsw/stack/linux64/pipetasks/201601.023gcf99090/bin/measurecoaddsources.py"", line 3, in /  [20160316t14:23:13.548969z]     measuremergedcoaddsourcestask.parseandrun()  [20160316t14:23:13.548999z]   file ""/home/build0/lsstsw/stack/linux64/pipebase/201601.06g7751869/python/lsst/pipe/base/cmdlinetask.py"", line 450, in parseandrun  [20160316t14:23:13.549011z]     resultlist = taskrunner.run(parsedcmd)  [20160316t14:23:13.549040z]   file ""/home/build0/lsstsw/stack/linux64/pipebase/201601.06g7751869/python/lsst/pipe/base/cmdlinetask.py"", line 192, in run  [20160316t14:23:13.549048z]     if self.precall(parsedcmd):  [20160316t14:23:13.549076z]   file ""/home/build0/lsstsw/stack/linux64/pipebase/201601.06g7751869/python/lsst/pipe/base/cmdlinetask.py"", line 279, in precall  [20160316t14:23:13.549087z]     task = self.maketask(parsedcmd=parsedcmd)  [20160316t14:23:13.549115z]   file ""/home/build0/lsstsw/stack/linux64/pipebase/201601.06g7751869/python/lsst/pipe/base/cmdlinetask.py"", line 369, in maketask  [20160316t14:23:13.549132z]     return self.taskclass(config=self.config, log=self.log, butler=butler)  [20160316t14:23:13.549160z]   file ""/home/build0/lsstsw/stack/linux64/pipetasks/201601.023gcf99090/python/lsst/pipe/tasks/multiband.py"", line 1008, in init  [20160316t14:23:13.549179z]     self.makesubtask(""measurement"", schema=self.schema, algmetadata=self.algmetadata)  [20160316t14:23:13.549206z]   file ""/home/build0/lsstsw/stack/linux64/pipebase/201601.06g7751869/python/lsst/pipe/base/task.py"", line 226, in makesubtask  [20160316t14:23:13.549846z]     subtask = configurablefield.apply(name=name, parenttask=self, keyargs)  [20160316t14:23:13.549901z]   file ""/home/build0/lsstsw/stack/linux64/pexconfig/201601.01/python/lsst/pex/config/configurablefield.py"", line 77, in apply  [20160316t14:23:13.549915z]     return self.target(args, config=self.value, kw)  [20160316t14:23:13.549943z]   file ""/home/build0/lsstsw/stack/linux64/measbase/201601.012gf26bc281/python/lsst/meas/base/sfm.py"", line 248, in init  [20160316t14:23:13.549954z]     self.initializeplugins(schema=self.schema)  [20160316t14:23:13.549985z]   file ""/home/build0/lsstsw/stack/linux64/measbase/201601.012gf26bc281/python/lsst/meas/base/basemeasurement.py"", line 298, in initializeplugins  [20160316t14:23:13.550004z]     self.plugins[name] = pluginclass(config, name, metadata=self.algmetadata, kwds)  [20160316t14:23:13.550032z]   file ""/home/build0/lsstsw/stack/linux64/measbase/201601.012gf26bc281/python/lsst/meas/base/wrappers.py"", line 15, in init  [20160316t14:23:13.550616z]     self.cpp = self.factory(config, name, schema, metadata)  [20160316t14:23:13.550647z]   file ""/home/build0/lsstsw/stack/linux64/measbase/201601.012gf26bc281/python/lsst/meas/base/wrappers.py"", line 223, in factory  [20160316t14:23:13.550660z]     return algclass(config.makecontrol(), name, schema)  [20160316t14:23:13.550688z]   file ""/home/build0/lsstsw/stack/linux64/measbase/201601.012gf26bc281/python/lsst/meas/base/baselib.py"", line 3401, in init  [20160316t14:23:13.552891z]     this = baselib.newpixelflagsalgorithm(args)  [20160316t14:23:13.552924z] lsst.pex.exceptions.wrappers.invalidparametererror:   [20160316t14:23:13.552967z]   file ""src/table/schema.cc"", line 563, in lsst::afw::table::key/ lsst::afw::table::detail::schemaimpl::addfield(const lsst::afw::table::field/&, bool)  [20160316t14:23:13.552986z]     field with name 'basepixelflagsflagclipped' already present in schema.   [20160316t14:23:13.553012z] lsst::pex::exceptions::invalidparametererror: 'field with name 'basepixelflagsflag_clipped' already present in schema.'  [20160316t14:23:13.553014z]   [20160316t14:23:13.613484z] scons:   [.scons/measure] error 1  [20160316t14:23:13.617577z] scons: building terminated because of errors.      please fix it.",1,val
DM-5474,Bugs in obs_subaru found by PyFlakes,"i ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that i am fixing as part of dm 5462)    ingest.py has undefined name day0    ccdtesting.py has at least three undefined variables: x, y and vig in the following:        ngood += pupilimage[y[good], x[good]].sum()    vig[i] = float(ngood)      crosstalkyagi.py has many undefined names, starting with makelist, estimatecoeffs",1,val
DM-5475,"Document investigation of logging, monitoring and metrics technologies and architecture",finish technote sqr007. related to dm4970,4,val
DM-5476,Revise FlagHandler," flaghandler is ""unpolished ... and a bit dangerous to the unwary"" (dm5247).  it could be improved by leveraging c11 features, replacing the default constructor with something that defines the (required) general failure flag, and allowing flags to be added individually.    a potential starting point is https:/jira.lsstcorp.org/browse/dm5247?focusedcommentid=45894&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment45894.",4,val
DM-5477,Firefly support for Camera team visualization needs (X16),"attend the weekly meeting with the camera team and uiuc development team, provide support in discussion and api usage. ",4,val
DM-5478,Write script to derive and collate QA metrics from data repository of processed data,i wrote a python script using stack components to derive qa metrics and collate other qa relevant information for a data repository of processed data.  this is currently output to a csv file that can be loaded into a sql database.,20,val
DM-5479,Wrote script to print the names of all visits that overlap a patch,in order to finish the idl workflow module for makecoaddtempexp i needed a program to say which visits overlap a given path.  that's what this script does.,5,val
DM-5480,Processing of COSMOS data - Part II,"continued work on processing and qa work on the cosmos verification dataset.  running processccdecam, making diagnostic plots, and nvestigating the results.  most recently i've  reprocessed the cosmos data through processccddecam using sdss as the astrometric and photometric reference catalog and am redoing the qa work on those results.",20,val
DM-5481,Write software to match up and combine data for sources in a processed data repository,"in order to fully check the outputs of the processed cosmos data i needed to combine the information on sources from multiple visits.  this code (written in idl for now) matching up sources astrometrically across different visits, combines all of the information on separate detections, and measures average quantities (phot and astrom) for unique sources.  the information is then output into four binary fits files.",10,val
DM-5482,Write presentation on verification datasets for AAS,prepared and gave a talk at the nsf booth at the florida aas meeting on the progress of the verification datasets effort.,5,val
DM-5483,Work on script to test the astrometric matcher,"we encouraged astrometric matching problems for the bulge verification dataset.  therefore, i wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works.  it worked well until ~80 arcsec.",5,val
DM-5484,SdssMapper.paf has wrong python type for processCcd_config, reports that sdssmapper.paf has the wrong python data type for the dataset processccd_config: it is lsst.obs.sdss.processccdsdss.processccdsdssconfig instead of lsst.pipe.tasks.processccd.processccdconfig,1,val
DM-5485,Work on plan to test specific algorithmic components of the stack,"after working on a script to test the astrometric matcher, i decided to put together a plan to run similar tests on our algorithmic code.  the rough plan is here:  https:/confluence.lsstcorp.org/display/sqre/stacktestingplan",2,val
DM-5486,"Work on putting together page of ""tips and tricks for using the stack""","due to the incomplete state of the stack documentation and tutorials, i decided to write down various ""tips and tricks"" for using the stack as i learn them.  https:/confluence.lsstcorp.org/display/sqre/tipsandtricksforusingthestack",2,val
DM-5487,Revise operations concept for Observation Processing System,"turn the l1 conops document into appropriate sections of ldm 230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (story points are for ktl drafting and initial contributions)",2,val
DM-5488,Field group updates,"after some work we have realized that the following needs to be done to field groups:     tabs group should have a field group smart wrapper component   field group needs to reinit on id change    remove mixin, use higherorder components instead   support a function for a value, this function will return a value or a promise   hidden fields  init field group with key/value object   subfield groups? study only, unless it is easy to implement.   maintain an option to keep unmount field value available   determine if initvalue needs to be passed around   passing fieldstate around too much   find reason for react warning every time popup is raised   look at promise code make sure it is working the way we think   if practical, remove all export default    fieldgroupconnector.  it is the high order component that replaces the mixin.   fieldgrouputils.js:  (~line 33): the field value would be a function on the file upload case. therefore the upload does not activate until validation. in the upload case the function would return a promise. however, it could return a value or an object with a value and a valid status. now the value key of a field can contain a promise or function or primitive. the function can return a primitive, a promise, or an object with primitive and status.    fftools.js lines 102158 you can see my experimenting with taking out the connector. it works fine and does eliminate one of the warning messages.    ",8,val
DM-5489,improvement of the north/east arrow on image,make the compass sticky when scroll the image,1,val
DM-5490,Develop operations concept for Batch Processing System,"develop a conops document that can be included as appropriate sections of ldm 230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (story points are for ktl drafting and initial contributions)",3,val
DM-5491,Develop operations concept for Data Backbone,"develop a conops document that can be included as appropriate sections of ldm 230 describing the data backbone that contains, manages, and provides access to the science data archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (story points are for ktl drafting and initial contributions)",3,val
DM-5492,Develop operations concept for Data Access Processing System,"develop a conops document that can be included as appropriate sections of ldm 230 describing the data access processing system that manages l3 computing in and interfaces to the data access center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (story points are for ktl drafting and initial contributions)",3,val
DM-5493,Develop functional breakdown for Observation Processing System,"write sections that can be incorporated into ldm 148 describing the functional breakdown of the observation processing system, including, for each major element:   overall function   inputs, outputs, and control interfaces   components used   descriptions of functions to be performed    (story points are for ktl drafting and initial contributions)",3,val
DM-5494,Develop functional breakdown for Batch Processing System,"write sections that can be incorporated into ldm 148 describing the functional breakdown of the batch processing system, including, for each major element:   overall function   inputs, outputs, and control interfaces   components used   descriptions of functions to be performed    (story points are for ktl drafting and initial contributions)",2,val
DM-5495,Develop functional breakdown for Data Backbone,"write sections that can be incorporated into ldm 148 describing the functional breakdown of the data backbone, including, for each major element:   overall function   inputs, outputs, and control interfaces   components used   descriptions of functions to be performed    (story points are for ktl drafting and initial contributions)",2,val
DM-5496,Develop functional breakdown for Data Access Center Processing System,"write sections that can be incorporated into ldm 148 describing the functional breakdown of the data access center processing system, including, for each major element:   overall function   inputs, outputs, and control interfaces   components used   descriptions of functions to be performed    (story points are for ktl drafting and initial contributions)",2,val
DM-5497,Develop DPS-WG documents,create documents needed to accomplish the goals of the dps wg.,24,val
DM-5498,Coordinate completion of operations concepts,coordinate the creation of a new version of ldm230 incorporating dpswg generated operations concepts.,2,val
DM-5499,Coordinate completion of functional breakdowns,coordinate the creation of a new version of ldm148 incorporating dpswg generated functional breakdowns.,2,val
DM-5501,Solve the metadata sanitization problem,"applications need access to visit specific metadata: e.g. pointing, airmass, exposure length.  this information is typically carried around in a fits header, but there are no conventions on spelling or even necessarily units of these metadata key, value pairs.  there needs to be a easy to use metadata sanitization process that allows data from many different systems to present a standardized interface to observation metadata to the algorithm code.",100,val
DM-5502,Collect usage of header metadata,"collect a comprehensive set of exposure oriented metadata used by science code.  this should also include metadata that is not currently needed but that could be utilized in the future.  in practice, i suspect this will involve looking for all calls to propertyset.get since that is how fits header metadata is currently passed around.",5,val
DM-5503,Implement single interface to sanitized exposure metadata,"currently metadata associated with exposures is accessed in a few different ways: through the calib object, through the wcs object, and through the metadata object.    in conjunction with dm 5502, which is to figure out what metadata is needed, this will provide a single interface to exposure oriented metadata.  one tricky thing is that the calib object has some subset of the metadata we'll need in a sanitized form, but we won't want to have to remember where to look for metadata.  if we can't extend the calib object to hold all sanitized metadata, we should create a new metadata object to store all sanitized metadata and remove the pieces from calib that are currently there.",10,val
DM-5505,Verification via precursor datasets,this epic covers covers timeboxed investigative activities into processing of precursor datasets with the lsst stack. ,26,val
DM-5506,DAX & DB Docs (ABH), add memman documentation (in ldm135)   refresh xrdssi documentation (in ldm135),6,val
DM-5507,"Design Discussions (AndyS, March)",nan,3,val
DM-5508,QA Tasks & Supertasks,"this epic covers stackside work for the squash mvp (dm5555)     (js:8, mwv:14)",22,val
DM-5509,alert production database next steps (April),place holder for additional alert production database work after investigate design task completes.  we should split this into smaller stories for a total of 18 points this cycle.,7,val
DM-5510,QA Tasks & Supertasks II,at this time this is epic is a bucket to keep track of backlog for validate_drp etc. ,22,val
DM-5511,"Design discussions (Brian, March)",nan,1,val
DM-5512,"Design Discussions (John, March)",nan,3,val
DM-5514,Validate shared scan implementation on IN2P3 cluster,nan,9,val
DM-5515,prepare Slack RFC,    https:/jira.lsstcorp.org/browse/rfc 140,1,val
DM-5516,"Design Discussions (Fritz, March)",nan,3,val
DM-5517,"Design Discussions (Nate, March)",nan,3,val
DM-5518,Create proposal & RFC for Butler API to define output dataset type,"this story represents a spike to  1. adhoc gathering of requirements to create butler api that allows a task to define an output dataset type 2. do any proofofconcept mock up needed 3. write an rfc & gather input  then  a. if there is major dissent, create another design spike story or  b. close the rfc, and green light work on dm4180",11,val
DM-5519,SQuaRE documentation & design documents,"this epic involves planning, design and usage documentation on square products and services.      (jmp:4,fe:15,js:6)",25,val
DM-5520,SQuaSH design proposal document,  document capturing situation as of beginning of x16 can be found at:    https:/dmtn016.lsst.io    further extension is planned in f16 to cover x16 development as well as consequences of the ldm151 rewrite.     ,10,val
DM-5521,Python wrappers for sphgeom,this issue is a prereq of dm3472,15,val
DM-5523,Weekly and monthly releases,  some manual process at the rate of 1 sp / month is still involved in the releases until the automating publishing process is complete. ,6,val
DM-5526,Add documentation to BinnedWcs,"dm5282 ported functionality from hsc to work in ""superpixels"" which are the result of binning in wcs. this functionality was introduced in binnedwcs.(cc/h). this functionality needs proper doxygen documentation added.",1,val
DM-5530,Documentation of Firefly functions and API (F16),we are concentrating on the coding in x16. this epic will be capture the effort to write the document for using firefly functions and api. ,40,val
DM-5532,Change star selectors to return stars instead of PSF candidates,"implement rfc154:   make star selectors tasks, but continue to use and prefer a registry   add an abstract base class for star selectors with the following methods:     selectstars abstract method that takes a catalog of sources and returns a lsst.pipe.base.struct containing a catalog of stars     run concrete method that takes a catalog of sources and an optional name of a flag field, calls selectstars to select stars, then sets the flag field (if given) for stars     makepsfcandidates make a list of psf candidates from a catalog of stars (does no selection, other than skipping stars that cannot be made into candidates, and logging the rejects)  ",4,val
DM-5533,Add HTM indexing to sphgeom,"to include python wrappers, in support of dm 5052",10,val
DM-5534,"Design Discussions (Fritz, April)",nan,6,val
DM-5535,"Design Discussions (Fritz, May)",nan,6,val
DM-5536,"DAX & DB Docs (Fritz, April)",nan,8,val
DM-5537,"DAX & DB Docs (Fritz, May)",nan,8,val
DM-5538,Finish data distribution prototype (April),nan,6,val
DM-5539,Finish data distribution prototype (May),nan,6,val
DM-5540,"Design Discussions (John, April)",nan,6,val
DM-5541,"Design Discussions (John, May)",nan,6,val
DM-5542,AFW rgb.py has undefined variable that breaks a test in some situations,"the rgb.py test is failing for me with current afw master:    tests/rgb.py  .e............  ======================================================================  error: testmakergbresize (main.rgbtestcase)  test the function that does it all, including rescaling    traceback (most recent call last):    file ""tests/rgb.py"", line 313, in testmakergbresize      with tempfile(filename, remove=true):  nameerror: global name 'tempfile' is not defined      ran 16 tests in 7.296s    failed (errors=1)      tempfile is definitely only used in line 313. it was introduced with commit c9864f49.    i'm not entirely sure how this is not picked up by jenkins as the test will run if matplotlib and scipy are installed and jenkins does have those.    ",1,val
DM-5543,"Design Discussions (AndyS, April)",nan,6,val
DM-5544,"Design Discussions (AndyS, May)",nan,6,val
DM-5545,Alert production database next steps (May),nan,7,val
DM-5546,"Design Discussions (Brian, April)",nan,2,val
DM-5547,"Design Discussions (Brian, May)",nan,2,val
DM-5548,persistence improvements to butler config system,"requirements:   easy to know what to provide   fails fast   has a way to be backward compatible with persisted configs & existing scripts    existing issues to fix:   currently config creation is verbose, difficult to read, and difficult to format properly    has the butler class hierarchy baked into the format",10,val
DM-5550,"Design Discussions (Nate, April)",nan,6,val
DM-5551,"Design Discussions (Nate, May)",nan,6,val
DM-5552,Add renderer option to js table,"tablepanel and basictable now accept optional renderers.  for each column, you can set a custom renderer for the header, cell, or both.  also, created several commonly used renderer for images, links, and input field.",2,val
DM-5553,Z-scale stretch for image display,the z scale stretch in current system is different from the one in ops,8,val
DM-5554,"Assist in document investigation of logging, monitoring and metrics technologies and architecture","assist with tech note sqr 007 and document investigation of logging, monitoring and metrics technologies and architecture.",2,val
DM-5555,SQuaSH MVP,"this is an epic that covers setting up a minimally viable qa environment for executing processing, calculating metrics, storing them and displaying them. this serves a dual purpose:     it results in a limited but still useful production service that developers can take advantage of   it allows us to assess our initial technology stack for suitability for further development.     the test case was picked to be a supertask version of the tests described in dm 4730 (informally cilauren) used during the merging of the hsc fork. this was meant to also allow us to use and give feedback on the supertask architecture. when it became obvious that we would not take delivery of that infrastructure in time for x16 work, we switched the test case to one of the kpms calculated in validatedrp. this switch does not affect the engineering aims of this prototype. the mvp based on validatedrp can eventually be extended to service other types of kpm measurement for regression testing and release characterisation.        (jh:32, jmp:16, js:32, mvw:14, af:20)      outcome: mvp stood up on squash.lsst.codes. currently collecting am1, am2 and pa1 kpms using validatedrp on validationdatacfht. after a short period of evaluation of the performance of the toolchains in production we will proceed with more data, more metrics and more features for f16.     ",100,val
DM-5556,SQuaRE ad-hoc developer requests,"this is a bucket epic for requests from developers/ science users that come up mid cycle and cannot wait until the new cycle, security vulnerabilities, critical bug fixes, etc.     (jh:16,jmp:8,fe:3)",27,val
DM-5559,Present Supertask design to DMLT,present the supertask design to the november 2015 dmlt in person meeting.    covers preparation of a presentation and related discussions preceding and immediately following the meeting.,6,val
DM-5560,Participate in October 2015 OCS-subsystems teleconference,"prepare for, attend, and follow up on the ocs subsystems teleconference on october 8, 2015.",2,val
DM-5561,Write tech note on modifications required to use py.test framework,"following the investigatory work into switching our python test files to be compliant with pytest, whilst still using unittest, a tech note needs to be written explaining the required changes.",10,val
DM-5562,"Participate in November 2015 OCS-subsystems teleconference (LSE-70, LSE-209)","prepare for, attend, and follow up on the ocssubsystems teleconference on november 11, 2015.  this story covers work related to lse70 and lse209; lse74 work was also done under a separate epic.",4,val
DM-5563,Participate in November 2015 OCS-subsystems teleconference (LSE-74),"prepare for, attend, and follow up on the ocssubsystems teleconference on november 11, 2015.  this story covers work related to lse74; lse70 and lse209 work was also done under a separate epic.",1,val
DM-5564,"Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209)","prepare for, attend, and follow up on the ocssubsystems teleconference on december 9, 2015. this story covers work related to lse70 and lse209; lse74 work was also done under a separate epic.",2,val
DM-5565,Participate in December 2015 OCS-subsystems teleconference (LSE-74),"prepare for, attend, and follow up on the ocssubsystems teleconference on december 9, 2015. this story covers work related to lse74; lse70 and lse209 work was also done under a separate epic.",1,val
DM-5566,"Review of LSE-70 and LSE-209 drafts, September 2015","arrange, prepare for, and attend a joint call with the camera team to review the endofsummer2015 drafts of lse70 and lse 209 from the ocs group.",3,val
DM-5567,CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209),review the lse70 and lse209 drafts submitted with change requests lcr567 and lcr568 in january 2016.,2,val
DM-5568,CCB review of LCR-603 (LSE-74),"review lcr603, ""lse74 document revision""",2,val
DM-5569,"LSE-70, LSE-209 refinements X16",there are open lcrs for cleanups to the versions of lse70 and lse209 approved by the ccb in february 2016.  an initial teleconference will be held on 30 march 2016 with the ocs group to discuss these.,4,val
DM-5578,making PSF candidates should be simpler,"the code to make psf candidates is too complicated and repeated in too many places (even after dm 5532). every time lsst.meas.algorithms.makepsfcandidate is called (except in a few tests) it is called as follows:                cand = measalg.makepsfcandidate(source, mi)              if cand.getwidth() == 0:                  cand.setborderwidth(borderwidth)                  cand.setwidth(kernelsize  2borderwidth)                  cand.setheight(kernelsize  2borderwidth)                im = cand.getmaskedimage().getimage()              max = afwmath.makestatistics(im, afwmath.max).getvalue()              if not numpy.isfinite(max):                  continue      this should to be centralized somewhere. i suggest adding this code to meas.algorithms.makepsfcandidate itself (which could delegate some work to a private function, if desired).",2,val
DM-5579,Add Error and Working feedback to FITS visualizer," add working message when plot is loading, (downloading..., plotting..., etc)   add error message when plot fails   for multi viewer remove and failed plot cells   work out if image select panel should become visible again.    any thing else the is plotting feedback related",6,val
DM-5580,Docgen draft from EA content for LSE-140,create a docgen from the lse 140 content in enterprise architect.,2,val
DM-5581,SQuaRE Communication and Publication Platforms Document and Presentation,http:/sqr011.lsst.io documents the various communication and publishing platforms that square operates on behalf of dm. this ticket will complete v1 of the document (dm4721 created a timeboxed first draft) and also include work to present the document to lsst management.,4,val
DM-5582,Support LCR-385,support getting lcr385 against lse78 through the ccb.,3,val
DM-5584,Create a reusable upload file component,this  component will upload and validate the file as part of the input's validation process.  it will return a token generated by the server which will resolve to the uploaded file if the upload success.   ,4,val
DM-5585,SQuaRE Communication and Publication Platforms Document and Presentation - Clone,this is a clone of dm 5581 tracking 's sps,5,val
DM-5586,Fix obs_decam butler level,"there is a bug in obsdecam/policy/decammapper.paf, causing some butler features for the ""visit"" level or above working incorrectly. the hdu key is irrelevant for the visit level or above, but wasn't included in the policy file.    because of this bug, the demotask in ctrlpool (ctrlpooldemo.py) runs incorrectly with decam data. it incorrectly treats dataref with different hdus as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). instead, each ccd image should be read once.        besides fixing the policy file, i also added an optional test that only runs if testdatadecam is set up. the part with level=""visit"" in the test fails without the ticket changes in the policy.    (p.s. the raw data file in testdatadecam is modified and has only 2 hdus.) ",3,val
DM-5588,Add lmfit package to the stack,"the current implementation of the new dipolefittask for ip_diffim uses lmfit to perform parameter estimation (leastsquares minimization). lmfit is essentially an api on top of scipy's optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. it would be nice to include this small, purepython package in the stack rather than investigating and re implementing the optimization using scipy or minuit2 (which are the two optimizers that i know of that are in the stack already).",4,val
DM-5590,Fix afw build issues with recent clang,"afw fails to build with recent versions of clang:      include/lsst/afw/image/maskedimage.h:553:65: error: 'loc' is a protected member of 'lsst::afw::image::maskedimage/::maskedimagelocatorbase/, boost::mpl::rangec/ > > > >,        boost::gil::memorybased2dlocator/, boost::mpl::rangec/ > > > >,        boost::gil::memorybased2dlocator/, boost::mpl::rangec/ > >  > >, reference>'                                       constvariancelocator(iter.loc.template get/())    and issues with statistics.i so far, more errors may turn up as these are cleared.    these problems are apparent with apple llvm version 7.3.0 (clang 703.0.29) (as shipped with the latest release of xcode, hence this now becoming an issue) and clang version 3.8.0 (branches/release38 262722) (a recent release from llvm; note that apple uses its own versioning scheme). clang version 3.7.1 (tags/release371/final) is not affected.",1,val
DM-5591,Archive in a box v1 (F16),"several times we were asked a question about firefly: great software. how could i use it for my data now?     we want to create a recipe and stubs of code (archive in a box) so others can take it, with minimal configuration changes and minimal customized data access code, to have a simple archive ui for their data. it will come with all the built in images and catalogs access, all the image, catalog, and xy plot functions. ",40,val
DM-5593,fix issue where butler repository search returns list for single item,"backwards compatible behavior is that when butler returns a single item, it is not in a list. a recent change (when the repository class was added) broke this behavior.     change it back so that if an operation in repository would return a list with a  single item, it pulls it from the list.    note this is only related to the case where a repository's parentjoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot.     ",1,val
DM-5594,Fix qserv service timeout issue,"after qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. investigate and implement a solution.",5,val
DM-5595,daf_persistence build failure on OSX,"i see the following build failure in dafpersistence on osx 10.11:    c o python/lsst/daf/persistence/persistencelib.so bundle f/ undefined suppress flatnamespace headerpadmaxinstallnames python/lsst/daf/persistence/persistencelibwrap.os llib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/mariadbclient/10.1.112gd04d8b7/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/pexpolicy/201601.04/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/pexlogging/201601.04/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/dafbase/201601.04/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/utils/201601.04/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/pexexceptions/201601.03/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/base/201601.03/lib l/private/tmp/ssd/swinbank/sharedstack/darwinx86/boost/1.59.lsst5/lib l/tmp/ssd/swinbank/sharedstack/darwinx86/miniconda2/3.19.0.lsst4/lib/python2.7/config ldafpersistence lboostserialization lmysqlclientr lpexpolicy lpexlogging lboostfilesystem lboostsystem ldafbase lutils lpexexceptions lbase lboostregex lpthread ldl lpython2.7  ld: file not found: libz.1.dylib for architecture x8664  clang: error: linker command failed with exit code 1 (use v to see invocation)  scons:   [python/lsst/daf/persistence/persistencelib.so] error 1  scons: building terminated because of errors.      this happens with the current master (3484020 at time of writing), but also with a recent weekly (3878625). ",1,val
DM-5604,Remove obsolete install scripts from ~/src/qserv/admin/tools/,internet free install scripts are unused and should be removed with related documentation.,1,val
DM-5605,runQueries.py fails on IN2P3 cluster,"launching runqueries.py produces some errors:    fjammes@ccosvms0070:~/src/qserv/admin/tools/docker/deployment/in2p3 (tickets/dm5402 =)$ ./runtestqueries.sh     decl                29.308806347275485   +    real    1m20.725s  user    0m0.004s  sys     0m0.012s  output directory: /afs/in2p3.fr/home/f/fjammes/runqueriesout  exception in thread thread12:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (2013, ""lost connection to mysql server at 'reading authorization packet', system error: 0"")    exception in thread thread23:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (1105, '(proxy) all backends are down')    exception in thread thread16:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (2013, ""lost connection to mysql server at 'reading authorization packet', system error: 0"")    exception in thread thread21:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (1105, '(proxy) all backends are down')    exception in thread thread17:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (2013, ""lost connection to mysql server at 'reading authorization packet', system error: 0"")    exception in thread thread19:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (2013, ""lost connection to mysql server at 'reading authorization packet', system error: 0"")    exception in thread thread18:  traceback (most recent call last):    file ""/usr/lib64/python2.7/threading.py"", line 811, in bootstrapinner      self.run()    file ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.target(self.args, self.kwargs)    file ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runqueries.py"", line 185, in runqueries      db='lsst')    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linuxx8664.egg/mysqldb/init.py"", line 81, in connect      return connection(args, kwargs)    file ""/qserv/stack/linux64/mysqlpython/1.2.3.lsst1/lib/python/mysqlpython1.2.3py2.7linux x8664.egg/mysqldb/connections.py"", line 187, in init      super(connection, self).init(args, kwargs2)  operationalerror: (2013, ""lost connection to mysql server at 'reading authorization packet', system error: 0"")  ",4,val
DM-5607,check & correct comparison operators in daf_persistence and daf_butlerUtils,"per comments in dm5593, an incorrect comparison operator was found, that used is instead of == in a string comparison (e.g. var is 'left' which is incorrect, it should be var == 'left'.  this needs to be corrected in repository (see dm5593 for details), and the rest of dafpersistence and dafbutlerutils should be checked for correct use of is vs. ==.",1,val
DM-5608,"plan and RFC for ""data repository based on version""","create way using repo of repos to get only the posix root in the returned cfg, update example/test code.  write an rfc, review with kt  post rfc & gather feedback.  incorporate feedback and/or create another design story if needed.",12,val
DM-5609,Investigate clang issues regarding friendship and protected members ,"in dm 5590, we worked around a problem in which clang 3.8 refused to access protected members of a cousin class given a friend declaration in the base. to our best understanding at time of writing, the code is valid: it seems possible that this is a bug in clang.    investigate what went wrong, produce a minimal test case, and (if appropriate) report this as an upstream bug.",3,val
DM-5610,"x16 Operations Planning in LOPT, TOWG, and DM replanning (ConOps development)","develop conops for dm system, including bulk batch system, data backbone, l3 hosting, etc. develop use cases for towg. continued planning for operations, focusing on data processing and products directorate.    don petravick, margaret gelman, hsin fang chiang, stephen pietrowicz, jaggi yedetore, paul wefel  ",54,val
DM-5611,x16 Joint Coordination Council,"coordination with cc in2p3.    don petravick, jason alt  ",7,val
DM-5612,Design specification and requirements analysis of Bulk Batch System,"functional breakdown of bulk batch system, including l2 processing, calibration processing, etc. detailed design, plan, and schedule.    don petravick, margaret gelman, jason alt, hsin fang chiang, stephen pietrowicz, rob kooper, paul wefel",68,val
DM-5613,Design specification and requirements analysis of Data Backbone,"functional breakdown of data backbone. detailed design, plan, and schedule.    jason alt, don petravick, margaret gelman, paul wefel",23,val
DM-5614,x16 middlware/workflow package definition and development,defining future middleware package to support science pipeline processing. maintaining and adding to current middleware packages. prototyping processing sequences with decam data.    ,100,val
DM-5616,x16 LSST Identity and Access Management Program development,nan,8,val
DM-5617,Further requirements analysis of the L1 System,"additional design specification of parts of the l1 system that we haven't looked at in detail yet, such as efd replication, observatory operations server, auxiliary telescope processing, telemetry processing, and commissioning support.    margaret gelman, stephen pietrowicz, james parsons, paul wefel  ",20,val
DM-5618,"WAN emulation testing, project 1","project 1 of wan emulation. see https:/confluence.lsstcorp.org/display/jp/wanemulatortestplan january2016 for details.    james parsons, paul wefel  ",21,val
DM-5620,L1 Basic Message Topology (x16),"includes system status and message dictionary, main programs for all l1 entities, and message interaction between l1 entities.",95,val
DM-5622,Network and Service Monitoring (Comfort Console),this is the development and integration work required for architecting and building a network monitoring center.,36,val
DM-5623,Procure FY16 capabilities,"quotes, discussions with vendors, details specs of procurements.   ",42,val
DM-5624,Deploy FY16 Cluster Services,nan,28,val
DM-5625,Migrate to distributed filesystem,nan,20,val
DM-5626,Deploy FY16 Verification Cluster,nan,36,val
DM-5627,Deploy FY16 Object Store Discovery Infrastructure,"deploy infrastructure for fy16 object store evaluation.    deliverable: object store discovery infrastructure  staff: 3 ncsa ici engineers (networking, storage, systems)  effort: 20 days  planned start: 6/1/2016  planned end: 6/30/2016",40,val
DM-5628,x16 ISO Work,nan,20,val
DM-5632,test run coaddDriver and multiBandDriver with DECam data,"preparation work to learn about ctrlpool and pipedrivers packages.       install ctrlpool and pipedrivers packages on my osx desktop (no slurm).    run the ctrlpool mpiexec example to verify if the mpi is working.   obtain a hsc data repo from cihsc as sanity checks.    construct a small decam data repo, using raw stripe82 data consisting of two visits, one band, one patch.   run ctrlpooldemo.py with the hsc data repo and then the decam data repo.   run the pipe_drivers scripts coadddriver and multibanddriver with the hsc data repo and then the decam data repo.    all with the default batch system smp to run on a single machine.",12,val
DM-5633,Add data products and config in obs_decam for multi-band processing,"add necessary data products and default config in order to run forcedphotccd, coadddrivertask, and multibanddrivertask with decam data. ",3,val
DM-5636,Python version checking in newinstall.sh is not quite right,there is a recent report on community where newinstall.sh reports that the python version is too old despite the user having a modern anaconda python in their path.  in commit e6fc9ed2 the code was changed to check $python for version compatibility but that is not correct as the python that will be used for the actual build is the python in their path. $python is defined purely as the python to use for eups installation.    in the reported error $python was not set and their /usr/bin/python was too old. confusingly the error message reporting the version problem actually reported the version information for the python in the path and not the $python python. the simple fix is to revert e6fc9ed2.    i already made significant comments on this topic in the original https:/github.com/lsst/lsst/pull/19 but i really do have to insist on either reverting that pr or at least fixing the error messages to use a consistent python (i'd argue that this is still wrong but at least consistent). the current situation is at best confusing and at worst pointless and wrong.    the version test only makes sense if we are testing that the default python in the path is the correct version to build the stack. $python was originally designed to allow a different python to be used to build eups. even that is no longer an issue as eups can work with python >= 2.6 now.,1,val
DM-5638,Make file upload show feedback when file is uploading,nan,2,val
DM-5640,Build a tool to automatically run autopep8 on LSST Stack,develop a lsstautopep8 command in https:/github.com/lsstsqre/sqrecodekit that can run https:/github.com/hhatto/autopep8 in an automated fashion across all of the lsst stack repositories according to the pep 8 exceptions determined in rfc 162.,1,val
DM-5641,finish up afw.table to astropy.table view support,"at an lsst/astropy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy tables of afw and https:/github.com/astropy/astropy/pull/4740.    before merging, we should add support for ""object"" columns for subclasses to hold e.g. footprints in sourcecatalog, and add some documentation.  we may also want to add a convenience method to return an astropy.table.table directly.",1,val
DM-5642,use AstroPy-compliant strings for units in afw.table,"with dm 5641, we'll soon be able to get astropy.table views into afw.table objects.  that will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected.",4,val
DM-5643,"add method to convert Property[Set,List] to nested dict","in interfacing with astropy it'd be useful to easily convert propertyset and propertylist to nested dict and ordereddict (respectively), converting elements with multiple values to lists in the process.",2,val
DM-5645,Add fine-grained authorization to ltd-keeper users,"the initial mvp of ltdkeeper had allor nothing authentication; any user was effectively an admin user. it would be useful have fine grained roles that each api user could have (for example, one api user might be able to add a build, but not create an edition or product or add another user). the phases of this ticket at:    1. design a set of roles that cover current functionality  2. add these roles to the user db model and user creation api  3. authorize users against these roles in specific api calls",2,val
DM-5647,Research & Design for object storage & transport factorization,"start research and design proposals & prototyping on the butler back end factorization problem; need to be able to configure butler to put and get different object types, different storage formats, and different storage locations. do intermediate kt reviews.  initial design and stubbed implementation to include the following: exposuref to/from fits file on local filesystem (posix) exposuref to/from memory sourcecatalog to/from database sourcecatalog to/from fits on local filesystem (posix) sourcecatalog to/from memory",9,val
DM-5648,Finish stubs and write role description for butler back end factorization,nan,8,val
DM-5649,visit AP team and work on processing DECam data,"march 1317, 2016. work on various topics about processing decam data:   improve documentations on processing raw decam data, especially about the steps of ingesting calibration data   identify future work on improving processing raw data. updates about instrumental signature removal tasks.   learn how to run difference imaging pipeline with decam data   try jointcal (simultaneous astrometry meas_simastrom package from in2p3) with decam data and identity necessary code changes for doing jointcal with decam data   use the preliminary jointcal astrometry results to examine decam data’s distortion    also more general discussions on data processing",8,val
DM-5650,SUIT vision document,writing down suit vision that the group has discussed and shaped in last year off and on.   suit will use it as guidance for system design.,4,val
DM-5651,run jenkins for PRs on all EUPS products - part I,nan,1,val
DM-5652,Implement RFC-167,implement rfc 167 for adding esutil to the stack.  this will be done in the same way as proposed to add scipy.,2,val
DM-5655,Reduce code duplication in StarSelectors,"both objectsizestarselector and secondmomentstarselector have logic to transform measured moments from pixel coordinates to tan_pixels in order to remove optical distortion.  that's generically useful for any star selector that works on measured moments, and we shouldn't have to repeat it everywhere it is used.",2,val
DM-5657,Improve Large Test Scale query script,this script is currently located in:   admin/tools/docker/deployment/in2p3/runqueries.py     here's some improvments:     use lsst/db instead of mysqlpython   externalize queries and other parameters in a config file   add an option to make script stop after a few queries (in order to have deterministic query results for large scale integration tests)   any other minor improvments...,8,val
DM-5658,Update tables of packages that depend on scipy,"now that the scipy package has been added (dm 5446), the table files of other packages to be fixed as soon as possible so that we have an idea of what is silently depending on scipy. these include afw, ipdiffim, measmodelfit, mopsdaymops, pipetasks, shapelet and sims_photutils. many of these are setupoptional that we should consider making mandatory. some will be setuprequired.",2,val
DM-5659,multiple dialog are not working well together,when several dialogs are up together.  the most recently click one should be one top. when table are in the dialogs such a fits header view. the scroll bars will go over other dialogs. this needs some though and work.  another thing when a message dialog is show because of a dialog error. it should center on the dialog.  update i don't think i will do the error centering now.  i am going to leave that and see if it is a real problem.,3,val
DM-5660,Add motivated model fits to validate_drp  photometric and astrometric scatter/repeatability analysis and plots,"implement wellmotivated theoretical fits to the astrometric and photometric performance measurements based on derivations from lsst overview paper.  http:/arxiv.org/pdf/0805.2366v4.pdf    photometric errors described by  eq. 5  sigmarand2 = (0.039  gamma)  x  gamma  x2  [mag2]  where x = 10(0.4(m m5))    eq. 4  sigma12 = sigmasys2  sigma_rand^2    astrometric errors   error = c  theta / snr    based on helpful comments from [zivezic]      i think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former i assumed and the latter i got from the value of your  analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles).  actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any “weird” behavior in  analyzed data (and you get the limiting depth, m5, even if you don’t go all the way to the faint end).     similarly, for the astrometric random errors, we’d expect        error = c   theta / snr,    where theta is the seeing (or a fit parameter), snr is the photometric snr (i.e. 1/err in mag), and c  1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing).   ",5,val
DM-5663,Config override fixes needed due to new star selector,"as of dm 5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal ci, which is how i missed this).",2,val
DM-5665,"Organize HSC docs ""hackathon""",liase with square to determine the most effective way to transfer hsc docs to lsst. organize a hackathon session for drp developers at which we get this done. bring doughnuts.,1,val
DM-5666,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.,2,val
DM-5667,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.  ,2,val
DM-5668,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.,2,val
DM-5669,Take part in HSC docs hacakthon,participate in hsc docs transfer hackathon.,2,val
DM-5670,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.,2,val
DM-5671,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.  ,2,val
DM-5672,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.  ,2,val
DM-5673,Take part in HSC docs hackathon,participate in hsc docs transfer hackathon.,2,val
DM-5674,Prepare detailed L2 plan,"at the scipiwg meeting of 23 & 24 march 2017,  presented an overview of his plans for l2 processing. the next step is to refine those plans and prepare a more detailed ""deepdive"" discussion of the l2 plans.",10,val
DM-5675,Cannot enable shapeHSM because RegistryField fails validation,"when running cihsc after settingup the measextensionsshapehsm, measextensionsphotometrykron and dependencies using setup v r . in the respective cloned folders, i get    cannot enable shapehsm (registryfield 'calibrate.detectandmeasure.measurement.plugins' failed validation: unknown key 'extshapehsmhsmmoments' in registry/configchoicefield  for more information read the field definition at:    file ""/home/vish/code/lsst/lsstsw/stack/linux64/pexconfig/201601.0+3/python/lsst/pex/config/registry.py"", line 179, in init      configchoicefield.init(self, doc, types, default, optional, multi)  and the config definition at:    file ""/home/vish/code/lsst/lsstsw/stack/linux64/measbase/2016_01.013 g779ee14/python/lsst/meas/base/sfm.py"", line 109, in /      class singleframemeasurementconfig(basemeasurementconfig):  ): disabling hsm shape measurements    find out why this is happening and find a fix  ",1,val
DM-5676,Wrap example C++ code with pybind11,same as dm 5471 but using pybind11,10,val
DM-5677,Wrap example code with cffi,"as per dm 5471, but using cffi.",10,val
DM-5681,Provide single-visit processing capability as required by HSC,"in dm 3368, we provided a means of running multiple processccd tasks across an exposure, but without performing global calibration etc as provided by hsc's processexposuretask.    please augment this with whatever additional capability is required to enable hsc data release processing.",2,val
DM-5685,processCcd.py is failing on some CFHT u band images,"processccd.py is failing on some u band cfht data, as reported by  on c.l.o: https:/community.lsst.org/t/testingdm4692thenew processccdtask/507/24    see that posting for sample data to reproduce the problem.",8,val
DM-5686,Accommodate pixel padding when unpersisting reference catalog matches,"the reference object loader in meas_algorithm's loadreferenceobjects.py grows the bbox by the config parameter pixelmargin:  doc = ""padding to add to 4 all edges of the bounding box (pixels)"" . this is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. this increase needs to be reflected in the radius set in the metadata fed into joinmatchlistwithcatalog().  ",2,val
DM-5688,Table performance on Firefly,"table seems to perform poorly on firefox. firefox gets into the complete refresh state only when the table is visible with charts only, fits view only or fits view and chart it does not happen    helpful article: http:/benchling.engineering/performanceengineeringwithreact/    changelog:   added react performance tools, react.addons.perf   fix some performance issues:     skip render of selection boxes when not needed.     skip rendering of xyplot options when not needed.     skip wasted render called for table cell and headers.     will do a more in depth investigation in another ticket.   refactor table code and it's state.     move all table related states into table_space.     create sub reducers for each data domain     rename and move functions to better describe what it's doing    added 'title' to table.    show mask while loading    ",6,val
DM-5689,Table needs to fire another action when data completely loaded,when the data for a table is completely loaded fire another action such as tablenewloadeddone. this way the xyplots and the image overlays know to go fetch the data.    4/22/2026 from the pull request:  added new action tablenew_loaded to table; fired when table is completely loaded.  added table error handling.  fix active table not updating after an active tab is removed.,2,val
DM-5691,AP Emergent work -- F16,there is emergent work that comes up as a side effect of other work.  this epic will capture that effort in f16.,19,val
DM-5692,Connect CatSim to StarFast simulation tool,"catsim can provide a fully realistic simulated catalog, which starfast could use as an input for simulations. this ticket includes writing the code to connect to the catsim database and updating the internal catalog format in starfast to be compatible with catsim.",4,val
DM-5693,Write StarFast interface to ProcessCCD,"the simulated images generated by starfast need to be able to be run through the lsst stack, to test and make use of the existing measurement, fitting, stacking, and image differencing capabilities.   this includes writing or updating a simulations obs package, and determining and supplying the required metadata.",6,val
DM-5694,Run StarFast simulated images through diffim,determine the metadata and dependencies needed to fully process two images simulated with starfast through diffim. ,2,val
DM-5695,Implement simple 1D DCR correction on simulated data,"nate lust wrote a simple dcr correction recipe that runs in 1d in an ipython notebook. this ticket is to re write the notebook in python modules that can be run on starfast simulated images prior to image differencing. for this ticket, the simulated images will be 2d, but dcr will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1d.",8,val
DM-5696,Add support for blank image,we need to add blank image support.,2,val
DM-5697,Extend simple DCR correction to 2D,"in dm 5695 a simple dcr correction was applied to simulated images in the case that the effect was purely along the pixel grid and could be reduced to 1d. this ticket extends that work to the general 2d case.  possible approaches include resampling the ""science"" image to match the ""template"", or including neighboring pixels and computing their covariance. ideally, multiple approaches will be implemented and tested.",6,val
DM-5698,Add astrometric errors to StarFast,"one concern with the proposed dcr correction is that it might fail in the presence of source position errors. this ticket is to add the capability to simulate a variety of types of position errors, such as atmospheric turbulence or an inaccurate wcs, to test the dcr implementation.",4,val
DM-5699,Run many sky simulations through DCR correction to find edge cases,"once a complete dcr correction prototype is finished, we will want to run many different sky simulations from starfast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. there are likely to be several thousand simulations needed which will take an as yet undefined number of cpu hours, but this ticket is for the work in setting up and analyzing the results from the run.",4,val
DM-5700,Put ImageSelectPanel into dropdown,currently the image select panel is in a dialog.  it also needs to be able to work in a dropdown.,4,val
DM-5701,Create toy composite (AST/GWCS) model with supported components,"to help us evaluate wcs options, we need to create a relatively complicated composite model in ast and gwcs, using a few models currently available within the existing packages. a minimal composite model to test these things would include:      fits linear transform    ccd distortion    optical model    fits tan wcs    the middle steps do not need to be realistic models, just something that we can use to compare ast's and gwcs's respective interfaces and capabilities for creating the composite model, and test for differences in their results. we can then use this model to evaluate performance when run on different numbers of pixels.",4,val
DM-5702,Create a new model in AST/GWCS to represent a complex distortion,"using lessons learned from dm5701, create a more complex distortion model that cannot be represented from the basic models in gwcs or ast. a good example for this might be a rapidly varying sinusoidal treeringlike function that is not well represented by the standard polynomial basis functions. this will test our ability to extend each framework with new models that have not yet been decided on.    once completed, we could plug this back into the composite model in dm5701.",8,val
DM-5703,Evaluate performance of AST/GWCS over a range of numbers of pixels,"once we have a composite distortion model from dm5701, evaluate the performance of ast and gwcs over a range of numbers of pixels, likely from ~100 through fullccd (4k^2).    as part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python only models in gwcs and whether bottlenecks could be worked around via optimizations in cython.",8,val
DM-5704,add cloudbees-folder support to puppet-jenkins ,nan,6,val
DM-5715,Produce document describing DRP parallelization use cases,"at various times in the past few months i've promised [gpdf], [petravick], , and probably a few others a document describing the parallelization needs for drp in greater detail.  my understanding of the plans for the eventual drp probably good enough to do this well now, and is unlikely to improve further in the near future (as that will require algorithmic research).    this needs to be prioritized with my other responsibility for documents that describe the drp system in other ways, most of which are oriented towards scientists and science pipelines developers.  the document on this ticket is essentially the description that would matter the most for the process control middleware team.",6,val
DM-5716,UI Consistency,there is a need to go though the entire ui and document inconsistencies with the old ui.,4,val
DM-5717,Firefly Result view architecture/component,"the result view architecture needs to be written.     a meeting needs to happen with david, gregory, xiuqin, trey, and tatiana to discuss this.   trey, loi, and tatiana should have a design meeting.    it should support some or all of the following ideas:     a search defining a new results view type   a search adding to an existing result view   a search replacing the results   any cleanup needs to happen.   some sort of controller that know which view should show and which view can be shown     new search panels easily added. maybe the html file defines which are visible",10,val
DM-5719,"Verification Cluster, Object Store Procurement",strategy design with pipeline and deployment teams. discussions of service description and levels of support. sufficient design to lead to procurement. discussions with vendors. quote selection. budget tracking. quote submission to finance. gco follow up questions. obfs follow up questions. finance follow up questions. overall tracking of purchase progression.,12,val
DM-5720,JIRA fixes,this tracks sps spent on jira requests. ,2,val
DM-5722,Add table client-side sorting,convert gwt's client side sorting to javascript.,4,val
DM-5723,make sure table can be resized properly,test table to make sure it can be resized under a variety of layout.,2,val
DM-5725,attend the weekly meeting with UIUC camera team,"while tatiana is the assignee of this ticket, xiuqin and gregory participate this weekly telecon semi regularly to lend support. ",2,val
DM-5726,attend the weekly meeting with UIUC camera team (May 2016),tatiana will attend the weekly meeting. xiuqin and gregory also attends when needed. ,2,val
DM-5728,Create django project and initial dashboard app,"this ticket captures the steps to create the django project for squash, its configuration and the dashboard app http:/sqr009.lsst.io/en/latest/    the planned tasks are:         implement the ``dataset``, ``visit`` and ``ccd`` tables in the django orm layer, as a minimum set      of tables for the dashboard app        prototype home page and dashboard pages      ",5,val
DM-5729,Config.loadFromStream suppresses NameError,"within a config override file being executed via config.load or config.loadfromstream, using a variable that hasn't been defined results in a nameerror exception, but this is silently suppressed and the user has no idea the following overrides have not been executed.",1,val
DM-5734,Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work,"  the testing data ""table_data.tbl"" in the testing tree was accidentally moved.  it should be added back so that ipacttabletest.java can run.     the request in jsontableutil was mistakenly moved out from the tablemodel by the the line       if (request != null && request.getmeta().keyset().size()>1)   .  the meta can be null but the request is not null, the request should be put into the tablemodel.     ",1,val
DM-5738,Move Camera creation out of CameraMapper base class,"with the new camerageom, it's considered desirable that each camera be able to define the serialization format for its static camera data.  despite this, it's still the base cameramapper that does loads it (at least for most cameras), going through a circuitous chain of policy files, obs  package paths, and python code.    it'd be vastly simpler for each mapper to simply build the camera object and assign it to self.camera in its own init_ method (most would simply delegate all the work to afw.camerageom.makecamerafrompath).  we could then remove all the camera entries from the paf files and make it much easier to follow the logic.    eventually, i think we need to be storing at least some components of the camera definition in the data repository (or something like a calibration repository associated with it), and that would require giving the mapper access to a partially constructed butler when its time to build the camera.  but we can save that for another day.  ",2,val
DM-5739,--clobber-config modifies input rerun,"using the clobberconfig option in a child butler repository can cause changes in the parent repository, as we try to rename files to back them up in the parent repository.    this is a critical bug because it can cause pipeline outputs to be unexpectedly modified.    it should be easy to fix, as it's just a matter of checking whether the files to be renamed backed up are in the output repository.    this was originally reported as https:/hscjira.astro.princeton.edu/jira/browse/hsc 1341  ",1,val
DM-5740,Create and deploy common Ansible roles for ELK,"create roles and deploy to ansible galaxy.    these are common roles for cloudinit (and any other future cloud dependencies), java (openjdkjdk) and an editors role.",3,val
DM-5741,Create and deploy Elasticsearch and Kibana Ansible roles,create roles and deploy to ansible galaxy.,3,val
DM-5742,Create and deploy an ELK system,"create a vagrant configuration and ansible role to configure and combine elasticsearch, logstash and kibana (elk).",36,val
DM-5743,"Create and deploy Logstash, Fluentd and Riemann Ansible roles",create roles and deploy to ansible galaxy.    create a role to combine all the individual projects together.,3,val
DM-5744,Create packer automation for ELK,build packer automation to create machine images to use for the elk system.,6,val
DM-5745,Implement ingestion code for the QA results,"the initial database model was implemented in dm5728 and outputs of the qa analysis code are being produced by the work described in http:/dmtn008.lsst.io/en/latest/    in this ticket we plan to implement and api for listing and creating jobs, metrics and measurements so that a job or the qa analysis code can register this information in the dashboard app.",4,val
DM-5746,Build parallel DCR simulator using GalSim,the result of dm 4899 was a simulation tool called starfast that can quickly make simulated images with realistic differential chromatic refraction. this ticket is to build an equivalent simulator using galsim to check the accuracy of results and benchmark speed and memory usage. ,6,val
DM-5747,SQUASH dashboard prototype design,squash dashboard prototype design is described here    http:/sqr 009.lsst.io/en/latest/,8,val
DM-5748,Upgrade mpi4py to latest upstream,"https:/bitbucket.org/mpi4py/ version 2.0 was released in october 2015 with a number of changes. we should upgrade. when upgrading, we should check whether it contains a proper fix for dm5409 and, if not, file a bug report upstream.    this issue should not be addressed until we have proper test coverage on code which uses mpi4py (dm3845).",1,val
DM-5750,Integration of Django and bokeh server,nan,5,val
DM-5753,XCode 7.3 can not link indirect dependencies that use @rpath,"with xcode 7.3 on os x we have difficulties resolving indirect dependencies when those dependencies are referenced using @rpath. this can be seen with qserv:    linking shared object build/libqservcommon.dylib  ld: file not found: @rpath/libboostsystem.dylib for architecture x8664  clang: error: linker command failed with exit code 1 (use v to see invocation)    where libboostsystem is being loaded via libboostthread:    $ otool l $boostdir/lib/libboostthread.dylib  /users/timj/work/lsstsw/stack/darwinx86/boost/1.59.lsst5fbf04ba888/lib/libboostthread.dylib:   @rpath/libboostthread.dylib (compatibility version 0.0.0, current version 0.0.0)   @rpath/libboostsystem.dylib (compatibility version 0.0.0, current version 0.0.0)   /usr/lib/libc+.1.dylib (compatibility version 1.0.0, current version 120.1.0)   /usr/lib/libsystem.b.dylib (compatibility version 1.0.0, current version 1226.10.1)      this problem is also found when doing a conda build of the stack because in conda all shared libraries are modified on creation to reference other libraries via the @rpath mechanism.    this bug has been reported to apple as http:/ and a https:/bugs.chromium.org/p/chromium/issues/detail?id=597459 indicates that the fix is to simply ensure that  l directives include a trailing slash.  ",4,val
DM-5756,Update Scons to v2.5.0,scons 2.5.0 came out over the weekend. there were many fixes to the dependency determination code. the next version of scons is intended to be 3.0 which will be the first version to support python 3. since we fully intend to switch to python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.,2,val
DM-5757,FitsHeader's resize and sorting,"dm 4494 has merged to the dev.  however, there are still two issues remained:   resize the popup with tabs does not work   sorting is depending on the basictable's sorting",1,val
DM-5759,TabPanel needs a way to keep it state between renders,the tabpanel and collapsiblepanel loses its state when it is rerendered.  it is going to have to have a way keeps it state. therefore it needs an option to take an id and keep it state in the store.      use case tabs of tables then the image plot goes to expanded mode.  the table tabs gets reset to the first one.    ,4,val
DM-5760,XYPlot needs to be expandable,make xyplot expandable,2,val
DM-5761,XYPlot should support selecting columns from a table,there are should be a way to display all the information about table columns in a table and allow user to choose a column using this table.,10,val
DM-5762,XYPlot: Optimize decimated plot aspect ratio,"currently decimation process assumes aspect ration 1. for decimated plots, the displayed area size (or user supplied value) needs to be used as an aspect ratio to approximate square bins.   when aspect ratio changes, decimation process needs to be redone.   to avoid server calls on resize, disallow flexible aspect ratio for decimated data.   ",6,val
DM-5763,XYPlot: decimation options,user needs to be able to control number of bins and bin size.,3,val
DM-5764,XYPlot: separate density plot from scatter plot,"at the moment we display data as scatter plot, when the number of points does not exceed decimation limit, and as density plot when the number of points does exceed this limit.    scatter plot and density plot should be separate charts. these are the reasons:   user should be able to create density plot with any number of points   chart type and display might be different for density plot in the future   scatter plot look should not change when the number of points exceeds decimation limit   scatter plot should support errors in the future",5,val
DM-5765,Remove unneeded imports in SConstruct,"there's an outstanding pull request from an external contributor (miguel de val borro) https:/github.com/lsst/sconsutils/pull/9 that makes some minor improvements to sconsutils by cleaning up the imports. somebody should review and (if appropriate) merge it. (or, at least, reply to our community!)",1,val
DM-5766,Implement spatial exposure selection task,"once dm3472 lands, it will be possible to write an image selection task that uses the sqlite 3 database produced by indexexposuretask (from https:/github.com/lsst/dafingest) to search for exposures overlapping a region (in particular, the spatial extent of a coadd patch). the rtreesearch method in testindexexposure.py (also from daf_ingest) has an example of how to perform spatial queries quickly.    i was originally scheduled to do something in this space, but paul mentioned that he had plans to refactor the image selection tasks already, and is much more familiar with the pipeline side of things than i am. therefore, i'm handing off the implementation of the pipeline task mentioned in dm3472 to him.",6,val
DM-5767,Create custom basic coaddition code,"create script to do the following:   takes a list of decam exposure numbers   for each ccd, loads the corresponding calexps   creates a naive pixelbypixel coadd of the underlying images   possibly either ands or ors the masks (though perhaps not necessary)   either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not   write the corresponding images out as coadded fits",1,val
DM-5768,Coadd CPB exposures,"identify sets of decam exposures from the cbp run and feed them to the coaddition script created in dm5767.    this will need to be redone each time a reprocessing is done as the script will run on calexps. i will do it once now, and then again after dm5465 is completed to satisfactory levels.",1,val
DM-5769,Write spot visualisation snippets,"write some snippets to aide in the processing and visualisation of the cbp data/analysis.    essentially, write some helper functions that you can throw sections of images at to help look at the shape of the cbp spots, as ds9 isn't great ideal this.    some nice features would be:    a function that takes a list of images or arrays, and plots them sidebyside, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. this would be as 2d colour plots.    a function that takes part of an image and displays it as a colourgraded surface.    a function that takes part of an image and displays it as a 3d barchart (as in root, but without using root because there is already enough evil in the world)",2,val
DM-5770,Investigate image processing for feature enhancement,"whilst looking at an individual spot from the cbp on decam i noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    this ticket is to investigate what image processing techniques will make these hardtosee features pop out so that they can be examined more closely.",2,val
DM-5771,Update config files,"dm46921 and dm5348 changed processccd to the point where past config files are no longer valid as stuff has moved a lot (see https:/community.lsst.org/t/backwardincompatiblechangestoprocessccdtaskandsubtasks/581)    this ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processccd",2,val
DM-5773,Firefly API plan and decision,"we need a plan for  all the firefly apis development in the new react/redux based js framework, including js api and python api.     backward compatibility   syntax format for js api   syntax format for python api   schedule      convert the existing api first      list of new ones to be added, when    ",2,val
DM-5775,Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable,"when an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%).   when the tabpanel is used, the table is put on tabpanel.  the table needs to access the size information of the outside container, ie,, the grandparent's width and height. the tabpanel has to pass the height and width to its child component.  without specifying the height and width in the tabpanel, by default, the auto is used.  when the width (height) is auto, it allows to use the child's width (height).  however, the child replies on the parent to provide such information.  when this circular relations occur, the default size of the child is used.  that is why the table component forever has 75px when it was put in the tabpanel.  to be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly. ",1,val
DM-5778,Document Configurable concept,"the configurable concept (a callable that takes a config as an argument) is a fairly important one in pexconfig as the guts behind registryfield and configurablefield, and it's mentioned several times in pexconfig's documentation, but it doesn't seem to be directly documented itself.",1,val
DM-5780,Assist schandra with ts_wep Luigi implementation,assist  with an initial implementation of his workflow using luigi.,1,val
DM-5781,"Port Data set info converter, part2","part 2 includes:  3 color  clean up on plot fail  clean up image in general  better row highlighting  other types of data layout (a fc type view)  artifacts (maybe part 3)   when image is small, like zoom it down 1/16x, behavior of selecting an image is not consistent. clicking on an edge will select one, but will not work on another.",10,val
DM-5782,"Include obs_cfht, obs_decam in lsst-dev shared stack",the shared stack on lsstdev provided in dm5435 does not contain the obscfht or obsdecam camera packages. please add them.,1,val
DM-5784,Port region serializer and data structures from GWT,"the region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util   regionfactory.java    region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd     containsoptions.java   global.java   regionfileelement.java   regparseexception.java   region.java   regionannulus.java   regionbox.java   regionboxannulus.java   regioncsys.java   regiondimension.java   regionellipse.java   regionellipseannulus.java   regionfont.java   regionlines.java   regionoptions.java   regionpoint.java   regiontext.java    regionvalue.java      note   do not port coordexception, there are other ways to do this.",8,val
DM-5787,Add support for SGE,jean coupon has requested support for sge in ctrl_pool.,1,val
DM-5791,Why is doSelectUnresolved an argument?,"the run method in the photocaltask has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration.  this is a good idea, but it should be configurable, i think. ",1,val
DM-5792,Support artifacts ,for now this is the artifacts for wise images.   we should look at the possibilities to generalize this. ,8,val
DM-5793,Convert  Mask support,nan,6,val
DM-5794,Support image and drawing layer subgrouping,nan,8,val
DM-5795,Add Python properties for getters and setters in afw::geom and shapelet,"i'm adding properties via swig %extend in much of afw::geom right now, because:    i think we've all agreed this is something we want, even if we haven't agreed how much effort we want to put into it.    i'm getting annoyed writing lots of parentheses for these getters and setters on dm5197.    i can get this done in a couple of hours on a weekend, so i don't need a t/cam to give me permission to spend my own time on it :)  ",1,val
DM-5797,Using 'CONSTANT' for background subtraction fails,"running processccd (on a decam file) with the following in the config file:      config.charimage.repair.cosmicray.background.algorithm='akimaspline'  config.charimage.background.algorithm='constant'  config.charimage.detectandmeasure.detection.templocalbackground.algorithm='constant'  config.charimage.detectandmeasure.detection.background.algorithm='constant'  config.calibrate.detectandmeasure.detection.templocalbackground.algorithm='constant'  config.calibrate.detectandmeasure.detection.background.algorithm='constant'      fails, and throws the following:      traceback (most recent call last):    file ""/home/mfisherlevine/lsst/pipetasks/bin/processccd.py"", line 25, in /      processccdtask.parseandrun()    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/cmdlinetask.py"", line 450, in parseandrun      resultlist = taskrunner.run(parsedcmd)    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/cmdlinetask.py"", line 199, in run      resultlist = mapfunc(self, targetlist)    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/cmdlinetask.py"", line 324, in call      result = task.run(dataref, kwargs)    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/mfisherlevine/lsst/pipetasks/python/lsst/pipe/tasks/processccd.py"", line 170, in run      dounpersist = false,    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/mfisherlevine/lsst/pipetasks/python/lsst/pipe/tasks/characterizeimage.py"", line 298, in run      background = background,    file ""/ssd/lsstsw/stack/linux64/pipebase/201601.06g77518698/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, args, keyargs)    file ""/home/mfisherlevine/lsst/pipetasks/python/lsst/pipe/tasks/characterizeimage.py"", line 356, in characterize      image  = estbg.getimagef()    file ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathlib.py"", line 5788, in getimagef      return mathlib.backgroundgetimagef(self, args)  lsst.pex.exceptions.wrappers.invalidparametererror:     file ""src/math/interpolate.cc"", line 61, in std::pair/, std::vector/ > lsst::afw::math::::recenter(const std::vector/&, const std::vector/&)      you must provide at least 1 point     file ""src/math/backgroundmi.cc"", line 196, in void lsst::afw::math::backgroundmi::setgridcolumns(lsst::afw::math::interpolate::style, lsst::afw::math::undersamplestyle, int, const std::vector/&) const      setting gridcolumns   lsst::pex::exceptions::invalidparametererror: 'you must provide at least 1 point ; setting gridcolumns   ",2,val
DM-5798,Pass butler to ref loader,"the design of the indexed reference catalogs requires a butler to be sent to the loader.  this requires passing the butler down through the chain of subtasks from the parent command line task.  in this case, i believe only calibratetask constructs sub tasks that requires a reference catalog.    this will also require moving the loader and indexer to meas_astrom, otherwise it will introduce a circular dependency.",4,val
DM-5799,Asinh stretch algorithm corerction,"in dm 2634, the asinh stretch algorithm  was implemented, but the behavior was not quite right. we need to figure out the issue and make it right. one possibility is that the understanding the relationship  of zero point  and black point, maximum point and white point. ",8,val
DM-5800,TabPanel:  Tab titles need to shrink to accommodate a large number of tabs.,should convert gwt's logic over to tabpanel.   shrink title as needed.   show full title on mouse over,4,val
DM-5802,Cmake in mariadbclient finds wrong libz,"when building mariadbclient, cmake identifies libz from a separate python installation than the one setup to run the stack. i have an anaconda installation on the disk, and a miniconda installation set up specifically for the lsst stack. during the building process cmake for some reason finds the alternate libz associated with that python installation.",1,val
DM-5803,fetchUrl is not handling post requests correctly.,parameters are not sent to the server when requests are posted via fetchurl.,2,val
DM-5804,Use aperture-corrected aperture flux in validate_drp,shift from psfflux flux/magnitude to aperturecorrected aperturebased mag/flux measurements for calculating photometric repeatibility.,1,val
DM-5805,Improve star/galaxy separation for validate_drp,"improve the star/galaxy separation for validate_drp.    many of the lsst srd kpms are defined for bright, isolated stars.  there is clear evidence that galaxies are being included in current runs (they have significantly higher photometric scatter at the same mag|snr).  improved star/galaxy separation will help generate better numbers    stretch goal:  include additional informative plots about how well the `extendedness` value in the catalogs is successfully separating stars and galaxies.",1,val
DM-5806,Add a paging bar to ImageMetaDataToolbarView,add a paging bar similar to the one for table to the imagemetadatatoolbarview.  this pages images instead of rows.,4,val
DM-5808,Ensure that variance plane in calexps is unchanged HSC⟷LSST,per discussion in hsc telecon 20160419.,1,val
DM-5810,Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector,"a couple recent changes to the stack break imagedifferencetask.     requires updates to only a few lines.     while i'm updating it to reflect the star selector api, i'm also changing the default star selector from secondmoment to objectsizestarselector (which i learned today is what the stack has been using by default for a while). ",1,val
DM-5819,Incorporate Price suggestions to make `validate_drp` faster,"increase the loading and processing speed of validatedrp following suggestions by     1. don't read in footprints  pass flags=lsst.afw.table.sourceionofootprints to butler.get    2. work on speed of calculation of rms and other expensive quantities.  current suggestions:  a. calcrmsdistances  b. multimatch  c. matchvisitcomputedistance  d. consider boolean indexing       objbyid =   to:     objbyid = dict(zip(self.reference[self.objectkey], self.reference))      note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re architect things to enable efficient processing beyond the memory on one node.",2,val
DM-5820,3 color and FITS header clean up,"there are some issues with three color when not using all three bands (i.e. using on green and blue):   mouse readout is not labeled correctly   fits head popup does not come up    other fits header popup issues:   if file size is too big then the text is wrapping   on safari, the resizable indicator in on every cell  ",6,val
DM-5821,Intermittent fault building ci_hsc through Jenkins,"occasionally (see e.g. https:/ci.lsst.codes/job/stackosmatrix/label=centos7/10437/console and https:/ci.lsst.codes/job/stackosmatrix/label=centos6/9594/console) the ci_hsc job in jenkins fails, reporting:    runtimeerror: dictionary changed size during iteration    the fault seems to be intermittent. please fix it.",3,val
DM-5822,Afw fails unit test for convolve depending on compiler optimisation level,"on osx 10.11.4 with apple llvm version 7.3.0 (clang703.0.29) afw fails test/convolve.py with the following error when either o0 or o1 is enabled but works fine for o2 and o3.      tests/convolve.py    .....ff/users/pschella/development/lsst/code/afw/python/lsst/afw/image/testutils.py:283: runtimewarning: invalid value encountered in isnan    nan0 = np.isnan(filledarr0)  /users/pschella/development/lsst/lsstsw/miniconda/lib/python2.7/sitepackages/numpy/lib/ufunclike.py:113: runtimewarning: invalid value encountered in isinf    nx.logicaland(nx.isinf(x), ~nx.signbit(x), y)  /users/pschella/development/lsst/lsstsw/miniconda/lib/python2.7/sitepackages/numpy/lib/ufunclike.py:176: runtimewarning: invalid value encountered in isinf    nx.logicaland(nx.isinf(x), nx.signbit(x), y)  f.f...  ======================================================================  fail: testspatiallyvaryinganalyticconvolve (main.convolvetestcase)  test inplace convolution with a spatially varying analytickernel    traceback (most recent call last):    file ""tests/convolve.py"", line 437, in testspatiallyvaryinganalyticconvolve      rtol = rtol)    file ""tests/convolve.py"", line 290, in runstdtest      self.runbasicconvolveedgetest(kernel, kerneldescr)    file ""tests/convolve.py"", line 317, in runbasicconvolveedgetest      dovariance = true, rtol=0, atol=0, msg=msg)    file ""/users/pschella/development/lsst/code/afw/python/lsst/afw/image/testutils.py"", line 201, in assertmaskedimagesnearlyequal      testcase.fail(""%s: %s"" % (msg, ""; "".join(errstrlist)))  assertionerror: basicconvolve(maskedimage, kernel=spatially varying gaussian analytic kernel using brute force) wrote to edge pixels: image planes differ: maxdiff=1.09176e38 at position (73, 18); value=1.09176e38 vs. 2825.0; nans differ    ======================================================================  fail: testspatiallyvaryingdeltafunctionlinearcombination (main.convolvetestcase)  test convolution with a spatially varying linearcombinationkernel of delta function basis kernels.    traceback (most recent call last):    file ""tests/convolve.py"", line 556, in testspatiallyvaryingdeltafunctionlinearcombination      rtol = rtol)    file ""tests/convolve.py"", line 290, in runstdtest      self.runbasicconvolveedgetest(kernel, kerneldescr)    file ""tests/convolve.py"", line 317, in runbasicconvolveedgetest      dovariance = true, rtol=0, atol=0, msg=msg)    file ""/users/pschella/development/lsst/code/afw/python/lsst/afw/image/testutils.py"", line 201, in assertmaskedimagesnearlyequal      testcase.fail(""%s: %s"" % (msg, ""; "".join(errstrlist)))  assertionerror: basicconvolve(maskedimage, kernel=spatially varying linearcombinationkernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxdiff=9.06659e36 at position (75, 29); value=9.06659e36 vs. 2865.0    ======================================================================  fail: testspatiallyvaryinggaussianlinercombination (main.convolvetestcase)  test convolution with a spatially varying linearcombinationkernel of two gaussian basis kernels.    traceback (most recent call last):    file ""tests/convolve.py"", line 523, in testspatiallyvaryinggaussianlinercombination      rtol = rtol)    file ""tests/convolve.py"", line 290, in runstdtest      self.runbasicconvolveedgetest(kernel, kerneldescr)    file ""tests/convolve.py"", line 317, in runbasicconvolveedgetest      dovariance = true, rtol=0, atol=0, msg=msg)    file ""/users/pschella/development/lsst/code/afw/python/lsst/afw/image/testutils.py"", line 201, in assertmaskedimagesnearlyequal      testcase.fail(""%s: %s"" % (msg, ""; "".join(errstrlist)))  assertionerror: basicconvolve(maskedimage, kernel=spatially varying gaussian analytic kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxdiff=1.22472e38 at position (74, 3); value=1.22472e38 vs. 2878.0; nans differ    ======================================================================  fail: testticket873 (main.convolvetestcase)  demonstrate ticket 873: convolution of a maskedimage with a spatially varying    traceback (most recent call last):    file ""tests/convolve.py"", line 623, in testticket873      rtol = rtol)    file ""tests/convolve.py"", line 290, in runstdtest      self.runbasicconvolveedgetest(kernel, kerneldescr)    file ""tests/convolve.py"", line 317, in runbasicconvolveedgetest      dovariance = true, rtol=0, atol=0, msg=msg)    file ""/users/pschella/development/lsst/code/afw/python/lsst/afw/image/testutils.py"", line 201, in assertmaskedimagesnearlyequal      testcase.fail(""%s: %s"" % (msg, ""; "".join(errstrlist)))  assertionerror: basicconvolve(maskedimage, kernel=spatially varying linearcombinationkernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxdiff=3.19374e38 at position (1, 46); value=3.19374e38 vs. 2774.0      ran 13 tests in 43.252s    failed (failures=4)  the following tests failed:  /users/pschella/development/lsst/code/afw/tests/.tests/convolve.py.failed  1 tests failed  scons:   [checkteststatus] error 1  scons: building terminated because of errors.  ",2,val
DM-5823,ECL_B1950 coordinate was not defined correctly,"the coordsys.js defined ecl_b1950 incorrectly.  when i was testing webgrid, the grid lines for  ecliptic b1950 were not right.  looked further, it was caused by wrong equinox value in its definition.",1,val
DM-5825,Add CI tests for obs_lsstSim,i propose:    1. create testdatalsstsim.  this will be based on 12 images from the current twinkles run 1 (or pre run 1):  2 epochs each of 6 filters.  (/)    2. add an optional test method to obslsstsim that runs if testdatalsstsim has been declared.  this is the way these tests are set up for the other obs  packages.    3. add testdatalsstsim dependency to lsstci (which already depends on obs_lsstsim).    this will then be run every time a full standard default jenkins build is processed.,2,val
DM-5827,Compare LSST and HSC pipelines through through multi-band coadd processing,"continue the work described in dm5301 through the standard ""multiband"" coadd processing workflow.    performing an endtoend comparison of the stacks will not be possible until measmosaic is fully operational on lsst (dm2674). however, until that point is reached, comparisons are still possible by either:     shepherding data through measmosaic and coaddition on hsc, then performing further processing and measurement using the lsst stack;   omitting meas_mosaic from the workflow altogether and performing endto end comparisons of the stacks without mosaicking.    obviously, neither of these will ultimately be adequate, but they should enable early identification of any major issues.",15,val
DM-5829,Create outline of Level 3 ConOps,create an outline of the sections of the level 3 conops document,2,val
DM-5830,Level 3 requirements flowdown,"document the flowdown of level 3 related requirements from srd, lsr, oss, and dmsr.",3,val
DM-5831,SUIT requirement flowdown,"go through the original requirement of suit,  put them in the categories:  done, tier1, tier2    ",6,val
DM-5832,LSE-140 post-CCB implementation,"following ccb approval of lse 140, perform minor document work required for full implementation (application of standard cover page, change log, etc.).",2,val
DM-5833,SUIT design diagramming,prepare initial set of sysml diagrams of the suit's relationship to other system components.,4,val
DM-5834,Prepare requirements and design for Fall 2016 SUIT deployments,prepare functional and quantitative requirements and the suit centric elements of design for the planned fall 2016 suit deployments (sdss stripe 82 and wise).,10,val
DM-5835,Prepare a draft of the SUIT deployment timeline,"prepare a draft schedule, with some detail for 2016 2017, for deployments of the suit into (test) production, including the datasets that will be served.",2,val
DM-5836,access to NCSA Nebular to setup servers for SUIT deployment,get three hosts in ncsa nebular system to deploy the current firefly application. the goal is workout the possible issues and identify the software needed to be installed for the hosts. clarify which team is responsible to install what third party software packages.,4,val
DM-5837,Document pipe_drivers,"please provide a minimal level of documentation for pipe_drivers, to include:     a doc directory with the usual content so that docstrings get generated by doxygen;   a package overview;    all docstrings should be appropriate for parsing by doxygen (ie, should start with """"""! where necessary).",2,val
DM-5839,horizon console interface broken,it appears that at some point in the last few months the horizon console interface has stopped working.  i am still able to access the console log output via the api/cli.,1,val
DM-5840,instance limit low vs available cores,the lsst project is currently at 81/100 instances but there are over 200 cores unused.  is it possible to increase the instance limit or are we being encouraged to use large instance flavors?,1,val
DM-5841,unable to list nebula lsst project users,"currently, [with some difficulty] it is possible to discover the user_id that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person.  this can make it difficult to 'self police' instances.    the administrative api endpoints are not publicly accessible and i doubt any end user has the appropriate permission. ",1,val
DM-5844,automate deployment of qa dashboard server and database instance,"add a qa server + rds instance to the terraform configuration for the jenkins demo sandbox for development purposes.  it may make sense to split this off to be an independent sandbox but that is very easy to do, if needed.",20,val
DM-5845,"ci_hsc fails with ""too many open files""","for example, with thanks to :                    cihsc: masterg78db638f21 .....................................................................................error (207 sec).   error building product cihsc.   exit code = 2   log is in /users/wmwv/lsstsw/build/cihsc/build.log   last few lines:  :::::  [20160425t19:25:59.824660z]     jobs.run(postfunc = jobspostfunc)  :::::  [20160425t19:25:59.824699z]   file ""/users/wmwv/lsstsw/stack/darwinx86/scons/2.3.5/lib/scons/scons/job.py"", line 113:  :::::  [20160425t19:25:59.824709z]     postfunc()  :::::  [20160425t19:25:59.824752z]   file ""/users/wmwv/lsstsw/stack/darwinx86/scons/2.3.5/lib/scons/scons/script/main.py"", line 1294:  :::::  [20160425t19:25:59.824767z]     scons.sconsign.write()  :::::  [20160425t19:25:59.824808z]   file ""/users/wmwv/lsstsw/stack/darwinx86/scons/2.3.5/lib/scons/scons/sconsign.py"", line 109:  :::::  [20160425t19:25:59.824816z]     none  :::::  [20160425t19:25:59.824869z]   file ""/users/wmwv/lsstsw/stack/darwinx86/scons/2.3.5/lib/scons/scons/dblite.py"", line 116:  :::::  [20160425t19:25:59.824878z]     none  :::::  [20160425t19:25:59.935601z] exception ioerror: (24, 'too many open files', '.sconsign.tmp') in /> ignored      possibly only happens on osx?",2,val
DM-5847,libxml build issue with mpich on OS X,"on os x with xcode installed mpich fails to build because it can not locate the libxml include files:      cc       topologyxmllibxml.lo   topologyxmllibxml.c:17:10: fatal error: 'libxml/parser.h' file not found   #include /            ^   1 error generated.    with pkgconfig 0.29.1 installed. the problem is that configure determines that libxml2.0 is available and is installed into /usr with a cflags of i/usr/include/libxml2. configure does not itself test whether those parameters are reasonable. with xcode there are no files installed into /usr/include and clang knows to look in specific sdk locations. when mpich builds it assumes that libxml2 can be found but fails to find it.    strangely, pkgconfig v0.28 does not seem to be able to find libxml 2.0 so there is no issue.    one solution is to install the command line tools but it might be more portable to attempt to disable libxml2.  ",2,val
DM-5848,"Investigate Jupyter internals, interactive widgets","in preparation for linking jupyter notebooks with firefly and other suit components, read jupyter documentation. learn how to build a sample widget or interactive dashboard in the jupyter framework",2,val
DM-5849,Investigate Ginga and Glueviz visualization tools,"ginga and glue (glueviz) are community visualization tools in python. become familiar with the capabilities of both, thinking from the point of view of using firefly for the display but using python for many other things.",2,val
DM-5854,Java array index out of bound error in VisSeverCommand.java,"the class filefluxcmdjson in visservercommand.java is calling                 string[] res = visserverops.getfileflux(fahary, pt);      however, when the mouse is outside the image, the visserverops.getfileflux(fahary, pt) returns:    new string[]    it is fine for a single band.  however, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands.      jsonobject obj= new jsonobject();              obj.put(""json"", true);              obj.put(""success"", true);                int cnt=0;              jsonobject data= new jsonobject();              for(band b : state.getbands())               data.put(""success"", true);      thus,  res\[cnt\] caused array index out of bound error.     to fix this issue, the for loop is changed as below:                          int cnt=0;              jsonobject data= new jsonobject();              band[] bands = state.getbands();              for (int i=0; i<res.length; i)              data.put(""success"", true);                jsonarray wrapperary= new jsonarray();              obj.put(""data"", data);              wrapperary.add(obj);      when the mouse is outside the image, the res returns a new string\[\]\, it is added to the jsonobject only once.  ",1,val
DM-5857,Make DipoleFitPlugin mask-safe,"the dipolefitplugin does not correctly handle bad pixels and other masks/flags. make it so it does so, and make tests to ensure it does so.",6,val
DM-5858,LSST the Docs Production Fall 2016,"dm5404 introduced lsst the docs as a production platform for continuous documentation delivery. this epic covers additional improvements to the platform, such as     implementation of a backup system for lsst the docs’ db      edition and build dashboards at the /v/ and /builds/ directories that help users find the appropriate version of the documentation site. these could be rendered with react from the api. this also serves as a ramping up exercise on ui elements that will be used on the squash dashboard and dochub, so learning time has been rolled into the estimate",42,val
DM-5859,Table: Add keyboard navigation, added arrow up/down to move between rows.   added page up/down to move between pages.     fixed table loading mask not showing   fixed pagingbar rendering more than it should    fixed annoying standardview missing unique key warning,2,val
DM-5860,Create obs_monocam,make a package to hold the description of moncam.,10,val
DM-5863,Minor tweaks to Cython and pybind11 tech notes,"i'll be making some superficial changes to the text of dmtn13 and dmtn14 for grammar, while updating links to the pythoncppchallenge repo (which has just moved from my private github to lsst dm).",1,val
DM-5864,Improve Qserv CI using multinode tests,"here's some tracks:    1. run multinode integration tests during qservdistrib ci build.  in order to do that we could create a qservtestmultinodes repository containing a build script which would launch multinode tests (for example see travis.yml)  i can do this on my side but i'll require a recent version of docker on the build machine.    2. i'll need your help to do next step:  each time command below succeed:    rebuild qservdistrib    publish this build to eups web repository and docker hub by running:    # bxxx is the build id and is available at the bottom of rebuild command standard output  publish b bxxx t qservdev qservdistrib  # then create and publish to docker hub image ""qserv/qserv:dev""  # which embed current build products and is used for all qserv deployment  # (baremetal, openstack, travis, developper machines)  $qservdir/admin/tools/docker/2builddevimage.sh      as a todo list, here's what could be done in an additional ticket, in the long term:      run multinode integration test inside jenkins instead of travis?     travis/gitub integration is very good, so i'm not sure this feature is still an active concern?     on the other hand travis has to download qserv/qserv:dev at each build, and if there's a timeout here, the build sometime fails. i don't know if this image can be cached in travis free version?      current procedure doesn't support yet tickets branch accross multiple repositories, (like qservxrootdqserv_testdata for example). do you think this feature would be easier to implement in jenkins?",10,val
DM-5866,Test lsst.log with pipeline tasks,"try to use lsst.log instead of lsst.pex.logging for a few science pipeline tasks, based on log u/ktlim/getlogger branch and dm3532. look into rfc29.",10,val
DM-5868,Literature research on image subtraction algorithms,"we need to get a good understanding of where the image subtraction implementation in the stack currently stands. this first requires an uptodate assessment of the literature, including becker et al. (2012), and zogy (2016). also, the ""preconvolution"" step.",8,val
DM-5869,Assessment of current state-of-the-stack diffim implementation,"the existing diffim implementation in the stack defaults to the (2000) version of the alard/lupton algorithm. other recent improvements such as ""preconvolution"", deltafunction basis, model selection via bic, others, seem to be implemented but are not turned on. we need a good understanding of the existing implementation so we can assess how straightforward it is to implement the zogy algorithm in real space in the stack.",6,test
DM-5870,Update testdata_subaru to support calib changes,merging dm 5124 broke obssubaru because the test data in testdatasubaru wasn't updated.  fix it.,1,test
DM-5872,"Incorporate ""Bickerton algorithm"" for detecting & masking satellite trails","in https:/hscjira.astro.princeton.edu/jira/browse/hsc 1272,  proposed an algorithm for detecting and masking satellite trails. this has undergone some review on hsc, but has never been incorporated into an hsc software or data release (and hence is not part of the ""hsc port"").    however: the algorithm is certainly relevant to lsst. please convert it to work with the lsst stack.",5,test
DM-5874,Produce document describing flavors of coadds," has requested a description of the different flavors of coadds, and the tradeoffs between the more experimental optimal coadds and the nonoptimal standard ones.    i'll try to do this as both a presentation for the dmlt and a dmtn (with essentially the same content).    this will include a bit of toymodel simulation work to try to predict some of the tradeoffs; this could make the discussion quite a bit more quantitative and i have an idea for how to do it that is pretty easy.",10,test
DM-5875,Propose text for alternate galaxy models in DPDD,write a paragraph or two describing alternatives to the constrained bulge+disk model currently in the dpdd.,1,test
DM-5877,Use Afterburners to clean up aperture correction logic,"this issue has several components; i'm combining them into a single issue because they need to be done atomically:    rewrite the base_classificationextendedness singleframeplugin/forcedplugin as an afterburnerplugin (and remove the old versions).    move the ""applyapcorr"" subtask out of singleframemeasurementtask and forcedmeasurementtask, making it instead a subclass of their parent tasks.     add afterburner subtask stages to processccdtask (within detectandmeasuretask) and the multiband tasks wherever measurement is currently being run.  the afterburner tasks should be run after aperture corrections are measured and/or applied.    after these changes, throughout the stack, whenever a measurementtask is run, we also run applyapcorrtask and afterburnertask (in that order), while possibly running measureapcorrtask immediately after the measurementtask.    this may or may not enable significant cleanups in detectandmeasuretask (i haven't looked closely).  if so, they should be done on this issue.    given all the moving parts, it's important to check that the actual behavior of the pipeline (in the aperture correction and extendedness values) does not change, so it might be useful to start by creating some reference outputs to compare against.",6,test
DM-5878,Add chi plots to validate_drp output to compare nominal error,"make histograms of deltas / nominal error.    where ""nominal"" error is that reported by the pipeline.    are they distributed with a sigma=1?",2,test
DM-5879,Remove use of Boost smart pointers throughout the Science Pipelines,replace all use of boost smart pointers through the stack with their standard library equivalents.    this will require an rfc.,10,test
DM-5880,Audit use of Boost in the stack and remove it where possible,"consider use of boost in the stack, and investigate where it can be eliminated by using std library equivalents. create tickets to remove it, then work on them.",10,test
DM-5881,Recover from processCcd refactor,"now that dm5771 is closed and processccd runs again, get back to the point where the things are not just not crashing but are actually working correctly (given that this is a nonstandard use case).",10,test
DM-5884,Create MySQL account for monitoring,a mysql account needs to be created in configuration procedure and on existing data on in2p3 cluster in order to enable elk access. mysql password/secret has to be shared accross all containers.,4,test
DM-5885,Create a JSON file for monitoring stack,"create a json or yaml file with:     qserv version   libraries/deps version    other idea welcome    this will interesting ""group by"" in monitoring tool (performance for each qserv or xrootd version for example)",3,test
DM-5886,Provide ngmix-based MCMC galaxy fitting,"dm 2250 provided for ""simple"" fitting of galaxies in single frame measurement using ngmix. extend this to fit galaxies using ngmix's mcmc sampling facilities. this may include defining a mechanism to store mcmc samples in source records.    this should be available through (an) lsstapps (or distrib) based meas_extension package(s).",60,test
DM-5889,"Suppress gcc warnings about ""unused local typedefs""","we should add \wno\unused\local\typedefs to our gcc options.  this cleans up the build significantly, because there's a flood of warnings of this type coming from boost.  if we suppress those, it might become possible to notice warnings that we care about.",1,test
DM-5893,LSST the Docs Fastly should redirect /en/latest/ to /,"previously we deployed documentation on read the docs. by default, read the docs would show the master version of documentation on ""/en/latest/"". many links with that endpoint may already exist. we should configure fastly to redirect such paths to ""/"".",1,test
DM-5894,LSST the Docs Fastly Courtesy Redirects for directory paths,currently if a user browses example.lsst.io/v/main/somedirectory instead of example.lsst.io/v/main/somedirectory they will receive an error.    we should develop a scheme where fastly can detect that such a path is a directory and redirect to the directory's index.html page.,4,test
DM-5897,Robustify coadd,"in running the processing from the twinkles data challenge in desc we found that it was very easy to use the wrong skymap when making a coadd.  since the coadd code doesn't even make a cursory check that the coordinate system it is using is the same as that of the coaddtempexps, it is very possible to mess this up.    adding a check that the coadd wcs is that of each input tempcoaddexp is would solve this.",2,test
DM-5898,Python EUPS package can use $PYTHON,the python eups package has a script that checks that the python being used is version 2.7. this script can optionally check $python rather than the python in the path but i am confused as to what that test is going to do for us. the problem is that sconsutils uses python and most of the shebangs use /bin/env python (although shebang rewriting on all platforms could help with that). i think the check script should have the $python support removed due to excessive confusion.    it would also help if the check script worked with python 3 so that the wrong python could be caught.,1,test
DM-5900,Create psutil EUPS package,add the python psutil package to the stack as python_psutil.,1,test
DM-5901,LTD Keeper: More Robust Edition purges,ltd keeper needs to purge fastly when an edition is rebuilt. currently the surrogatekey for the build is also used to cover editions. this means that the key needed to purge an edition is the same as that for an build. hence purging an edition means that the system needs to purge the surrogate key of the previous build.    we're seeing situations where the surrogate key that keeper is purging is not the one that needs to be purged. a more robust configuration would be for each edition to have a stable surrogatekey that can be unambiguously purged.    this ticket covers the following work:    # diagnose the issue.  # enable alembic migrations for flask (flaskmigrate)  # add a surrogatekey column to the edition model  # change the s3 copy rebuild code to change the surrogatekey header  # change the rebuild code to purge based on the edition's surrogatekey.,3,test
DM-5903,Finish technical note on galaxy shear experiments,"in the review of dm 5447 we decided it made sense for  to take over finishing the technote, in particular providing an introduction and concluion with more context.",4,test
DM-5904,Create focus script,"in dm3368, we stripped out the focus calculation since it's not camerageneric, and the scatter/gather isn't necessary for general processing.  we need to reinstate the focus calculation in its own scatter/gather script.",2,test
DM-5905,Refactor DipoleFitPlugin classification into separate Classification plugin,"currently the new dipolefitplugin runs measurement and then classification from a single measurement method. the classification should be moved out to a separate plugin. this will require more information be stored in the measrecord, in order to do the classification separately. given that complication, evaluate whether this is even worthwhile.",6,test
DM-5906,Remove qmeta::QueryId and use global qserv::QueryId,remove the qmeta::queryid and use the typedef of queryid in global/inttypes.h instead. also try to verify that queryid is used instead of uint64_t where applicable.,6,test
DM-5907,Replace the heap in ScanScheduler with a list.,"scanscheduler is using heaps to order tasks by chunkid. this makes it difficult to add tasks to actively running chunks, which causes a significant delay to the start of query execution. using a list of buckets of tasks where each bucket is for one chunkid will make it easy to add tasks to chunks being. within each bucket it will still be necessary to order tasks by tables used in the query.",9,test
DM-5908,Alter the worker thread pool to allow threads to leave the pool and continue.,there are times when it is desirable for a thread to continue but effectively leave the thread pool and be considered finished by the scheduler. util::threadpool needs to modified to do this.,9,test
DM-5909,"After the first result set is returned, have the thread leave the pool.","when large results are returned from the worker to the czar, the thread should leave the thread pool and the task should indicate to the scheduler that it is done.   ",6,test
DM-5910,Add code to the czar to throttle incoming large results.,the czar needs code to limit the number of tasks sending back large results at any given time.,9,test
DM-5911,Fix circular references in Mapper objects,"whilst running tests with pytest and the new file descriptor leak checker it became clear that mapper objects were not freeing their resources when they were deleted. in particular, the registry objects remained and the associated sqlite database files were opened. this led to pytest running out of file descriptors when large test suites were being executed.    the problem turns out to be the dynamically created map functions. these are created as functions (not bound methods) attached to an instance. since they are not bound methods the instance object (self) has to be passed in to closure. this leads to self containing a reference to a function that contains a reference to self and this prevents the mapper from ever being garbage collected (leading to all the resources being retained).    a short term fix is pass the mappers into the closures using weakref.    eventually it would be nice to consistently make the map_ items bound methods rather than attaching them as functions but that is beyond the scope of this ticket.",1,test
DM-5912,"Add ""everything"" scan",add a low priority scanscheduler to the worker to handle very slow scans or scans that do not  work well on the other schedulers.,3,test
DM-5913,Add ability for workers to switch slow queries to the everything scan.,give the workers the ability to move user queries to the everything scan if they are taking too long to complete a task or several tasks.,9,test
DM-5914,Document planned implementation of toy model of Lupton(ZOGY),"develop a better understanding of the planned implementation of zogy in real space by implementing the kernel correction in kspace and investigating its characteristics when transformed back into real space. do this either symbolically (if possible) or numerically in an ipython notebook. first in 1d, then in 2 d, both assuming a constant kernel. include documentation in the ipython notebook describing the current understanding of how this will be implemented",6,test
DM-5915,Decide how to rework afw:Wcs guts with AST,"following the tobewritten recommendation for dm 4157, we plan to rework the guts of afw:wcs to use ast. we need to decide how afw:wcs will use ast, whether as a wrapper or as a complete replacement with ast.    the product is a design",8,test
DM-5916,Decide how to rework XYTransform/GTransfo guts with AST,"we want to better connect our other transforms with the wcs system, which means reworking the guts of xytransform/gtransfo to work with ast. this could involve making one or both of them a wrapper, complete replacement, or writing a converter that turns our transform object into an ast map or frameset.    the product is a design",10,test
DM-5917,Design an API for the new Wcs and Transform system,"we need to design a new api for the wcs/transform system. this is somewhat independent of the question of how the lowlevel code is used: we want a clean and simple api that lets the components of the stack create, manipulate, use, and persist the necessary transformations. related to this question is whether we will still need skytopixel/pixeltosky or whether the necessary operations with those can be subsumed into some frametoframe transformation (e.g. pixeltopixel or tanto tan).    the product is an rfc",16,test
DM-5918,What transforms do we currently need?,"in order to use ast in the stack, we may need to add mappings to it. we also need to be able to describe our transforms at a high level so that we know how to create them.    we need a list of the currently necessary transformations (e.g. from afw:wcs, xytransform, gtransfo and any other relevant stack packages), and some concrete ideas about the kinds of transforms we may need in the future. these should be described in a highlevel mathematical manner, independent of our wcs/transform system.    this can be informed by dmtn005 and the requirements section of dmtn 010",4,test
DM-5919,Describe our composite mappings and transformation endpoints (Frames),"to use ast in the stack, we need to be clear what our different transformations (ast:mappings) and endpoints (ast:frames) are going to be so we can create the chain of transformations (ast:framesets) that will be used throughout the stack. this applies to both images and camerageom. we may want to produce similar descriptions for other stack objects.    this ticket is the highlevel frames equivalent to the mathematical mapping description in dm5918.    this will help us determine how we can put our current input/output image frames into the new system.",4,test
DM-5920,Create DCR metric using new dipole measurement,"in order to evaluate dcr correction algorithms we need a metric that defines the severity of dcr in a residual image. this ticket is to run the new dipole measurement task on simulated difference images affected by dcr, and to define a useful metric. the result will be a brief technical note defining the process and the metric, with a few examples.",6,test
DM-5921,Clarify how to work with ci_hsc's astrometry_net_data,"cihsc's readme.rst contains https:/github.com/lsst/cihsc/blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec/readme.rst#referencecatalog on declaring & setting up the included reference catalogue data.    i believe this was rendered obsolete by dm5135, which automatically sets up the reference catalogue when ci_hsc itself is set up. attempting to follow the documentation therefore produces confusing warning messages, and may break things.    please check if my understanding is correct and, if so, fix the documentation.",1,test
DM-5922,Rework camera geometry to use the replacement for XYTransform,"as part of overhauling xytransform we will likely need to replace the way we describe the transformations supported by camera geometry and detector. this is likely to include a new way of describing the coordinate frames (e.g. pixel. focal_plane and pupil).    if we adopt ast (as seems likely) then these frames will be ast frames, the transforms will be ast mappings and the collection described by camera and detector will be one or more ast framesets.    an rfc for the redesigned api for camera geometry will be required and this ticket is to implement the resulting design.",8,test
DM-5923,Support arbitrary sky rotation angles in StarFast,"currently, if a region of sky is simulated in starfast the stars must always have the same x,y coordinates (before dcr effects). this ticket is to support arbitrary rotations and offsets of the simulated stars to mimic realistic repeated observations of the same field.",2,test
DM-5924,Improve overscan correction,"overscan correction can be improved.  specifically, some systems have sharp discontinuities in the bias section.",6,test
DM-5925,Implement fringe correction in ISR,there is an initial implementation of fringe correction in the obs_subaru package.  it should be ported and generalized.,6,test
DM-5926,networking in strange state for newly created instances,"when starting a new instance, occasionally something strange seems to happen with the  network setup.  the instance will come up but is inaccessible (icmp, ssh). when this happens, the console log shows that a dhcp address was obtained and cloudinit injected sshkeys, so it isn't a total network setup failure.    i have seen this happen a few times in the last couple of weeks but i can't reliably reproduce it.  i'm wondering if neutron is logging anything interesting when this happens.    this failure mode happened  again a few minutes ago with 7adffa827221454cacfe5f21cdd34ea8.  which i killed and recreated as instance b6f64981099b46e5a27ee3694372f447 with the same private ip address.   the new instance is accessible as expected.",1,test
DM-5927,API errors when trying to start up multiple instances,i am attempting to start up 20 m1.medium instances without floating ips to take available of the new instance cap from dm5840.  this consistently fails after starting a few instances with an http 403.      error creating openstack server: expected http response code  field                                     manual                                                                                                                                               osextaz:availabilityzone             0                                                                                                                                                    osextsts:taskstate                   error                                                                                                                                                ossrvusg:launchedat                  none                                                                                                                                                 accessipv4                                                                                                                                                                                   addresses                                                                                                                                                                                    created                                                                                                                                                    flavor                                  b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700                                                                                             id                                      centos7slurm20160422210744 (7364ada7263e4fb0a9f4219ab19e0be0)                                                                                 keyname                                slurmslave4                                                                                                                                         osextendedvolumes:volumesattached    8c1ba1e0b84d486fbe7a665c30030113                                                                                                                     properties                              error                                                                                                                                                updated                                 83bf259d1f0c4f458e03f9002f9b4008                                                                                                                    |   +        ,2,test
DM-5928,April 2016 LAAIM work,"drafted documentation for web sso capabilities: https:/confluence.lsstcorp.org/display/laaim/web+sso  began testing new ncsa iam capabilities (group management, user selfregistration).  registered ncsa with incommon as a suborg of uiuc to ease future idp/sp registrations.  attended local ncsa lsst coordination meetings.",4,test
DM-5929,April Work for ConOps,"work on developing, editing and providing feedback for various conops.  converted existing conops to new format.",33,test
DM-5930,Replace exiting DipoleMeasurementTask with DipoleFitTask,"the goal of this ticket is to replace the existing dipolemeasurementtask with the new  dipolefittask, subsequent to ticket dm 5413.    tbd: does this include completely removing all remnants of dipolemeasurementtask code?",6,test
DM-5931,Test planned implementation of Lupton(ZOGY) algorithm in real space,develop a (1d?) simple toy model and test the effects of the correction for varying i1 and i2 noise levels and different image psfs and matching kernel(s). this will be done first in an ipython notebook.    see dm5914.,6,test
DM-5932,Trial implementation of Lupton(ZOGY) in stack,the lupton reinterpretation of the zogy algorithm in realspace is essentially a postconvolution that implements noise whitening (or decorrelation) of the image difference. we will make a first pass at implementing this in the existing diffim codebase in order to perform future evaluations on real data.,16,test
DM-5933,Replace jointcal.StarSelector with meas_algorithms.starSelector,jointcal has its own custom star selector. this should be removed and replaced with a star selector based on measalgorithms.starselector. a good choice might be measalgorithms.objectsizestarselector.,2,test
DM-5934,Update developer guide with Astropy guidance,once rfc 178 is adopted the developer guide has to be updated to include guidance as to how astropy can be used in the stack (similar to how boost is documented).,1,test
DM-5935,Package Astropy for the stack,once rfc 178 is adopted astropy needs to be packaged in an eups container. given the complexity of astropy dependencies the packaging will be done as for numpy and scipy by checking that astropy is available (v1.1 will be the minimum version).,1,test
DM-5936,Make afw rgb unit test PEP440 compliant for matplotlib check,"if a user has a version of matplotlib installed from a git clone, the afw rgb unit test fails at the matplotlib version check. the versioning scheme for this type of install is determined by pep 440. make the unit test properly handle this type of version comparison.",1,test
DM-5937,April work for middleware,participated in requirements definition,1,test
DM-5938,Redirect non HTTPS requests on LSST the Docs to TLS,see https:/docs.fastly.com/guides/securingcommunications/allowingonlytlsconnectionstoyour site,1,test
DM-5939,Pre-release versions of matplotlib 2.0 break afw unit tests,"in the afw rgb unit test, testwritestarslegacyapi checks to make sure that a file name with an unknown extension raises a value error. in current version of matplotlib, saving a file with an unknown extension causes this error:      valueerror: format ""unknown"" is not supported.  supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff.      in matplotlib 2.0 prerelease the file is saved as a png when an unknown extension is specified. since the write call success the unit test fails as it is expecting a failure.     if nothing depends on this behavior, the unit test should probably be removed.",1,test
DM-5940,Create new build based on the converted firefly code.," remove all of the gwt code except for a few remaining files.   create separate build for the new firefly viewer, leaving the old fftools as it was before the js conversion.    repackage files as needed moving forward.",8,test
DM-5941,Private network not available across all instances,"i'm setting up an elk system. part of that is an elasticsearch system. when i bring up the system the private network is bisected. i attempted creating a security group, in case that was a problem but it didn't help. note that the work around is to create security groups or use a firewall and use floating ips. this is far from ideal. i think the right solution is to use the private network.    example:    first section pes1 pes3 pesk      vagrant@es1:$ ifconfig  ens3      link encap:ethernet  hwaddr fa:16:3e:47:28:a7            inet addr:10.0.42.30  bcast:10.0.42.255  mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe47:28a7/64 scope:link            up broadcast running multicast  mtu:1454  metric:1            rx packets:363265 errors:0 dropped:0 overruns:0 frame:0            tx packets:304215 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            rx bytes:95396177 (95.3 mb)  tx bytes:238466304 (238.4 mb)    lo        link encap:local loopback            inet addr:127.0.0.1  mask:255.0.0.0            inet6 addr: ::1/128 scope:host            up loopback running  mtu:65536  metric:1            rx packets:850 errors:0 dropped:0 overruns:0 frame:0            tx packets:850 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            rx bytes:138411 (138.4 kb)  tx bytes:138411 (138.4 kb)    vagrant@es1:$ ping 10.0.42.32  ping 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  64 bytes from 10.0.42.32: icmpseq=1 ttl=64 time=0.284 ms  64 bytes from 10.0.42.32: icmpseq=2 ttl=64 time=0.266 ms  64 bytes from 10.0.42.32: icmpseq=3 ttl=64 time=0.265 ms  64 bytes from 10.0.42.32: icmpseq=4 ttl=64 time=0.302 ms  c   10.0.42.32 ping statistics   4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms  vagrant@es1:$ ping 10.0.42.34  ping 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  64 bytes from 10.0.42.34: icmpseq=1 ttl=64 time=0.333 ms  64 bytes from 10.0.42.34: icmpseq=2 ttl=64 time=0.325 ms  64 bytes from 10.0.42.34: icmpseq=3 ttl=64 time=0.322 ms  64 bytes from 10.0.42.34: icmpseq=4 ttl=64 time=0.319 ms  c   10.0.42.34 ping statistics   4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms  vagrant@es1:$ ping 10.0.42.31  ping 10.0.42.31 (10.0.42.31) 56(84) bytes of data.  from 10.0.42.30 icmpseq=1 destination host unreachable  from 10.0.42.30 icmpseq=2 destination host unreachable  from 10.0.42.30 icmpseq=3 destination host unreachable  c   10.0.42.31 ping statistics   4 packets transmitted, 0 received, 3 errors, 100% packet loss, time 3017ms  pipe 3  vagrant@es1:$ ping 10.0.42.33  ping 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  from 10.0.42.30 icmpseq=1 destination host unreachable  from 10.0.42.30 icmpseq=2 destination host unreachable  from 10.0.42.30 icmpseq=3 destination host unreachable  c   10.0.42.33 ping statistics   4 packets transmitted, 0 received, 3 errors, 100% packet loss, time 3008ms  pipe 3  vagrant@es1:$ ping 10.0.42.35  ping 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  from 10.0.42.30 icmpseq=1 destination host unreachable  from 10.0.42.30 icmpseq=2 destination host unreachable  from 10.0.42.30 icmpseq=3 destination host unreachable  c   10.0.42.35 ping statistics   5 packets transmitted, 0 received, 3 errors, 100% packet loss, time 3999ms  pipe 3      second section pes2 pes4 plfr      vagrant@es2:$ ifconfig  ens3      link encap:ethernet  hwaddr fa:16:3e:6f:30:2c            inet addr:10.0.42.31  bcast:10.0.42.255  mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe6f:302c/64 scope:link            up broadcast running multicast  mtu:1454  metric:1            rx packets:196344 errors:0 dropped:0 overruns:0 frame:0            tx packets:160561 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            rx bytes:47667399 (47.6 mb)  tx bytes:7135345 (7.1 mb)    lo        link encap:local loopback            inet addr:127.0.0.1  mask:255.0.0.0            inet6 addr: ::1/128 scope:host            up loopback running  mtu:65536  metric:1            rx packets:97268 errors:0 dropped:0 overruns:0 frame:0            tx packets:97268 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            rx bytes:8558096 (8.5 mb)  tx bytes:8558096 (8.5 mb)    vagrant@es2:$ ping 10.0.42.33  ping 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  64 bytes from 10.0.42.33: icmpseq=1 ttl=64 time=0.311 ms  64 bytes from 10.0.42.33: icmpseq=2 ttl=64 time=0.309 ms  64 bytes from 10.0.42.33: icmpseq=3 ttl=64 time=0.300 ms  c   10.0.42.33 ping statistics   3 packets transmitted, 3 received, 0% packet loss, time 2000ms  rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms  vagrant@es2:$ ping 10.0.42.30  ping 10.0.42.30 (10.0.42.30) 56(84) bytes of data.  from 10.0.42.31 icmpseq=1 destination host unreachable  from 10.0.42.31 icmpseq=2 destination host unreachable  from 10.0.42.31 icmpseq=3 destination host unreachable  from 10.0.42.31 icmpseq=4 destination host unreachable  c   10.0.42.30 ping statistics   5 packets transmitted, 0 received, 4 errors, 100% packet loss, time 4014ms  pipe 3  vagrant@es2:$ ping 10.0.42.32  ping 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  from 10.0.42.31 icmpseq=1 destination host unreachable  from 10.0.42.31 icmpseq=2 destination host unreachable  from 10.0.42.31 icmpseq=3 destination host unreachable  from 10.0.42.31 icmpseq=4 destination host unreachable  from 10.0.42.31 icmpseq=5 destination host unreachable  c   10.0.42.32 ping statistics   5 packets transmitted, 0 received, 5 errors, 100% packet loss, time 4023ms  pipe 3  vagrant@es2:$ ping 10.0.42.34  ping 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  from 10.0.42.31 icmpseq=1 destination host unreachable  from 10.0.42.31 icmpseq=2 destination host unreachable  from 10.0.42.31 icmpseq=3 destination host unreachable  c   10.0.42.34 ping statistics   4 packets transmitted, 0 received, 3 errors, 100% packet loss, time 3006ms  pipe 3  vagrant@es2:$ ping 10.0.42.35  ping 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  64 bytes from 10.0.42.35: icmpseq=1 ttl=64 time=0.387 ms  64 bytes from 10.0.42.35: icmpseq=2 ttl=64 time=0.278 ms  64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms  c   10.0.42.35 ping statistics   3 packets transmitted, 3 received, 0% packet loss, time 1998ms  rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms      this can be reproduced by sourcing your openstack credentials and running this https:/gist.github.com/jmatt/7b6eb6a042c4e63531d40d1a68069f33. use vagrant ssh pes1 to connect to the pes 1 instance.  ",2,test
DM-5942,Public elasticsearch configuration using SSL/TLS and basic auth,create a (relatively) secure way to use elasticsearch from outside of nebula using ansible. see:    https:/ nginx    note that this will not allow another elasticsearch to join but will allow the usual admin and client queries using basic auth.,1,test
DM-5943,Add Git refs to Jobs Table of QA Dashboard,the level 0 qa db should know what stack git refs correspond to each job. this will enable plots to filter jobs based on development ticket so that a developer can understand how a branch compares to master.    this ticket will add jobs to the schema (http:/sqr009.lsst.io/en/latest/#level0 qa) and create the necessary migration script.,5,test
DM-5945,Implement validate_drp plot in Bokeh as proof-of-concept for QA Dashboard,this ticket will implement a plot from validatedrp in the qa dashboard as a proofofconcept for how existing matplotlib plots can be re implemented in bokeh with data from the qa database.    stretch goals (maybe for a future ticket) will be to overplot the validatedrp output of one job against another’s to understand performance changes.,2,test
DM-5946,SUIT design ,"finish suit design, produce a design document",75,test
DM-5947,Jupyter widget using Firefly,"jupyter widget using firefly visualization components.   to better support the user community in using jupyter notebook with python packages, we want to start exploring the process of creating jupyter widget, using firefly visualization capabilities. ",26,test
DM-5948,Workspace design,workspace design     ,30,test
DM-5949,Testing framework setup and more unit test,testing framework setup and more unit test code   ,45,test
DM-5951,Make obs_subaru PEP8 (pyflakes) compliment,"running pyflakes on obs_subaru revels many places where the code is not (lsst specific) pep8 compliment. actual coding bugs reviled by pyflakes were fixed in dm5474, however many of the formatting issues need to be fixed. once dm4740 and dm 4668 are done, all remaining code should be brought into coding standard compliance.",3,test
DM-5952,Initial draft(s) of the Data Backbone ConOps,"initial drafts of the data backbone concept of operations. versions 0.1, 0.2. produced document that is ready for friendly, internal review although document is not complete.",8,test
DM-5953,Internal review of Data Backbone ConOps and revision(s),nan,2,test
DM-5954,Watch Boot Camp materials,videos from the https:/community.lsst.org/t/dmbootcamp announcement/249 cover a lot of topics a newbie like me is interested in.,4,test
DM-5955,Build LSST Software Stack from the source,create a virtual machine with functioning lsst software stack to have an environment where i can see and play with its code.,3,test
DM-5958,Integration Environment: top level design,"working with sui/qserv teams to create a plan that includes deployment schedules, levels of support, discussions of administrative requirements in integration environment, detailed documentation before procurement, security reviews, reviews with deployment team.",26,test
DM-5959,Integration Environment: Procurement,discussions with vendors. quote selection. budget tracking. quote submission to finance. gco follow up questions. obfs follow up questions. finance follow up questions. overall tracking of purchase progression.,10,test
DM-5960,Node delivery to racking,"node delivery acceptance, unpacking, inventory, rack builds, pdu placement, node racking, bios updates, power connections, ",16,test
DM-5961,Networking,"unpacking networking equipment, inventory, ordering/procure/unpack cabling, equipment racking, cabling, config creation and management.",6,test
DM-5962,OpenStack deployment,"this activity is outside of lsst project; this story is a placeholder for demonstrating progression toward completion of the epic.    some activities that are likely to occur:  software installation, os installation/updates, raid configurations, software configuration, monitoring and notification system(s) installation and configuration, integration testing and friendly uses evaluation. ",24,test
DM-5963,Compute Upgrade,discussions and planning for the creation of an lsst service only tenant for better isolation (hence protection) from ad hoc services. further discussions with lsst dm interested stakeholders about this feature. policy development for use and monitoring of use of this feature.,10,test
DM-5964,Object Storage Installation,discussions and planning for allocating the 1pb storage increase within nebula between object storage and block storage. further discussions with lsst dm interested stakeholders about this feature. policy development for use and monitoring of use of this feature.,5,test
DM-5966,Remove use of Boost smart pointers in meas extensions,"removal of boost smart pointers in dm 5879 missed some meas extensions which are not built as part of lsstdistrib. namely: measextensionsshapehsm, measextensionssimpleshape and measextensions_photometrykron.  update these too.",1,test
DM-5967,Provide docker swarm POC for Qserv containers orchestration,nan,10,test
DM-5968,Split secondary index loading from qserv_data_loader.py to separate unit test,"to isolate development and validation of secondary index loading strategies, encapsulate loading of ""pure"" secondary index data via qservdataloader.py, without using entire datasets.",8,test
DM-5969,Deploy any secondary index loader mods into qserv_data_loader.py,"following completion of dm5968, any modifications to the toplevel data loading procedure for the secondary index need to be deployed back to the main qservdataloader.py driver.",5,test
DM-5970,Slurm deployment preparation,installation and familiarization with the supported level of service of slurm installation. includes rolling that configuration into puppet modules/manifests. also includes documentation of the installation and configuration requirements.    this is monthofapril work; a new story continues into may.,20,test
DM-5972,Inventory to racking,"inventory, unpacking, bios updating, racking, rack building, pdu installation, power cabling, os installation, raid configuration, xcat deployment planning, puppet deployment planning, puppet module/manifest creation, puppet config versioning, cabling strategies discussions, additional cable purchases (minor).",30,test
DM-5973,Update developer guide with pytest guidance,now that dm 5561 explains how to migrate to pytest compatibility the developer guide must be updated to state how to use pytest in unittests.,5,test
DM-5976,Change SubtractBackgroundConfig.isNanSafe default to True, suggests that the default value for subtractbackgroundconfig.isnansafe be changed from false to true.,1,test
DM-5977,Create and deploy Beats for Logstash and Elasticsearch.,create and deploy beats for logstash and elasticsearch. these beats are used to transport logs and monitoring data to elk.    see: https:/,3,test
DM-5978,Miscellaneous Nebula service items for x16,"this story is for miscellaneous nebula service items that do not have individual lsstdm jira issues (often handled in the rt ticket system)  including account creation requests, reporting on hanging/errant processes for cleanup, response & communiques on security incidents, etc. ",8,test
DM-5979, tests in testArgumentParser.py fail Jenkins run-rebuild on nfs,"(1) testoutputs fails because paths are compared literally  jenkins runrebuild #139 failed with pipebase  https:/ci.lsst.codes/job/runrebuild/139/console    fail: testoutputs (main.argumentparsertestcase)  test output directories, specified in different ways    traceback (most recent call last):    file ""tests/testargumentparser.py"", line 497, in testoutputs      self.assertequal(args.input, datapath)  assertionerror: '/nfs/home/lsstsw/stack/linux64/obstest/201601.03gafa6dd010/data/input' != '/home/lsstsw/stack/linux64/obstest/201601.03gafa6dd010/data/input'      please make the comparison more robust.     (2) file descriptor leaks  jenkins runrebuild #138 failed with pipebase  https:/ci.lsst.codes/job/runrebuild/138/console    fail: testfiledescriptorleaks (lsst.utils.tests.memorytestcase)    traceback (most recent call last):    file ""/home/lsstsw/stack/linux64/utils/201601.02g97a6e33/python/lsst/utils/tests.py"", line 133, in testfiledescriptorleaks      self.fail(""failed to close %d files"" % len(diff))  assertionerror: failed to close 1 files        file open: /nfs/home/lsstsw/build/pipebase/.nfs000000000a20a3f700005679    this test passes on local disk.  ",2,test
DM-5980,update jenkins to 2.x,nan,10,test
DM-5983,Stop cleanly MySQL if configuration step fails,next scripts doesn't stop cleanly mysql if configuration step fails:      admin/templates/configuration/tmp/configure/scisql.sh  admin/templates/configuration/tmp/configure/tools/sql loader.sh  ,2,test
DM-5984,Use RO MySQL account in qserv_testdata,"it seems integration tests datasets are now loaded with loader. so using mysql root account is no more required in integration tests, an account with select access on test databases (like qsmaster), should be enough.     furthermore, all code related to mysql writes can be removed from     python/lsst/qserv/tests/sql/cmd.py  python/lsst/qserv/tests/sql/connection.py  ",4,test
DM-5985,Add unicode support for Qserv password,qserv password must be encoded in ascii for now in qserv meta.conf. unicode passwords should be supported.,6,test
DM-5986,Use sagas in place of side-effects in chart-related controllers ,"replace sideeffects with saga and cleanup chart related controllers (tablestats, xyplot and histogram).",3,test
DM-5988,Support Monocam reduction,"monocam is being used on a telescope, and we want to reduce the data obtained.  this is made difficult by the fact that the camera and the telescope are not talking to each other so the usual header keywords are in separate files from the data.",6,test
DM-5991,A look at the overall performance of the application,investigate the overall performance of the application and improve it where possible.  it is pointed out that triview is especially slow compare to expanded.  need to investigate.,4,test
DM-5992,Reception and Placement,"receive, unbox, inventory, inspect, build racks, rack nodes and power.",10,test
DM-5993,Networking Configuration,"unbox, inspect, rack, power networking equipment, cables ordered, alternate cable purchase options tested, initial switch configuration(s), cabliing.",15,test
DM-5994,Provisioning,"os + updates installation, imaging building, stateless/stateful node provisioning, test of image, security configurations, networking bandwidth tuning, file system tuning, file system installation and tuning.",39,test
DM-5995,Disaster Recovery Implementation,testing/practicing recovery of node/image/software after various types of faults.,30,test
DM-5996,Documentation,document each component sufficient enough for transfer of knowledge and system recovery as needed.,5,test
DM-5997,Capability Validation,review that design was implemented successfully including recover and supporting documentation exists.,10,test
DM-5998,Security Vetting,review of capability by site security team,3,test
DM-5999,Acceptance by Stakeholders,"review with stakeholders (target users, release manager, others as necessary) to confirm that capability fulfills original requirements. ",2,test
DM-6004,Acceptance by Stakeholders,"review of services (compute, storage, networking) by lsst project before considering work final.",5,test
DM-6005,Procurement,nan,8,test
DM-6006,Reception and Placement,nan,3,test
DM-6007,Networking Configuration,nan,10,test
DM-6008,Provisioning,nan,30,test
DM-6009,Disaster Recovery Implementation,nan,20,test
DM-6010,Documentation,nan,15,test
DM-6011,Capability Validation	,nan,15,test
DM-6012,Security Vetting,nan,10,test
DM-6013,Acceptance by Stakeholders,nan,20,test
DM-6014,Reception and Placement,nan,2,test
DM-6015,Networking Configuration,nan,3,test
DM-6016,Provisioning,nan,10,test
DM-6017,Disaster Recovery Implementation,nan,4,test
DM-6018,Documentation,nan,4,test
DM-6019,Capability Validation,nan,5,test
DM-6020,Security Vetting,nan,6,test
DM-6021,Acceptance by Stakeholders,nan,6,test
DM-6022,Lazy load related chart data on table data update,"when new table data received, the related chart data should be updated only for the components on display. hidden components' data should be lazily updated when a component becomes visible.",3,test
DM-6025,ingest.py throwing away errors,"line 118 of ingest.py has a problem try block which is currently just throwing away errors, which has made for some confusing/frustrating debugging.    this should be changed to either warn or raise, but not silently dispose of errors.",1,test
DM-6026,Make it possible to distinguish TABLE_NEW_LOADED actions triggered by sort,"it would be beneficial to have in the tablenewloaded payload a  trigger field, which would differentiate actions triggered by sort (where  data do not change, only their order) or filter from other loads. we don't  need to reload table statistics or histogram on sort. but we do need to to  reload them on filter.      created table_sort action to distinguish sorting from filtering.  sorting should not reload xyplot nor catalog overlay.    also:   disable history when in api mode.   ensure tablemeta.source reflects the file on the server.   fix tablepaneloptions not resetting columns selection.   remove 'fits data' tab when no images available.    fix 'coverage' appearing when it should.",2,test
DM-6028,Validation is not performed on unchanged fields,"currently, validation is performed only if a field has changed. we need to be able to validate all fields on form submit.    the issue is not limited to initial (ex. empty) value being invalid. the invalid message is lost when a field is unmounted/remounted.    you can test the following way:   http:/localhost:8080/firefly/;a=layout.showdropdown?view=anydatasetsearch   open chart settings, enter 1000 into x/y ratio  the field is shown as invalid    switch to histogram and back, the invalid message is gone, the field appears to be valid    another test case is example dialog tab 'x 3', 'x 3'  tab test field initial value 88 is invalid (it should be between 22 and 23), but it appears valid. ",2,test
DM-6029,Error message is not shown,the error message is not showing consistently when mouse is over tha exclamation icon.,1,test
DM-6030,Investigate possibilty of cosmic ray muons (etc) for precision gain calibration,"in the era of cbps, we care about absolute system throughput, and thus need to accurately know the gain of amplifiers in the ccds.    initially, this can be done by labbased fe55 characterisation (modulo the nonlinearity, though that itself will need to be need to be characterised and corrected for), but changes in the relative gains of the various amplifiers need to be monitored, and this must be done in a way that is not degenerate with the optical transmission in any way.    theoretically it should be possible to use cosmic ray muon tracks, and tracks from radioisotope contamination of the glass/dewar, to measure the (change in the) relative gains of the amplifiers.    early work has shown that this does work in principle, but this ticket is for some further effort to see whether this method can provide the necessary accuracy given the amount of data available remains to be seen.    this ticket would normally need to be significantly more points, but as it builds on earlier work, it can, at least for initial results, be done quite cheaply.    initial investigation will inform further work.    1st order: histogram all pixels in dark images after careful bias subtraction. look at shape of spectrum. fit some arbitrary function, and correlate with fe55 gain measurements.    2rd order: separate muon tracks from soft electron tracks/nuclear recoil events. can cutting one of more of these types out improve the correlation? or can treating them separately improve the resolution?    3rd order: recalculate the histogram in terms of the de/dx for the muon tracks, i.e. taking into account their track lengths and thus angle through the silicon.",10,test
DM-6031,Create documentation for bright object masks,"the bright object mask code ported from hsc bought the ability to mask regions, during coaddition, by providing mask files. how to create these files, and where they should be placed in the file system is documented on and hsc ticket (https:/hscjira.astro.princeton.edu/jira/browse/hsc1351) but not on the lsst side. the information on how to create and use bright object masks should be put into lsst documentation.",3,test
DM-6033,Support sphinxcontrib-bibtex in technotes,allow bibtexbased references in technotes using https:/github.com/mcmtroffaes/sphinxcontribbibtex.    dmtn010 will be used as a pilot case.,1,test
DM-6036,Produce and ingest master calibs for USNO monocam data.,"use the construct .py scripts added to pipe_drivers to produce temporally relevant master biases, darks, flats (and fringe frames?) for the recent usno observing with monocam.    a small amount of hacking will be required due to the fact that the current ingestion model assumes that each ccd frame has a usno counterpart which tells about the telescope pointing etc, but the bias frames do not have these.    once the master calibs are produced, get them ingested.",2,test
DM-6037,Reduce sky data from USNO monocam run,"using the master calibs produced in dm6036, push all the monocam data through processccd.    others will run sanity checks on the output (initial astrometry & photometry). from there i believe people will look at using the data to test jointcal & sim_astrom etc, but this ticket just related to the initial reduction.    as more data comes in from the 2nd telescope, a little further hacking may be necessary to keep everything running. some of this will likely be hacky or need oneoff solutions/header modification, hence the higherthannormal number of story points assigned to what one might expect to be an hour long job.",3,test
DM-6038,Monocam bias structure analysis,"estimates of the noise in the bias frames coming from the usno monocam run around ~20e  rms. this is higher than with the same readout configuration in the lab, and could be due to several things.    this ticket is to take a look at a few bias frames and investigate the structure of the noise. if it is periodic and at a constant phase then using master biases will significantly improve the snr in the calexps produced, but if it is not, then whether or not bias frames should be used at all should be considered.",1,test
DM-6039,Download temporally relevant raw calibs for CTIO DECam data,"go to the noao portal and download a sufficient number of darks, biases and flats (in each band) from around the time of the ctio trip to produce master calibs.",1,test
DM-6040,Create and ingest master calibs for DECam CBP reduction,"having collected data in dm6039, push all this through the master calib creation scripts and ingest master calibs into registry.    this is a necessary but notnecessarilysufficient ingredient for making progress on dm5465.",2,test
DM-6041,Functional use cases for tools,"work with vandana desai (irsa) to tabulate science use cases for tools. then transform the science use cases to functional use cases (""this is how we want the tool/interface to behave"").    this work is  to identify common functional and scientific use cases between ptf/ztf and lsst to inform lsst on how the suit web portal might be organized for user interaction with lsst data. ",1,test
DM-6042,Add viewer launching API,"add ability to launch the viewer and load images, xyplots, and tables from the api. we use to call this firefly.getexternalviewer()  also, we have this concept of 'root path' through out the code. the api use can set a root path so he can use his when we are cross site. need to implement.  ",6,test
DM-6043,Change mouse readout to use supports MouseReadoutCntlr & add an API readout,"change mouse readout to use mousereadoutcntlr.      use verysimplemousereadout as a reference.    change to use mousereadoutcntlr for options instread of imageplotcnltr    add a second (vertical) mouse readout to be used in the api mode. the readout should be set into apiutilimage.jsx, initautoreadout.  it should replace verysimplemousereadout       & add an api readout",10,test
DM-6048,Bundle up more HSC data for validate_drp,we would like to include a larger set of hsc data for validation.  i tested this while in tucson.  my working dir was /tigress/pprice/frossie.  the raw and processed data should be stuffed into validationdatahsc,1,test
DM-6050,Table caching optimizations,"we need to avoid duplicate requests which result from minor differences in tablerequest parameters, which are not used to get data.  for example, loading catalog table, which triggers table statistics, and then getting an xy plot, results in 3 requests, returning identical data.    1. requestclass=serverrequest; tblid=tblid1; usertargetworldpt=10.68479;41.26906;eqj2000;m31;ned; searchmethod=cone; catalog=wiseallwisep3aspsd; requesteddataset=wiseallwisep3aspsd; radius=200; use=catalogoverlay; catalogproject=wise    2. requestclass=serverrequest;requesteddataset=wiseallwisep3aspsd; catalog=wiseallwisep3aspsd; use=catalogoverlay; usertargetworldpt=10.68479;41.26906;eqj2000;m31;ned; catalogproject=wise; radius=200; searchmethod=cone    3. requestclass=serverrequest; tblid=xyplottblid 1; catalog=wiseallwisep3aspsd; use=catalogoverlay; usertargetworldpt=10.68479;41.26906;eqj2000;m31;ned; searchmethod=cone; requesteddataset=wiseallwisep3aspsd; catalogproject=wise; radius=200; decimate=decimate=ra,dec,10000,1,,,,    the difference between 1 and 2 is tblid parameter. the difference between 2 and 3 is tblid and decimate parameters. as well as the order of the parameters. none of which change the catalog search result.    test case: test searches, test catalog, allwise source, radius=200",2,test
DM-6051,Add extendedness vs. star selector test to single-visit validation in ci_hsc,"cihsc has a test that verifies that extendedness as measured on coadds broadly agrees with the star selection done for psf estimation on individual frames.  this tests a bunch of stuff, including aperture corrections on the coadds and propagation of flags from visits to coadds.    it doesn't test that aperture correction vs. extendedness logic is correct in processccd.py, but just copying this test to the appropriate validation function in cihsc should do the trick.  this is currently broken, but should be fixed in dm 5877.",2,test
DM-6052,Improve password management for Qserv MySQL accounts,"qserv passwords management for mysql account (i.e. root, monitor, qsmaster) should be improved. see wmgr password management to have a good example. furthermore qsmaster use currently empty password, this must be fixed.",8,test
DM-6053,Allow use of other MySQL account thant 'qsmaster',"'qsmaster' value can't be changed in qservmeta.conf, this must be fixed      diff git a/admin/templates/installation/qservmeta.conf b/admin/templates/installation/qservmeta.conf  index 81b6ca9..203ebda 100644   a/admin/templates/installation/qservmeta.conf   b/admin/templates/installation/qservmeta.conf  @@ 103,7 103,7 @@ usermonitor = monitor   passwordmonitor = changemetoo      # used to access qserv data and metadata (like indexes)  userqserv = qsmaster  userqserv = qservdata      above change leads to next error in integration tests:      154 [0x7f1beacf9700] error lsst.qserv.sql.sqlconnection null  connecttodb failed to connect!  154 [0x7f1beacf9700] error lsst.qserv.sql.sqlconnection null  runquery failed connecttodb: start transaction  20160510 14:55:09,684  root  critical  exception occured: error from mysql: (999) error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=xxxxxx, db=qservcssdata, socket=]  traceback (most recent call last):    file ""/home/dev/src/qserv/bin/qservdataloader.py"", line 274, in /      loader = loader()    file ""/home/dev/src/qserv/bin/qservdataloader.py"", line 225, in init      cssinst = css.cssaccess.createfromconfig(config, """")  csserror: error from mysql: (999) error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=xxxxxx, db=qservcssdata, socket=]  20160510 14:55:09,810  lsst.qserv.admin.commons  critical  error code returned by command : qservdataloader.py v config=/qserv/stack/linux64/qservtestdata/2016011g7b107917/datasets/case05/data/common.cfg host=127.0.0.1 port=5012 secret=/home/dev/qservrun/git/etc/wmgr.secret deletetables config=/qserv/stack/linux64/qservtestdata/2016011g7b107917/datasets/case05/data/object.cfg cssremove skippartition chunksdir=/home/dev/qservrun/git/tmp/qservtestcase05/chunks/object config=/qserv/stack/linux64/qservtestdata/2016011g7b107917/datasets/case05/data/object.cfg emptychunks=/home/dev/qservrun/git/var/lib/qserv/emptyqservtestcase05qserv.txt qservtestcase05qserv object /qserv/stack/linux64/qservtestdata/2016011g7b10791+7/datasets/case05/data/object.sql /home/dev/qservrun/git/tmp/qservtestcase05/chunks/object/object.txt   error    ",4,test
DM-6054,Minor updates in suptertask from following DMTN-002,some examples in the dmtn 002 seem slightly out of date.    update supertask documentation and code to catch up with some recent developments in the stack. ,2,test
DM-6057,v12.0 [Winter 2016 / Extra 2016] release,this is the ticket for the v12.0 release prep.     edit: release announcement at https:/community.lsst.org/t/lsststackversion120winter2016extra2016 release/874  ,12,test
DM-6062,Launch integration tests using Docker+Openstack,vagrant and packer will be replaced by openstack python api and cloudinit which are more flexible.     a pseudodns will be provided thanks to avahi/mdns   only one public/floating ip willl be used (in order to allow booting large clusters laters),10,test
DM-6063,Fix how aperture correction is applied, committed a fix jan 15 to how aperture correction is applied that i accidentally lost when refactoring in dm4692. https:/github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 restore the fix.    the need for this fix suggests a design flaw in measurement that will be fixed as part of dm5877,1,test
DM-6064,"Define, design, and RFC repository refactor.",includes support for    butler manages input & output repos   repo tagging  ,18,test
DM-6065,design work for butler storage & format factorization,nan,5,test
DM-6069,Camera team visualization support (F16),support the camera team to use firefly for their visualization needs.,12,test
DM-6071,L1 Messaging path status,"all principle entities for l1 are in place and the messaging is working as intended. the message dictionary includes some message types for prototyping and will likely double in size as the interface between ocs is firmed up in the coming weeks. the implementation thus far is a 'ready, set, go' set of states. test files from 10 forwarders are sent to 10 distributors thru a wan emulation device, and the result can be timed. the dmcs component is a simple cli to initiate messages as currently written. this component will be expanded as requirements are determined.    still under development is a component layer between the condor controller and the ncsa foreman entity so that resource availability can be queried and provide a communication link for ancillary information as needed.",40,test
DM-6072,Li prototype code and the Wan Emulator,"l1 forwarder components and distributor components are located on opposite sides of the emulator (the long haul network component) and move files across the path when configured and  given a 'go' signal via messaging. forwarder/distributor pairs are set up dynamically for each file transfer (similar to a readout event). results are temporarily forwarded to a status queue sink, where messages are processed for the publishing of results.",30,test
DM-6073,Pass background to NoiseReplacerTask,"implement rfc 180:    `noisereplacertask` wants some statistics about the background that was subtracted from the exposure, but it gets these in a fragile and roundabout fashion: it expects the code that measures the background to put the mean and variance into the exposure's metadata, using special keys. it is difficult to enforce correctness because background is measured several times while processing an exposure.    to solve this, pass the background directly to `noisereplacertask`. this will require passing the background through the various measurement tasks, which will require small changes to code that calls the measurement tasks.    in addition, remove computation of background statistics from the background fitting code.",4,test
DM-6074,Add RegistryField support to Task.makeSubtask,as part of implementing rfc 183 add support for tasks specified in lsst.pex.config.registryfield to lsst.pipe.base.task.makesubtask  ,2,test
DM-6075,Document the need for abstract base tasks for tasks,"as part of rfc183 document the fact that variant tasks should have a common abstract base class that defines the api. if we add future tasks that we feel are likely to have variants, then we should create an abstract base class.    candidates include star selectors, psf determiners and isr tasks.    note that this applies to tasks lsst provides in its stack, not to variants users produce and other obscure oneoff code.    also document the desire that tasks with anticipated many variants, such as star selectors, and psf determiners should be in registries. this explicitly excludes tasks such as isr where only one task is likely to be useful for a given set of data.  ",2,test
DM-6076,Create a registry for star selectors,create a registry for star selectors and use the registry instead of configurablefield in tasks that call a star selector.    update config overrides in obs_  packages and unit tests accordingly.,3,test
DM-6077,Change PSF determiners into tasks,"psf determiners are already configurables, and some benefit from having a log. take the logical next step and make them instances of lsst.pipe.base.task.",1,test
DM-6078,Aperture correction fails to measure a correction for the final plugin in the list and reports misleading errors,"since the refactoring of dm4692, runs of processccd.py detail the following in their logs:      processccd.charimage.detectandmeasure.measureapcorr warning: only 0 sources for calculation of aperture correction for 'basepsfflux'; setting to 1.0  processccd.charimage.detectandmeasure.measurement: measuring 65 sources (65 parents, 0 children)   processccd.charimage.detectandmeasure.measurement.applyapcorr: applying aperture corrections to 1 flux fields  processccd.charimage.detectandmeasure.measurement.applyapcorr: use naive flux sigma computation  ...  processccd.calibrate.detectandmeasure.measurement.applyapcorr: applying aperture corrections to 2 flux fields  processccd.calibrate.detectandmeasure.measurement.applyapcorr: use naive flux sigma computation  processccd.calibrate.detectandmeasure.measurement.applyapcorr warning: could not find basegaussianfluxflux or basegaussianfluxfluxsigma in apcorrmap        processccd.charimage.detectandmeasure.measureapcorr: measuring aperture corrections for 2 flux fields  processccd.charimage.detectandmeasure.measureapcorr warning: only 0 sources for calculation of aperture correction for 'basepsfflux'; setting to 1.0  processccd.charimage.detectandmeasure.measureapcorr warning: only 0 sources for calculation of aperture correction for 'basegaussianflux'; setting to 1.0  processccd.charimage.detectandmeasure.measurement.applyapcorr: applying aperture corrections to 2 flux fields  processccd.charimage.detectandmeasure.measurement.applyapcorr: use naive flux sigma computation  ...  processccd.calibrate.detectandmeasure.measurement.applyapcorr: applying aperture corrections to 3 flux fields  processccd.calibrate.detectandmeasure.measurement.applyapcorr: use naive flux sigma computation  processccd.calibrate.detectandmeasure.measurement.applyapcorr warning: could not find extphotometrykronkronfluxflux or extphotometrykronkronfluxfluxsigma in apcorrmap      i can confirm that for the latter, running hsc data with the fix on dm6063, the aperture corrections are being measured and applied for the psfflux and gaussianflux measurements, but not for the kronflux measurements.      looking at the output from the current ""expected"" values for the lsstdmstackdemo we see that there is an offset in the psfgaussian fluxes, implying the gaussian fluxes are not being measured (and hence not applied):      from this i conclude that the aperture corrections are indeed being measured for all but the final entry in the plugin list.  this implies that the report of ""only 0 sources for calculation of aperture correction for 'xxxxxflux'; setting to 1.0"" is incorrect for all but the final plugin measurement.    the demo previously successfully calculated aperture corrections and, after the logic fix of dm4836, applied them in the correct order:      the sources of these issues and fixes for them are the goal of this issue.",2,test
DM-6079,description of archive in a box,"please put in more detailed description of the ""archive in a box"" concept",1,test
DM-6082,Add Sublime Text configuration tips to Developer Documentation,"[rowen] and [parejkoj] have some good tips about setting up sublime text.  [jsick] suggested that we add these configuration tips to the developer documentation.    http:/developer.lsst.io/en/latest/#parttools    both want to include info about recommended packages, but also the linter configurations to support the dm styles.    i paste in here various helpful parts from the hipchat software development room discussion of this.  both verbatim, and summarized.    1. install package control    2. packages:  git, gitgutter, sidebarenhancements, sublimelinter, sublimelinterflake8, sublimelinterhtmltidy, sumnumbers, gist, brackethighlighter, trailingspaces, trimmer, omnimarkuppreviewer, restructuredtextimproved, markdown editing, colorsublime    3. themes:  sunburst color scheme      vim users:  vintageous  + mac os x configuration:  defaults write com.sublimetext.3 applepressandholdenabled bool false  so that holding down 'j' moves downward.  note that vintageous is not a complete implementation of vim, but it at least allows enough basics so that one doesn't go crazy switching back and forth.    link the subl command to /usr/local/bin     quick tips:  ""optionselect (to select blocks) and select something then cmdd are both extremely useful for modifying lots of things at once.""    ""similarly, ctrlshiftup/down arrow.""    ""cmdclick on multiple lines to have multiple synchronized cursors""    configurations:  1. [rowen]'s sublimetext preferences file: https:/jira.lsstcorp.org/secure/attachment/27846/preferences.sublimesettings  2. configuration flake8 so that it works in the linting can take a bit of work if flake8 isn't in your default path.  see sublimelinter.sublimesettings attachment for 's configuration: https:/jira.lsstcorp.org/secure/attachment/27845/sublimelinter.sublime settings    the above are useful, but we'll need someone to detail the linter stuff more.",1,test
DM-6083,Enable websocket client to pickup channel parameter from url,send websocket channel information via url.  keep channel information on browser reload.    this is needed for firefly python api and external (when firefly viewer is invoked trough url) api.  ,1,test
DM-6086,JSON Schema for metric data from validate_drp to be ingested by the QA Dashboard app,"a welldefined json schema is needed for validatedrp’s json output so that it can be easily, and consistently ingested into the qa database. the schema will also make the json more selfdescribing, and potentially useful for other tools to build upon as well.    the schema is being drafted in a thread at https:/community.lsst.org/t/jsonschemafor squash/777?u=jsick. once an informal consensus is reached the schema will be implemented in validatedrp on this ticket.",8,test
DM-6087,jenkins job to execute validate_drp and push results to qa dashboard,"this is the initial jenkins job that ""ties"" all the components together.    it needs to:     execute validatedrp   push metadata about the jenkins build to qa dashboard    push the validatedrp metrics to qa dashboard",7,test
DM-6089,Use fixed width integer types from std instead of boost,the following fixed width integer types are used in the stack:     boost::int16t   boost::int32t   boost::int64t   boost::int8t   boost::uint16t   boost::uint32t   boost::uint64t   boost::uint8t    this ticket aims to replace them with their equivalents from cstdint.,1,test
DM-6098,draw a diagram of DRP data flow,"study jim bosch's diagrams and descriptions (parallelization in data release production, data release production top level overview), consider inputs/outputs of high level pipelines and parallelization of the drp, draw a diagram to illustrate the data flow. ",4,test
DM-6099,Improve afw.table Astropy view support,"dm5641 completed the first version of astropy view support, but there is still room for improvement:    make footprint s in sourcecatalog s available as a dtype=object column.  same for psf , wcs , calib in exposurecatalog.     use astropy's coordinate classes for coord fields (may require an rfc to determine how much we want to use astropy's coordinate classes).  ",4,test
DM-6100,afw/tests/rgb.py fails due to .ttf files,"afw/tests/rgb.py fails for me with the below error. we likely shouldn't be trying to track system resources like fonts, as we don't have any control over them.      [20160512t19:46:12.528961z] failed test output:  [20160512t19:46:12.536029z] tests/rgb.py  [20160512t19:46:12.536057z]  [20160512t19:46:12.536070z] ...s......ss...f.  [20160512t19:46:12.536106z] ======================================================================  [20160512t19:46:12.536138z] fail: testfiledescriptorleaks (lsst.utils.tests.memorytestcase)  [20160512t19:46:12.536173z]   [20160512t19:46:12.536192z] traceback (most recent call last):  [20160512t19:46:12.536261z]   file ""/users/parejkoj/lsst/lsstsw/stack/darwinx86/utils/2016_01.04g52f464f/python/lsst/utils/tests.py"", line 134, in testfiledescriptorleaks  [20160512t19:46:12.536330z]     self.fail(""failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """"))  [20160512t19:46:12.536352z] assertionerror: failed to close 2 files  [20160512t19:46:12.536356z]  [20160512t19:46:12.536391z]   [20160512t19:46:12.536404z] ran 17 tests in 3.451s  [20160512t19:46:12.536407z]  [20160512t19:46:12.536424z] failed (failures=1, skipped=3)  [20160512t19:46:12.536445z] file open: /library/fonts/nisc18030.ttf  [20160512t19:46:12.536479z] file open: /system/library/fonts/apple color emoji.ttf  [20160512t19:46:12.536495z] the following tests failed:  [20160512t19:46:12.539928z] /users/parejkoj/lsst/lsstsw/build/afw/tests/.tests/rgb.py.failed  [20160512t19:46:12.540060z] 1 tests failed  ",1,test
DM-6102,implement basic oauth2 authentication for qa-dashboard,"per discussion at the sqre co working session on thursday, we agreed to implement minimal authentication for the mvp version of the qa dashboard as an external reverse proxy, such as https:/github.com/bitly/oauth2_proxy.",6,test
DM-6105,Implement the post_save mechanism to update bokeh sessions when new data is available,in tickets/dm 5750 the bokeh python library was integrated in the squash django project. in order to exemplify its use the kpm ci chart is showing only hardcoded values for now.    in this ticket we will implement methods to read the data from the database and the post_save mechanism to update the bokeh session when new data is available.   ,4,test
DM-6106,color map in visualization,"the four new colormaps introduced in matplotlib last year  http:/bids.github.io/colormap/    d3js cmap: http:/bl.ocks.org/mbostock/3289530    d3 supports cielab (lab) and cielch (hcl) color spaces, which are designed for humans rather than computers. http:/bl.ocks.org/mbostock/3014589    ",4,test
DM-6107,Firefly performance profiling and code refactoring if needed,we need to dedicate some effort in each cycle to do the performance profiling and code refactoring needed to improve performance,22,test
DM-6108,More work on firefly viewer layout control,"more work needs to be done on the triview layout controlling:     when there is a table with image meta data is loaded, the images need to show with the meta data tab selected   when any data is pushed then drop downs needs to close   when a table a catalog table is loaded and there are no plots then then the triview should be up with the coverage tab selected. when there is plots then the coverage tab should not be selected.   we need a way to remove load a table and then only see the table, same with xyplots   catalog and image meta data are determined by looking at the data.  however, we might need this logic in a single function   when a table is loaded and we cannot determine what type it is then the table and the xyplots only should some up.    when all data is deleted the default tab should open.  (in irsaviewer case the is the select image panel)",8,test
DM-6111,Browsers should cache editions for a shorter time period than Fastly,"currently we set cachecontrol: maxage=31536000 so that fastly caches uploads from ltd mason for a year on its pops. this has the sideeffect of also having browsers potentially cache documentation on the client for up to a year. in practice, browsers churn through their cache space more quickly, but i've noticed that safari has no cap on its cache space, and therefore can hold onto pages for a long time.    the solution is to set a surrogatecontrol max age to 1 year, and have cachecontrol: maxage=0, private, must revalidate. this will be done on ltd keeper during the copy phase of a build into an edition (since it is reasonable for a client to cache a build forever), but then give us the flexibility to update an edition instantly.    in the future we may want a more nuanced solution where css and javascript, for example, are cached longer on the browser.",1,test
DM-6112,Provide minimal documentation for meas_extensions_photometryKron,"please provide a minimal level of documentation for measextensionsphotometrykron, to include:   a doc directory with the usual content so that docstrings get generated by doxygen;   a package overview;    all docstrings should be appropriate for parsing by doxygen (ie, should start with """"""! where necessary).  ",1,test
DM-6113,Calibration Products Pipeline work during F17,"this will need to be properly fleshed out before scheduling: for now, it's a bucket for stories that [mfisherlevine] is unlikely to complete in x16. [rhl], [mfisherlevine] & [swinbank] to provide further definition.",65,test
DM-6118,The color stretch dialog box does not work properly,"there are a few issues in the color stretch dialog box:  # when the asinh or gamma algorithm is selected,  the asinh parameters and gamma parameters are always reset to the default.  the user entered values can not be kept and used.   # when there are two or more images, when the lower/upper range in one of the image is set, the lower/upper range in all the rest images are set to the same lower/upper range.    # the rangevalues are always reset each time when the color stretch dialog is open.  ",5,test
DM-6119,"update ""newinstall.sh"" nebula images & docker containers - w_2016_20",nan,1,test
DM-6121,Remove old DipoleMeasurementAlgorithm from imageDifference.py,"currently the new algorithm is run alongside the old. this ticket will deprecate the old algorithm, making the new one the default. this will be done after the new algorithm is vetted on real data (dm 5412).  it may also be blocked by the pending sfm overhaul so that it can be implemented as a standard registered plugin.",4,test
DM-6123,Build SFM housing for PSF approximation using ngmix code,"build a measurement plugin which allows psf approximation to be done using ngmix.  after consulting with erin, it was decided that this would make use of the em code and would produce as its output some variable number of gaussians.  these will be turned into multishapeletfunction outputs.    a suitable set of configuration options and output failure flags will also be provided.",6,test
DM-6124,Testing ngmix Psf plugin with CModel,test that the ngmix psf approx plugin works correctly in our measurement framework by testing it with cmodel and comparing results with those produced with shapeletpsfapprox.,6,test
DM-6125,Do robustness tests of ngmix PSF approx plugin,"run tests on the ngmix psf approx plugin similar to those which were run on shapeletpsfapprox.  we will test both for how long the plugin takes to run, and how often it fails.    note previous report on cmodel and spa was dm 4368",6,test
DM-6126,LSST's version of Astrometry.net doesn't build on Ubuntu 16.04,"reproduced building on ubuntu 16.04.    https:/groups.google.com/forum/#!topic/astrometry/adcjhfmyhpe    the current version (0.67) does build successfully standalone.    these two patches fix 0.5.0:  https:/github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff and https:/github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff  though only the first one is needed, i believe the second one is part of the build already.",1,test
DM-6127,ngmix has no license,"ngmix does not have a license, which means we shouldn't distribute it. work with erin sheldon to see if he is willing to add one.",1,test
DM-6128,Expanded view not doing fit/fill consistently ,"expanded view not doing fit/fill consistently. sometimes is seems to fit/fill and resize it correctly, other times it stays at the zoom level.  it should always fit/fill and change zoom level with resize when in expanded mode. (unless zoom type is force_standard).",2,test
DM-6130,Fix docker git script: providing both -R and QSERV_DIR make it fails,nan,2,test
DM-6133,mpi4py does not compile under Yosemite due to hardcoded MACOSX_DEPLOYMENT_TARGET,"mpi4py build on yosemite (mac os x 10.10) fails with     build.log:[20160517t16:51:55.847161z] error: $macosxdeploymenttarget mismatch: now ""10.9"" but ""10.10"" during configure      for details see attached build log.    the macosxdeploymenttarget is being set in ups/eupspkg.cfg.sh      [serenity mpi4py] cat ups/eupspkg.cfg.sh  # if macosxdeploymenttarget is not set, we force it to be at least 10.9  # (mavericks). this is the earliest version of os x expected to work with  # release 11 of the lsst stack.  # this works around dm5409, wherein mpi4py was attempting to use an os x 10.5  # sdk, based on querying anaconda, and failing.  export macosxdeploymenttarget=$      what is it that is supposed to be setting macosxdeployment_target?  and why is it not set at the time when ups/eupspkg.cfg.sh is run, but is set to 10.10 by the time the actually compilation is done?   ",1,test
DM-6134,Fix style of catalog panel , finish up dm 5388 ticket by fixing the ui style of the panel,4,test
DM-6135,Migrate VO search panel,vo search panel is a tab part of the catalog drop down panel that should be migrated. this was an outlier of dm 5388 ticket.,16,test
DM-6136,Review and connect validation part to the input area field component,catalog panel (dm 5388) needed an input area for polygon input search but it needs a review and connect the validation reducer to it.,6,test
DM-6137,Add input based on catalog DD table,"add a panel to use dd catalog information to allow user to input catalog constraints as editable table component.    this is to get a table with one or more extra column which are input field. the extra column needs  a different default cell renderer (textcell)  see tablerender.js.  this table component needs to be hooked up to the fieldgroup or some way so it can be used in the catalog search panel.   the catalog search panel will need to be adapted to display this table constraints to fully complete the search query options.    the implementation:  it contains the redesigned panel based on irsa current ops catalog search. a table with input constraints and sql area input are added.   there is a lot in that pr. in particular, usage of table renderers and 'fieldgroup' together with a changed basictable component to be able to get the value from input field custom column 'constraints'.   the panel will be reviewed also by irsa and some input requirements were left out for now (mainly aesthetic details). another leftout is the text are component and the validation of it. that should be addressed in the other ticket dm 6136. ",12,test
DM-6138,Change Fields groups to handle other actions better,"the fields group can be out of sync with actions if they are trying to use store data when that actual value is changes.  this is a classic side effect issue.  it can be solved with sagas.    our current example.  the color panels updating from the plot when the activeplotid changes.    more to do:   field groups need a sega to more effective respond to out side actions   the dispatchchangefieldgroup needs better, more documented parameters   update multiple fields at the same time.   should we have the field group support reset to init state? probably not, but look into it.   change init values?   check example dialog and see if the large/smaller example is validating correctly.",2,test
DM-6139,Change server side hardcopy code to work better with the non-GWT call,the server hard to make a hard copy now takes a staticdrawinfo object.  we want to use only a region array.  change the server side to support this.,2,test
DM-6140,Produce tech note describing detailed project management procedures,write a technical note describing the detailed project management procedures derived by the pmpwg. source material is 's document at https:/github.com/lsst/ldmpmt/.,8,test
DM-6141,Drawing layer improvement to handle mouse selection,"drawing layers are not handling and sharing the mouse quite  right.  also the mechanism to determine to is priority for the mouse needs work as well. this is all necessary to make markers work correctly, since every marker is an individual drawing layer.     also, the draw layer utilities are all in plotviewutil.js.  they need to be moved to something closer to the draw layers.",4,test
DM-6142,Client side Hardcopy support for png with drawing layer overlay,add all hard copy support so we can create a png with all the overlays.,6,test
DM-6145,jenkins/qa terraform destroy fails if there is an existing rds final snapshot,aws appears to prevent the overwrite of an existing final rds snapshot.  this scenario may arise when creating/destroying a dev env multiple times.  this can be avoided by disabling the final snapshot when destroying an rds instance.  one way to resolve this would be to add a terraform var to signal this is his is a development env.,2,test
DM-6146,Evaluate performance of dipole fitting in crowded regions,"there are legitimate concerns about performance of the new dipole fitting algorithm in crowded fields. this will be evaluated (on real data? if no existing data, then realistic simulated data) and contrasted with other possible alternatives. this is in response to zejlko's concern and suggestion that dipolefittask should constrain only positions using pre subtraction images, and only fit fluxes using the diffim.",8,test
DM-6147,Set SUSPECT mask in ISR task and make saturation a double,"implement rfc 190 and mask suspect pixels:    add selectlevel to lsst.afw.camerageom.ampinfocatalog, as a double, and add support for it to lsst.ip.isr.isrtask, analogously to masking saturation: iif suspectlevel is not nan then set the suspect flag for pixels above the suspect level.    also change the type of saturation in the ampinfocatalog from int to double, so that the existing test for nan actually works",5,test
DM-6149,Reduce memory utilization in mysql proxy,jon is trying to run tests with large result which kills proxy/czar because it runs out of virtual memory. would be nice to reduce memory use and find a way not to keep query result in memory.,2,test
DM-6151,Failure to fail when fallbackFilterName is None,"when no fallbackfiltername is set, we can get a confusing error message when failing to load a calib:    runtimeerror: unable to retrieve dark for  and no fallback filter specified: unknown value type for filter: /    this is unrelated to the calib load failure, and merely reflects the fact that fallbackfiltername=none.",1,test
DM-6153,long term re-plan,"we need to make the long term plan for fy2017  fy2019, ready for comcam; and more fy2020 fy2022, ready for operation.",30,test
DM-6154,New features in histogram ,". 1d histogram, maybe a special case of density plot  . new way to calculate the bin size?  ",10,test
DM-6155,Attend SciPi WG meeting,attend the science pipelines working group meeting in seattle.,8,test
DM-6156,Attend SciPi WG phonecon,attend the science pipelines working group meeting by video con.,3,test
DM-6157,Flesh out MOPS work,work with lynne jones and colin slater to flesh out the high level mops design to a point where we can plane the risk associated with each component.,3,test
DM-6158,Attend SciPi WG F2F in Tucson,attend the science pipelines working group face to face meeting in tucson.,3,test
DM-6159,Flesh out the Level 1 processing diagram,andy and i need to make sure we understand the level 1 processing.,10,test
DM-6160,Adding an int to the end of CzarConfig causes a segfault error.,adding and int to the private members of czarconfig causes a segfault when czar::czar() calls log_config(logconfig);. gdb shows logconfig is the correct string value but somewhere in log4cxx something is corrupted and causes a segfault.    adding an int to the end of czar (the class where czarconfig is instantiated) does not cause the issue. ,6,test
DM-6162,Investigate how the diffim decorrelation correction works for the case of non-uniform PSFs and noise,"it is not clear whether, or how, the l(zogy) postconvolution kernel (pck; see dm5914) will work for nonuniform psfs or noise/variance. this will be investigated using the simple implementation from dm5914.     tasks:  1. determine whether variation in the pck across the field is significant enough to matter for typical lsst images  2. if it does matter, investigate options for performing interpolation of the pck across the field, or via calculating the pck across the field from the spatially varying matching kernel. ",8,test
DM-6165,Extend the capabilities of the StarFast simulator beyond the minimum needed for DCR algorithm testing,during development of the starfast simulator for dcr correction algorithm testing several addition features were identified that would make it more general and useful. these capabilities will enable a wider range of testing of new image differencing algorithms and give greater confidence in the accuracy of the results for dcr simulations. ,14,test
DM-6166,Time AST and compare to our WCS code,"time tan sip for our code and for ast, in order to get a sense of the performance impact of switching to ast for our wcs implementation.",3,test
DM-6167,Create DMBP project in jira,"create a new project in jira (dm baseline plan), spec provided here: https:/confluence.lsstcorp.org/display/dm/projmgmtwg%3athenew+dlp  i am sure we will fine tune it, but it is a (hopefully good) start.",2,test
DM-6168,Wrap afw using pybind11,"experiment with using pybind11 (rather than swig) to expose afw, and the packages it depends on,  to python.    the concrete result of this epic is an assessment of the utility gained by wrapping the rest of the stack in pybind11 and an estimate of the time that would be required to carry out that work. if those goals are reached without completing the work on afw, we can claim success. in particular, if it becomes clear early in the epic that there is no long term utility here, we should abort the rest of the work.",80,test
DM-6169,Participate in DM replanning process,"participate in the ongoing dm replanning process. this includes contributions to both the scipiwg and pmpwg, including such documentation writing or other tasks as the chairs of the those groups request, as well as resource loading and delivering the complete plan.",100,test
DM-6170,PSF fitting study,"investigate the https:/github.com/rmjarvis/piff/ psf modelling code. experiment with applying it to realistic lsst (or precursor) data. discuss whether it is an improvement over existing techniques, and a recommendation as to whether it should be adopted in the stack. (nb writing the code to incorporate it into the stack is not a requirement of this epic, but may be a desirable sideeffect).    if piff is still too immature for this to be useful, investigate kendrick smith's https:/github.com/lsstdm/measextensionshscpsf/ code.",15,test
DM-6171,Service technical debt accumulated in earlier cycles,service technical debt accumulated in earlier cycles.,100,test
DM-6172,F16 DRP emergent work,handle emergent work during f16.,55,test
DM-6173,Serve as chair of the IVOA Time Domain Interest Group, will serve as chair of the ivoa time domain interest group through may 2017. this epic captures work associated with that activity in f16.,20,test
DM-6174,Prepare for and participate in SBAG meeting,by request of http:/ this epic captures work associated with preparation for that meeting.,20,test
DM-6175,Visualization tools for Science Pipelines,[nlust] will collaborate with the suit group on development of appropriate visualization tools in support of the work of the science pipelines group during f16.    [xiuqin] will provide further specification of success criteria.,20,test
DM-6176,Optimal coaddition,"experiment with building ""optimal"" coadds, as defined by http:/dmtn015.lsst.io/en/latest/.    the aim here is to be able to generate coadds for experimentation with measurement algorithms. the expected output is appropriate mathematical formalism and prototype code. polished integration of this facility with the lsst stack is not a requirement (but may be a useful by product) of this work.",100,test
DM-6177,Increase memory locked amount in container,"in order to lock memory, the memory locking limit within the container for the qserv worker needs to be raised. my understanding is the container uses whatever is the host setting so the limit has to be set for the container user and whatever the user is inside the container. the particular limits is:    memorylocked 64 kbytes    notice that by default it's 64k. that needs to be raised to say 75% of the real machine size. i wouldn't make it unlimited as a memlock mistake may crash the whole machine. the limits are specified in ""/etc/security/limits.conf"". you will know that you are successul when you ssh into the container as the qserv worker user and the ""limit"" command tell you have can lock lots of memory.    we would also set the capipclock privilege but setting the soft/hard limit above should be good enough. so, let's start with that. ",2,test
DM-6178,Add eups version for Qserv for stack package version,"for stack packaged qserv version, version needs to be retrieved and added to monitor.yaml using next command:    dev@clrinfopc04:~/src/qserv$ eups list qserv s v  local:/home/dev/src/qserv    indeed, pkgautorversion doesn't work in this case, i.e. with no git repos",2,test
DM-6179,Support Python 3 migration,"support the migration of the dm code to python 3. this includes writing transition documentation, integration of a new scons, migrating a handful of low level packages and liaising with the teams on their packages.    the final outcome of this epic is that everything would be in place for the migration at the august all hands meeting.",40,test
DM-6180,Update LSE-61 requirements and traceability,"with the updates to dpdd and ldm151 in the early part of f16, there is a need to update lse61 (dmsr) such that it can directly trace requirements from oss+dpdd through lse 61 and down to implementation ldm documents.    this will require substantial rewrites of many of the existing requirements and possible addition of new requirements. it may also be necessary to add annotations to dpdd and other ldm documents to provide traceability anchors for dmsr.    the outcome of this epic is a new baselined dmsr approved by ccb.",40,test
DM-6181,reST roles for JIRA References,"add :jira:`dm1234`type roles to documenteer so that jira tickets, epics and rfcs can be referenced easily from all of our sphinx based projects.",2,test
DM-6183,sourceSelector needs a schema in ImageDifferenceTask,imagedifference.py crashes with a vague error on initializing the sourceselector task. the problem turns out to be that sourceselector needs a schema passed in.,1,test
DM-6184,Add a python 3 Jenkins instance,we need a jenkins instance where the default python in the path is python3 (where version >= 3.4 with 3.5 preferred). the underlying os does not matter.    a prerequisite of this is a modification to the lsstsw bin/deploy script to allow python3 to be installed (miniconda3 eups package?) rather than python2.    modifying the eups scons and python packages is outside the scope of this ticket. a build of a third party eups package is sufficient demonstration of the capability.,12,test
DM-6185,Get jointcal running on minimum data,"it is very important for other teams to have a version of jointcal running to remove the sensitivity on the errors in astrometric reference catalogs.  the suggestion is to get jointcal running with cfht, hsc, decam and lsstsim.",100,test
DM-6186,Provide input for the update of LDM-151,we need to update the level 1 portions of ldm 151 to be both more descriptive and close to what we actually plan to deliver.  this will involve breaking down things to a finer level of planning as well as delivering content for the document.,38,test
DM-6188,"First draft of overview (""vision"") document",see https:/dmtn 016.lsst.io,3,test
DM-6189,Complete update to LDM-230,"complete an update to ldm230 and submit to tct for rebaselining.  along the way, contribute to and review other documents needed by dps wg.",10,test
DM-6190,Update sizing/cost model,"contribute to the updating of the sizing/cost model, including fixing known bugs, synchronizing the inputs and models to fit the current baseline, and investigating changing the modeling technology.",10,test
DM-6191,Refine SuperTask design document,deliver a refined supertask design document including reslicing to accommodate changes in the axis of parallelization between supertasks making up a composite supertask.,13,test
DM-6192,Update LSE-75 DM-TCS ICD,submit an lcr to update lse 75 to reflect current thinking on telemetry feedback from dm to the tcs.,5,test
DM-6193,Update LSE-72 DM-OCS ICD,submit an lcr to update lse 72 to reflect changes discovered by work at ncsa to support early integration tests.,5,test
DM-6194,Update LSE-68 DM-Camera DAQ ICD,"submit an lcr to update lse 68 to reflect understandings developed between mike huffer and ncsa about the interface, including the image deletion policy for the camera data buffer.",5,test
DM-6195,Provide input to Commissioning Plan,provide input based on understanding of the dm interfaces to the commissioning plan being developed by chuck claver.,5,test
DM-6196,SQuaSH capability extension: multiple testdata service,"this epic covers work to deliver the following improvements to the squash prototype stood up in x16:     drilldown 1 level (time series>histogram)     multiple testdata options (requires jenkins, backend, dashboard extension)     processccd + validate_drp pseudoworkflow     pseudoprovenance (track manifest.txt  real lsst provenance system will be swapped in for extensive functionality when available)   ",92,test
DM-6197,Update LSE-76 Summit ICD,submit an lcr to update lse 76 based on summit rack and power needs obtained from ron lambert.,2,test
DM-6198,Ad-hoc Docs & Comms requests,timeboxed epic for incycle adhoc developer or management requests. in the first half of fy16 most of these are likely to be deveoper guide related. ,12,test
DM-6199,Stack API documentation ,stack api doc generation  > pipelines.lsst.io,20,test
DM-6201,Resource load F16 part II,resource load for second half of f16    (sp estimate from first half),6,test
DM-6202,"SQuare Requirement, Design, & Review Docs for DM","this is an epic to track the work required on for dm baselined documentation from square staff, including any associated with working group / replan etc    [fe: 45% mwv: 45% dn: 10%]    ",35,test
DM-6203,Releases and Release Engineering Improvements,  [50% fe 50% mjp],16,test
DM-6204,Build/CI/Deploy improvements requested by the DAX/Qserv team,  requests for build/ci/deploy improvements initiated by the dax/qserv team prioritised on request from the dm project manager.     ,24,test
DM-6205,Build/CI/Deploy improvements requested by the Architecture Team,  build/ci/deploy improvements requested by the architecture team prioritised by request from the dm system architect.     they cover predominantly support for the python3 support. ,4,test
DM-6206,CI Improvements: Jenkins 2 upgrade etc,"  this epic covers a timeboxed maintainance of the jenkinsbased ci system, including the jenkins 2 upgrade as well as the required updates to the jenkinspuppet module. it also may include work done as part of dm 6204 brought over to the apps ci service. ",8,test
DM-6207,CI/Build/Deploy improvements for Sims,this is a timeboxed effort to prioritise support requests from the sims group,4,test
DM-6208,SQuaRE services disaster recovery,this is a timeboxed effort to test and improve backups and disaster recovery for square services. it is unlikely to be sufficient in itself. ,8,test
DM-6209,Ad-hoc developer requests,"this is a bucket epic for adhoc developer requests that cannot be postponed till the next planning cycle. in the event that it is underutilised for this purpose, it will be assigned to technical debt dm5850",8,test
DM-6210,Improve OSX support,nan,16,test
DM-6211,Gitlfs maintenance - protocol upgrade etc,nan,8,test
DM-6212,Slack migration,nan,8,test
DM-6213,Conda binary distribution improvements,nan,16,test
DM-6214,logging.lsst.codes improvements,nan,8,test
DM-6215,Verification dataset exploratory work,[dn 50% af 50%],16,test
DM-6216,F16 DAX Services Containers & Ops,nan,10,test
DM-6217,F16 DAX Services Improvements,nan,10,test
DM-6218,F16 NCSA Dax Services Deploy,nan,10,test
DM-6219,F16 Replan,nan,33,test
DM-6221,F16 Support SUIT for Prototype DAC,nan,40,test
DM-6222,F16 L1 DB Prototype I,nan,89,test
DM-6223,F16 NCSA Stripe 82 Image Ingest,nan,36,test
DM-6224,F16 Butler Repository Refactor,"per kt, the parent/peer repository relationship scheme was not an exact fit for what we need. we discussed and decided that butler should manage its own input and output repositories. also discussed with kt and gregory was the ability to select inputs by 'tagging' repositories. the design discussion with the larger group is captured in rfc 184.",40,test
DM-6225,F16 Butler Storage & Format Refactor,"we want a pluggable architecture that allows code that uses butler to be able to define the the storage format and location from configuration and/or run time code.  (maybe it's implicit in this epic, but we need to define, design, rfc, and implement this feature.)",35,test
DM-6226,F16 Butler Composite Dataset Design,"do design, rfc, and some prototype code for loading and saving ""composite datasets"" via butler.    composite dataset definition: a python objects loaded by butler from file/database/etc that is persisted in more than one physical location (e.g. more than one file on disk). those objects should also be able to be written to more than one physical location   the design should support this but the initial implementation may not be required to have this. ",50,test
DM-6227,F16 Butler Repo of Repos Design,nan,20,test
DM-6228,F16 VO Standards Investigation,nan,10,test
DM-6230,F16 Qserv Loader Improvements,nan,100,test
DM-6231,F16 Qserv Containers and Ops,nan,16,test
DM-6232,F16 QServ Improvements,nan,82,test
DM-6233,F16 NCSA Qserv Deploy,nan,34,test
DM-6234,F16 NCSA Stripe 82 Catalog Ingest,nan,20,test
DM-6235,"Take part in LDM-151 Progress Meeting, 2016-05-27",nan,1,test
DM-6236,"Take part in LDM-151 Progress Meeting, 2016-05-27",nan,1,test
DM-6237,"Take part in LDM-151 Progress Meeting, 2016-05-27",nan,1,test
DM-6238,Familiarization with RHL calibration documentation,nan,4,test
DM-6239,The grid labels are not placed in the right position when the coordinate is Ecliptic coordianates,the algorithm to calculate the label position does not work well for the ecliptic coordinate system.  the algorithm needs to be modified to work for all the coordinates.,2,test
DM-6240,Support API interaction with Regions,"we now need more fine grain controls over regions:    from api, user can:     load region file   delete region layer   add a region entry to a layer   delete a region entry from a layer    when this ticket is complete, region conversion should be completed.",10,test
DM-6241,Implement the ZOGY extension to the A&L algorithm in the stack,"dm 5422 provided a test implementation of the real space extension to the a&l algorithm for a correction kernel motivated by the zogy paper.  this epic is to take that test algorithm and incorporate it so that it can be used by the diffim tasks.    the first step will be to incorporate it using a static psf t compute the correction kernel.  the second will be to evaluate how that affects the resultant difference image.  this should also include an estimate of the overall performance relative to the base a&l algorithm by examining runtime, false positive rate, and accuracy of noise estimation in both the detection threshold and the reported measurement snr.",30,test
DM-6242,Study spatial variability of ZOGY correction,dm 6241 looked at how the correction term to the a&l algorithm performs under the simplifying assumption that the science image psf is spatially invariant (though the matching kernel is spatially varying).  this epic will focus on how to extend the correction to include spatially varying terms.,38,test
DM-6243,Study the impact of having a spatially invariant decorrelation correction factor to A&L,the initial implementation of the a&l + noise whitening correction term assumes a single matching kernel and variance value(s) for the image(s) in the correction kernel.  we should assess how well that assumption performs in simulated and real images.  one test would be the variance and covariance in the noise as a function of position in a set of typical images.,8,test
DM-6244,Assess performance of the decorrelation correction to A&L,"study the performance when using the (currently, spatially invariant) correction term to the base a&l algorithm in terms of runtime, detection threshold, reported measurement noise, and false positive rate for similarly tuned versions of both the base algorithm and that with the correction applied.",6,test
DM-6245,Compare competing algorithms for correcting DCR in template images,dm5455 provides an implementation of a correction algorithm that depends on a matrix inversion approach to correct for dcr.  this should provide another approach for comparison (potentially a more forward modeling based approach).    compare the algorithms in a simplified system in 2d where dcr is along one axis.  the algorithms should be extended to arbitrary rotations.  the bakeoff will be repeated in the case of arbitrary rotations.    the result should be a recommendation as to the algorithm to use for dcr correction in template images.,49,test
DM-6246,Vertical overscan off by one again,"in dm5524 [price] fixed the vertical overscan by directly editing the amp info catalogs, but didn't mark the camera generating code as bad. in dm6147 i regenerated the files, reintroducing the problem. the problem seems to be a subtle bug in the camera generating code. rather than try to fix it, i'll convert the fixed catalogs directly and mark the generating code as broken. [price] will issue an rfc that suggests a better way to handle generating amp info and once that is dealt with we can come up with a more permanent fix (e.g. delete the generating code or fix it).",1,test
DM-6247,DRP Outline for LDM-151,"write outline for data release production section of ldm 151, using the drp data flow diagram as the organizing principle.",2,test
DM-6248,"DRP Top-Level Diagram and Descriptions, Draft 1","insert the content from the drp data flow diagram on confluence into ldm151, adjusting it to the outline developed on dm6247.",2,test
DM-6249,Implement competing algorithm,implement a competing (potentially a forward modeling approach) algorithm for correcting dcr in templates.,12,test
DM-6250,Extend competing algorithm to arbitrary rotation angles.,nan,6,test
DM-6251,Convert DRP Top-Level Diagram to standard conventions,"dm6248 adds a large, complex diagram that will need to be cleaned up and converted to use the same conventions and colors as other diagrams in ldm151.",2,test
DM-6252,Do bakeoff between the two algorithms in simplified case,"the original matrix inversion technique and the competing technique will likely have different sensitivities.  this should be a comparison of the algorithms, likely based on numbers of dipoles, along with performance (memory and runtime) considerations.    this will be done on 2 d images with dcr along one axis.",6,test
DM-6253,Bakeoff between algorithms extended to arbitrary rotation.,redo bakeoff in the case of arbitrary rotation in dcr effect.,6,test
DM-6254,Develop standard conventions and colors for LDM-151 diagrams,"we want diagrams in ldm 151 to have consistent notation and colors, and to be produced using the same tool.  someone needs to look at the diagrams produced so far to gather requirements, decide on and document these conventions, and select the tool we'll use to produce them.",4,test
DM-6255,Improve detail for for DRP imchar/jointcal in LDM-151,write more detailed descriptions and possibly draw a rough diagram for the single frame processing and simultaneous calibration components of data release production.    does not necessarily involve turning this section into prose.,6,test
DM-6256,"Improve detail for DRP background matching, coaddition, and diffim in LDM-151",nan,4,test
DM-6257,Improve detail for DRP coadd processing in LDM-151,nan,2,test
DM-6258,Improve detail for DRP object characterization in LDM-151,"includes coadd measurement, multifit, and forced photometry.    could be faster to write than other sections because we can lift from ""blendedmeasurement"" document that already exists in ldm151 repo; could be harder because that document has already exposed a number of unresolved questions that may need to be addressed (by at least getting agreement among pundits on the best bet approaches) before we can plan.",4,test
DM-6259,Improve detail for DRP afterburners and level-3 gathering in LDM-151,nan,2,test
DM-6260,Cleanup and standardize DRP imchar/jointcal diagrams,"dm6255 will produce some rough, draftlevel diagrams that will need cleanup and standardization.",1,test
DM-6261,"Cleanup and standardize DRP background matching, coaddition, and diffim diagrams","dm6256 will produce rough diagrams that will require cleanup and standardization.     has made some suggestions for the current diagram that i'll implement on this issue, so i'm assigning it back to me.  i'll also go ahead and integrate his updated drp overview diagram (currently on confluence) into ldm151 here.  ",1,test
DM-6262,"Cleanup and standardize DRP detection, association, and deblending diagrams","dm6257 will produce rough, draftlevel diagrams that will require cleanup and standardization.",1,test
DM-6263,Cleanup and standardize DRP object characterization diagrams,"dm6258 will produce rough, draftlevel diagrams that will require cleanup and standardization.",1,test
DM-6264,Cleanup and standardize DRP afterburners and level-3 gathering diagrams,"dm6259 will produce rough, draftlevel diagrams that will require cleanup and standardization.",1,test
DM-6265,Audit DRP LDM-151 for correct handling of chromaticity,"correctly handling wavelengthdependent photometric and psf effects is one of the biggest qualitative differences between the current stateoftheart and what we have in mind for lsst, and that makes it easy to get wrong.  we need to make sure all steps that produce highquality fluxes or rely on highquality psfs have access to object colors and a reasonable approach to using them.  ",2,test
DM-6266,Upgrade cfitsio and deal with long keyword handling,"to implement rfc105, we need to figure out how we are handling long fits header keywords, before we can upgrade to cfitsio 3.38 or newer. there may be other fitsrelated idiosyncrasies in the stack that may be brought to light while upgrading, as 3.38 has changed how it handles some of the nonstandard conventions.    see some of the notes in dm4115 for problems encountered while attempting to upgrade to the 3.38 beta.",20,test
DM-6269,Attend HTCondor Week,attend htcondor week with  to learn about condor and pegasus  http:/research.cs.wisc.edu/htcondor/htcondorweek2016/index.html,20,test
DM-6270,Review of Workflow Systems,"review different workflows and write a final comparison report. the plan is to look at up to 8 workflows. current list of workflows:   pegasus, htcondor   panda   swift   ...    each review should take around 3 days. the goal is to review the workflows systems with:   longevity, how long has the system been around, what is the funding?   use cases, who is using it?   scale, how large of a workflow has it used?   code, is the code open, how is the developer community, does it have python bindings?   gui, does it have some easy way to monitor the workflow?   can we generate workflows programmatically?   what clusters are supported?    ideally we should also try and get it up and running and maybe even generate a dummy workflow. discussion could be how we can prototype drp and use that as a use case.    deliverable: evaluation report  staff: rob kooper, hsinfang chiang, matias carrasco kind, mikolaj kowalik, steve pietrowicz  effort: 25 days  planned start: 6/1/2016  planned end: 6/30/2016",50,test
DM-6271,Audit DRP LDM-151 for correct handling of crowded fields,"'s background is in extragalactic science on high latitude fields, and he frequently forgets to think about how algorithms will perform in crowded stellar fields.  when the first draft is complete, we should have someone experienced in that area read closely to check that he hasn't made any incorrect algorithmic assumptions as a result.",1,test
DM-6272,Reception and Placement,nan,7,test
DM-6273,Statement of Work ,nan,12,test
DM-6274,Propose extension of SuperTask functionality for workflow package,"should see what is needed to add to supertask so we can use it with workflows. after this is decided we should create a rfc for implementation.    deliverable: rfc to extend supertask  staff: rob kooper, matias carrasco kind, mikolaj kowalik  effort: 5 days  planned start: 6/1/2016  planned end: 6/22/2016",10,test
DM-6275,Implementation of Supertask RFC,"this should implement the rfc written in dm6274.  note that this activity is independent of the work to complete the supertask and activator prototype in dm6418.    deliverable: deliverables based on outcome of rfc  staff: matias carrasco kind, mikolaj kowalik  effort: 15 days  planned start: 7/1/2016  planned end: 7/31/2016",30,test
DM-6276,ConOps for Workflow/Middleware,"create a conops for workflow, this will depend on some decisions made about l2 conops.    deliverable: conops document for workflow  staff: rob kooper, hsin fang chiang, matias carrasco kind, steve pietrowicz, jason alt, margaret johnson  effort: 15 days  planned start: 7/1/2016  planned end: 7/31/2016",30,test
DM-6277,Proof of Concept Implementation of Workflow System,"this should start the implementation of the workflow system. this will be a proof of concept only.    deliverable: proof of concept workflow implementation code  staff: hsin fang chiang, mikolaj kowalik, rob kooper, steve pietrowicz  effort: 20 days  planned start: 8/1/2016  planned end: 8/31/2016",40,test
DM-6278,Investigate proper precision for afw::image::Image pixel transforms,"the various pixel based transforms in afw/src/image/image.cc were converted from using boost::lambda to c11 lambda per dm 6091.    at many places the previous implementation contained implicit casts (through boost::ret) of intermediate results to pixelt (e.g. float).  in particular this affects opperations such as result = l  c r where l is the left hand side image, r is the right hand side image and c a double constant.  when calculated at double precision (e.g. without the casts, which are not needed with c+11 lambdas) the result is slightly different and this causes tests/testprocessccd.py to fail on self.assertalmostequal(psfiyy, 2.17386182921239, places=7) which is only equal up to the fifth place.    in order to not break existing behaviour i added explicit casts to pixelt for intermediate results. but this approach is questionable as the end result will be less accurate then possible. the aim of this ticket is to decide which approach is best:    1. calculate at full precision and modify the test case.  2. cast intermediate results to final precision (as it is done now).  3. do something else?",1,test
DM-6279,Fix possible logic error in pex_policy dictionary,investigate and fix the following warning in pex_policy.      src/dictionary.cc:312:9: warning: logical not is only applied to the left hand side of this comparison [wlogicalnotparentheses]      if (!gettype() == policy::policy) / should have checked this at a higher level                      src/dictionary.cc:312:9: note: add parentheses after the '!' to evaluate the comparison first      if (!gettype() == policy::policy) / should have checked this at a higher level                     (                          )  src/dictionary.cc:312:9: note: add parentheses around left hand side expression to silence this warning      if (!gettype() == policy::policy) / should have checked this at a higher level          ^          (         )  src/dictionary.cc:312:20: warning: comparison of constant 'policy' (5) with expression of type 'bool' is always false [wtautologicalconstantoutofrange compare]      if (!gettype() == policy::policy) / should have checked this at a higher level  ,1,test
DM-6280,The labels in HMS formate are wrong in WebGrid,the labels in hms format no longer show hh:mm:ss anymore.  the porting introduced the bug.  ,1,test
DM-6283,Fix mismatched-tags warnings in meas_modelfit,the following warnings are produced in meas_modelfit. fix them.      include/lsst/meas/modelfit/unitsystem.h:90:1: warning: 'localunittransform' defined as a struct here but previously declared as a class [wmismatchedtags]  struct localunittransform ,1,test
DM-6284,Remove swig special casing for obsolete boost features,in utils the file python/lsst/p_lsstswig.i defines special cases for boost features that are removed as part of dm 5880. this ticket removes the special cases.,1,test
DM-6285,Chart API: external API and the API for histogram,need to support external api and the api for histogram    for now only addxyplot and showplot are implemented,4,test
DM-6286,"Charts (XY plot, histogram) Container",need to be able to view multiple charts simultaneously in the chart area    user would like to see multiple xyplots from the same data displayed at the same time. one example will be to display different colorcolor plots using all 4 bands data from wise catalog.  it is also possible we want to display histogram data at the same time as colorcolor plots.,10,test
DM-6287,Charts refactoring,"i'd like to do some cleanup, which would facilitate further development. this includes:   moving chart related code to a separate package (now it is in visualize)   converting components created with react.createclass to es6 classes    reorganize store and controllers to have all charts related things under 'charts'. now we have 'charts' for charts ui, xyplot for xyplot charts, histogram for histogram charts, and tblstats for table statistics.      fixed bugs     missing chart mount action, when a chart is removed and then recreated on the same table    steps to reproduce: load a table (default scatter plot created), create histogram, delete scatter, create new scatter.       the last scatter did not produce mount action, and the plot was not tracking table changes, like filter.     undefined shows as a label when no server call is necessary    steps to reproduce: load table (default scatter created), clear options and choose the same columns , click apply.    ""undefined"" are shown as axis labels",4,test
DM-6288,Chart options display,"make chart options ""in place"" popup, similar to table options for consistent look. it will also alleviate resizing, because the chart size won't need to change when options are open.",3,test
DM-6289,Chart options reset and clear,need to support reset and clear for plot and histogram options.    should be nobrainer after dm6138 (update multiple fields at the same time),3,test
DM-6290,Attend SBAG Meeting,"meeting runs tues 28 to thurs 30 june; that means we'll likely lose nate for the whole week, given travel.",8,test
DM-6291,Read materials related to SBAG prep and attend telecon,"read up material to prepare for sbag, and discuss readings with mario, lynne, and zeljko.",4,test
DM-6293,Fix error in cmodel related to computing LinearTransforms,"when running cmodel in cihsc, the cmodel plugin throws the error:    processccd.charimage.detectandmeasure.measurement warning: error in modelfitcmodel.measure on record 775961510756221246:     file ""src/geom/lineartransform.cc"", line 66, in const lsst::afw::geom::lineartransform lsst::afw::geom::lineartransform::invert() const      could not compute lineartransform inverse   lsst::afw::geom::singulartransformexception: 'could not compute lineartransform inverse'      this seems to be causing some aperture corrections to fail, as there are no sources to compute the corrections from. investigate why this error is being thrown. if it is a bug, fix it, if the code is not handling situations it should then make the algorithm more robust.",4,test
DM-6294,Add support for pybind11 to build system,add pybind11 as third party package to the stack. update sconsutils to support building with pybind11. use daf_base datetime to demonstrate that this works.,8,test
DM-6295,Unit test for coadds in pipe tasks detects too many sources,"the unit test for pipe tasks creates a dozen stars, to use in coaddition testing. however the results of running the test show over a hundred sources found. investigate why the extra sources are being detected, and fix to increase the robustness of the test. if this relates to other sections of the codebase (deblender) investigate if it is appropriate to make changes to those components to make them more robust instead of creating a simple hack in the unit test.",5,test
DM-6296,Wrap afw::geom with pybind11,"the generated wrappers will live parallel to the swig wrappers. this ticket only covers the c wrappers themselves, not the python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until dm 6168 is complete.",5,test
DM-6297,Wrap afw::detection with pybind11,"the generated wrappers will live parallel to the swig wrappers. this ticket only covers the c wrappers themselves, not the python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until dm 6168 is complete.",5,test
DM-6298,Wrap afw::math with pybind11,"the generated wrappers will live parallel to the swig wrappers. this ticket only covers the c wrappers themselves, not the python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until dm 6168 is complete.",5,test
DM-6300,Extend galaxy shear fitting results to cover ngmix,nan,10,test
DM-6301,Write example meas_base plugin in Python,"during x16, new functionality was exposed to python plugins in measbase. write a complete pedagogical example. it should go beyond our current pure python plugins to demonstrate use of:     flaghandler;   safecentroidextractor;    other relevant, undocumented functionality.    this should be added to the package level documentation for measbase, so it appears in some extended version of https:/lsst web.ncsa.illinois.edu/doxygen/xmasterdoxydoc/measbase.html.",6,test
DM-6302,Wrap pex_exceptions with pybind11,"while wrapping these packages, pay particular attention to exception translation (see jim's bullet point 3: https:/jira.lsstcorp.org/browse/rfc182?focusedcommentid=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment 48644).",4,test
DM-6303,Wrap ndarray with pybind11,note particularly jim's second bullet point at https:/jira.lsstcorp.org/browse/rfc182?focusedcommentid=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment 48644.,12,test
DM-6304,Wrap afw dependencies in pybind11,"everything that isn't base, utils, pex_exceptions (dm6302), ndarray (dm6303).",10,test
DM-6306,Executable test in utils needs to test an executable,in dm 4036 all the test binaries were removed as no longer being needed. this had the unfortunate side effect that the testexecutables.py test no longer tests anything. this ticket will be used for adding a test file.,1,test
DM-6307,input outlines to LDM-151 for AP,write outlines into the ldm 151 document for the alert production pipelines.,4,test
DM-6308,Upload JSON from validate_drp to SQuaSH REST API on Jenkins,"this ticket covers work to build a python package/script whose role is to take json output from validatedrp (dm6086), shim it into the json schema currently expected by the squash rest api (http:/sqr009.lsst.io), and post the data to the api's /jobs endpoint.    this tool also adds additional metadata to the ‘job’ document, including the build id and versions of packages as run by validatedrp (see dm 5943).",2,test
DM-6309,Update LDM-151 with SDQA Skeleton and Outline,1. update the ldm151 draft with the sdqa skeleton from the dmlt + scipipelines working group discussions of may 1620.  implement as bullet points in a semi coherent list. (/)    2. clean up list. (/),1,test
DM-6310,Transform SDQA bullets to prose,nan,1,test
DM-6311,Create 1st-level block diagrams for SDQA,nan,2,test
DM-6312,Update Scons to v3.0,scons v3 is the scons version that supports both python 3 and python 2.7. this ticket is for updating scons and ensuring that the python 2.7 stack still builds.    this work depends on the scons developers delivering a new scons by mid july. whilst work is ongoing it may be necessary to help out with the port if we wish to meet our python 3 target.,4,test
DM-6313,Create miniconda3 EUPS package,newinstall.sh currently installs miniconda via eups. to replicate that functionality in python3 we need to create a miniconda3 package. this package should be almost identical to miniconda2.    requires that lsstsw first be updated to support python 3.,1,test
DM-6314,Port lsstsw to Python 3,get lsstsw working with python 3:   update the deploy script to allow a python 3 python to be installed and modify the version checking code.   demonstrate that lsstsw rebuild will successfully build and install a thirdparty nonscons package.,5,test
DM-6315,Write Python 3 porting guide,porting the stack to python 3 is not as simple as blindly running futurize. a guide has to be written explaining the issues and providing guidance on when to accept futurize suggestions and when to ignore them. this guide will be written as a tech note.,10,test
DM-6316,Update newinstall.sh to support Python 3,newinstall.sh currently insists on installing and checking for python 2.7. this needs to be changed to allow python 3.    requires sconsutils works with python 3 as the lsst eups package is installed as part of newinstall.sh.,1,test
DM-6317,Update developer guide to include Python 3,update the developer guide to indicate that python 3 must be supported and that code must run on python 2.7 and 3.    this ticket will reference the tech note delivered as part of dm 6315. writing extensive user documentation on the future package is beyond the scope of this ticket.,2,test
DM-6319,Port sconsUtils to Python 3,sconsutils has to be modified to ensure it works with python 3. additionally swig calls must be changed to trigger python 3 mode.,3,test
DM-6320,Port utils to Python 3,ensure that the utils package will work with python 3.,2,test
DM-6322,Port base package to Python 3,ensure that base works with python 3.,2,test
DM-6323,Lead Python 3 migration at All Hands Meeting, prepare for all hands meeting.   present plan to developers.   advise developers doing migration.   contribute fixes as required.,8,test
DM-6325,Replace BOOST_STATIC_ASSERT with static_assert,replace booststaticassert with static_assert from c11.,1,test
DM-6326,reST roles for mock code references,"add mock code reference roles so that authors can add semantics to their writing without attempting to make actual references to api documentation that does not yet exist. covers all roles in the python domain, and supports tilde syntax for collapsing the namespace.",1,test
DM-6331,Shifting F16 milestones to S16,"per 's request, provide kevin with a list of milestones which we will not address in f16. reschedule them to s16 in jira.",1,test
DM-6343,Update LDM-151 introduction to reflect new structure,"some proposals made on dm6247 to change the structure of ldm151 (add algorithmic components section, move overview into production specific sections, add notation section to introduction) were accepted at the live meeting on 6/27.  this issue rewrites the introduction accordingly.",1,test
DM-6345,Firefly Python API scope and decision,python api to use firefly visualization components and other functions.   this story is to come up with a good plan for the rest of the development work to related to python api. ,2,test
DM-6346,User installation and operation instructions for conda ,create documentation for the stack conda binaries created in dm 5415 as part of the science pipelines documentation,3,test
DM-6347,Add FlagDecorator to support FlagHandler in Python,"dm4009 added the c and swig changes needed to allow the flaghandler to be used from python.  during review, nate suggested that a decorator class could be used to improve the use of this code in python.  this ticket will be to review nate's decorator and confirm that it is the correct model for pythononly plugins.    we will also modify the unit test in dm4009 and the empsfapprox plugin in dm6123 to use the decorator.",2,test
DM-6348,Write Calibration Products Production section of LDM-151,write photometric calibration pipeline section of ldm 151,20,test
DM-6349,Replace cameraGeom PAF files,"paf files have long been deprecated, but continue to be used for describing the camera geometry.  we need to replace the paf camerageom files used for cfht megacam, decam, lsstsim and sdss, and the scripts used to convert these files to fits files for reading by the mappers.  they might be replaced by a configuration like yaml, or pure python.",10,test
DM-6350,Generate camera description at build time,"camera geometry used to be defined using paf (policy) files, which are now deprecated. as part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the paf files to the new camera geometry configuration scheme which uses fits files and a python file to describe the camera. these scripts are still part of the obs packages, and some people rely on them for making changes to the camera description. on the other hand, the generated fits files and python file are also first class members of the obs packages. this means that we have two sources of the same information, which is dangerous.    for obslsstsim, obsdecam, obscfht and obssdss, we want these scripts to be the primary source of information.  this means we should delete the generated files, and create them at build time.  we should also standardise the name of the script used to generate these.",3,test
DM-6351,Add skeleton words to LDM-151 for AP,we need to flesh out the algorithmic components and narrative sections to the point of having ~1 sentence per paragraph in the finished document.,10,test
DM-6352,Use the HTM based reference catalogs in tests,"in order to move a.net out of meas_astrom to make it a true dependency, we need to replace its use in tests.",4,test
DM-6356,Add linearity correction to obs_decam,add linearity correction from dm 5462 to obs_decam using the standard linearity tables.  ,4,test
DM-6357,"Take part in LDM-151 Progress Meeting, 2016-06-03",nan,1,test
DM-6358,"Take part in LDM-151 Progress Meeting, 2016-06-03",nan,1,test
DM-6359,"Take part in LDM-151 Progress Meeting, 2016-06-03",nan,1,test
DM-6360,"Update ""Using Boost"" section in DM Developer Guide to prefer standard library by default","implement rfc 185 by updating the ""using boost"" section in dm developer guide to prefer standard library by default.",1,test
DM-6361,Replan (June),nan,11,test
DM-6362,Replan (July),nan,11,test
DM-6363,Replan (August),nan,11,test
DM-6364,Design DAX containers,nan,6,test
DM-6365,L1 DB Prototype (June),placeholder for l1 database prototyping in june 16  to be replaced with actual stories of same sp total,14,test
DM-6368,Adjust version check of EUPS python package to allow v3,to enable python 3 support of the stack the eups python stub package needs to allow python 3.    ,1,test
DM-6369,Test DIA simulation script with Postgres,i will be useful to compare mysql and postgres performance for use in l1. after dm 6918 is complete (means works with mysql) verify that it can also run against postgres. ,5,test
DM-6370,L1 DB Prototype (August),placeholder story for work in this epic in august  replace with detailed stories at same sp load,14,test
DM-6373,Improve skeleton for LDM-151 Algorithmic Components,for all subsections in  algorithms components owned by :    provide enough bullet points to capture scope.    add bullet points for subtly difficult aspects of components.    add extra level subsubsubsection level for measurement.    create matrix of measurement algorithms and contexts.  ,1,test
DM-6375,New image visualization functions (F16),"to support the pipeline qa and build the first web portal, there will be new functions need to be developed.  this epic is to collect those functions.",40,test
DM-6376,Implement DAX containers,implement containerized dax services,4,test
DM-6377,SPIE conference 2016,activity related to attending the spie meeting in june 2016. deliverable is a report on the conference.,15,test
DM-6378,Persist output of simple DCR correction,dm 5695 will create transfer matrices stored as numpy arrays. this ticket extends that work to determine a useful format and write functionality to persist those arrays.,2,test
DM-6379,Investigate CAOM,"investigate observation model interfaces and storage, and applicability of caom",4,test
DM-6380,Attend SPIE conference,this ticket relates to attendance at the spie meeting in edinburgh the last week of june.,10,test
DM-6381,ADQL support in dbserv,work on understanding coordinate systems in adql and implement the adql >qserv rewriter,6,test
DM-6382,Generate template DCR images,dm 5695 will create transfer matrices that can be used to create template images of a field at arbitrary airmass. this ticket is to write the code to generate and persist those template images.,4,test
DM-6383,Use template DCR images for image differencing,"dm 6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate differential chromatic refraction in image differencing. this ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation.",2,test
DM-6384,Create and deploy Conda binaries for v12.0 release build,deploy mac os x and centos 5 conda binaries for v12 to a conda repository (http:/conda.lsst.codes/stack/current).,21,test
DM-6385,Create CLI tool to add mac users.,create a cli script to add and delete mac os x users. somehow this is a many step process on mac os x.,1,test
DM-6387,Deploy Conda repository to S3,deploy conda binaries to s3 using their static website feature. http:/conda.lsst.codes/stack/current.,4,test
DM-6388,Create Ansible automation to run the conda build,create an ansible deploy to automate conda binary builds. target mac os x and centos5.,5,test
DM-6389,Create CentOS5 Conda binaries,create centos5 conda binary builds using docker then push them to the s3 static website.,2,test
DM-6392,Text on variability characterization for LDM-151,expand the variability characterization algorithmic section of ldm 151.,1,test
DM-6394,DM Replanning: ConOps Development,"development of concept of operations documents for various dm services, including data backbone, aa system, l3 hosting, and batch processing for the commissioning phase.    deliverable: conops documents  staff: don petravick, margaret johnson, jason alt, steve petrowicz, hsin fang chiang, jagadeesh yedetore, jim basney, alex withers, robert gruendl (roughly 0.5 effort each)  effort: 33 days  planned start: 6/6/2016  planned end: 8/31/2016",66,test
DM-6395,Data Backbone conops iteration 1: create raw draft (internal),write a raw draft of the concept of operations for data backbone services. in this iteration the document is developed in google docs following the conops template.,3,test
DM-6396,Data Backbone conops iteration 2: group review to produce first draft,"review raw draft of concept of operations for the data backbone services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,test
DM-6397,Data Backbone conops iteration 3: larger review to produce second draft,"review first draft of data backbone services conops within data processing architecture working group, bringing in relevant experts.    input from review is incorporated into a second draft.",6,test
DM-6398,Data Backbone conops formatting: convert second draft to reStructuredText,"when the data backbone services conops is in a solid state, convert the google doc to restructuredtext following dm's documentation versioning process.",2,test
DM-6399,DM replanning: NCSA WBS broken down to Level 3 WBS elements,"break down ncsa restructured wbs to level 3 (02c.07.xx.xx) sufficient for integration into pmcs.    deliverable: draft wbs for project level change processes to accommodate.  staff: don petravick, margaret johnson, jagadeesh yedetore, santanu chaudhuri (roughly 0.5 each)  effort: 5 days  planned start: 6/1/2016  planned end: 6/30/2016",10,test
DM-6400,DM replanning: phased WBS elements for minimal data archiving of camera data and minimal transport via data backbone to NCSA,"phased breakdown of work activities to construct minimal archiving of camera data and transport via data backbone to ncsa. includes release of claim on camera buffer.    deliverable: phased wbs for minimal data archiving of camera data and minimal transport via data backbone to ncsa.  staff: don petravick, margaret johnson, james parsons, steve petrowicz, jagadeesh yedetore, felipe menanteau  effort: 20 days  planned start: 7/1/2016  planned end: 7/31/2016",40,test
DM-6401,DM replanning: instantiating Project Reporting Group and planning & reporting process,"instantiating ncsa's project reporting group and planning and reporting processes.    deliverable: project reporting normalized to new methods.  staff: don petravick, margaret johnson, santanu chaudhuri, jagadeesh yedetore  effort: 10 days  planned start: 6/6/2016  planned end: 6/30/2016",20,test
DM-6403,DM replanning: participation in PM working group,"participation in project management working group for dm replanning.    deliverable: deliverables to pm working group.  staff: santanu chaudhuri, don petravick, margaret johnson  effort: 3 days  planned start: 6/1/2016  planned end: 6/30/2016",6,test
DM-6404,Operations Planning in LOPT and TOWG,"participation in lopt and towg for lsst operations planning to get to conops.    deliverable: deliverables to get to conops  staff: don petravick, margaret johnson  effort: 12 days  planned start: 6/6/2016  planned end: 8/31/2016",24,test
DM-6405,Verification Planning with Systems Engineering,"extend concepts of operations to include adequate verification. will involve coordination with lsst systems engineering team.    deliverable: verification plan descriptions in conops documents  staff: don petravick, margaret johnson, jason alt, paul wefel, steve petrowicz, jagadeesh yedetore  effort: 27 days  planned start: 6/6/2016  planned end: 8/31/2016",54,test
DM-6406,Install and configure GNU/Linux on my office desktop.,nan,4,test
DM-6407,Main prototype for all L1 entities (F16 part 1),main program prototypes for all entities in the l1 prompt processing system. this epic continues work started in x16 and covers work in the first part of the f16 cycle.    deliverable: major component hierarchy and all 'has a' objects list.  staff: jim parsons + 2 summer students (at 50% each)  effort: 15 days  planned start: 6/1/2016  planned end: 7/31/2016,30,test
DM-6408,L1 Startup Scaffolding,scaffolding to remotely start and stop all l1 prompt processing system entities.    deliverable: initial scripts starting and stopping all entities remotely  staff: jim parsons + 2 students (50% each)  effort: 8 days  planned start: 7/1/2016  planned end: 7/31/2016,16,test
DM-6409,L1 System Status Message Dictionary,message formats and contents between all l1 prompt processing system entities. this epic continues work from x16.    deliverable: enumeration of message formats and contents as needed.  staff: jim parsons + 2 students (50% each)  effort: 10 days  planned start: 6/1/2016  planned end: 6/30/2016,20,test
DM-6410,Learn about design principles and usage of data processing tasks.,"as we are working on designing a new workflow framework i should read available documentation and look through the existing code base regarding data processing tasks to learn more how they are written, work, and interact among themselves.",10,test
DM-6411,Message interaction between all L1 entities (F16),"basic messaging and interactions, including data dictionary and message patterns.    deliverable: all communication, data, and reporting paths specified and implemented  staff: jim parsons + 2 students (50% each)  effort: 15 days  planned start: 6/1/2016  planned end: 8/31/2016",30,test
DM-6412,Basic Framework for L1 System Health and Status,"basic framework for l1 prompt processing and archiving system health and status display, including status event recorder.    deliverable: first draft of health checks via message; plan for remote diagnostics  staff: jim parsons + 2 students (at 50% each)  effort: 10 days  planned start: 7/1/2016  planned end: 8/31/2016",20,test
DM-6414,ConOps/Planning Document for the OCS-DM interface,detailed concept and technical implementation plan for dm interface with ocs.    deliverable: conops document for ocs  staff: jim parsons  effort: 7 days  planned start: 8/1/2016  planned end: 8/31/2016,14,test
DM-6415,ConOps/Planning Document for the Camera-DM interface,detailed concept and technical implementation plan for dm interface with the main camera.    deliverable: conops document for camera  staff: jim parsons  effort: 7 days  planned start: 8/1/2016  planned end: 8/31/2016,14,test
DM-6416,ConOps/Planning Document for Base Archiving API,detailed concept and technical implementation plan for base archiving api.    deliverable: conops document for base archiving api  staff: jim parsons  effort: 5 days  planned start: 8/1/2016  planned end: 8/31/2016,10,test
DM-6417,Specification for Comfort Dashboard and Alarms,"detailed specifications for comfort dashboard and alarms, including interactions with human operations, and some technology prototyping.    deliverable: detailed specifications  staff: jagadeesh yedetore, santanu chaudhuri, tbh  effort: 21 days  planned start: 6/13/2016  planned end: 8/31/2016",42,test
DM-6418,Final version of Supertask and Activator prototype implementation,"final version of supertask and activator prototype implementation. this is not the final version of supertask, just what was initially designed.     deliverable: final prototype  staff: matias carrasco kind, mikolaj kowalik  effort: 6 days  planned start: 6/6/2016  planned end: 7/15/2016",12,test
DM-6419,Propose and discuss workflow selection,"propose and discuss workflow selection.    deliverable: rfc for workflow selection  staff: hsin fang chiang, mikolaj kowalik, rob kooper, steve pietrowicz  effort: 5 days  planned start: 7/1/2016  planned end: 7/15/2016",10,test
DM-6420,"Investigate Shifter, HTCondor, preemption, and file cleanup",evaluate use of containers to configure htcondor slots for drp tasks. investigating shifter on blue waters.    deliverable: investigation report  staff: greg daues  effort: 10 days  planned start: 6/1/2016  planned end: 8/31/2016,20,test
DM-6421,Evaluate use of dynamic slot mechanism in HTCondor for DRP tasks,evaluate use of dynamic slot mechanism in htcondor for drp tasks.    deliverable: evaluation report  staff: steve pietrowicz  effort: 5 days  planned start: 6/1/2016  planned end: 6/30/2016,10,test
DM-6422,Python 3 migration work,"migrate existing packages in the lsst stack to support python 3 compatibility. some work will occur during the allhands meeting.    deliverable: migration of existing packages for python 3 compatibility  staff: steve petrowicz, jim parsons, matias carrascokind, mikolaj kowalik, hsin fang chiang  effort: 1 days  planned start: 8/1/2016  planned end: 8/31/2016",2,test
DM-6423,Data Backbone: explore overheads and costs of staging model,explore overheads and costs of staging model from data backbone into data caches.    deliverable: assessment and characterization of staging component of orchestration  staff: steve pietrowicz  effort: 15 days  planned start: 8/1/2016  planned end: 8/31/2016,30,test
DM-6425,Analyze existing implementation of Supertask,look through the code base of current prototype of supertask and activator to understand better limits of existing design of data processing task and how they are being addressed.,6,test
DM-6426,Understand how EUPS works,read developer's guide tutorial and official documentation of eups to understand how it works to manage stack's packages easily.,2,test
DM-6427,Data Backbone: produce abstract API to ingest data into L1 archive ,implement an abstract api to ingest data into the l1 archive in the data backbone.    deliverable: abstract api in github  staff: steve pietrowicz  effort: 4 days  planned start: 6/1/2016  planned end: 6/30/2016,8,test
DM-6428,Data Backbone: initial analysis of OpenStack and Ceph object store APIs,initial analysis of openstack and ceph object store apis in data backbone.    deliverable: analysis report  staff: steve pietrowicz  effort: 10 days  planned start: 6/1/2016  planned end: 6/30/2016,20,test
DM-6429,Data Backbone: initial analysis of DDN WOS object store API,initial analysis of ddn wos object store api in data backbone.    deliverable: analysis report  staff: steve pietrowicz  effort: 10 days  planned start: 7/1/2016  planned end: 7/31/2016,20,test
DM-6430,Object Store and WAN Data Interchange Evaluation,"evaluate data interchange over the wan with object store technology.    deliverable: evaluation report  staff: jason alt, tbd set hire  effort: 25 days  planned start: 6/6/2016  planned end: 8/31/2016",50,test
DM-6431,Commission the evaluation framework on one machine at NCSA,commission the evaluation framework on one machine at ncsa.,10,test
DM-6432,Evaluate at additional sites and/or by simulation using WAN emulation techniques,additional sites and/or simulation using wan emulation techniques.,30,test
DM-6433,Produce evaluation report,wan data interchange evaluation report.,5,test
DM-6434,Cost Model Updates for FY17,semi annual updates of costing forecast in sizing model.    deliverable: rfc for proposed cost updates  staff: jason alt  effort: 10 days  planned start: 6/6/2016  planned end: 6/30/2016,20,test
DM-6435,Service Management for F16 (part 1),"provide service management for lsst development resources at ncsa, including nebula openstack cluster, lsst dev, and relevant fy16 procurements. interface with functional groups to provide support for dm services.    deliverable: services  staff: greg daues  effort: 6 days  planned start: 6/1/2016  planned end: 8/31/2016",12,test
DM-6436,Deploy FY16 Integration Environment,"deploy infrastructure for fy16 sui/qserv integration environment (a.k.a., prototype dac).    deliverable: secure sui/qserv integration environment  staff: 3 ncsa ici engineers (networking, storage, systems)  effort: 65 days  planned start: 6/1/2016  planned end: 6/30/2016",100,test
DM-6437,Convert GWT projection and Coorindate Conversion routine to JavaScript,convert booth's projection code and judy bennet's coordinate conversion routines to pure javascript  ,8,test
DM-6438,Test the new JS convertion and projection routines against the java versions,"now that the gwt algorithmic code has been converted it needs to be validated. set up the test for both the projections routines and the coordinate conversion routines.    task details   unit test should be run on the java and javascript side   a unit test should exist for all 10 projections. more might be necessary since there are variation within a projection type.   we need to bring booth in for details of each projection and each variation.   booth has example file somewhere.   xiuqin ran coordinate conversion test in to past.  she (and maybe booth) have the best understand of what that test should be.    the same input file should be used for the java and javascript side.   the same output (if possible) should be produced.      status so far    projections     gnomonic  somewhat tested   orthographic somewhat tested   aitoff  somewhat tested   sfl  somewhat tested   plate   somewhat tested   linear   arc   car   ncp   cea      coordinate conversion.    it appears to work when run in firefly, however the code is not fully covered.    switching between gwt and pure javascript    to switch between gwt and js edit visutil.js and change the following line:  export const usegwt= false;    true uses the old code, false used the new code.    entry points and directories       coordinate conversion:  dir: firefly/src/firefly/js/astro/conv, entry point: visutil.js, convert(), line 104, also see line 26    projection: dir: firefly/src/firefly/js/visualize/projection, entry point:_ webplot.js, makewebplot(), line 124  ",12,test
DM-6439,Remove GWT from build,after the code is tested remove the gwt from the build. should check with  to make sure the boolean to enable gwt has been removed from visutil.js and webplot.js,4,test
DM-6440,May 2016 LAAIM work,gave input on iam design for fy16 integration environment.  discussed iam replication requirements with stakeholders.  attended local ncsa lsst coordination meeting.,4,test
DM-6441,Create Ansible automation to run the conda build,complete ansible implementation started in dm 6388.,4,test
DM-6443,Measure photometric and astrometric precision for DECam COSMOS dataset,measure the photometric and astrometric precision for the decam cosmos dataset and determine the sources of extra systematic scatter.,26,test
DM-6444,"For the 3_build-git-image.sh, pass -j$(nproc) to scons to speed up the build process",nan,2,test
DM-6445,Verification CoDR preparation,just capturing fe's sps  towards this. ,6,test
DM-6446,Remove boost dependencies where possible,"in x16/dm 5580, we removed boost from a number of packages. however, we may not have rigorously updated their dependency lists to indicate where boost is no longer required. please do so.",1,test
DM-6447,Revise and improve DMTN-020,"an initial version of dmtn020, describing project management practices, was produce in dm6140. revise and update that based on feedback from the dm project manager, dm project controls specialist, dm technical managers, and others.",10,test
DM-6448,Deploy FY16 Storage Expansion (part 2),"deploy infrastructure for fy16 storage expansion. this epic covers followon work to dm3830.    deliverable: storage expansion  staff: 5 ncsa ici engineers (networking, storage, systems)  effort: 45 days  planned start: 6/1/2016  planned end: 7/31/2016  ",90,test
DM-6449,Deploy FY16 Nebula Expansion (part 2),"deploy infrastructure for fy16 nebula expansion. this epic covers followon work to dm3832.    deliverable: expanded services  staff: 3 ncsa ici engineers (networking, storage, systems)  effort: 10 days  planned start: 6/1/2016  planned end: 7/31/2016",20,test
DM-6450,Deploy FY16 Cluster Services (part 2),"deploy infrastructure for fy16 cluster services. this epic covers followon work to dm5624.    deliverable: cluster services deployed  staff: 3 ncsa ici engineers (networking, storage, systems)  effort: 10 days  planned start: 6/1/2016  planned end: 6/30/2016  ",20,test
DM-6451,Deploy FY16 Verification Cluster (part 2),"deploy infrastructure for fy16 verification cluster. this epic covers followon work to dm5626.    deliverable: verification cluster  staff: 5 ncsa ici engineers (networking, storage, systems)  effort: 65 days  planned start: 6/1/2016  planned end: 6/30/2016  ",100,test
DM-6452,L1 System Mock 1: Butler component,implement a mock program that receives the level 1 processing system data stream to simulate butler integration.    deliverable: mock program  staff: felipe menanteau  effort: 5 days  planned start: 7/1/2016  planned end: 7/31/2016,10,test
DM-6453,L1 System Mock 2: Archive component,integrate mock api to ingest data into archive that organize data spatially on tape.    deliverable: mock api  staff: felipe menanteau  effort: 5 days  planned start: 8/1/2016  planned end: 8/31/2016,10,test
DM-6454,"Investigate the feasibilty of hosting an extracted, transformed and loaded database at archive site instead of a full EFD","investigate the feasibilty of hosting an extracted, transformed and loaded (etl) database at archive site instead of a full efd. gather sufficient details to support change request to eliminate full efd at archive site. address evident concerns relating to the increased data volume in the reformatted efd and better integration into operational context, including the data backbone.     deliverable: sufficient details to support a change request  staff: steve peckins  effort: 16 days  planned start: 7/1/2016  planned end: 8/31/2016    ",32,test
DM-6455,Investigation of workflow and interface tools in the OpenStack environment,"investigation of workflow and interface tools in the openstack environment. this is learning to support the eventual toolkits anticipated in the sui for production deployment at the dac at ncsa.    deliverable: investigation report  staff: matias carrasco kind, 2 students (at 50% each)  effort: 20 days  planned start: 6/15/2016  planned end: 8/15/2016",40,test
DM-6456,create description of features in storage APIs,"the apis for the storage brokers we're looking into are similar, but don't have a 1 1 correspondence.  write up the features offered by the apis, and see where there is overlap.",4,test
DM-6457,Design and RFC for Repository Refactor,drive the rfc for repo refactor to completion (this includes a lot of design work),20,test
DM-6458,Expand skeleton in LDM-151,we need to flesh out the skeleton text to contain full descriptions of the algorithms and pipelines we expect the baseline design to use.,20,test
DM-6459,"productize ""Repository Refactor""","after rfc184 is closed: implement, unit tests, review, document, submit.    when this story closes, i think rfc184 status is supposed to be changed from adopted to implemented.",20,test
DM-6460,Ramp-up adminstrative capability of qserv for deployment of SUI prototype system,"new staff will be on boarded who has no prior experience with qserv. the goal is to ramp up to provide administration for the deployment of the prototype dac, and to foster development of documentation within the qserv project that facilitates administration and usability of the product as a component in the lsst systems.    deliverable: administration of qserv for sui prototype system  staff: steve peckins  effort: 15 days  planned start: 6/1/2016  planned end: 8/31/2016",30,test
DM-6461,App logging framework migration work,"app logging framework migration work. enhance the lsst::log package, prepare and do a rfc, migrate codes to use lsst::log, deprecate pex_logging.    deliverable: framework migration  staff: hsin fang chiang  effort: 20 days  planned start: 7/1/2016  planned end: 8/31/2016",40,test
DM-6462,"Emergent middleware work (F16, part 1)","reserve of effort to handle minor middlewarerelated work that emerges during the f16 cycle, juneaugust.    deliverable: tbd middleware fixes  staff: steve pietrowicz, hsinfang chiang, mikolaj kowalik, matias carrascokind, rob kooper  effort: 10 days  planned start: 6/6/2016  planned end: 8/31/2016",20,test
DM-6463,Please provide how-to-reproduce instructions for LSST/HSC comparison epics,"for all the stories describing comparisons between hsc and lsst results (notably dm5301 and dm5827), please provide instructions describing the steps to reproduce the comparison. in particular, include:     a list of any tweaks that had to be applied to the code;   non default configuration options;    exactly which comparisons were made.",2,test
DM-6464,Compare CModel results from LSST and HSC,"demonstrate that the hsc and lsst stacks produce consistent results for cmodel measurement. account for (and fix, where relevant) differences.",4,test
DM-6465,Compare Kron results from LSST and HSC,"demonstrate that the hsc and lsst stacks produce consistent results for kron measurement. account for (and fix, if relevant) any differences.",4,test
DM-6466,Compare meas_mosaic-ed HSC and LSST coadds,"the previous comparison of coadds on hsc and lsst was performed without an operable lsst based meas_mosaic. when one becomes available, demonstrate that mosaicking is consistent between hsc and lsst; describe, account for, and (where possible) correct differences.",5,test
DM-6467,Account for noise replacement differences between LSST and HSC,"in dm 5827,  wrote:    in most of these plots you can see some scatter at relatively bright magnitudes... these are likely getting different pixel values when we replace objects with noise which is causing these changes.    check that this is the case, and, if so, explain why the noise replacement is different.",5,test
DM-6470,convert jenkins-ebs-snapshot job to use credentials for aws keys,"at present, this job is being templated by puppet to inject the keys in plain text which are they converted by jenkins to stored secrets if/when the job is edited and resaved via the jenkins ui.  this means that the credentials may be leaked.",1,test
DM-6471,Conda eups packages don't work if eups is already configured,"when  attempted to use the conda repository he ran into a problem installing and using the packages because he already had an active eupsdir and eupspath.    when the eups package is installed and linked, it should warn users when these environment variables are set.    i'm open to another solution but would prefer it doesn't change the eups package behavior. changing behavior goes against the best practices for conda and more generally packaging.",1,test
DM-6473,Possible image related issues in firefly viewer,"image meta data tab   images cannot be remove, but in expanded mode, it can.   selecting image no longer highlight table.  the reverse works fine.   visualize/saga/imagemetadatawatcher.js:272 returns 1.   when a nonmeta table is selected, images are shown, but not the toolbar.   after table is removed, images are still there.    image external api does not mix well with firefly viewer.   firefly viewer uses 'triviewimages’ viewerid while api has no viewerid.  as a result, images loaded by api will be lost once table or other searched data are returned.    catalog overlay are drawn outside of the images.    more issues:      it's possible to select distance tool and then area selection. first drag would define area selection, all the following line. a click would be defining a 0 length line, even if point selection is enabled.  ",8,test
DM-6474,Restore star selector registry,"restore the registry for star selectors that was lost in dm 5532, now that tasks in registries can be used as subtasks.    also use the registry where appropriate.",2,test
DM-6475,Install conda psutil instead of LSST's version, requests that conda lsst uses conda's psutil. currently we use our own version.,1,test
DM-6476,Report and work around conda repository change,this needs to be worked around by either using a different version of condabuild or addressing the changes to the conda recipe structure. i also want to comment and/or create an issue on condabuild so they know that such changes are affecting users.    see:    https:/github.com/conda/condabuild/issues/1003  https:/github.com/conda/condabuild/pull/1004    https:/github.com/conda/condabuild/commit/b4ec0e0659d8f376042d4fc391616bf235996cf5    https:/github.com/mjuric/condalsst/commit/6a552b6f9cada2530681cfdc4a9f67add261ff99 but that fix will be broken as soon as conda build #1004 is merged.,1,test
DM-6487,Form validation regression issues,"recent changes in 'dev' made some of the components stop working.  we need to click through firefly viewer to identify the problems and fix it.  below are a few that i've spotted.    data sets menu:  form fail validation when they should not.  filters and upload file should be nullable.    catalogs classic menu:  form fail validation without any visual indications.  valid parameter is false when passed into validupdate in completebutton.    xyplot(scatter plot) options:  beside x and y, everything else should be optional(nullable).  even after entering a valid value into all of the fields, 'ok' still fail validation similar to above where 'valid' parameter is false when passed into completebutton.    there may be more, please do a quick search to make sure all usage of completebutton is good.   ",4,test
DM-6490,Investigate calibration zeropoint offset between HSC vs. LSST processCcd.py runs,"as reported in dm4730, while the scatter between single frame processing measurements of the same dataset on the hsc vs. lsst stacks is quite good (rms = 0.009 mag between gaussian fluxes, for example, in the figure shown on that ticket), there is a clear offset (0.0166 mag in the figure shown) in the zeropoint between the two stacks (it is systematic, i.e. no trend with magnitude).  the cause may well be due to slight differences in the reference stars selected for calibration.  we also speculated about differences in slot definitions used in the calibrations steps (e.g. for aperture corrections, psfex, etc...), so i have rerun visit 1322 through both stacks having forced all apertures used in calibration to be the same, namely a circular aperture of 12 pixels measured using the sinc algorithm (as opposed to ""naive"").  i have attached the processccd.py config files for the two runs so my settings can be reproduced.    also of note, i am using a measalgorithms branch on the hsc stack with the following commit:      commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917  author: lauren macarthur /  date:   wed jan 13 16:35:59 2016 0500        hack to allow flux.aperture to be used in apcorr            since it does not seem possible to access the nth element of a      schema element that is an array in the context of setting a config      override, this allows for flux.aperture to be set as      calibrate.measureapcorr.reference and it sets it to index 4 (which      corresponds to a radius of 12 pixels) in the init.  this was      selected to match the current lsst default.    diff git a/python/lsst/meas/algorithms/measureapcorr.py b/python/lsst/meas/algorithms/measureapcorr.py  index 9f6c599..f1fa99d 100644   a/python/lsst/meas/algorithms/measureapcorr.py   b/python/lsst/meas/algorithms/measureapcorr.py  @@ 81,6 81,9 @@ class measureapcorrtask(lsst.pipe.base.task):       def init(self, schema, kwds):           lsst.pipe.base.task.init_(self, kwds)           self.reference = keytuple(self.config.reference, schema)          if self.config.reference == 'flux.aperture':              print ""note: setting aperture correction flux to flux.aperture[4] ==> radius = 12 pixels""  +            self.reference.flux = self.reference.flux[4]           self.tocorrect = {}      i attach some of the figures comparing the psf fluxes from these runs which compare the output of the two stacks having matched the two src catalogs.  there are two sets: 1) having adjusted the flux for each source to the zeropoint calculated in the calibration and stored as fluxmag0 2) having adjusted the flux for all sources to a common zeropoint (zp=33.0, chosen to roughly match the calibrated zp).  note that my figures do include aperture corrections (in dm5301, many of the plots show fluxes preaperture correction).  i have also included plots that directly compare the aperture corrections applied (difference in mag units).  finally, i also include plots comparing the 12 pixel circular aperture mags (i.e. to which no apcorr is added).    clearly, the zeropoint determined in the calibration of the two stacks differs between the two stacks and, in particular, there seem to be some very problematic ccds where the differences are particularly significant (~0.05 mag, and not always in the same direction).  please investigate the source of this discrepancy.",8,test
DM-6491,Investigate offset in baseline zeropoint between LSST vs. HSC stack reductions for some HSC visits,"dm6490 reports on an offset between the calibration zeropoints between hsc vs. lsst processccd.py runs.  here we report another, additional, offset seen in certain hsc visits.  it is not seen in the figures shown in dm6490 for visit 1322.  however, here attach the same figures for visit 19696, run with identical setups/configs for both stacks as in dm6490, where we see an additional offset in the ""common zp"" figures (i.e. all fluxes have been scaled the same zp=33.0 for comparison).    a best guess at present is that the calibration frames are different between the hsc and lsst stacks for the timeframe of this visit; e.g. were the inputs ingested exactly the same for both sets?  did the bug in regards to flagging on the flats noted in dm5124:    i found a difference in the codes doing the statistics: the hsc code uses a hard coded mask ignore list of detected only, while the lsst code uses a configurable mask ignore list that defaults to detected,bad (and the default isn't overridden). this produces a large difference on ccds with bad amps (e.g., ccd=9).  there's a smaller difference on ccd=49 because the number of bad pixels is smaller. also note that the scaling of one ccd (like ccd=9) can affect others because we force the normalisations to correspond to that which we get from solving the system of m exposures of n ccds.    have a greater impact on these calibs?    please investigate the cause of this offset.",8,test
DM-6492,Review LTS-210,nan,2,test
DM-6494,Better error messages from the camera mapper when a template cannot be formatted,"the cameramapper produces a very unhelpful traceback if it cannot format a template string with the provided data id dict. for example:    traceback (most recent call last):    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipetasks/12.0.rc13gb785bf9/bin/processccd.py"", line 25, in /      processccdtask.parseandrun()    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipebase/12.0.rc11g832266b/python/lsst/pipe/base/cmdlinetask.py"", line 450, in parseandrun      parsedcmd = argumentparser.parseargs(config=config, args=args, log=log, override=cls.applyoverrides)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipebase/12.0.rc11g832266b/python/lsst/pipe/base/argumentparser.py"", line 479, in parseargs      self.processdataids(namespace)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipebase/12.0.rc11g832266b/python/lsst/pipe/base/argumentparser.py"", line 577, in processdataids      dataidcontainer.makedatareflist(namespace)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipebase/12.0.rc11g832266b/python/lsst/pipe/base/argumentparser.py"", line 126, in makedatareflist      dataref=dr)]    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/pipebase/12.0.rc11g832266b/python/lsst/pipe/base/argumentparser.py"", line 935, in dataexists      return butler.datasetexists(datasettype = datasettype, dataid = dataref.dataid)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafpersistence/12.0.rc11gc553c114/python/lsst/daf/persistence/butler.py"", line 288, in datasetexists      locations = self.repository.map(datasettype, dataid)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafpersistence/12.0.rc11gc553c114/python/lsst/daf/persistence/repository.py"", line 392, in map      return self.doparents(repository.domap, args, kwargs)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafpersistence/12.0.rc11gc553c114/python/lsst/daf/persistence/repository.py"", line 325, in doparents      res = func(parent, args, kwargs)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafpersistence/12.0.rc11gc553c114/python/lsst/daf/persistence/repository.py"", line 405, in domap      loc = self.mapper.map(args,  kwargs)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafpersistence/12.0.rc11gc553c114/python/lsst/daf/persistence/mapper.py"", line 169, in map      return func(self.validate(dataid), write)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafbutlerutils/12.0.rc16/python/lsst/daf/butlerutils/cameramapper.py"", line 284, in mapclosure      return mapping.map(mapper, dataid, write)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafbutlerutils/12.0.rc16/python/lsst/daf/butlerutils/mapping.py"", line 123, in map      path = mapper.mapactualtopath(self.template, actualid)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafbutlerutils/12.0.rc16/python/lsst/daf/butlerutils/cameramapper.py"", line 732, in mapactualtopath      return template % self.transformid(actualid)  typeerror: %d format: a number is required, not nonetype      it is unclear what string was being formatted with what data, making the problem difficult to diagnose and correct.    i suggest changing line 732 of cameramapper.py from:    return template % self.transformid(actualid)    to something like the following:    try:      transformedid = self.transformid(actualid)      return template % transformedid  except exception as e:      raise runtimeerror(""failed to format %r with data %r: %s"" % (template, transformedid, e))      here are the last few lines of the same traceback after applying this change:        path = mapper.mapactualtopath(self.template, actualid)    file ""/users/rowen/uw/lsst/lsstsw/stack/darwinx86/dafbutlerutils/12.0.rc1+6/python/lsst/daf/butlerutils/cameramapper.py"", line 735, in _mapactualtopath      raise runtimeerror(""failed to format %r with data %r: %s"" % (template, transformedid, e))  runtimeerror: failed to format '%(date)s/%(filter)s/decam%(visit)07d.fits.fz[%(hdu)d]' with data : %d format: a number is required, not nonetype      a bit wordy, but it is much easier to figure out what went wrong.    i have stumbled across this problem twice in the last few weeks, so i consider this change fairly important. the first time it was caused by a defective format string in a paf file. this time i'm not yet sure what is causing it, but at least i have something to go on.",1,test
DM-6497,Assist IN2P3 engineer in loading DC2013 data sample,"bogdan vulpescu, in2p3 engineer, tried to load dc2013 data sample. fabrice help was required to install qserv in multinodes and understand dataloading system.    some issues have been found and will be reported in future tickets:     a script to publish loaded data (i.e. insert db name in qservw_worker.dbs) would be useful   mysql client might break proxy if option are not provided correctly (a bug report will be available soon)",3,test
DM-6498,Assist IN2P3 student in using Openstack and following LSST coding standards," has written a code to automate qserv cluster boot on openstack cloud. soma support was required to understand and solve cloud init and openstack issues, qserv container deployment and lsst coding standards.",6,test
DM-6500,convert irsaviewer to react.js,"convert irsaviewer to use the new firefly library built on react/flux.    also made these changes:  remove irsa footer from fireflyviewer  filter by selected rows  auto correct table’s filter input.  multiple columns sort via sortbycols  fix menu item not showing selected  ife automatically builds firefly.    to test, make sure you pull ife repos as well.  same branch name on ife.",10,test
DM-6501,Regrid needed to for WebGrid,"when compute the points for the grid lines, there is no guarantee that the number of points will all be the same.  however, the points can be regrided to ensure all the lines have the same number of points.",6,test
DM-6502,setup test framework,"need to decide what to check so we have a consistent testing, requirement is opensource   language   license   maturity   funding   ease of install   dependencies   os requirements   ease to create a workflow   ability to execute on clusters/laptop   test with simple lsst workflow   willingness to meet and answer our questions   open bug reporting site   speed at which bugs are resolved   size of community, external collaborators   how big a graph can it support?   is shared filesystem required for data, or can it take care of data transfer   does it support mpi other parallel code?   smart wrt to data available on node (optional)  ",2,test
DM-6503,pegasus,review pegasus workflow management system (https:/pegasus.isi.edu) against criteria defined in the epic.,6,test
DM-6504,Swift,review http:/swift lang.org/main/index.php scripting language against criteria defined in the epic.,6,test
DM-6505,final report,nan,4,test
DM-6506,panda,nan,6,test
DM-6510,Verification Plan Systems Engineering Status Review,"this follows on from  [dm 5315] and covers collating comments from dmlt, submitting the status report and document to systems engineering and dealing with the comments.     ",4,test
DM-6513,"Remove unsused ""version.h"" file and associated code","this code seems obsolete and unused:      qserv@clrinfopc04:~/src/qserv (tickets/dm5967)$ grep r ""version.h""    admin/tools/docker/git/src/qserv/sitescons/genversion.py:# genversion.py : declare a builder for global version headers.  admin/tools/docker/git/src/qserv/sitescons/genversion.py:    """"""construct a version header from git describe output and store  admin/tools/docker/git/src/qserv/core/modules/sconscript:versionfile = env.command(['global/version.h'], none, genversion.buildversionheader)  core/modules/sconscript:versionfile = env.command(['global/version.h'], none, genversion.buildversionheader)  ",3,test
DM-6514,Minor fixes to linearization,"dm5462 added linearization to isrtask but had a few loose ends which this ticket aims to correct:   i intended to enable linearization by default, but somehow lost that change.   i intended to update obstest to use null linearization, but i forgot and the previous item meant i didn't catch the omission   it turns out that the butler data proxy object will not work with functors (attempting to call the retrieved item results in an error, rather than resolving the proxy). this is easily worked around by using immediate=true when retrieving linearizers. this didn't show up until dm 6356 because obsdecam is the only camera that uses linearization lookup tables, and obs_subaru avoids the problem by not returning a proxy.  ",1,test
DM-6516,Convert footprint support,convert the footprint support from the gwt code,20,test
DM-6518,Fix scheduler delays caused by mlock call in memman.,"locking tables in memory with mmap and mlock greatly increases scan query speeds but makes the worker scheduler unresponsive to interactive queries. this also tends to have only one scheduler (fast, slow, medium) running at a given time.",8,test
DM-6519,Temp local background broken,the temp local background feature has been broken and needs to be fixed.,1,test
DM-6520,Prepare an RFC about logging migration,"summarize rfc29, evaluate technical details, prepare working examples, reraise rfc 29 or file a new rfc before the migration.  some implementation may be done before the new rfc.  ",10,test
DM-6521,Enhance lsst.log by having a Log object and Python interface ,"based on branch u/ktlim/getlogger in log and requests from dm 3532, implement a lsst::log python interface through log objects, and allow controllability of logger names and levels in python.  ",7,test
DM-6524,Capture ProjMgmt WG Long Term Planning conclusions in DMTN-020,the projmgmt wg is going to agree on a strategy for long term planning. make sure it's captured in dmtn 020.,3,test
DM-6527,Statement of Work ,nan,10,test
DM-6528,Networking Configuration,nan,5,test
DM-6529,Investigate single frame processing astrometry failures/poor solutions on some HSC chip/visits.,"the astrometric solution of some visit/ccd combinations for hsc data are failing or finding very poor solutions.  this typically occurs for the outermost (highly fringed) ccds (e.g. 100..103, 95).  i provide some sample output below.      processccd.calibrate.astrometry.refobjloader: loaded 71 reference objects  processccd.calibrate.astrometry.matcher: filterstars purged 0 reference stars, leaving 71 stars  processccd.calibrate.astrometry.matcher: purged 4436 unusable sources, leaving 288 usable sources  processccd.calibrate.astrometry.matcher: matched 6 sources  processccd.calibrate.astrometry.matcher warning: number of matches is smaller than request  processccd.calibrate.astrometry: matched and fit wcs in 1 iterations; found 6 matches with scatter = 0.000 + 0.000 arcsec        20160610t17:13:54: processccd.calibrate.astrometry: found 80 catalog sources  20160610t17:13:54: processccd.calibrate.astrometry: matching to 119/148 good input sources  20160610t17:13:55: processccd.calibrate.astrometry: matched 20 sources  20160610t17:13:55: processccd.calibrate.astrometry warning: number of matches is smaller than request  20160610t17:13:55: processccd.calibrate.astrometry: 20 astrometric matches for 100, 031  20160610t17:13:55: processccd.calibrate.astrometry: refitting wcs  20160610t17:13:55: processccd.calibrate.astrometry: astrometric scatter: 0.038076 arcsec (with nonlinear terms, 20 matches, 0 rejected)        processccd.calibrate.astrometry.refobjloader: loaded 68 reference objects  processccd.calibrate.astrometry.matcher: filterstars purged 0 reference stars, leaving 68 stars  processccd.calibrate.astrometry.matcher: purged 2206 unusable sources, leaving 225 usable sources  processccd.calibrate.astrometry.matcher: matched 4 sources  processccd.calibrate.astrometry.matcher warning: number of matches is smaller than request  processccd fatal: failed on dataid=:     file ""src/sip/createwcswithsip.cc"", line 142, in lsst::meas::astrom::sip::createwcswithsip/::createwcswithsip(const std::vector/&, const lsst::afw::image::wcs&, int, const lsst::afw::geom::box2i&, int) [with matcht = lsst::afw::table::match/]      number of matches less than requested sip order   lsst::pex::exceptions::lengtherror: 'number of matches less than requested sip order'        20160610t17:20:11: processccd.calibrate.astrometry: found 84 catalog sources  20160610t17:20:11: processccd.calibrate.astrometry: matching to 137/162 good input sources  20160610t17:20:11: processccd.calibrate.astrometry: matched 19 sources  20160610t17:20:11: processccd.calibrate.astrometry warning: number of matches is smaller than request  20160610t17:20:11: processccd.calibrate.astrometry: 19 astrometric matches for 103, 1_31  20160610t17:20:11: processccd.calibrate.astrometry: refitting wcs  20160610t17:20:11: processccd.calibrate.astrometry: astrometric scatter: 0.086131 arcsec (with nonlinear terms, 18 matches, 1 rejected)      other failed visit/ccd combos:  visit=19684 ccd=101  visit=30488 ccd=95: runtimeerror: unable to match sources    this may simply be due to some threshold in the configs that is rejecting more stars on the lsst side, but this is not confirmed.  please investigate the cause of these failures.",6,test
DM-6533,LDM-151 adjustments,"adding text to ldm 151 where appropriate, working around the structure defined by jim et al.",2,test
DM-6538,Write DMTN describing Lupton diffim decorrelation,"write a technote describing the analysis and implementation of the lupton(zogy) difference image decorrelation correction.    a new technote has been set up, it will be: http:/dmtn 021.lsst.io",12,test
DM-6540,Propose track to improve container infrastructure,"qserv uses docker for deployment, this ticket will add track on how to improve container deployment and management.",10,test
DM-6541,lsst-dev shared stack should provide release builds,"the shared stack on lsst dev (etc) currently only provides tagged weekly builds of the lsst stack. releases, rcs, etc are not included. please update the build script so that they are.    nb simply including these builds is easy enough by changing the versionglob regular expression in sharedstack.py. however, the current version is selected by a lexicographic sort of available versions. that works well enough for weekly builds (w2016xx is less than w2016xx+1), but fails with other tags. better use a sort based on the date the tag was created on the http server instead, perhaps.",1,test
DM-6542,Prevent external viewer from popup blockers.,"currently, when external viewer launched, it is blocked by pop up blockers. need to change polling logic to a pushed solution so the 'launch' action can happen immediately. ",3,test
DM-6544,Release Note integration for 12_0 Stack Release,transcribe v12_0 release notes prepared by development teams on confluence into the pipelines documentation sphinx project.    pipelines documentation is published with lsst the docs to https:/pipelines.lsst.io.,3,test
DM-6545,Setup mononode test environment for initial learning about installations,how to load data and perform queries.  investigate dax interface to qserv.,4,test
DM-6546,Add queryId to messages at start and end of user queries.,"the queryid, ""qi=xxx:"", needs to be added to log messages that are useful for analysis. of primary interest are messages that indicate that a query has begun or finished, such as ""discarded userqueryselect"".",1,test
DM-6547,Capture proposed epic review procedure in DMTN-020,see notes at https:/confluence.lsstcorp.org/display/dm/projmgmtmeeting20160614,1,test
DM-6548,Capture release policy in DMTN-020,"capture the policy for releases (all work to be done 2 weeks before end of cycle, release at the cycle changeover) in dmtn 020.",1,test
DM-6552,Attend SBAG prep meeting at UW, will travel to uw for preparatory discussions in advance of this month's sbag meeting.,8,test
DM-6554,"Take part in LDM-151 Progress Meeting, 2016-06-13",nan,1,test
DM-6555,"Take part in LDM-151 Progress Meeting, 2016-06-13",nan,1,test
DM-6556,"Take part in LDM-151 Progress Meeting, 2016-06-13",nan,1,test
DM-6557,"Take part in LDM-151 Progress Meeting, 2016-06-20",nan,1,test
DM-6558,"Take part in LDM-151 Progress Meeting, 2016-06-20",nan,1,test
DM-6559,"Take part in LDM-151 Progress Meeting, 2016-06-20",nan,1,test
DM-6561,Fix order of flags in Kron photometry,the flags are not added to the flag handler in the correct order for kron photometry.,1,test
DM-6563,Clean-up rerun documentation,"following dm4443, there are a few ambiguities in the new  rerun documentation. fix them.",1,test
DM-6566,Make updateSourceCoords and updateRefCentroids more visible,implement rfc 197 to make updatesourcecoords and updaterefcentroids more visible,1,test
DM-6568,"Further prep for SBAG meeting, attend video telecon with Heidi et al.",read back ground materials on lsst moving object simulations. this will be used to prepare for both the sbag meeting and to come up with questions that need clarification in the preparatory telecons.,5,test
DM-6569,Remove the extra init method from the SourceDetectionTask,"sourcedetectiontask defines both init(self, schema=none, kwds) and init(self, schema=none, kwds). the first exists purely because of a doxygen bug that makes \copydoc init fail. however,     copydoc init    works. remove the non dunder init method and update the documentation with    \copydoc init  .",1,test
DM-6575,Refactor Known Issues and Metrics pages in Pipelines Docs,make known issues and metric report both toplevel pages. link to installation issues from installation page.    see https:/pipelines.lsst.io/v/dm6575/index.html,1,test
DM-6577,Convert jointcalTask unittest into a validation measure,"now that jointcal has some basic unittests that check whether the relative and absolute astrometry are less than some value, we should convert those tests into validation measures a la https:/github.com/lsst/validatedrp. this would help us track whether we are actually improving things as we tweak the algorithm and the mappings that we fit.",4,test
DM-6578,Initial tests running HTCondor jobs utilizing Shifter,"we start with initial tests of shifter, with the first goal to  submit pbs jobs on the blue waters test system utilizing shifter that start htcondor master/startd daemons on compute nodes.  these daemons will communicate to a remote htcondor central manager (e.g., running on the nebula openstack) and glide in to join a working pool.  the setup will then be tested with simple payload jobs (these  submitted from a nebula instance running the schedd)  that verify access to the lsst stack within the udi (user defined image).",4,test
DM-6580,Understand and ensure variance plane compliance with diffim decorrelation,"understand how the variance plane should be adjusted in the decorrelation (zogy) correction, and ensure it is being done correctly.",8,test
DM-6581,Decrease warning messages in dipoleFitTask,the dipolefittask was spitting out too many warnings. change many of those to debug statements and remove the `lmfit` userwarnings.,1,test
DM-6582,Design a metadata system for LSST code and documentation repositories (technote),"this ticket involves the research and design of a metadata system for describing lsst code and documentation repositories. such metadata would be leveraged by dochub and lsst the docs (see https:/sqr011.lsst.io) and would reside as a yaml/json file in a resource’s github repository.    http:/jsonld.org is of particular interest. i’m also consulting with github, ads, zenodo, and cfa library on making a sustainable system.    note: this story should be moved to a dochub epic.",3,test
DM-6588,Adapt qa analysis script for LSST vs. HSC coadd processing comparison,the analysis script was adapted for single visit processing comparisons in dm4393 and dm4730.  do the same here for coadd processing comparisons.,4,test
DM-6589,Fill out Software Primitives section of LDM-151,nan,2,test
DM-6591,Implement exception translators in upstream pybind11,pybind11 does not currently support translation of custom exceptions. this ticket tracks work done on upstream pybind11 (internal fork https:/github.com/lsstdm/pybind111) to implement this functionality. it should support functionality equivalent to (but not necessarily with the same api) as boost python exception translators (http:/,4,test
DM-6593,firefly api related issues due to irsa integration.," firefly_loader.js mistakenly uses relative path to load dependencies when it should resolve it via location of the loading script.   tablepanel should render html content as html by default.   paging bar style does not show correctly in irsa html   row height does not resize to the icon size, the old api did. and the default row selectable is set to false in the old api.   the help button needs to be added on top of the table panel.   the expand button does not function as expected (open a full table panel).",6,test
DM-6596,Write command-line driver tutorial for LSST@Europe2 meeting,"this will be done as dmtn 023 so the results are preserved for posterity.    this may be somewhat redundant with the work mandeep gill is doing in translating hsc docs, but i need it now; we can merge later.",1,test
DM-6598,Prepare presentation for SPIE,write the presentation for the spie conference. date of presentation: 26th june.,2,test
DM-6600,Clean up naming of multiband tasks and scripts,"several of the multiband processing tasks and files in pipetasks and pipedrivers have inconsistent names:    some task names do not agree with the script names.    words like ""coadd"" and ""merged"" are not consistently used.     actually making these changes is trivial, but the work also requires creating and shepherding an rfc.",1,test
DM-6601,Port change to EXP-ID handling,"from https:/hscjira.astro.princeton.edu/jira/browse/hsc1409:    due to an operational reason (to meet the requirement of subaru fits dictionary), the definition of expid is soon to be changed in the data acquisition side.  in the new definition, expid is set to 'hsce%08d' where the letter 'e' is fixed as requested in the dictionary, and the number part corresponds to exactly the same number as our familiar 'visit'.  obs_subaru:ingest.py needs to be updated to include this rule.  the data taken with this change so far are:  hsca07441200hsca07441757  hsca90925200hsca90929557      the change made as part of hsc1409 introduces a new code path for the updated data, while old data continue to be supported with the old code path.",1,test
DM-6603,Reporting improvements,read and critique jacek's  lpm document and the more manual oriented work from john swinbank.  phone con with kevin w.r.t. reporting channel for equipment expenses in jira (as opposed to the now clear separate distinct financial channel).  worked out checklist and principals for revised wbs. ,6,test
DM-6607,Install packstack to test OpenStack Object Storage API,nan,2,test
DM-6608,Finalize v12 Pipelines release documentation,add the release announcement and finalize other documentation details in pipelines.lsst.io for the v12 release.,1,test
DM-6609,Review LDM-135 (LSST Database Design),nan,4,test
DM-6610,Further refine alert generation pipelines sections,"there is much more information in the document, but the pipelines sections need to be refined.  we also need to give the software primitives a go over.",20,test
DM-6611,Update X16/W16 release notes for qserv and dax services,nan,2,test
DM-6612,Make HSC processing without bright object catalogs easier,"obssubaru enables bright object masks by default, as that's desirable for hsc production runs.      however, when hsc data is processed without bright object masks available (as will happen in most go observations and development use), multibanddriver.py will fail because the brightobject mask plane is not present but the basepixelflags algorithm is configured to make use of it. this is confusing, and it also requires the definition of a configuration file to fix the problem because basepixelflags cannot be configured directly on the commandline.    some possibilities for fixing this:    add the bright_object mask plane in assemblecoadd if domaskbrightobjects is true but the external catalog is not found.  this will make the pixelflags operation a silent noop.    allow configuration options to allow pixelflags algorithm to silently skip some flags if the appropriate masks are not available.    i am sure there are other options as well.  ",2,test
DM-6614,Include Kron parameters in algorithm metadata,"the kron code doesn't set the algorithm metadata.  e.g.    algmetadata.set(""extphotometrykronkronfluxnradiusforflux"",                  config.plugins[""extphotometrykron_kronflux""].nradiusforflux)    ",1,test
DM-6616,"update ""newinstall.sh"" nebula images & docker containers - v12_0",nan,1,test
DM-6620,Cannot instantiate LoadAstrometryNetObjectsTask without Config object,"one should be able to create a loadastrometrynetobjectstask without passing a config object, if one only wants the default configuration. currently it raises typeerror:      traceback (most recent call last):    file ""testjointcal.py"", line 79, in setup      refloader = loadastrometrynetobjectstask()  typeerror: init() takes at least 2 arguments (1 given)      if the config object really is a kwarg, it should default none and create a default config, so that one doesn't have to do, e.g.:      loadastrometrynetobjectstask(loadastrometrynetobjectsconfig())  ",1,test
DM-6621,cleanup non-survey-generic python in jointcal,"jointcal.py current does things like:      for dataref in datarefs:      if dataref.dataid[""visit""] == int(visit) and dataref.dataid[""ccd""] == int(ccd):          ...      this is not survey generic, and is probably not the best way to identify data blocks anyway. this, and other non generic things in jointcal.py should be cleaned up so they work across surveys.",4,test
DM-6622,make jointcal integration/validation test for hsc,"we need an integration/validation test for jointcal on hsc data, to show that jointcal can run safely on hsc data processed through the stack.",10,test
DM-6623,make jointcal integration/validation test for cfht,"we need an integration/validation test for jointcal on cfht data, to show that jointcal can run safely on cfht data processed through the stack.",10,test
DM-6624,make jointcal integration/validation test for DECam,"we need an integration/validation test for jointcal on decam data, to show that jointcal can run safely on decam data processed through the stack.",10,test
DM-6625,make jointcal integration/validation test for lsstSim,"we need an integration/validation test for jointcal on lsstsim data, to show that jointcal can run safely on lsstsim data processed through the stack.",10,test
DM-6627,Fix base_* stuff in CcdImage.cc,"ccdimage.cc currently has hard coded a bunch of getschema().find(""baseblah"").key things. these should either be replaced with ""slot "", config.blahname, or dealt with at a higher level (e.g. not loading all those values directly inside of ccdimage::loadcatalog).    once this is done, we should delete the comments at the top of the file.",1,test
DM-6628,Reimplement diffim decorrelation as task,reimplement the image decorrelation as a subtask rather than a direct call to a function.,6,test
DM-6629,validate_drp: design and implement an API for metric measurements and serializations,"validatedrp computes metrics and generates json that, through the https:/github.com/lsst/postqa tool, is submitted to the squash rest api for persistence and display in a web app.    a previous ticket, dm 6086, we bolted on a json serialization scheme compatible with squash. however, this approach was not well integrated with validatedrp. we want a framework/api where serialization is handled consistently and integrally with metric computations. this includes the semantic serialization of computational parameters and reduced datasets.    this api can be applied beyond validate_drp as a means for metrics and integration tests to be submitted to squash as well.",22,test
DM-6630,Support ingesting reference catalogs from FITS files,support a means of ingesting index reference catalogs from fits tables (e.g. sdss catalogs).,2,test
DM-6631,Single-frame processing tasks are no longer usable without a Butler,"adding a butler argument to the constructor signatures for characterizeimagetask, calibratetask, and processccdtask makes these tasks difficult to use without a butler.    the fix is to make the butler argument optional (with a default of none), while adding another argument that allows a fullyconstructed reference object loader to be provided directly instead.    this is closely related to dm6597, which has the opposite problem: pipe_drivers' singleframedrivertask doesn't take a butler argument, but it needs to in order to provide one to processccdtask.    i have a fix for this just about ready, but i'd like to add some unit tests that verify we can run all of these tasks both from the command line and directly before calling it complete.",3,test
DM-6632,Make match and flag propagation more reusable,"we have two bits of code for doing spatial matches and propagating flags:    propagatevistflagstask: propagates flags from individual visit catalogs to coadd catalogs, and depends on a butler to do so (reasonably; it includes the smarts to load the appropriate catalogs, so it has to do i/o).    calibratetask.copyicsourcefields: propagates fields from icsrc to src, but is only usable as part of calibratetask.    both of these should delegate at least some of their work to new class (possibly a task) that manages the schemas, schemamappers, and crossmatching necessary to do this work.  this new class should be reusable without a butler and without constructing any higherlevel tasks.",4,test
DM-6633,"HSC ISR configuration file is applied to ProcessCcdTask, not IsrTask","obs_subaru/config/hsc/isr.py has its config options specified relative to processccdtask's config hierarchy, not isrtask's.  this allows the isr task to be retargeted in this file, but it will prevent isrtask from being run as a cmdlinetask directly.    isr task retargeting should be moved to config/processccd.py, allowing the config/isr.py level to be moved to the appropriate level.",1,test
DM-6634,Add JIRA-wrangling howto to DMTN-020,expand https:/dmtn020.lsst.io/v/dm6447/#jira maintenance to describe best practices for t/cams working with jira. include:     appropriate labels;   teams;    ... other things?,1,test
DM-6638,LTD Keeper: Auto slug for edition paths deals with underscores,"had a bug where utils.autoslugifyedition did not replace underscores with a dash, and therefore failed utils.validatepathslug. this created a silent breaked where a branch like u/rowen/r12_patch1 did not get an edition created for it.    this ticket adds this replacement code and adds a test for such a case.",1,test
DM-6640,IsrTask is not a valid CmdLineTask,"isrtask is a commandline task, but its run method does not take a dataref (it instead has a rundataref method.  this is inconsistent with other cmdlinetasks and more importantly breaks parseandrun.    i'm committing a small workaround on dm6631 to get parseandrun working, but the ultimately method names should be made consistent across cmdlinetasks.  that will require an api change and hence an rfc.",1,test
DM-6642,Make list of elements for consideration for planning packages,create a detailed checklist for developing the planning packages for the replan and wbs restructuring.,2,test
DM-6643,RADICAL-Pilot,review http:/radicalpilot.readthedocs.io/en/latest/index.html against criteria defined in the epic.,6,test
DM-6644,Makeflow,review makeflow against criteria defined in the epic.   http:/ccl.cse.nd.edu/software/makeflow/,6,test
DM-6645,pinball,review https:/github.com/pinterest/pinball workflow management system.,6,test
DM-6646,CloudSlang,review cloudslang against criteria defined in the epic.   http:/cloudslang docs.readthedocs.io/en/v0.9.60/index.html,6,test
DM-6647,Adapt qa analysis script to apply corrections measured by meas_mosaic,dm2674 involves getting hsc's measmosaic working with the lsst stack.  this issue consists of adapting the analysis.py script of dm4393 & dm 4730 to (optionally) apply the astrometric and photometric solutions derived running measmosaic to the individual visits before comparison.  this is useful in general and is specifically useful in comparing the meas_mosaic results between the hsc and lsst stacks.,2,test
DM-6650,Management level review of two products of the Management process working group,"reviewed https:/github.com/lsst/ldmpmt/blob/integration/index.rst and https:/dmtn020.lsst.io/v/dm6447/    made extensive markup of ldmpmt,  delivered to mario juric.  assessed dm 6447,  which show promise of an actual workable manual, though not complete.",1,test
DM-6651,Move new reference loader so meas_astrom can use it and perform some cleanup,"the new reference object loader code lives in pipetasks, which means it cannot be directly used by code in measastrom. this will hamper separating astrometry.net out of measastrom, because unit tests need reference catalogs and measastrom cannot depend on pipe_tasks.    also, i'd like to take a cleanup pass on the module names, so the new code is easier to find, and improve the unit tests.",2,test
DM-6652,Remove database hack,"dm5988 introduced a hack in reading the raw files: we use a database to cache metadata from the shutter files and update the camera files at read time.  the camera files have now been ""sanitised"" (updated with the appropriate metadata), and it's time to remove the hack.     writes:    data is on lsstdev in:    /nfs/lsst2/photocaldata/data/monocam/sanitised9/1m3/1m3    raw calibs are in:    /nfs/lsst2/photocaldata/data/monocam/sanitised9/1m3/calibs    regarding what i want: everything to be the same, but with a normal ingest, i.e. no splicing, just taking everything that is needed from one set of files. some points to note:     should be able to ingest all the raws and calibs files, and register their object types to allow processing with these as ids (inc. pipedrivers scripts)   pipedrivers master calib scripts should still run (and their outputs still be ingestable)    processccd should run  ",2,test
DM-6653,implement the active target,"when a dialog such as catalog search is displayed, it should be able to pick up the active target or the coordinates from a highlighted row in a table. please, implement the mechanism that will automatically pick up those coordinates and pre fill the search form for you.",6,test
DM-6656,ffApi image related issues found by irsa integration,"for external image viewer, the default rangevalues causes problem, i.e. other defaults not set. (fixed)  global default does not always apply to external image viewer(fixed dm7016)  the gator implementation related to coverage map   (1) default symbol size, shape, color setting is different from that of original map   (2) cannot specify the shape, size, and color through api;   (3) cannot specify the shape, size, and color of a search center through api;   (4) does not display any image and source when the table has only one ra,dec values, for example:  one table with one position value or one table with many records but has the same ra,dec values. moved to [dm7001]   (5) the sources on coverage map are not clickable. however, on table and plot are clickable and work fine. (fixed)  ",6,test
DM-6657,ffApi XYplot related issues found by irsa integration," default  symbol size, shape,and color setting is different from that of original version.   no xy plot options popout windows    the plot displays nonascii characters on the panel (for example: â fitâ â )   miss filter dialog on the plot panel comparing with the original version.     does not accept default column names for the plot.",6,test
DM-6660,CR finder does not care about XY0 of input image,"port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 1391:    the current version of cr finder does not care about xy0 of the input image and when i try to run cr finder on warped (difference) image, psf cannot be properly extracted.    and:    i have noticed that the center of warped image is a gap between ccds and psf estimation there will fail. so get psf without specifying the position is good enough. psf class will select the best position.  ",1,test
DM-6661,"ConfigDictField says ""Inequality in keys for..."" even if I give 2 same configurations","from https:/hscjira.astro.princeton.edu/jira/browse/hsc1401:    config.py:    from lsst.meas.photocal.colorterms import colortermgroupconfig    for key in ['i', 'i2', 'y', 'r', 'n1', 'n2', 'n3', 'z']:      root.calibrate.photocal.colorterms.library[key] = colortermgroupconfig.fromvalues({})    this comamnd line    rm fr output ; for i in  ; do processccd.py ./hsc output output c config.py  ; done    raises following error    20160601t02:43:45: processccd fatal: comparing configuration: inequality in keys for calibrate.photocal.colorterms.library: ['z', 'i', 'i2', 'r', 'y', 'n1', 'n2', 'n3'] != ['n3', 'i', 'i2', 'r', 'y', 'n1', 'n2', 'z']  20160601t02:43:45: processccd fatal: failed in task initialization: config does match existing config on disk for this task; tasks configurations must be consistent within the same output repo (override with clobberconfig)    ",1,test
DM-6663,Study iPlant as a potential candidate for workspace implementation,nan,2,test
DM-6664,Investigate why afw.table.IdFactory doesn't allow reserved=0,"setting reserved=0 when constructing a source id factory (as would be logical when there is no exposure id to reserve bits for) strangely doesn't work; it seems to be necessary to reserve at least one bit.  this may be a signedness problem (we use signed 64 bit integers for ids to appease fits, which is unfortunate), but we should be careful just reducing the number of available bits, as this could break code that expect to read ids already written to disk.    note that any change to this code in afw.table may require changes to code in daf.butlerutils.exposureidinfo as well.",2,test
DM-6665,set up unit test for projection in Java,"while working on dm 6438 (set up unit test for projection in javascript), we realized we should have a parallel unit test system set up for java code, to keep the two systems in sync. ",6,test
DM-6667,Data Backbone conops iteration 4: submit to TCT,"submit the document for tct change control. process is tbd.     if it is not accepted by tct, further work is not in the scope of this epic, and would need to be planned in the ev system.",1,test
DM-6668,Data Backbone conops: develop engineering considerations for BOE for work package,"based on the data backbone services conops, develop a list of engineering considerations for making a boe for the data backbone planning package.",3,test
DM-6669,Authentication & Authorization conops iteration 1: create raw draft (internal),write a raw draft of the concept of operations for authentication and authorization services. in this iteration the document is developed in google docs following the conops template.,5,test
DM-6670,Authentication & Authorization conops iteration 2: group review to produce first draft,"review raw draft of concept of operations for the aa services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,test
DM-6671,Authentication & Authorization conops iteration 3: larger review to produce second draft,"review first draft of aa services conops within data processing architecture working group, bringing in relevant experts.    input from review is incorporated into a second draft.  ",6,test
DM-6672,Authentication & Authorization conops formatting: convert second draft to reStructuredText,"when the aa services conops is in a solid state, convert the google doc to restructuredtext following dm's documentation versioning process.",2,test
DM-6673,Authentication & Authorization conops iteration 4: submit to Systems Engineering,"submit the document for tct change control. process is tbd.    if it is not accepted by tct, further work is not in the scope of this epic, and would need to be planned in the ev system.",1,test
DM-6674,Authentication & Authorization conops: develop engineering considerations for BOE for work package,"based on the aa services conops, develop a list of engineering considerations for making a boe for the aa planning package.  ",3,test
DM-6675,Level 3 Hosting conops iteration 1: create raw draft (internal),write a raw draft of the concept of operations for level 3 hosting services. in this iteration the document is developed in google docs following the conops template.,4,test
DM-6676,Level 3 Hosting conops iteration 2: group review to produce first draft,"review raw draft of concept of operations for the l3 hosting services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,test
DM-6677,Level 3 Hosting conops iteration 3: larger review to produce second draft,"review first draft of level 3 hosting services conops within data processing architecture working group, bringing in relevant experts.    input from review is incorporated into a second draft.  ",6,test
DM-6678,Level 3 Hosting conops formatting: convert second draft to reStructuredText,"when the l3 hosting services conops is in a solid state, convert the google doc to restructuredtext following dm's documentation versioning process.",2,test
DM-6679,Level 3 Hosting conops iteration 4: submit to TCT,"submit the document for tct change control. process is tbd.    if it is not accepted by tct, further work is not in the scope of this epic, and would need to be planned in the ev system.",1,test
DM-6680,Level 3 Hosting conops: develop engineering considerations for BOE for work package,"based on the l3 hosting services conops, develop a list of engineering considerations for making a boe for the l3 hosting planning package.",3,test
DM-6681,Batch Processing for commissioning conops iteration 1: create raw draft (internal),write a raw draft of the concept of operations for batch processing services for the commissioning phase. in this iteration the document is developed in google docs following the conops template.,4,test
DM-6682,Batch Processing for commissioning conops iteration 2: group review to produce first draft,"review raw draft of concept of operations for the batch processing for commissioning services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,test
DM-6683,Batch Processing for commissioning conops formatting: convert first draft to reStructuredText in a technical note,"when the batch production for commissioning services conops is in a solid state, convert the google doc to a dm technical note in restructuredtext.",1,test
DM-6684,Batch Processing for commissioning conops: develop engineering considerations for BOE for work package,"based on the batch services for commissioning services conops, develop a list of engineering considerations for making a boe for the batch services planning package.",3,test
DM-6685,"Planning package for Management, Engineering and Integration with engineering judgement BOE based on RACI diagram","following list of elements for consideration (dm 6642), estimate planning packages for management, engineering and integration wbs element.    boe is derived from raci document, which list roles and responsibilities of line management, reporting group, steering group, and area technical leads.",1,test
DM-6686,Planning package for L1 Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for level 1 services wbs element.    boe is derived from detailed plan for prompt processing and archiving services created in february and engineering judgement based on conops documents.",1,test
DM-6687,Planning package for Batch Production Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for batch production services wbs element.    boe is derived from engineering judgement based on conops documents.  ",1,test
DM-6688,Planning package for Data Backbone Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for data backbone services wbs element.    boe is derived from engineering judgement based on conops documents.  ",1,test
DM-6689,Planning package for Data Access Hosting Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for data access hosting services wbs element.    boe is derived from engineering judgement based on conops documents.",1,test
DM-6690,Planning package for Common Workflow/Middleware with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for common workflow/middleware wbs element.    boe is derived from engineering judgement based on conops documents.",1,test
DM-6691,Planning package for Misc. Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for miscellaneous services wbs element. an example is the authentication and authorization services.    boe is derived from engineering judgement based on conops documents.",1,test
DM-6692,Planning package for Development Support Services with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for development support services wbs element.    boe is derived from engineering judgement.",1,test
DM-6693,Planning package for ITC and Fabric Provisioning and Operation with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for itc fabric provisioning and operation wbs element.    boe is derived from engineering judgement.  ",1,test
DM-6694,Planning package for Service Management with engineering judgement BOE,"following list of elements for consideration (dm 6642), estimate planning packages for service management wbs element.    boe is derived from engineering judgement based on itil methodology.",1,test
DM-6695,Submit change request,submit formal change request to restructure ncsa wbs in pmcs.,1,test
DM-6696,Revise Level 1 ConOps    ,revise the level 1 conops to incorporate the minimal required functionality of level 1 services: minimal data archiving of camera data and minimal transport via data backbone to ncsa.,4,test
DM-6697,Build draft design,produce functional design breakdown from the revised conops (dm 6696).,16,test
DM-6698,Articulate the design in format needed for planning,articulate the design created in (dm 6697) into the format needed for planning.,12,test
DM-6699,Produce revised WBS,"based on articulated design, revise wbs to incorporate phase.",10,test
DM-6700,Discuss elements of RFC,discuss elements of rfc (technical details and scope).,6,test
DM-6701,Produce RFC,write up and submit rfc.,2,test
DM-6702,Respond to RFC comments and update RFC as needed,respond to rfc comments and update rfc as needed.,2,test
DM-6705,Select workflow based on conops and review of workflow systems,"based on use cases/requirements gathered in dm6270 and evaluation reports completed in dm6276, select workflow system.",2,test
DM-6706,Discuss elements of RFC,"discuss elements of workflow rfc (technical details, scope, requirements).",4,test
DM-6707,Produce RFC,write up and submit rfc.,2,test
DM-6708,Respond to RFC comments and update RFC as needed,respond to rfc comments and update rfc as needed.,2,test
DM-6709,Pull down and install OCS SAL code in prep for ConOps development,"at the camera workshop in mid june, ocs team members suggested that dm pull down their service abstraction layer software and gain familiarity with it. the user manual is being studied before compiling the software and running it with dm software as a means of simulating planned telescope & site processes and how dm will interact with them.  most of the work for this epic will be conducted in august. this story captures our prep work.",4,test
DM-6710,Monitoring plan for Startup procedure,identify startup processes to be monitored for health and to provide notification for startup failure.,1,test
DM-6711,L1 entity prototypes,this story addresses the need to separate the processes that connect to the daq and retrieve the image data from the processes that forward the image data to ncsa. requirements for this component and the component that formats the image data into a file which includes associated metadata were discussed at length during the camera workshop this month. prototypes for these component processes are underway.,16,test
DM-6712,Message Dictionary additions,"message types for system bookkeeping acknowledgements as well as report messages were added to the existing dictionary and the means for acting upon these message types are being added to component prototype code.  in addition, needed changes were made to the existing dictionary so all reporting entities write more complete details to their report message queues.",4,test
DM-6713,Amendments to message interaction,proper acknowledgements began being added to the messaging system this month.,6,test
DM-6714,Camera Workshop attendance,work on preliminary specific additions to the camera interaction conops took place this month during attendance at the camera workshop,6,test
DM-6715,Use Shifter+HTCondor  in processing Stripe82 ref data at modest scale,"to test out processing at modest scales (~ 100  1000 cores)  utilizing shifter+htcondor on machines like bw, organize processing (processccd of obssdss) of stripe82 data similar to that used in the lsstdmstackdemo (run=4192 field=300).",6,test
DM-6723,Add tests for order of flags to all measurment plugins,"in the measbase framework, we independently define an enumeration of available flags https:/github.com/lsst/measextensionsphotometrykron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/include/lsst/meas/extensions/photometrykron.h#l82 and a set of table fields for storing flags https:/github.com/lsst/measextensionsphotometrykron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/src/kronphotometry.cc#l422. we implicitly assume that these are declared in the same order, but do not, in general, enforce this.    in dm6561, these were found not to be in the same order in measextensionsphotometrykron. setting a flag based on a bad result would therefore set the wrong flag in the output table.    in the dm6561 solution, we introduced a test for this which is specific to the photometrykron codebase. however, the basic structure of the test would be easily extended to cover all measbase plugins to ensure this error can never occur. do so.",3,test
DM-6725,"Message refinement , in light of development #1",nan,12,test
DM-6726,Default chart and other optimizations,"these are the changes to support defalt chart and single chart type (as for irsa release)   remove chart selection from chart area   use dropdown for chart selection (can be omitted if single chart type is used)   populate current values in chart options   support clear and reset in chart options   for tables, connected to charts, if no default parameters are specified, default chart is an xy plot with catalogcoordcols (used to produce an overlay) for catalogs or two first numeric columns for other tables.   label, matching column expression, and unit, matching table model, are default parameters for both app and api now.  ",6,test
DM-6727,Message refinement #2,nan,12,test
DM-6728,Camera workshop attendance,"attend camera workshop, meeting did not fully address the need.  travel to slac.",8,test
DM-6729,Summarize meeting results into Concepts of Operation,nan,2,test
DM-6730,ConOps for Comfort Console and System Monitor,"this is the first definition of the concept of operation of the comfort console and system monitor piece of the dm system. this application will play a key role in fault detection and correction as well as monitor actively the state (and sub state) of all the components in the dm system. based on the role of the operator, he / she will be able to dig down into the faults and take corrective action. an action dashboard will provide a hierarchical view of the state of the system and its components at any point in time.",20,test
DM-6731,Definitions of Alarms and Actions,this is a task to define all the failure cases and alarms that they should generate. it will also define the action that will be taken in the event of an alarm / fault and who will be taking the action.,30,test
DM-6732,"Process reference data on ""lsstdev pool"" for reference",nan,2,test
DM-6733,Reference processing on NERSC Cori Shifter implementation,nan,4,test
DM-6736,Write test jobs and submit files,nan,2,test
DM-6737,Investigate HTCondor configuration wrt dropping of nodes in backfill scenario,nan,4,test
DM-6738,Run test jobs and evaluate,nan,2,test
DM-6740,Write final report,nan,2,test
DM-6741,Make SuperTask data-aware,nan,10,test
DM-6742,Add workflow features,nan,10,test
DM-6743,Select existing tasks for prototyping conversion to workflow supertask,nan,1,test
DM-6744,Convert selected tasks,convert a cmdlinetask into supertask,10,test
DM-6745,Finish gathering input from DM representatives,nan,3,test
DM-6746,Compile input,nan,2,test
DM-6747,Review conops template,nan,1,test
DM-6748,Iteration 1: Write raw draft based on input gathered from DM representatives,nan,6,test
DM-6749,Iteration 2: group review to produce first draft,nan,6,test
DM-6750,Iteration 3: larger review to produce second draft,nan,8,test
DM-6751,Iteration 4: final draft and convert to reStructuredText to produce tech note,nan,4,test
DM-6752,Service Management for F16 June,dividing f16 service management  ~ monthly.,4,test
DM-6753,Service Management for F16 July, dividing f16 service management  ~ monthly.  ,4,test
DM-6754,Service Management for F16 August, dividing f16 service management  ~ monthly.,4,test
DM-6755,Write test programs to exercise Swift API with OpenStack ,nan,2,test
DM-6756,Write test programs to exercise Swift API with Ceph,nan,2,test
DM-6757,Benchmark Swift command line tool for objects less than 5GB,nan,4,test
DM-6758,Benchmark Swift command line tool for objects greater than 5GB,nan,4,test
DM-6759,Benchmark Swift custom tool for objects less than 5GB,nan,4,test
DM-6760,Benchmark Swift custom tool for objects greater than 5GB,nan,4,test
DM-6761,Analyze results and write report,nan,4,test
DM-6762,Find and read documentation for OpenStack Swift API,nan,1,test
DM-6763,Find and read documentation for Ceph API,nan,1,test
DM-6764,Write abstract API,nan,4,test
DM-6765,Research existing API,nan,2,test
DM-6766,Find and read documentation for DDN WOS API,nan,1,test
DM-6768,Create description of features in storage APIs,nan,2,test
DM-6769,Write test programs to exercise Swift API with DDN WOS,nan,2,test
DM-6770,Benchmark Swift command line tool for objects less than 5GB,nan,2,test
DM-6771,Benchmark Swift command line tool for objects greater than 5GB,nan,2,test
DM-6772,Benchmark Swift custom tool for objects less than 5GB,nan,2,test
DM-6773,Benchmark Swift custom tool for objects greater than 5GB,nan,2,test
DM-6774,Analyze results and write report ,nan,3,test
DM-6775,Write test programs to exercise object stores,nan,6,test
DM-6776,Benchmark Swift command line tool for objects less than 5GB,nan,2,test
DM-6777,Benchmark Swift command line tool for objects greater than 5GB ,nan,2,test
DM-6778,Benchmark Swift custom tool for objects less than 5GB,nan,2,test
DM-6779,Benchmark Swift custom tool for objects greater than 5GB ,nan,2,test
DM-6780,Write report ,nan,6,test
DM-6781,remove SizeMagnitudeStarSelector,"the sizemagnitudestarselector is still in meas_algorithms, but it is unused and likely no longer works. we should either remove it, or update it to be fully supported.    the same holds true for any other c based star selectors we still have lying around.",1,test
DM-6783,Add support for deriving from Python exception types to pybind11,dm 6302 adds support for custom exception translators to pybind11. however exceptions mapped do not inherit from python baseexception or higher. this prevents exceptions from being raised and caught with except exception as e in python. this behaviour also occurs with boost python and swig (we hack around it with a pure python wrapper).    this ticket aims to solve the problem by adding support for inheritance from python exception types to pybind11.,4,test
DM-6784,Port meas_extensions_convolved from HSC,hsc has a new measurement extension: measextensionsconvolved.  this performs aperture photometry with the psf degraded to nominated seeings (similar to how galaxy photometry is commonly done these days).    relevant hsc tickets are https:/hscjira.astro.princeton.edu/jira/browse/hsc1395 and https:/hscjira.astro.princeton.edu/jira/browse/hsc1408.,5,test
DM-6785,Port parent/child measurement from HSC,"the deblender sometimes gets into trouble with cluster galaxies, and the deblended fluxes aren't accurate.  in that case it helps to have measurements on the image without any deblending having been performed.  this is a feature used in hsc's mid2016 production run afterburner, ticket https:/hscjira.astro.princeton.edu/jira/browse/hsc1400.  this feature should be ported for use in lsst.",5,test
DM-6788,Document meas_extensions_ngmix,"measextensionsngmix has no useful documentation, not even a doc directory. add some.    this should include at least an overview of the package contents, a description of its capabilities, and instructions on enabling it within the meas_base framework. the package should have a readme.",2,test
DM-6789,Provisioning,nan,15,test
DM-6790,Product Acceptance,nan,15,test
DM-6791,Disaster Recovery Implementation,nan,20,test
DM-6792,Documentation,nan,5,test
DM-6793,Capability Validation,nan,5,test
DM-6794,Security Vetting,nan,5,test
DM-6795,Acceptance by Stakeholders,nan,10,test
DM-6796,Capability Design,nan,10,test
DM-6797,Gathering product pricing,nan,10,test
DM-6798,Updating LDM-143,nan,5,test
DM-6799,Acceptance into baseline,nan,5,test
DM-6800,Design,nan,10,test
DM-6801,Implementation,nan,10,test
DM-6802,Capability Design,nan,15,test
DM-6803,Procurement,nan,10,test
DM-6804,Reception and Placement,nan,10,test
DM-6805,Networking Configuration,nan,10,test
DM-6806,Provisioning,nan,25,test
DM-6807,Disaster Recovery Implementation,nan,10,test
DM-6808,Documentation,nan,10,test
DM-6809,Capability Validation,nan,20,test
DM-6810,Security Vetting,nan,10,test
DM-6811,Acceptance by Stakeholders,nan,10,test
DM-6812,Qserv container crashes on Openstack using up to date CentOS/docker setup,"    [qserv@lsstfabricejammesqserv0 ~]$ docker run it net=host e ""qservmaster=lsstfabricejammesqserv0"" qserv/qserv:devmaster bash    qserv@lsstfabricejammesqserv0:/qserv$ /qserv/run/bin/qservstart.sh   info: qserv execution directory : /qserv/run  starting mysql  [fail.] manager of pidfile quit without updating file. ... failed!  [failing xrootd.[....] : manager of pidfile quit without updating file. ... failed!   failed!  see startup logfiles : /qserv/run/var/log/xrootdconsole.log, /qserv/run/var/log/worker/xrootd.log  [failing cmsd.[....] : manager of pidfile quit without updating file. ... failed!   failed!  see startup logfiles : /qserv/run/var/log/xrootdconsole.log, /qserv/run/var/log/worker/cmsd.log  [ ok ing mysqlproxy..  [failing qservwatcher failed!  see startup logfile : /qserv/run/var/log/qservwatcher.log  [ ok ing qservwmgr.    # here error log can be different sometimes...  qserv@lsstfabricejammesqserv0:/qserv$ cat /qserv/run/var/log/mysqld.log  ...  20160627 23:50:00 140703880873792 [note] innodb: waiting for purge to start  20160627 23:50:00 140703880873792 [note] innodb: 5.6.27 started; log sequence number 1661735  20160627 23:50:00 140703880873792 [note] plugin 'feedback' is disabled.  20160627 23:50:00 140703105521408 [note] innodb: dumping buffer pool(s) not yet started  20160627 23:50:00 140703880873792 [note] server socket created on ip: '::'.  20160627 23:50:00 140703880873792 [note] /qserv/stack/linux64/mariadb/10.1.11.lsst2/bin/mysqld: ready for connections.  version: '10.1.11mariadb'  socket: '/qserv/run/var/lib/mysql/mysql.sock'  port: 13306  source distribution  20160627 23:50:04 140703665879808 [note] /qserv/stack/linux64/mariadb/10.1.11.lsst2/bin/mysqld: normal shutdown    20160627 23:50:04 140703665879808 [note] event scheduler: purging the queue. 0 events  20160627 23:50:04 140703088736000 [note] innodb: fts optimize thread exiting.  20160627 23:50:04 140703665879808 [note] innodb: starting shutdown...  20160627 23:50:06 140703665879808 [note] innodb: shutdown completed; log sequence number 4432991  20160627 23:50:06 140703665879808 [note] /qserv/stack/linux64/mariadb/10.1.11.lsst2/bin/mysqld: shutdown complete    160627 23:50:06 mysqldsafe mysqld from pid file /qserv/run/var/run/mysqld/mysqld.pid ended  160629 19:36:11 mysqldsafe starting mysqld daemon with databases from /qserv/data/mysql  20160629 19:36:11 139694065747776 [note] /qserv/stack/linux64/mariadb/10.1.11.lsst2/bin/mysqld (mysqld 10.1.11mariadb) starting as process 143 ...  20160629 19:36:12 139694065747776 [note] innodb: using mutexes to ref count buffer pool pages  20160629 19:36:12 139694065747776 [note] innodb: the innodb memory heap is disabled  20160629 19:36:12 139694065747776 [note] innodb: mutexes and rw_locks use gcc atomic builtins  20160629 19:36:12 139694065747776 [note] innodb: memory barrier is not used  20160629 19:36:12 139694065747776 [note] innodb: compressed tables use zlib 1.2.8  20160629 19:36:12 139694065747776 [note] innodb: using sse crc32 instructions  20160629 19:36:12 139694065747776 [note] innodb: initializing buffer pool, size = 128.0m  20160629 19:36:12 139694065747776 [note] innodb: completed initialization of buffer pool  20160629 19:36:12 139694065747776 [error] innodb: ./ibdata1 can't be opened in readwrite mode  20160629 19:36:12 139694065747776 [error] innodb: the system tablespace must be writable!  20160629 19:36:12 139694065747776 [error] plugin 'innodb' init function returned error.  20160629 19:36:12 139694065747776 [error] plugin 'innodb' registration as a storage engine failed.  20160629 19:36:12 139694065747776 [note] plugin 'feedback' is disabled.  20160629 19:36:12 139694065747776 [error] unknown/unsupported storage engine: innodb  201606 29 19:36:12 139694065747776 [error] aborting  ",10,test
DM-6813,Track statistics about user queries and tasks running on chunks,nan,9,test
DM-6814,Move queries to different scheduler if too slow,nan,8,test
DM-6815,Update LSST full-stack processing configuration to match best practice from HSC,"in preparation for running an endtoend comparison of large scale processing with the hsc and lsst stacks, we need to update the configuration to reflect currently understood best practice.    in general, we expect the default hsc configuration to be better understood and ""battletested"" given that it has been used for sciencegrade data releases.    audit the default configuration of the full lsst stack (from processccdtask through multiband coadd processing). where lsst defaults differ from hsc, update the lsst configuration to match the hsc equivalent unless there's a clear reason why lsst's default should be different. when it's not appropriate to update the lsst configuration, add an override to obs_subaru.    in some cases, the lsst and hsc stacks have diverged so that a direct transfer of configuration options isn't possible. where an equivalent can be found, take advantage of it. otherwise, stick with existing lsst defaults.",10,test
DM-6816,"Process HSC ""RC"" dataset through the LSST stack","process the ""rc"" dataset used to verify hsc data releases through the lsst stack using the configuration specified by dm 6815.",4,test
DM-6817,Compare HSC and LSST processing of RC dataset,"using the script enhanced in dm6588, compare hsc and lsst (dm6816) processing of the rc dataset.",8,test
DM-6818,Quality check LSST processing of RC dataset,perform a quality analysis on the lsst processing of the rc dataset (dm 6816) in the same way as would be performed before an hsc data release.,5,test
DM-6819,Resolve CModel issues with aperture corrections,while working on dm 4202 it became apparent that the aperture corrections calculated and applied by cmodel were too large. this ticket is intended to trace down where the failure is occurring and correct it.,4,test
DM-6820,Develop resource loaded plan for executing DRP sections of LDM-151,nan,20,test
DM-6821,Deliver DRP slide deck for LSST Director's Review,required by 20160712.,10,test
DM-6822,Add meas_extensions_ngmix to lsst_distrib,primarily so it can enjoy the benefits of regular ci runs.,2,test
DM-6824,Use meas.algorithms.astrometrySourceSelector in measOptimisticB,"now that there is a working astrometrysourceselector (just merged in meas_algorithms from dm 5933), we should get matchoptimisticb working with it. this would entail replacing matchoptimisticb.sourceinfo with astrometrysourceselectortask and tweaking the latter to do whatever matchoptimisticb needs, and removing sourceinfo.",2,test
DM-6828,Deliver sections for  Operations Use Case Report,"for each diagram covering a key use case, provide a narrative interpretation of the key concepts being conveyed, including significant operational implications from the concepts being presented.    fill in the table for the assigned use case areas.    key use cases/concepts include: l1 production, l2 production, itc incident response, itc problem management.",6,test
DM-6829,Deliver sections for Concept of Operations,"contribute to concept of operations sections about chilean, ncsa, and cc in2p3 facilities. describing the ""nuts and bolts"" basics and summarize each facility's role in the lsst operational system.",3,test
DM-6830,Investigate effects of turning on the Brighter-Fatter correction for single-frame processing of HSC data,"in the process of comparing hsc vs. lsst stack singleframe processing runs, we have been running with the brighterfatter correction (bfc) turned off.  the reason for this to begin with was that is was not yet available on the lsst stack when we started these comparisons.  we also want to isolate as many features as possible in order to confidently assess their individual effects  the functionality was ported on dm4837 with a default of dobrighterfatter=false.  this issue is to continue the singlevisit run comparisons (see e.g. dm5301, dm6490, dm6491) with bfc turned on on both stacks.    in particular, we are finding that slight differences in the reference stars selected for a given ccd can result in significantly different psf models.  also, it was noted in dm4960 that lsst seems to select reference stars to a brighter cutoff than hsc.  if a given field has a larger fraction of bright stars considered in the psf modeling, it is conceivable that it will be more significantly influenced by the bf effect, thus causing the large ccdtoccd variations seen in, e.g. dm6490 (https:/jira.lsstcorp.org/secure/attachment/28213/comparevisitv1322diffbasepsffluxskyzp.png).",6,test
DM-6831,Wrap base with pybind11,split off from dm 6302.,2,test
DM-6832,Wrap utils with pybind11,split off from dm 6302.,2,test
DM-6833,add 'placeholder' attribute to the input element,an attribute called placholder is available in html element / to give a hint to the user of what can be entered. the placeholder text must not contain carriage returns or line feeds.      add it as proptype to / component.,1,test
DM-6834,Write report on SPIE conference,write a report on my visit to the spie conference in edinburgh.,3,test
DM-6835,Learning about Openstack,sahand progress on learning about openstack,4,test
DM-6836,Create a python interface to access OpenStack,sahand progress on getting the interface to access the openstack interface using nova client,3,test
DM-6837,Data Backbone Conops  iteration 1 prep:  Create a list of service endpoints,"create a for list of service endpoints, with service considerations, and deliver to the development file tree. del with new ambiguities from the camera meeting at slac by listing the ""summit data services""  for both main camera and spectrograph as service endpoints,  since this may increase the functionality required, and it seems prudent to flow any of these requirement into further processes, since they seem likely.",2,test
DM-6838,Learning about Spark,sahand progress on getting familiar with spark and use of the interface to  create a small spark cluster in openstack,4,test
DM-6839,Learning about Docker,sahand progress on learning docker containers and potential automatic deploy in openstack,3,test
DM-6840,Set up and install Spark,sahand progress on getting spark installed ,2,test
DM-6841,Learning about Kubernetes,sahand progress on learning about automatic deploy and scalability of docker containers using kubernetes ,2,test
DM-6842,Deal with emergent related requests  affecting operations planning in June,"there emergent request for comment emerged in june.     1) the interim project manager,  directed that the project begin an investigation into amazon wen service due to contacts he developed at a data base orient workshop he sponsors.  formulating  a response required a review of the service offered by aws, and inquiring about the validity of pursing an evaluation of just one vendor in a marketplace that has many vendors, and a deciding that an appropriate amount of work was to send additional ncsa staff to an aws workshop to gain a similar appreciation of aws as was gained at the database meeting at slac.  (authority of interim project manager to insist on immediate action was also sorted out)    2) request to understand computing capabilities at alternate site from the deputy director.  support for for alternate site capabilities are documented in the  the emerging l2 batch concept of operations a copy of which was shared (though draft status noted)     3) processed a summary of the camera meeting which occurred at slac. did not find  conclusions that related to a concept of operations.    in particular we could not understand it there was a call for computing and a summit data service to support disconnected operations,  or if this was a mere optimization in the system to relocate the acquisition and forwarding infrstructure to the summit, with no other changes.",4,test
DM-6843,Learning about Swift and HDFS,sahand progress on storage objects to be used in openstack,4,test
DM-6844,Learning about Openstack and Jupyter,di progress on learning these web technologies,5,test
DM-6845,Installing JS9 in Openstack server,nan,2,test
DM-6846,Learning about SocketIO and HTML REST API,di progress on communication technologies for the web,4,test
DM-6847,Integrating Jupyter and JS9 for FITS visualization,di progress in getting js9 to work in jupyter notebook,6,test
DM-6848,Write wrapper API for JS9 and Jupyter,di progress in writing a wrapper to interact between js9 within jupyter,5,test
DM-6849,Understand the installation and administrative processes,"review and gain administrative insight using the processes encoded in test scripts and other relevant features based on investigations in the prototype installations. for example, we may observe steps in test scripts to gain understanding of capabilities behind the scripts.    provide comments on documentation where deemed helpful.",9,test
DM-6850,Liaison with deployment effort,interact with qserv developers supporting deployment and ncsa's service provisioning environment. learn and investigate aspects of qserv administration present in test deployment but not previously covered.,10,test
DM-6851,Setup multinode test environment for initial learning about installations ,setup up one master node and one worker node.,3,test
DM-6852,Update Activator to reflect recent changes in CmdLineTask,nan,4,test
DM-6853,Discussion regarding  'quanta' definition in SuperTask,nan,2,test
DM-6854,Finalize documentation and current issues of prototype,"after updating some latest changes, need to update documentation to explain the extend of this supertask and activator initial implementation.",4,test
DM-6855,TBD related emergent work in July,nan,5,test
DM-6856,TBD related emergent work in August,nan,5,test
DM-6857,Document that the catalog returned from star selectors is a view,"star selectors return a catalog whose records are shallow copies of the input catalog records. document the shallow copy aspect. this is important for two reasons:   the user should know   implementers must be told this, because if the records are deep copies then the code that sets a flag for stars will not set a flag in the input catalog, which loses most of the point of setting that flag.",1,test
DM-6858,Mapper tests require modification when new datasets are added,"https:/community.lsst.org/t/centrallydefinedbutler datasets/841 a new way to define datasets common to all cameras in daf_butlerutils, but modifying these yaml files require explicit lists of datasets to be modified in tests/cameramapper.py.    if these tests are still useful, they need to depend on a minimal set of dataset definitions instead of the real ones.",1,test
DM-6859,Participation according to direction from interim project management,"given directions from interim project management, participation consisted of direct conversations with kevin and jacek plus background work talking to staff related to assembling a plan.",5,test
DM-6860,Refine simple 1D DCR correction,"dm 5695 created a functional implementation of a simple dcr correction algorithm. while it appears to successfully create template images with airmass and dcr matched to science images, it is computationally inefficient and appears to introduce new artifacts to the template image. this ticket is to enhance the simple algorithm in several ways:   convert to sparse matrices where possible   use variance weighting of the images   propagate masked pixels correctly   refine the algorithm to mitigate the new artifacts",6,test
DM-6861,Understand how to render conops documents in Sphinx,learn how to render conops documents in restructuredtext. prototype conops template and for delivery into technical control team sphinx engineering environment.,1,test
DM-6862,Raw draft of System Monitor and Comfort Display,produce raw draft of conops for review by steering committee. includes operational components and connectivity for the system monitoring services that will monitor devices from the summit to ncsa.,3,test
DM-6863,Add verification feature to L1 conops,nan,7,test
DM-6864,Add verification test to L1 plan,nan,7,test
DM-6865,Add verification test to L1 design,nan,7,test
DM-6866,Add verification feature to Data Backbone conops,nan,10,test
DM-6867,Add verification feature to L2 conops,nan,7,test
DM-6868,Add verification feature to Authentication & Authorization conops,nan,10,test
DM-6869,Liaison with Systems Engineering,nan,6,test
DM-6870,Appreciate amount of effort needed to run preliminary planning exercise,run planning process with local staff to appreciate amount of effort needed.,5,test
DM-6871,Review evaluation criteria with CC-IN2P3,review evaluation criteria with fabio during his visit from ccin2p3 to ncsa.   https:/drive.google.com/open?id=1xhj6kafennhcyrpskb6bcxyxgfs_9cl3brx9wch1se,1,test
DM-6872,Create evaluation plan from evaluation criteria,turn criteria into tabular comparison chart and respect test implementation constraints.,4,test
DM-6873,Estimate amount of effort needed to run detailed planning exercise,"run through process of detailing activities down to story size requested by the lsst evm system.     do detailed estimation of conops development and a sample of technical areas, and extrapolated based on number of epics, size of staff, and complexity of mission. total = 100 hours for 3 months of activities for current staff size.",2,test
DM-6874,Design framework for integrating procurement activities with invoices,respond to request to relate equipment charges to acquisition strategy document procurement activities.,1,test
DM-6875,Design framework for reporting and steering meetings,"run the process with staff to assess and supervise technical status, the appropriateness of work compared to architectural vision, consistency with ncsa general acumen, and status vs. plan.",6,test
DM-6876,TBD processes coordinated with impending hire,"design and implement critical processes defined in the raci document, coordinated with impending hire.",6,test
DM-6879,Address concerns with source side (Dave Mills),"work with dave mills and others to understand architecture and use of ""source"" efd. the goal is to understand the amount of volume of data that would be in reformatted efd that otherwise would not have been, should we proceed with the proposed change.",8,test
DM-6880,Address concerns with target side (SLAC),"understand permissions and protections that would be in the reformatted efd that otherwise would not have been, should we proceed with the proposed change.",8,test
DM-6881,Address internal concerns,understand whether the file annex should be kept in the same cluster as efd as opposed to general files in the data backbone.,6,test
DM-6882,Incorporate into ConOps and any draft design notes,"incorporate concerns, solutions and agreements into conops and any draft design notes.",6,test
DM-6883,Address additional emergent concerns ,address tbd additional emergent concerns ,2,test
DM-6884,Rework MemMan to be inline with the qserv worker Scheduler.,split the memory mapping function from the memory locking function to allow the scheduler to initiate locking without blocking. add additional memory tracking improvements in line with current thinking. reduce lock contention. add logging.,4,test
DM-6886,forcedPhotCoadd.py fails on CFHT data due to a CModel bug,"hello,    forcedphotcoadd fails while running on cfht data due to a cmodel bug. here is an example on the error message that we get:      python: src/cmodel.cc:1368: void lsst::meas::modelfit::cmodelalgorithm::measure(lsst::afw::table::sourcerecord&, const lsst::afw::image::exposure/&, const lsst::afw::table::sourcerecord&) const: assertion `measrecord.getfootprint() >getarea()' failed.  aborted      adding the following lines in cmodel.py (in cmodelforcedplugin.measure, before the call to self.algorithm.measure) allows to go around the problem for the time being, which seems to arise for null value of the number of pixel in a given footprint:      if not measrecord.getfootprint().getarea():      raise valueerror(""measrecord.getfootprint().getarea(): 0. no pixel in this footprint."")  ",1,test
DM-6890,deploy jenkins python env support,nan,1,test
DM-6892,Access to system with LSST stack,secure access to machine(s) with the lsst stack. this includes installation on local desktop/laptop.,2,test
DM-6893,Controlled Test of LMSimpleShape using high SNR objects,"some issues came up during dm 6300 which indicated that a more controlled set of tests would be required than the random great3sims tests to understand the behavior of ngmix lmsimpleshape.      lmsimpleshape appears to fail computing moments on low snr objects.  it also shows pretty wide variation in shear bias which did not show up with cmodel.    the needed tests with would include controlled profiles (gauss, dev, and exp), controlled snr, and controlled q, theta, and flux.  this should separate out the causes of failure and shear variation which we have seen.",6,test
DM-6894,Ensure DipoleFitTask uses correct PSF(s) in case when Decorrelation is turned on,"diffim a&l decorrelation (dm6241) modifies the diffim psf, but leaves the ""presubtraction"" images used by dipolefittask as they were. ensure that the correct psfs are being used for dipole fitting when decorrelation is turned on (and actually, in all cases).",8,test
DM-6897,Get data stream from socket into a fits file,get data stream (module?) from jim into a fits file than can be loaded subsequently to the butler.,2,test
DM-6898,Load known image data format into the Butler,use some type of known data (image) to load and test into the buttler. data types might include decam mef images of single plain image files from simulations.,2,test
DM-6899,Assemble data stream from socket to lsst-stack pipeline,connect all of the parts together.,4,test
DM-6900,ci_hsc failure: insufficient PSF sources classified as stars,"since https:/ci.lsst.codes/job/cihsc/396/, the regular cihsc build has been failing with:    [20160705t23:59:53.929169z]  fatal: at least 95% of sources used to build the psf are classified as stars (49 > 50): fail  [20160705t23:59:53.929201z] traceback (most recent call last):  [20160705t23:59:53.929238z]   file ""/home/build0/lsstsw/build/cihsc/bin/validate.py"", line 3, in /  [20160705t23:59:53.929249z]     main()  [20160705t23:59:53.929317z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [20160705t23:59:53.929334z]     validator.run(dataid)  [20160705t23:59:53.929375z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 163, in run  [20160705t23:59:53.929394z]     self.validatesources(dataid)  [20160705t23:59:53.929436z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 201, in validatesources  [20160705t23:59:53.929451z]     0.95 psfstars.sum()  [20160705t23:59:53.929510z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 91, in assertgreater  [20160705t23:59:53.929547z]     self.asserttrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [20160705t23:59:53.929587z]   file ""/home/build0/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 82, in asserttrue  [20160705t23:59:53.929614z]     raise assertionerror(""failed test: %s"" % description)  [20160705t23:59:53.929660z] assertionerror: failed test: at least 95% of sources used to build the psf are classified as stars (49 > 50)    this error appears to be related to dm5877: when i reverted to versions of pipetasks, measbase, measalgorithms, ipdiffim, measextensionsphotometrykron and obssubaru predating that ticket landing, the error vanishes.    i note that  reports that he does not see these failures on his (os x) system, but i can reproduce them on linux: we should investigate if that's just a coincidence, or if there is perplatform variation here.",1,test
DM-6902,VO search doesn't trigger coverage image nor overlay,"while migrating the vo search panel, i found the follwing problem: once the table gets back, no image coverage or overlay is rendered.  one problem from irsa simple cone search result is that the vo table doesn't contain the right ucds expected. the second problem when the vo table does contain the proper ucds is that the metainfo is not set.   the edu.caltech.ipac.firefly.server.query.searchmanager#jsontablepartrequest doesn't set the attributes from the datagroup object into the tablemeta data as it is done previously in ops by edu.caltech.ipac.firefly.server.query.searchmanager#getrawdataset    please, add the metainfo object missing to the table and set the proper catalogoverlaytype, and catalogcoordcols needed so the coverage image and overlay can be rendered.",4,test
DM-6903,Add an option to label ccd serial number on the showVisitSkyMap.py plot ,(actual assignee: samuel piehl)     sometimes it is useful to know where the ccds are on the plot. add an option to label the ccd numbers. ,1,test
DM-6904,Create DCR visualization tools,several visualization tools will be very helpful to fully understand the effect of dcr correction algorithms and their failure modes.    a function that generates difference images with the sources used for calibration and/or psf fitting marked.   a visualization that indicates the spectral type of each source in an image. this could be a mask overlay where the color corresponds to the spectral type.    a visualization of the coarse spectral resolution model built for dcr correction,5,test
DM-6905,Locate the test dataset for PDAC,locate and evaluate a dataset of sdss stripe82 which is going to be used for testing the prototype dac.,2,test
DM-6907,XYPlot density plot with log scale - bin size is not reflected correctly,"xyplot with the large number of points does not display correctly when log scale is selected. when log scale is selected, binning on the server should be using the logs, so that the bins are the same size on the log scale. ",4,test
DM-6908,Filter editor on a chart toolbar,need to add filter editor to the chart toolbar. filter editor should be without selectable rows.,4,test
DM-6909,Filtering from expanded mode cancels expanded mode,"when a table is filtered from the expanded mode, the layout is changed back to unexpanded.    it looks like the issue is more general: table actions trigger layout changes, which are not always right. for example, tableremove action while in a dropdown makes the  dropdown to get closed. i've traced it to fireflylayoutmanager.js:layoutmanager generator function.    test sequence in firefly:    when a table is loaded, open ""charts"" dropdown, select col link for x, then select col link for y. (at this point the previous table is removed).   tableremove action on the second click triggers dropdown to go away.   ",2,test
DM-6914,git-lfs.lsst.codes certificate is expired,"per reports on hipchat, the tls certifcate on gitlfs.lsst.codes was not upgraded to the new  .lsst.codes cert.      john swinbank  9:52 am  @josh @jmatt i'm seeing the following, which i think might be the same as @srp's error above. any ideas?  get https:/gitlfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99: x509: certificate has expired or is not yet valid  followed by git lfs failing.  (i assumed it was user error on my part at first, but if so it's coincidence that steve's git lfs fails at the same time.)      ",1,test
DM-6915,"jointcalRunner passing tract to jointcal, which had tract removed from run()","when cleaning up jointcal for testing, i removed tract from jointcal.run(), but did not remove it from the return list of jointcalrunner.gettargetlist(). tract isn't actually used anywhere in jointcal.run(), so we should be able to just remove it from gettargetlist's return.    keeping those two in sync may be a bit tricky without a unittest that compares them.",4,test
DM-6916,Documenteer seeds Git revision date and branch name if not present in metadata.yaml,"if last_revised and version are not present in metadata.yaml, then the git commit date and branch name should be used while building metadata instead.    also updates lssttechnotebootstrap to take advantage of automated metadata for new projects.",1,test
DM-6917,Write User Guide for new validate_drp metric/measurement API,"dm 6629 provided a new api for consistently specifying metrics, their specification, and reporting results of measurements.    this api can, and should, be used beyond validate_drp for any code that wants to submit metadata to squash. this ticket will provide user documentation on the api base classes to help other developers write new metrics and measurements.",4,test
DM-6918,Implement script to simulate AP workflow,"to understand better the load on l1 database i need a more or less adequate set of queries running against the databases. apgenerated queries should be a good start so a simple script that simulates what ap does will be very helpful. sure i don't need any actual image processing or alert production, only the parts which read/write data to the database on pervisit basis.",10,test
DM-6919,"Please rename ""afterburners""","in dm4887 we introduced a new measurement postprocessing system which we called ""afterburners"".    the term ""afterburner"" is overloaded and applied in multiple contexts. to save confusion, please rename this system to something less ambiguous. best if we can do this soon, before this usage spreads.",1,test
DM-6922,Upgrade to new stack install procedure for containers,"lsst stack install has evolved: https:/pipelines.lsst.io/install/newinstall.html#  release container creation script needs to be update.  latest docker version will be tested, as  reported cmd line options have changed.",2,test
DM-6923,Apply distortion when searching for astrometric reference objects,"while investigating dm 6529 i found that lsst generally finds fewer reference objects than hsc when doing astrometry.  for the ccds on the edge of the focal plane the number of stars was typically very low causing frequent failures.  i found that in the hsc code, there is a distortion being applied that shifts the exposure bounding box when getting objects from the reference catalog.  this distortion is not being applied in the lsst code.",1,test
DM-6924,Resurrect obs_file,obs_file needs to be resurrected.  this is partially due to the reorganization of processccd.  my take is to try to make the ingest script read the files and ingest them keyed on the filename.  then the dataid will be just the filename.  hopefully we can then mock all the other info needed for processing in a general way.  calibration (astrometric and photometric will be off by default).  ,8,test
DM-6925,star selector and PSF determiner are selecting stars that are not valid point sources,"when turning on cmodel a more robust extendedness classifier relieved that many of the stars being used as psf candidates were being classified as extended as shown in the attached plot. this plot was generated from the output of cihsc. work should be done to determine why these stars are mistakenly being selected and fix the bad behavior. additionally the https:/github.com/lsst/ci_hsc/commit/6daf43ca41b6d192b6e1dbedb60cde0bec90b615, where the success criteria for validate sources in validate.py should be reverted from 85% to 95%.",4,test
DM-6928,HSC backport: Include PSF moments in the output tables,"this is effectively a port of https:/hscjira.astro.princeton.edu/jira/browse/hsc 110 though, due to the considerable differences in bookkeeping for the sdssshape code, this will be more of a reimplementation.  ",4,test
DM-6933,access policy for PDAC,this is a prototype dac (pdac) and the access to it is limited. we need to draft a access policy. ,1,test
DM-6934,Startup Scaffolding Machine Requirements - Base,"plan base machine needs for startup, including those processes that must run together on the same physical host (such as databases resident in memory needed by other processes).",2,test
DM-6935,Plan Base site machine startup pattern,"list all dependencies for specific processes that must be previously up an running. establish the settings for 'time zero' on the startup timeline, such as purging queues, clearing specific data stores, arranging file system for startup, etc.",2,test
DM-6936,Prep for and attend Kubernetes meeting,"kubernetes is a machine provisioning application that has potential to assist lsst startup scaffolding such as just in time personality assignment, persisting config settings, etc.",1,test
DM-6937,Download and install Kubernetes,implement a kubernetes instance running on the nebula cluster and begin configuration for testing base site startup behavior.,4,test
DM-6938,Evaluation of Kubernetes for Startup Scaffolding,"after kubernetes is running as a simulated base site start up and provisioning tool, begin evaluation with fault injection such as the need for hot swap machine failover, sudden changes to network topology and name server entries, etc.",3,test
DM-6939,Set up proposed start up tools and procedure for NCSA L1 components,"if kubernetes is the answer for startup provisioning which it is hoped to be, apply it to ncsa l1 machine startup requirements.",2,test
DM-6940,Final Startup Scaffolding document,"this is expected to be a document specifying the final startup scaffolding disposition. if specification is not final, it will assess which requirements for this epic were not reached.",2,test
DM-6941,First round of updates to DRP LDM-151 sections from reviews,"will address comments from [swinbank], [rhl], and probably .",4,test
DM-6942,Explore and experiment the process of creating a Jupyter widget,study the jupyter notebook and understand the concept of jupyter widget. try to make a simple jupyter widget that works with firefly visualization.,4,test
DM-6944,Integrate multiple-backgrounds concept into LDM-151,it's recently become apparent that we need to at least consider using different background estimation techniques for different kinds of measurements.  this is will require some thought to work into our current processing plans.,4,test
DM-6945,Add text to algorithmic components sections in LDM-151,"while [swinbank] has commented that the outlines are probably good enough for planning work (and i thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [rhl]'s first review pass, so i think i should flesh that out with text sooner rather than later.    in this issue, i'll stick to sections that no one else has added text for, but eventually i'll also need to work with  and perhaps others to ensure that section has a consistent level of detail and focus.",8,test
DM-6949,Firefly has problem to render in other browsers than Chrome,"couple of problem using firefly in  safari:   the components appears blank,    in firefox:    image and xyplot are not aligned (gator).    the alignment can be reproduced in my chrome and safari.  search parameters: allwise source catalog, m81 100arcsec.   ",1,test
DM-6952,Table problems,"table component has couple of problems:    #  scrambled table values after column selection and saving the table, then reset mess up the table. saving the table and reset makes the table comes back.  #  table display no longer redefines column names in the table based on the column label, e.g. ""field size"" instead of ""s_fov"".  #  downloaded file is not a valid ipac table.  #  filtering table does not change image overlay or plot until table is saved. at that point, the filtering works, but the plot symbol changes (happens when result is decimated, datapoints > 5000?)  #  in edit table options, it's unclear what the reset button resets to.  ",6,test
DM-6953,Image problems are grouped in this ticket,"image viewer has a couple of issues:     there is no panner for the image (is it missing from the api or is it a bug in calling it?)   no readout value from thumbnail image   image does not have toolbar or layers control (gator)  probably api options to be used or missing? > move to dm7001   markers don't show up in png download > moved to dm6980   the top bar readout doesn't include units for the pixel flux   clicking on the expand icon deletes the image (in api only)   clicking on the expand icon diabled the expand mode of table and xy plot (in api only)    image button ask you for a position and displays that, wiping out whatever image brought you to irsa viewer in the first place. it should give another tile of the same field, drawn from the selected data set. (irsaviewer only)   need mode to draw grid without labels   rangevalues messing up image display          old:  var external= firefly.getexternalviewer();              external.setdefaultparams() ;    new:  xtviewer.setdefaultparams() ;   the ""rangevalues"" were taken out for now   ""rangevalues""  : firefly.util.image.serializesimplerangevalues(""sigma"", 2,8,""linear"")    ",14,test
DM-6954,XY plot problems found,"implement the items listed here:   min/max options are now gone after migration, need to be added.   use the expression for x, y column as the default label, otherwise the read out could be confusing.   label changes for decimation: xbins and ybins:  number of xbins, number of ybins  shrink the size (to 1/5?)  of the blue dots for data representation.  i do like the circle when the point is highlighted.   the units on the plot are indicated with a comma, e.g. “dec, deg”, should be ""dec (deg)"" as before   need to confirm again ()   making a change to the plot (e.g. ra = ra  1), then clicking on the gears to close makes the shading legend disappear. it also “quantizes” the plot (not any more)  what happens now is the plot appears without legend after a search. then making a change to the plot (e.g. ra = ra  1 or filtering the table), then applying makes the legend appears/disappears. expanding the table and collapsing it, make the legend disappears.  step to reproduce: catalog search on 2mass around m16 with 10' radius.  greyscale introduced, where different colors represent different numbers of points. after filtering, the points change color to blue.  (this is because the it is not decimated any more)  clicking on plot gears makes plot unusably small.  (not applicable any more since gear now brings up the options in popup) ",6,test
DM-6955,Message Dictionary Adjustment.,"audit format of existing messaging and adjust according to 'wants' not task 'needs'...that is, msg body format that exists now is sufficient to fulfill tasks, but destination components must receive a broader description of overall system state. this will allow all components to log a more comprehensive snapshot of current state and is needed for troubleshooting. these additions to the message dictionary will be configurable like a logging priority levels function, and additions to message payload can be turned off for typical nightly operation.",4,test
DM-6956,'ACK' (Acknowledgement) message formats,"enumerate ack messages for all primary message types. prototype both blocking and nonblocking acknowledgement aggregator that works via timeout, behavioral change, etc. this is related to dm6411",4,test
DM-6957,Adding ACK messages to existing code framework,new entries in message dictionary must be added and tested with the existing messaging code base.,4,test
DM-6958,Documentation for new message types,add new ack message types to existing dictionary documentation.,2,test
DM-6959,Messages as objects,"consider creating a dictionary of code objects to represent messages. currently message bodys are built on the fly   evaluate pros and cons of switching to a message factory pattern. message types are not a very extensive list, but switching to object implementation could increase maintainability of code.",2,test
DM-6960,Overlay health check code,implement and overlay health check mechanism on existing messaging control code. prototype and gather timing information to determine optimal frequency of checks and the location in the exposure cycle when these checks should occur.,8,test
DM-6961,Fault Injection in the form of unsuccessful health checks for components,build testing mechanism to inject faults into into health framework and stub code to address health check failures,4,test
DM-6962,Create policy for health check failure,"document policy regarding action to take when various components are found to be unhealthy. this can vary depending when 1 component type (forwarder) is offline versus 21 forwarders offline. in addition, plans must be formulated for addressing the point in the exposure cycle when the health failure occurs.",4,test
DM-6963,Implement health failure policy,formalize the prototypical implementation of health checks and associated policy into 'what to do' actions,4,test
DM-6964,Make a proposal for API support for representation of relationships between table columns,"end users and the suit need to be able to determine a variety of relationships between columns in the tabular data products produced by lsst.  the particular example motivating this ticket is the need to answer the question ""where in the table is the uncertainty data for column 'x'?"".    the answer could be:   ""there isn't any""   ""a symmetric gaussian uncertainty is in column 'sigmax'""   ""asymmetric gaussian uncertainties are in columns 'sigmaplusx' and 'sigmaminusx'""   ""'x' is correlated with 'y' and the covariance matrix is in 'covarxx', 'covarxy', and 'covaryy'""     ideally we would find a way for these relationships to be defined when the apps code generates its afw.table outputs, discoverable through an api usable in the afw.table context, exportable to the database, and made available to end users and the suit.  it should be usable whether the data are delivered to end users as reconstituted afw.table objects or as tables in common python formats (at least astropy tables).    it should assist the suit in determining how to (automatically, though optionally) display uncertainty data when the primary data are requested.    this ticket expresses the idea that a solution that consists purely of a documented convention about prefixes to the string names of columns is inadequate.  we would like to avoid having to write code implementing that convention in, potentially, hundreds of places, and we would like to avoid requiring that end users know these conventions in order to see proper displays with error bars.  ",3,test
DM-6966,Flesh out software primitives,jim has put together a fairly complete software primitives section.  this task is to read it over from the perspective of alert production and expand/refine where necessary.,4,test
DM-6967,Level 2 conops formatting: convert draft to reStructedText,nan,1,test
DM-6968,create a shared stack on NFS for use with  the current local condor pool,"it is well known that building, setting up a stack, and interactive devel work with those operations on nfs has performance issues.  hence the official shared stack on lsstdev uses /ssd .    however,  a shared stack on nfs is useful and adequate for one important  use case    users need a stack that can be used for small productions on the local condor pool currently available  on lsstdev.   for this use case multiple ""source""/""setups"" on a node/against the file system  can be avoidable by using a script to directly declare the environment.  runorca /ctrlorca supports this feature.       while gpfs is coming soon, there is expected to be a transition period of 2 3 months and so the nfs file system and a stack on it can serve users for an interim period.   if building a shared stack on nfs is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on nfs. ",1,test
DM-6969,Fixes to LoadIndexedReferenceObjects,bug fixes for using the new loadindexedreferenceobjecttask and its associated components.,2,test
DM-6970,Add tests for bindings of Eigen::Array and ndarray::EigenView," has discovered that we don't have test coverage for converting less common eigen types to python.  this is not urgent, but it should be fixed.",1,test
DM-6971,Qserv 2016_07 release, update release notes   publish docs and bump version numbers,1,test
DM-6972,Fix Qserv install doc and scripts for new newinstall.sh,update qserv install docs per new info at https:/pipelines.lsst.io/install/newinstall.html,1,test
DM-6973,"Fix metadata date problem in LDM-{463,152,135}",these docs are currently borken in ci  just need to have the dates reformatted in their metadata.yaml,1,test
DM-6974,Type of IngestIndexedReferenceTask_config wrong in obs_ paf files,"in dm 6651 i moved the new htm indexed reference catalog code from pipetasks to measalgorithms, but didn't do a complete job. the type of ingestindexedreferencetaskconfig in obs paf files still must be updated.",1,test
DM-6975,Document release milestone changes in DMTN-020,please add a note to dmtn020 describing the changes to release milestones discussed at the https:/confluence.lsstcorp.org/display/dm/dmleadershipteammeeting201607 18.,1,test
DM-6976,watch for Highcharts update ,"there is an issue in the density plot for displaying the legends. highcharts does not support the setting of the symbol size in the legends. so when the symbol size is too small or too large, the legends are not displayed.     we don't want to do too much workaround currently. this ticket is to watch for the highcharts update. ",1,test
DM-6977,verification and test of the Bayesian histogram calculation on server side,we need to set up some unit tests of the bayesian histogram calculation.     first we need to do some verification of our algorithm with scientists.  i think we can find some known data sets and the results with help from scientists.   the unit tests should include several different distribution of input data. ,10,test
DM-6978,Update qserv for changes in Log interface,dm6521 improved log class interface by replacing some static methods with nonstatic. qserv is currently using couple of static methods which were retained in log class for the duration of this migration. once updated log package is released update qserv code to use new non static methods and remove static methods from log class after that.,2,test
DM-6980,Markers don't show up in PNG download,markers don't show up in png download,4,test
DM-6982,Fix oversampling settings in psfex,"the current settings in psfex will only turn on oversampling only if the seeing is < 0.5"", even if you have configured it do oversampling. this needs to be changed so that everything is determined by the config parameters.    we have also seen on hsc data that oversampling in general does not work well in psfex.  we need to change the current configuration which does 2x oversampling to just use the native pixel scale.",1,test
DM-6983,ci_hsc failure: AttributeError: 'Butler' object has no attribute 'repository',"following https:/community.lsst.org/t/imcheckinginbutlerchangesrelatedtorfc184/959, cihsc is failing as follows:      [20160720t07:57:31.954576z] traceback (most recent call last):  [20160720t07:57:31.954643z]   file ""/home/jenkinsslave/workspace/stackosmatrix/compiler/gcc/label/centos7/python/py2/lsstsw/build/cihsc/bin/validate.py"", line 3, in /  [20160720t07:57:31.954664z]     main()  [20160720t07:57:31.954732z]   file ""/home/jenkinsslave/workspace/stackosmatrix/compiler/gcc/label/centos7/python/py2/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [20160720t07:57:31.954756z]     validator.run(dataid)  [20160720t07:57:31.954825z]   file ""/home/jenkinsslave/workspace/stackosmatrix/compiler/gcc/label/centos7/python/py2/lsstsw/build/cihsc/python/lsst/ci/hsc/validate.py"", line 155, in run  [20160720t07:57:31.954851z]     self.validatedataset(dataid, ds)  [20160720t07:57:31.954923z]   file ""/home/jenkinsslave/workspace/stackosmatrix/compiler/gcc/label/centos7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in validatedataset  [20160720t07:57:31.954956z]     mappers = self.butler.repository.mappers()  [20160720t07:57:31.954991z] attributeerror: 'butler' object has no attribute 'repository'  [20160720t07:57:32.023212z] scons:   [.scons/ingestvalidation903342100] error 1      see e.g. https:/ci.lsst.codes/job/stackosmatrix/13274/compiler=gcc,label=centos 7,python=py2/console.    please fix it. ",1,test
DM-6984,Suggest logging migration in daf_persistence and daf_butlerUtils,use lsst::log instead of pex::logging in dafpersistence and dafbutlerutils,2,test
DM-6985,Suggest logging migration in afw,suggest a changeset with lsst::log instead of pex::logging in afw,5,test
DM-6986,Suggest logging migration in pipe_tasks and meas packages,suggest changesets using lsst::log instead of pex::logging,5,test
DM-6987,Write up a description of Composite Datasets based on input from KT,"write a description of composite datasets as i understand them based on the email kt sent on may 20 (attached), and on conversation i had with kt and fritz on july 20.",2,test
DM-6988,Review Composite Dataset description document with stakeholders,"review the composite dataset description document with [jbosch] and [parejkoj], and any others who may be interested (e.g. post on community or do an rfd)",2,test
DM-6989,ctrl_events/tests/EventAppenderTest.py fails Jenkins run-rebuild,"ctrl_events/tests/eventappendertest.py started failing on jenkins ""runrebuild"" last night:   https:/ci.lsst.codes/job/runrebuild/354/console    all test cases in eventappendertest.py did run and pass, but it failed with a segmentation fault in the end.     jenkins ""runrebuild"" uses a stack on nfs on lsstdev (/nfs/home/lsstsw).  the same test passes on regular jenkins (stackosmatrix).      ",3,test
DM-6990,Improve testing in SQuaSH prototype,this ticket captures some testing practices from https:/ep2013.europython.eu/media/conference/slides/obeythetestinggoatrigoroustddforwebdevelopmentwithdjangoandselenium.pdf  that we intend to use in the squash prototype.      use selenium to test user interactions  (functional tests)   use unittest module for unit tests    include some documentation about testing in sqr009 ,5,test
DM-6991,Add a script to summarize what visits are in what patches,"(actual assignee: samuel piehl)     have a script to show what visits are in what tracts/patches. this is especially useful for running coadd making and processing (e.g. makecoaddtempexp, assemblecoadd) with runorca and htcondor, as the dataid of the jobs need to be specified. so this script's output will be in the format of a runorca input file.     ",5,test
DM-6992,Extend SQuaSH dashboard to work with multiple datasets,"currently squash dashboard works only with a fixed dataset. we want to ingest measurements of metrics computed by validate_drp for multiple test data e.g cfht, decam and hsc. in order to handle multiple datasets, we need a new model in squash  and extended the job api to ingest the measurements for different datasets. the user must be able to selected in the interface the dataset to be displayed.",3,test
DM-6996,produce a draft document of SUIT requirements,"after combing through the current suit requirements, we feel that we need to reorganize and rewrite the suit requirements to be in line with suit vision document.    this story will be producing the first draft of the rewrite. ",20,test
DM-6998,Problems with MemoryTest ordering,"memorytestcase (or a derivative thereof) must be run as the last of all tests in a module in order to properly catch leaks.    https:/developer.lsst.io/coding/pythontesting.html#memoryandfiledescriptorleaktesting implies, and https:/sqr012.lsst.io/#memorytest, that this can be achieved by listing it as the last test case in the file.    this works for py.test, but not when using plain old unittest: the latter does not, so far as i can see, guarantee any sort of ordering as a matter of principle, and, in practice, it sorts things lexicographically (it uses whatever order it gets from running dir() on the test module, and i don't think that's guaranteed to be anything in particular).    for example, consider https:/github.com/lsst/measalgorithms/blob/master/tests/testastrometrysourceselector.py. i made the following change to introduce a memory leak:       a/tests/testastrometrysourceselector.py   b/tests/testastrometrysourceselector.py  @@ 70,8 70,9 @@ class testastrometrysourceselector(lsst.utils.tests.testcase):           self.sourceselector = sourceselector.sourceselectorregistryhttps:/github.com/lsst/sconsutils/blob/f9763768d999cefa4c26b9f3418c28394dfb38df/python/lsst/sconsutils/tests.py#l133, and i'm pretty sure that this is hardwired into the muscle memory of many developers. in these cases, memory tests written following current guidelines won't be being properly executed.    ",3,test
DM-6999,Use lsst::log in pipe_base and pipe_tasks,"per rfc 203, switch from using pex.logging to lsst.log in pipebase and pipetasks (stage 2)",8,test
DM-7000,Remove pex_logging dependency on pipe_tasks,nan,3,test
DM-7001,Gator / Image Vis issue,"issues with coverage:      toolbar icon not showing up   if only one point that is no coverage image (or one table with many records but has the same ra,dec values)   can’t repeat: in expanded mode, magnifier fails when image fills the visible space entirely (seems to affect 'coverage' image only)   missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view. => move to: dm7068   can’t repeat: if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either.   in laptop screen size, the toolbar is not fully visible, scrolling from left to right only move the background but not the expanded panel.   can”t repeat:in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage image, can be reproduced in http:/localhost:8080/firefly/demo/ffapihighleveltest.html (btw, it happens in finderchart in ops on any image in expanded mode ( ? ) )   readout is sometimes off the screen    expanded then return to normal: zoom is not adjusted correctly    if we find a way to repeat the items marked 'can't repeat' they should go into another ticket.  maybe in dm7068 if it is still opened.  ",4,test
DM-7003,Match across filters -- Make color-color diagram,add the capability to match across filters.    1. create color color diagrams  2. analyze performance metrics as a function of color.,4,test
DM-7004,Add ellipticity measurement to validate_drp,"calculate the ellipticity, and the residual ellipticity (moments  psf).    add to calculated srd statistics.    this will involve thinking about things on an imagebyimage basis, which is the natural and largely srdspecified way for considering ellipticity.",4,test
DM-7005,Show the list of packages that changed from build to build linked to the git url of the latest commit,motivated from the deviation seen from build 156 to 157 in  https:/squash.lsst.codes/am1 (caused by a commit in meas_algorithms package) we can show the list of packages that changed in the current build with respect to the previous build by comparing the git commit shas and return a list of tuples with the package name and git url.,2,test
DM-7006,Update squash to use bokeh 0.12.1,"bokeh 0.12 was just released and some issues are being fixed, before updating the bokeh version used in squash we propose to wait for 0.12.1 release.  ",1,test
DM-7007,Investigate coverage of S13 databases found so far,look at databases located at ncsa so far to assess if they cover the full survey. the databases to be evaluated are mentioned in https:/dev.lsstcorp.org/trac/wiki/summer2013/configandstacktestingplans/instructions the s13 drp dataset was split into two regions with an overalp used for crosssite verification:   ncsa:  40< r.a. < 10   in2p3: 5 < r.a. < +55    hence a goal of this task is to identify which previously located candidate databases and files correspond to either or both of these ranges.,4,test
DM-7008,Check boost.python building with Python 3,we may want to disable boost.python in the build. there are hints that there are problems with python3.5.,1,test
DM-7009,std::string construction from NULL pointer in ctrl_events,"i was browsing through ctrlevents package and found couple of instances in the headers where std::string instance is constructed from null pointer:  https:/github.com/lsst/ctrlevents/blob/master/include/lsst/ctrl/events/receiver.h#l87  https:/github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/transmitter.h#l81    i suspect that this code is never executed and those methods are overridden in subclasses because that construct will very likely crash when executed (std::string does not support construction from zero pointer, it will try to read from that pointer). even if it's not executed it's better to change to return empty string or, if those two classes are never instantiated, make them abstract and make the methods pure virtual.  ",1,test
DM-7010,Builds should be optimised by default,"by default, our builds are not optimised (o0), which requires everyone who doesn't want to wait until the heat death of the universe to set sconsflags=""opt=3"", but other packages that are built with scons may not recognise this.  this default is also contrary to the standard practise for opensource software, which is that by default builds are optimised.  i will change the default optimisation level to opt=3 from the current opt=0.  i will also add support for og.    this change was approved in rfc202.",1,test
DM-7011,Run DECam data through proccessCcd.py and imageDifference.py,nan,2,test
DM-7012,assign initial responsibilities in LDM-151,assign first thoughts responsibilities to all software primitives and algorithmic components.  this is my take.  john will have his own take.,2,test
DM-7014,Memory cache leak in firefly server,the visualization system is not update the memory accounting for the caching system.,2,test
DM-7015,Analyze segmentation fault in EventAppenderTest,analyze the bug described in dm 6462,4,test
DM-7016,Big image not showing working message when the load,"this is a problem with uploads, large image loads, and atlas.   when a big image is loading the user does  not get feedback.  the problem is the the ui is not creating the imageviewer soon enough.",2,test
DM-7017,Firefly JavaScript API documentation to support Camera team,convert api documentation and code examples to get camera team started with the converted firefly fits viewer. ,6,test
DM-7018,Firefly distribution build,"we need to support regular firefly distribution builds (with bundled tomcat server),  similar to the builds we did in lsst firefly repository before the conversion.    this is to get camera team started with new api.",2,test
DM-7019,Setup standalone Firefly build using IPAC github,modify the existing fireflystandalone build in jenkins to use ipac's github.  make sure github autoreleases still works.,3,test
DM-7021,Update pex_exceptions to support Python 3,pex_exceptions needs to be updated to support python 3.,1,test
DM-7022,Package an experimental Firefly widget,the aim is to package an experimental jupyter widget with limited functionality so that it can be installed like other jupyter widgets. only a small set of python and javascript code will need to be packaged  the widget will connect to a firefly server. the https:/github.com/jupyter/widget cookiecutter provides a template.  ,4,test
DM-7024,Add more features to JS9 Wrapper ,"di progress on adding extra features to js9, including load and saving regions in the local notebook server, same with files. ",4,test
DM-7025,Investigate the option to use websockets used by jupyter to explore bi-directional communication ,di progress on understanding the possibility of using websocket locally to communicate with js9 instances on local server. in this case we wouldn't need an external server and communication can be bi directional. now is only in one direction (mostly) when running js9 locally ,3,test
DM-7026,Setup up a cluster with kubernetes,sahand progress on installing and deploying a cluster automatically with kubernetes. after this is completed we will use a user case example of running in cluster managed by kubernetes/spark,6,test
DM-7028,Port daf_base to Python 3,changes necessary to get daf_base to work with python 3.,1,test
DM-7029,Image Bugs noticed in the API testing," image is coming with one draw layer. (i can delete this draw layer and nothing changes on the image)     when draw layer is deleted, and no more layers are present, the layers dialog should be closed. (it stays with nothing to display, you have to click x to close it.)     after selecting an area in one viewer, i select an area in another viewer, then move the mouse to the first one:      147 imageviewerlayout.jsx:314 uncaught typeerror: cannot read property 'x' of null at imageviewerlayout.jsx:314   nothing works after that. i have to reload.     selection appears with an offset, if the page, which contains the viewer is scrolled. (load the attached script, press 'start selection tracking' click to select a point, then scroll page a bit down, then click to select another point  it shows down from where it should be.)     i have 2 image viewers in separate divs. selected line in one, then selected line in the other. the line from the first one disappeared, but its label is still there. (see attached image.)     selection is working differently from distance. to select in another plot, i need to press selection again. i don't need to press ruler again to select new distance in another plot.     it's possible to select distance tool and then area selection. first drag would define area selection, all the following line. a click would be defining a 0 length line, even if point selection is enabled. move to dm6473     the payload.attvalue of changeplotattribute action is using worldpt for area and point selections (when payload.attkey is 'selection' or 'activepoint'), but imagept for line selection (when payload.attkey is 'activedistance') how can i make them all use image coordinates?        ",6,test
DM-7030,Update xrootd from upstream,nan,4,test
DM-7031,Assign initial responsibilities in LDM-151,assign first thoughts on responsibilities to all software primitives and algorithmic components. this is my take. simon will have his own take.,2,test
DM-7032,Estimate resource requirements for Software Primitives,meet with jim & simon. discuss the software primitives section of ldm 151: clarify any ambiguities and perform an initial resource loading estimate.,3,test
DM-7033,Estimate resource requirements for Software Primitives,meet with simon & john. discuss the software primitives section of ldm 151: clarify any ambiguities and perform an initial resource loading estimate.,3,test
DM-7034,Estimate resource requirements for Algorithmic Components,meet with jim & simon. discuss the algorithmic components section of ldm 151: clarify any ambiguities and perform an initial resource loading estimate,3,test
DM-7035,Estimate resource requirements for Algorithmic Components,meet with simon & john. discuss the algorithmic components section of ldm 151: clarify any ambiguities and perform an initial resource loading estimate,3,test
DM-7036,Port pex_policy to Python 3,changes needed to make pex_policy work on python 3,1,test
DM-7037,Check endianness in ndarray/numpy conversions,"as reported on https:/community.lsst.org/t/howtorunthedmstackonsimulatedfitsimages/892/9, our imagef(array) constructor will accept arrays with nonnative endianness and interpret them as native.  this probably means the array converters in ndarray aren't including byte order when checking whether a passed array's dtype matches the expected c type.  ",2,test
DM-7038,Setup JSDoc generation for the API portion of Firefly,"we need generate and publish jsdoc for firefly javascript api, both high and low level.",4,test
DM-7039,Familiarization with Footprint redesign,"familiarize yourself with the rfc 37 driven footprint redesign. start thinking about ideas for how you could implement it and what the transition plan from the current footprints might be.    a great outcome would be to propose a set of stories which would tackle the new footprint development effort.    a good outcome would not be to have the stories ready to go, but to be well prepared for a discussion with [jbosch] & [swinbank] where we'll come up with some stories as a group.",5,test
DM-7040,Stars selected by starSelector change when number of cores varies,"sogo mineo writes in https:/hscjira.astro.princeton.edu/jira/browse/hsc1414:    see the following lines:    measalgorithms/hsc4.0.0/python/lsst/meas/algorithms/objectsizestarselector.py:466  in objectsizestarselector.selectstars():        if psfcandidate.getwidth() == 0:          psfcandidate.setborderwidth(self.borderwidth)          psfcandidate.setwidth(self.kernelsize  2self.borderwidth)          psfcandidate.setheight(self.kernelsize  2self.borderwidth)      in reduceframes, these lines set the width of psfcandidate to be 21  for the first time the execution reaches there.    when the first ccd image has been processed, the worker process  continues to process another ccd image, and the execution reaches  here again.  this time, psfcandidate.getwidth() is 41, because  psfexpsfdeterminer has set it to be 41, and the value has been  retained because the width is a static member.  and so, for the second  ccd image, the width of psfcandidate is not 21 but 41.    since psfcandidates are widened, stars positioned at edges of images  are rejected.  it results in a smaller number of psf candidates than expected.    only ccd images that are initially given to the worker processes  are processed with psfcandidate.getwidth() == 21. the other ccd images are  processed with psfcandidate.getwidth() == 41.  when the number of smp cores changes, ccd images are processed with different  parameters.    the change in the number of psf candidates results in different psf, a different  result of image repair, and different catalogs.      the line numbers are different on the lsst side because of refactoring (objectsizestarselector.py:466 has moved to starselector.py:148), but the bug is still present.  the main problem appears to be that the psfcandidate elements are static, are being set in both the star selector and the psf determiner and one of those is conditional on what the value is.  i will investigate moving the static class variables to instance variables   the desired size appears to vary by context, so it shouldn't be a class variable.",2,test
DM-7044,Additional constraints on reference band selection for multiband,"reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signaltonoise in the high priority bands.  https:/hscjira.astro.princeton.edu/jira/browse/hsc 1411 points out that some additional qualifications, such as success for major measurements (e.g., cmodel and kron), would be helpful.",3,test
DM-7046,Prototype python cache for weak_ptr and weakref objects.,"there is a requirement for composite datasets that components of composites be cached and shareable, and we expect to use an object cache for this.   we have identified that we need to be able to cache c weak_ptr and python weakref in the same cache in an opaque way. this needs some prototype r&d.",2,test
DM-7047,Port pex_config to Python 3,work involved in ensuring that pex_config passes all tests on python 3 and legacy python.,1,test
DM-7048,validate_drp is failing because it's accessing butler internals that have changed,need to change obs_decam's ingest task to use the newer class hierarchy to get the root of the butler's single repository. (longer term there should be a butler api for this or the task should get the value of root from somewhere else),1,test
DM-7049,Move patch/tract and config mapping definitions to daf_butlerUtils,"implement rfc204 by adding new entries for all patch/tract and config mapping definitions to .yaml files in dafbutlerutils, and removing any such entries that are identical to the common ones from .paf files in obs  packages.    i think the ""common"" entry can usually be defined by consensus between any two of obscfht, obsdecam, and obssubaru (and frequently all three).  if there are any patch/tract or config datasets for which no two cameras agree, i think we should use obs_subaru's definitions (but i doubt there are any such cases).    entries that are not identical to the common ones should not be removed on this issue (that should make this change entirely backwards compatible), but should be documented in new percamera issues for later standardization.",4,test
DM-7050,LTD Keeper: Use Google Cloud Platform SQL,"currently ltd keeper uses a sqlite db. this ticket will migrate that db to google’s cloud platform’s managed sql. this solution provides automatic backups, and provides flexibility to run multiple ltd keeper pods. google’s sql makes sense since ltd keeper is run on google cloud platform.",5,test
DM-7051,conda installation from the stack channel brings in astropy 1.2,if you do a conda install of lsst_sims from http:/conda.lsst.codes/sims you get astropy1.1.1.  if you do the same install from http:/conda.lsst.codes/stack you get astropy1.2.1.  this is a problem for the sims stack since the sncosmo package (on which we depend for our simulations of supernova light curves) is sensitive to which version of astropy you are running.    was it intentional that the two channels deliver different versions of astropy?,2,test
DM-7053,Assemble a complete database with S13 DRP catalogs,create a database populated with complete catalogs resulting from processing of the sdss stripe 82 data at both ncsa and in2p3 sites. the database has to be created at ncsa on the following database server:   lsstdb.ncsa.illinois.edu    the database name will be:   gaponsdrpstripe82    the database will be populated with the contents of the following databases:   ncsa (lsstdb.ncsa.illinois.edu):    dauessdrpstripe82ncsa  dauessdrpdedupebyfilter0  dauessdrpdedupebyfilter1  dauessdrpdedupebyfilter2  dauessdrpdedupebyfilter3  dauessdrpdedupebyfilter4     in2p3 (ccdb02.in2p3.fr):    lsstproddc20132  lsstproddedupebyfilterg  lsstproddedupebyfilteri  lsstproddedupebyfilterr  lsstproddedupebyfilteru  lsstproddedupebyfilter_z      additional requirements:   the duplicate entries (due to the overlap in the ra range) will need to be carefully assessed and eliminated.   the referential integrity of the resulted database will need to be tested  ,16,test
DM-7054,Kick-off meeting,[nlust] & [swinbank] will meet with the sui/t team on 20160726 and discuss how we can best engage with them.,1,test
DM-7055,fix miscellaneous table issues,# disable sorting when content is html  # table options: auto adjust all column width based on content  # table refractoring: exposing more actions to saga.  # renamed a few actions to better reflect what it's doing.  # added tablefilter  #  added document for sequence of actions where applicable.  # update build script to skip buildclient when possible.  # catalog overlay should not use table id in drawing layer description. (see the attached screenshot.) it should be using metaconst.catalogoverlay_type attribute value,4,test
DM-7056,Wrap afw::table with pybind11,"following the same pattern as dm6926, dm6297, etc.",8,test
DM-7057,Complete afw port to pybind11,nan,10,test
DM-7059,Plot sources/source density on WCS quiver plots,"we should show the individual sources (size/color scaled by s/n?) that went into the jointcal fit, and/or a density map of the sources (scaled by s/n?) under the quiver plots. this will help distinguish areas with poorly constrained fits or where the tan sip function diverges, from those where the new wcs really is odd.",2,test
DM-7060,Plot old/new jointcal WCS vs. tangent plane,"to better understand the jointcal wcs vs. the original single frame tansip, we need quiver and heatmap plots of each wcs (old and new) separately vs. a tangent_pixel or related ""nondistorted"" projection. this will let us compare the original single frame fit with jointcal's fit.    this probably could be done with camerageom, but would be easier with the upcoming new wcs/transform system, since it may involve pulling out a new frame.",4,test
DM-7061,"Plot ""real"" distortion by comparing with reference catalog","to compare the old wcs and jointcal's fit with the ""real"" distortion, we can use the matched reference catalog to plot a quiver diagram or an interpolated heat map showing how far each star is from its reference star. we may have to think about how to select objects for this plot, since centroiding errors would make it not so useful.    this would probably be most useful for lsstsim, since that has an infinite precision reference catalog.",4,test
DM-7062,Support work related to PDAC effort,"this issue captures emergent work to support for example dm 6905 , for which i spent some cycles locating datasets of the 2013  sdrp, staging some files off of bw tape through globus online and unpacking to /nfs/scratch,  etc.    this effort may not fit exactly as 'emergent middleware',  but it was roughly the best fit at this time. ",3,test
DM-7063,support work for testing shared stack in NFS,"it was realized that the ""shared stack of lsstdev"" was not actually usable on the local condor pool due to /ssd usage.   in response to this,  an effort for a second shared stack on nfs  was initiated in dm 6968.  this issue captures the emergent work of pipeline testing to validate the new stack of that issue. ",2,test
DM-7065,Extend functionality of experimental Jupyter widgets for Firefly,"a package for experimental jupyter widgets for firefly is being developed in  https:/github.com/caltech ipac/firefly_widgets . using the firefly javascript api for images and tables, add some further useful functionality for demonstration purposes.",4,test
DM-7066,Port pex_logging to Python 3,work required to get pex_logging working on python 3. will also include some package cleanups.,1,test
DM-7067,Break joincal's link to upstream lsst_france repo,"lsst/jointcal is still linked to the upstream repo at lsst_france. i believe all the relevant changes have been ported. it's time to break that upstream link, so that pull requests can be made in a more obvious fashion.",1,test
DM-7068,Firefly API bugs 2,"issues    gator:   missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view.     the gator multiobject search seems having problem with the coverage image.    atlas:     if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either. large drawing layers block viewer from becoming active, wfirst footprint or wfc3/ir cause the problem.    in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage  not a bug, magnifier is disable when zoom level is above 8x     api:    in api, it is not possible to drag dialogs (ex. drawing layers).    all firefly (found by tatiana):     the highlight should of catalog or coverage overlay should just change if you are close to the point. for now i am setting it to 20 pixels   catalog overlay should not use table id in drawing layer description. (see the attached screenshot.) it should be using metaconst.catalogoverlaytype attribute value. moved to dm 7055     after using distance tool in one plot, then the other, clicking again in the first plot does not make it active. (you have to click on the border of the plot to make it active, clicking inside does not help.)    this seems to cause strange behavior, when selections do not work as expected. for example: select ruler, select some lines alternating first and second plot. unselect ruler, select area icon. selecting in the first plot will still show distance. i had to delete distance tool drawing layer or click on the border for things to start showing area selection.    in general, there is some confusion with active plot, when i have two viewers. should a plot become active as soon as mouse enters is? otherwise in readout, compass thumbnail will still show active plot, while readout thumbnail could use another plot.  note from trey: this is the same problem as the marker/footprint in atlas described aboved",4,test
DM-7069,Port daf_persistence to Python 3,work relating to getting daf_persistence to run on python 3. includes some code modernization.,1,test
DM-7070,Move consts from top of Associations.cc into JointcalConfig,there are three values at the top of associations.cc under a todo comment that should be lifted up into jointcalconfig so they can be configured at runtime. it would be good to try to add tests to check different values for them (and possibly just remove usnomatchcut).,1,test
DM-7071,Fix Django admin interface ,"django admin interface is useful to edit db entries in squash if needed, e.g decam measurements that were incidentally pushed to the dashboard during x16.    a bug was found using the admin interface in development mode, due to a bad field returned by the jobs model.    this ticket is to capture the fix for this bug, this new git ref will be deployed for better control of data in squash database.    ",1,test
DM-7072,"visit DRP team, June 2016",travel to princeton june 13 17 and meet with the drp team; work and learn about l2 processing; discuss the workflow requirements and use cases. ,8,test
DM-7073,Install ESXi on lsst-dm-mac.lsst.org,install and configure esxi on the mac pro server.,7,test
DM-7074,Install Mac OS X Mountain Lion on ESXi,"install, configure and snapshot mac os x mountain lion. unfortunately this is required to install any other mac os vm on esxi.",1,test
DM-7075,Install Mac OS X Yosemite on ESXi,"install, configure and snapshot mac os x yosemite. this requires configuring the vmx file then installing mac os x mountain lion and upgrading.",2,test
DM-7076,Install Mac OS X El Capitan on ESXi,"install, configure and snapshot mac os x el capitan. this requires configuring the vmx file then installing mac os x mountain lion and upgrading.",2,test
DM-7077,Install MacOS Sierra on ESXi,"install, configure and snapshot macos sierra. this requires configuring the vmx file then installing mac os x mountain lion and upgrading.",1,test
DM-7078,Firewall and SSH configuration on ESXi,"figure out and configure the firewall, ssh server and ssh client for esxi.    this isn't especially well documented since it's part of vmware vsphere.    this part specifically was time consuming since most users by vsphere.",2,test
DM-7079,Upgrade panopticon to 5.0.0-alpha4,this upgrade requires moving from topbeat to metricbeat which requires some minor rework and upgrading the entire system at once.,2,test
DM-7080,Doxygen isn't updating,"the current build of our doxygen documentation, as displayed at https:/lsst web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/index.html, is labelled ""generated on mon jun 27 2016 03:52:22 for lsstapplications"". at time of writing, that's more than a month ago. important additions to the documentations made during the last month are missing.  ",1,test
DM-7081,Airflow,review https:/github.com/apache/incubator airflow workflow management system against criteria defined in the epic.,6,test
DM-7082,deploy django admin interface fix,test and deploy django admin fix from dm 7071.,1,test
DM-7083,Install MySQL and PostgreSQL servers on ccqserv124,"time to run my incomplete l! prototype on real hardware, for that i need mysql and postgresql servers on a dedicated machine in in2p3 cluster (ccqserv124). probably start with installing what comes with the system repos before trying latest and greatest stuff.    both servers need to be configured to allow me create databases/tables, and i only need to enable connections from localhost.  ",1,test
DM-7084,Astropy views not available on Catalog subclasses,somehow the asastropy isn't being inherited by basecatalog subclasses in python; it's probably getting messed up by the fact that catalog is a template and this is added at the swig level.,1,test
DM-7088,Image select panel not yet working correctly with coverage,the image select panel needs to be able to modify the coverage image.,4,test
DM-7090,"IrsaViewer catalog panel, labels and input fields moved as you type","catalog search panel in irsaviewer, the target panel label, feedback, and input box are jumping as input is being typed.  their position should be fixed.",2,test
DM-7091,F16 Qserv Release Mgmt,"developer work to support the monthly and endofcycle qserv releases.  includes compiling release notes, updating package dependencies, updating installation docs, minor fixes in support of new compilers, etc.",20,test
DM-7094,"Develop Sphinx configuration for Pipelines Documentation, including MVP HTML/CSS Template","this ticket will kickoff a pilot implementation of pipelines documentation in sphinx. specific goals are    1. develop template for sphinxready doc/ directories in packages (based on sqr006)  2. setup a mvp sphinx template that works well with numpydoc and astropy automodsumm. simply porting astropy’s sphinx template would be pragmatic.  3. documenteerdriven configuration for sphinx.    these will be mvps, and iterated upon in later tickets that implement sphinx api docs for stack packages.",1,test
DM-7103,Run DAX containers at NCSA,"this is an initial step to manually launch the containerized dax services on the new pdac cluster.  this is meant to expose container configuration, account setup, privilege, logging, debugging, etc. issues.",4,test
DM-7104,support PDAC Qserv deploy,"support john in adapting scripts and methodology as necessary to support qserv deploy on the pdac cluster at ncsa, as is currently done at in2p3.  ",8,test
DM-7105,Qserv 2016_08 release,nan,1,test
DM-7106,PDAC Qserv Deploy,"configure cluster and adapt scripts and methodology as necessary to support qserv deploy on the pdac cluster at ncsa, as is currently done at in2p3.  ",13,test
DM-7107,Deliver revised slides for Joint Status Review,"deliver a modified version of the slides from the july 2016 joint directors (sic) review, plus service any other requests from project management.",6,test
DM-7108,Provide updated F16 DRP plan for PMCS ingest,"only the first three months of f16 were concretely resource loaded and ingested into pmcs at the start of the cycle. a provisional plan was loaded for the remaining three months. check, refine and update than plan as necessary.",4,test
