issuekey,title,description,storypoint,split_mark
MESOS-313,Report executor terminations to framework schedulers.,"the scheduler interface has a callback for executorlost, but currently it is never called.",2,train
MESOS-336,Mesos slave should cache executors,"the slave should be smarter about how it handles pulling down executors.  in our environment, executors rarely change but the slave will always pull it down from regardless hdfs.  this puts undue stress on our hdfs clusters, and is not resilient to reduced hdfs availability.",5,train
MESOS-343,Expose TASK_FAILED reason to Frameworks.,"we now have a message string inside taskstatus that provides human readable information about taskfailed.    it would be good to add some structure to the failure reasons, for framework schedulers to act on programmatically.    e.g.    enum taskfailure ",8,train
MESOS-487,Balloon framework fails to run due to bad flags,"i suspect this has to do with the latest flags refactor.    [vinod@smfdbkq03sr4 build]$  sudo glogv=1 ./bin/mesostests.sh gtestfilter=""balloon"" verbose  warning: logging before initgooglelogging() is written to stderr  i0529 22:28:13.094351 31506 process.cpp:1426] libprocess is initialized on 10.37.184.103:53425 for 24 cpus  i0529 22:28:13.095010 31506 logging.cpp:91] logging to stderr  source directory: /home/vinod/mesos  build directory: /home/vinod/mesos/build    we cannot run any cgroups tests that require mounting  hierarchies because you have the following hierarchies mounted:  /cgroup  we'll disable the cgroupsnohierarchytest test fixture for now.    note: google test filter = ballooncgroupsnohierarchytest.rootcgroupsnohierarchymountunmounthierarchy:  [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from cgroupsisolatortest  [ run      ] cgroupsisolatortest.rootcgroupsballoonframework  using temporary directory '/tmp/cgroupsisolatortestrootcgroupsballoonframeworkpwwde1'  launched master at 31574  failed to load unknown flag 'builddir'  usage: ltmesosmaster [...]    supported options:    allocationinterval=value     amount of time to wait between performing                                     (batch) allocations (e.g., 500ms, 1sec, etc) (default: 1secs)    cluster=value                 human readable name for the cluster,                                    displayed in the webui    frameworksorter=value        policy to use for allocating resources                                    between a given user's frameworks. options                                    are the same as for userallocator (default: drf)    [no]help                     prints this help message (default: false)    ip=value                      ip address to listen on    logdir=value                 location to put log files (no default, nothing                                    is written to disk unless specified;                                    does not affect logging to stderr)    logbufsecs=value              how many seconds to buffer log messages for (default: 0)    port=value                    port to listen on (default: 5050)    [no]quiet                    disable logging to stderr (default: false)    [no]rootsubmissions         can root submit frameworks? (default: true)    slaves=value                  initial slaves that should be                                    considered part of this cluster                                    (or if using zookeeper a url) (default: )    usersorter=value             policy to use for allocating resources                                    between users. may be one of:                                      dominantresourcefairness (drf) (default: drf)    webuidir=value               location of the webui files/assets (default: /usr/local/share/mesos/webui)    whitelist=value               path to a file with a list of slaves                                    (one per line) to advertise offers for;                                    should be of the form: file:/path/to/file (default: )    zk=value                      zookeeper url (used for leader election amongst masters)                                    may be one of:                                      zk:/host1:port1,host2:port2,.../path                                      zk:/username:password@host1:port1,host2:port2,.../path                                      file:/path/to/file (where file contains one of the above) (default: )  master crashed; failing test  /home/vinod/mesos/src/tests/balloonframeworktest.sh: line 31: kill: (31574)  no such process  ../../src/tests/script.cpp:76: failure  failed  balloonframeworktest.sh exited with status 2  [  failed  ] cgroupsisolatortest.rootcgroupsballoonframework (2031 ms)  [] 1 test from cgroupsisolatortest (2031 ms total)    [] global test environment tear down  [==========] 1 test from 1 test case ran. (2031 ms total)  [  passed  ] 0 tests.  [  failed  ] 1 test, listed below:  [  failed  ] cgroupsisolatortest.rootcgroups_balloonframework     1 failed test  ",1,train
MESOS-598,Also check 'git diff --shortstat --staged' in post-reviews.py.,we current check if you have any changes before we run post reviews.py but we don't check for staged changes which iiuc could get lost.,1,train
MESOS-708,"Static files missing ""Last-Modified"" HTTP headers","static assets served by the mesos master don't return ""lastmodified"" http headers. that means clients receive a 200 status code and redownload assets on every page request even if the assets haven't changed. because angular js does most of the work, the downloading happens only when you navigate to mesos master in your browser or use the browser's refresh.    example header for ""mesos.css"":        http/1.1 200 ok      date: thu, 26 sep 2013 17:18:52 gmt      contentlength: 1670      contenttype: text/css    clients sometimes use the ""date"" header for the same effect as ""lastmodified"", but the date is always the time of the response from the server, i.e. it changes on every request and makes the assets look new every time.    the ""lastmodified"" header should be added and should be the last modified time of the file. on subsequent requests for the same files, the master should return 304 responses with no content rather than 200 with the full files. it could save clients a lot of download time since mesos assets are rather heavyweight.",2,train
MESOS-723,Expose total number of resources allocated to the slave in its endpoint,this could be useful information if there are bugs in master/slave that causes slaves to overcommit its resources.,2,train
MESOS-752,SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave test is flaky,"[ run      ] slaverecoverytest/0.reconciletasksmissingfromslave  checkpointing executor's forked pid 32281 to '/tmp/slaverecoverytest0reconciletasksmissingfromslavent1btb/meta/slaves/2013101519131677734335153314910/frameworks/2013101519131677734335153314910000/executors/0514b52f3c174ee5ba16635198701ca2/runs/97c9e2ccceea40a8a915aed5fed1dcb3/pids/forked.pid'  fetching resources into '/tmp/slaverecoverytest0reconciletasksmissingfromslavent1btb/slaves/2013101519131677734335153314910/frameworks/2013101519131677734335153314910000/executors/0514b52f3c174ee5ba16635198701ca2/runs/97c9e2ccceea40a8a915aed5fed1dcb3'  registered executor on localhost.localdomain  starting task 0514b52f3c174ee5ba16635198701ca2  forked command at 32317  sh c 'sleep 10'  tests/slaverecoverytests.cpp:1927: failure  mock function called more times than expected  returning directly.      function call: statusupdate(0x7fffae636eb0, @0x7f1590027a00 64byte object /)           expected: to be called once             actual: called twice  over saturated and active  command exited with status 0 (pid: 32317)  ",1,train
MESOS-786,Update semantics of when framework registered()/reregistered() get called,current semantics:    1) framework connects w/ master very first time > registered()  2) framework reconnects w/ same master after a zk blip > reregistered()  3) framework reconnects w/ failed over master > registered()  4) failed over framework connects w/ same master > registered()  5) failed over framework connects w/ failed over master > registered()     updated semantics:    everything same except   3) framework reconnects w/ failed over master > reregistered(),3,train
MESOS-830,ExamplesTest.JavaFramework is flaky,identify the cause of the following test failure:    [ run      ] examplestest.javaframework  using temporary directory '/tmp/examplestestjavaframeworkwsc7u8'  enabling authentication for the framework  i1120 15:13:39.820032 1681264640 master.cpp:285] master started on 172.25.133.171:52576  i1120 15:13:39.820180 1681264640 master.cpp:299] master id: 2013112015132877626796525763234  i1120 15:13:39.820194 1681264640 master.cpp:302] master only allowing authenticated frameworks to register!  i1120 15:13:39.821197 1679654912 slave.cpp:112] slave started on 1)@172.25.133.171:52576  i1120 15:13:39.821795 1679654912 slave.cpp:212] slave resources: cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.822855 1682337792 slave.cpp:112] slave started on 2)@172.25.133.171:52576  i1120 15:13:39.823652 1682337792 slave.cpp:212] slave resources: cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.825330 1679118336 master.cpp:744] the newly elected leader is master@172.25.133.171:52576  i1120 15:13:39.825445 1679118336 master.cpp:748] elected as the leading master!  i1120 15:13:39.825907 1681264640 state.cpp:33] recovering state from '/tmp/examplestestjavaframeworkwsc7u8/0/meta'  i1120 15:13:39.826127 1681264640 statusupdatemanager.cpp:180] recovering status update manager  i1120 15:13:39.826331 1681801216 processisolator.cpp:317] recovering isolator  i1120 15:13:39.826738 1682874368 slave.cpp:2743] finished recovery  i1120 15:13:39.827747 1682337792 state.cpp:33] recovering state from '/tmp/examplestestjavaframeworkwsc7u8/1/meta'  i1120 15:13:39.827945 1680191488 slave.cpp:112] slave started on 3)@172.25.133.171:52576  i1120 15:13:39.828415 1682337792 statusupdatemanager.cpp:180] recovering status update manager  i1120 15:13:39.828608 1680728064 sched.cpp:260] authenticating with master master@172.25.133.171:52576  i1120 15:13:39.828606 1680191488 slave.cpp:212] slave resources: cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.828680 1682874368 slave.cpp:497] new master detected at master@172.25.133.171:52576  i1120 15:13:39.828765 1682337792 processisolator.cpp:317] recovering isolator  i1120 15:13:39.829828 1680728064 sched.cpp:229] detecting new master  i1120 15:13:39.830288 1679654912 authenticatee.hpp:100] initializing client sasl  i1120 15:13:39.831635 1680191488 state.cpp:33] recovering state from '/tmp/examplestestjavaframeworkwsc7u8/2/meta'  i1120 15:13:39.831991 1679118336 statusupdatemanager.cpp:158] new master detected at master@172.25.133.171:52576  i1120 15:13:39.832042 1682874368 slave.cpp:524] detecting new master  i1120 15:13:39.832314 1682337792 slave.cpp:2743] finished recovery  i1120 15:13:39.832309 1681264640 master.cpp:1266] attempting to register slave on vkone.local at slave(1)@172.25.133.171:52576  i1120 15:13:39.832929 1680728064 statusupdatemanager.cpp:180] recovering status update manager  i1120 15:13:39.833371 1681801216 slave.cpp:497] new master detected at master@172.25.133.171:52576  i1120 15:13:39.833273 1681264640 master.cpp:2513] adding slave 20131120151328776267965257632340 at vkone.local with cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.833595 1680728064 processisolator.cpp:317] recovering isolator  i1120 15:13:39.833859 1681801216 slave.cpp:524] detecting new master  i1120 15:13:39.833861 1682874368 statusupdatemanager.cpp:158] new master detected at master@172.25.133.171:52576  i1120 15:13:39.834092 1680191488 slave.cpp:542] registered with master master@172.25.133.171:52576; given slave id 20131120151328776267965257632340  i1120 15:13:39.834486 1681264640 master.cpp:1266] attempting to register slave on vkone.local at slave(2)@172.25.133.171:52576  i1120 15:13:39.834549 1681264640 master.cpp:2513] adding slave 20131120151328776267965257632341 at vkone.local with cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.834750 1680191488 slave.cpp:555] checkpointing slaveinfo to '/tmp/examplestestjavaframeworkwsc7u8/0/meta/slaves/20131120151328776267965257632340/slave.info'  i1120 15:13:39.834875 1682874368 hierarchicalallocatorprocess.hpp:445] added slave 20131120151328776267965257632340 (vkone.local) with cpus():4; mem():7168; disk():481998; ports():[3100032000] (and cpus():4; mem():7168; disk():481998; ports():[3100032000] available)  i1120 15:13:39.835155 1680728064 slave.cpp:542] registered with master master@172.25.133.171:52576; given slave id 20131120151328776267965257632341  i1120 15:13:39.835458 1679118336 slave.cpp:2743] finished recovery  i1120 15:13:39.835739 1680728064 slave.cpp:555] checkpointing slaveinfo to '/tmp/examplestestjavaframeworkwsc7u8/1/meta/slaves/20131120151328776267965257632341/slave.info'  i1120 15:13:39.835922 1682874368 hierarchicalallocatorprocess.hpp:445] added slave 20131120151328776267965257632341 (vkone.local) with cpus():4; mem():7168; disk():481998; ports():[3100032000] (and cpus():4; mem():7168; disk():481998; ports():[3100032000] available)  i1120 15:13:39.836120 1681264640 slave.cpp:497] new master detected at master@172.25.133.171:52576  i1120 15:13:39.836340 1679118336 statusupdatemanager.cpp:158] new master detected at master@172.25.133.171:52576  i1120 15:13:39.836436 1681264640 slave.cpp:524] detecting new master  i1120 15:13:39.836629 1682874368 master.cpp:1266] attempting to register slave on vkone.local at slave(3)@172.25.133.171:52576  i1120 15:13:39.836653 1682874368 master.cpp:2513] adding slave 20131120151328776267965257632342 at vkone.local with cpus():4; mem():7168; disk():481998; ports():[3100032000]  i1120 15:13:39.836804 1680728064 slave.cpp:542] registered with master master@172.25.133.171:52576; given slave id 20131120151328776267965257632342  i1120 15:13:39.837190 1680728064 slave.cpp:555] checkpointing slaveinfo to '/tmp/examplestestjavaframeworkwsc7u8/2/meta/slaves/20131120151328776267965257632342/slave.info'  i1120 15:13:39.837569 1682874368 hierarchicalallocatorprocess.hpp:445] added slave 20131120151328776267965257632342 (vkone.local) with cpus():4; mem():7168; disk():481998; ports():[3100032000] (and cpus():4; mem():7168; disk():481998; ports():[3100032000] available)  i1120 15:13:39.852011 1679654912 authenticatee.hpp:124] creating new client sasl connection  i1120 15:13:39.852219 1680191488 master.cpp:1734] authenticating framework at scheduler(1)@172.25.133.171:52576  i1120 15:13:39.852577 1682337792 authenticator.hpp:83] initializing server sasl  i1120 15:13:39.856160 1682337792 authenticator.hpp:140] creating new server sasl connection  i1120 15:13:39.856334 1681264640 authenticatee.hpp:212] received sasl authentication mechanisms: crammd5  i1120 15:13:39.856360 1681264640 authenticatee.hpp:238] attempting to authenticate with mechanism 'crammd5'  i1120 15:13:39.856421 1681264640 authenticator.hpp:243] received sasl authentication start  i1120 15:13:39.856487 1681264640 authenticator.hpp:325] authentication requires more steps  i1120 15:13:39.856531 1681264640 authenticatee.hpp:258] received sasl authentication step  i1120 15:13:39.856576 1681264640 authenticator.hpp:271] received sasl authentication step  i1120 15:13:39.856643 1681264640 authenticator.hpp:317] authentication success  i1120 15:13:39.856724 1681264640 authenticatee.hpp:298] authentication success  i1120 15:13:39.856768 1681264640 master.cpp:1774] successfully authenticated framework at scheduler(1)@172.25.133.171:52576  i1120 15:13:39.857028 1681264640 sched.cpp:334] successfully authenticated with master master@172.25.133.171:52576  i1120 15:13:39.857139 1681264640 master.cpp:798] received registration request from scheduler(1)@172.25.133.171:52576  i1120 15:13:39.857306 1681264640 master.cpp:816] registering framework 20131120151328776267965257632340000 at scheduler(1)@172.25.133.171:52576  i1120 15:13:39.862296 1680191488 hierarchicalallocatorprocess.hpp:332] added framework 20131120151328776267965257632340000  i1120 15:13:39.863867 1680191488 master.cpp:1700] sending 3 offers to framework 20131120151328776267965257632340000  registered! id = 20131120151328776267965257632340000  launching task 0  launching task 1  launching task 2  i1120 15:13:39.905390 1680191488 master.cpp:2026] processing reply for offer 20131120151328776267965257632340 on slave 20131120151328776267965257632341 (vkone.local) for framework 20131120151328776267965257632340000  i1120 15:13:39.905825 1680191488 master.hpp:400] adding task 0 with resources cpus():1; mem():128 on slave 20131120151328776267965257632341 (vkone.local)  i1120 15:13:39.905886 1680191488 master.cpp:2150] launching task 0 of framework 20131120151328776267965257632340000 with resources cpus():1; mem():128 on slave 20131120151328776267965257632341 (vkone.local)  i1120 15:13:39.906422 1680191488 master.cpp:2026] processing reply for offer 20131120151328776267965257632341 on slave 20131120151328776267965257632342 (vkone.local) for framework 20131120151328776267965257632340000  i1120 15:13:39.906664 1680191488 master.hpp:400] adding task 1 with resources cpus():1; mem():128 on slave 20131120151328776267965257632342 (vkone.local)  i1120 15:13:39.906721 1680191488 master.cpp:2150] launching task 1 of framework 20131120151328776267965257632340000 with resources cpus():1; mem():128 on slave 20131120151328776267965257632342 (vkone.local)  i1120 15:13:39.907171 1680191488 master.cpp:2026] processing reply for offer 20131120151328776267965257632342 on slave 20131120151328776267965257632340 (vkone.local) for framework 20131120151328776267965257632340000  i1120 15:13:39.907419 1680191488 master.hpp:400] adding task 2 with resources cpus():1; mem():128 on slave 20131120151328776267965257632340 (vkone.local)  i1120 15:13:39.907480 1680191488 master.cpp:2150] launching task 2 of framework 20131120151328776267965257632340000 with resources cpus():1; mem():128 on slave 20131120151328776267965257632340 (vkone.local)  i1120 15:13:39.907938 1680191488 slave.cpp:722] got assigned task 0 for framework 20131120151328776267965257632340000  i1120 15:13:39.908473 1680191488 slave.cpp:833] launching task 0 for framework 20131120151328776267965257632340000  i1120 15:13:39.914427 1682874368 slave.cpp:722] got assigned task 1 for framework 20131120151328776267965257632340000  i1120 15:13:39.914594 1680728064 slave.cpp:722] got assigned task 2 for framework 20131120151328776267965257632340000  i1120 15:13:39.914844 1681801216 hierarchicalallocatorprocess.hpp:590] framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632341 for 1secs  i1120 15:13:39.915292 1682874368 slave.cpp:833] launching task 1 for framework 20131120151328776267965257632340000  i1120 15:13:39.915424 1681801216 hierarchicalallocatorprocess.hpp:590] framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632342 for 1secs  i1120 15:13:39.915685 1681801216 hierarchicalallocatorprocess.hpp:590] framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632340 for 1secs  i1120 15:13:39.915828 1680728064 slave.cpp:833] launching task 2 for framework 20131120151328776267965257632340000  i1120 15:13:39.917840 1680191488 slave.cpp:943] queuing task '0' for executor default of framework '20131120151328776267965257632340000  i1120 15:13:39.917935 1679118336 processisolator.cpp:100] launching default (/users/vinod/workspace/apache/mesos/build/src/examples/java/testexecutor) in /tmp/examplestestjavaframeworkwsc7u8/1/slaves/20131120151328776267965257632341/frameworks/20131120151328776267965257632340000/executors/default/runs/375b31a970934db1964de6b425b1e4b4 with resources ' for framework 20131120151328776267965257632340000  i1120 15:13:39.922019 1679118336 processisolator.cpp:163] forked executor at 3268  i1120 15:13:39.922703 1679118336 slave.cpp:2073] monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3268  i1120 15:13:39.929134 1682874368 slave.cpp:943] queuing task '1' for executor default of framework '20131120151328776267965257632340000  i1120 15:13:39.929323 1682874368 processisolator.cpp:100] launching default (/users/vinod/workspace/apache/mesos/build/src/examples/java/testexecutor) in /tmp/examplestestjavaframeworkwsc7u8/2/slaves/20131120151328776267965257632342/frameworks/20131120151328776267965257632340000/executors/default/runs/2bd0e75da2b94ae6be089782612309a5 with resources ' for framework 20131120151328776267965257632340000  i1120 15:13:39.931243 1682874368 processisolator.cpp:163] forked executor at 3269  i1120 15:13:39.931612 1681801216 slave.cpp:2073] monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3269  e1120 15:13:39.931836 1681801216 slave.cpp:2099] failed to watch executor default of framework 20131120151328776267965257632340000: already watched  i1120 15:13:39.936460 1680728064 slave.cpp:943] queuing task '2' for executor default of framework '20131120151328776267965257632340000  i1120 15:13:39.936619 1681801216 processisolator.cpp:100] launching default (/users/vinod/workspace/apache/mesos/build/src/examples/java/testexecutor) in /tmp/examplestestjavaframeworkwsc7u8/0/slaves/20131120151328776267965257632340/frameworks/20131120151328776267965257632340000/executors/default/runs/16d600dada86461491cb58a7b27ab534 with resources ' for framework 20131120151328776267965257632340000  i1120 15:13:39.941299 1681801216 processisolator.cpp:163] forked executor at 3270  i1120 15:13:39.942179 1681801216 slave.cpp:2073] monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3270  e1120 15:13:39.942395 1681801216 slave.cpp:2099] failed to watch executor default of framework 20131120151328776267965257632340000: already watched  fetching resources into '/tmp/examplestestjavaframeworkwsc7u8/2/slaves/20131120151328776267965257632342/frameworks/20131120151328776267965257632340000/executors/default/runs/2bd0e75da2b94ae6be089782612309a5'  fetching resources into '/tmp/examplestestjavaframeworkwsc7u8/1/slaves/20131120151328776267965257632341/frameworks/20131120151328776267965257632340000/executors/default/runs/375b31a970934db1964de6b425b1e4b4'  fetching resources into '/tmp/examplestestjavaframeworkwsc7u8/0/slaves/20131120151328776267965257632340/frameworks/20131120151328776267965257632340000/executors/default/runs/16d600dada86461491cb58a7b27ab534'  i1120 15:13:40.372573 1681801216 slave.cpp:1406] got registration for executor 'default' of framework 20131120151328776267965257632340000  i1120 15:13:40.373258 1681801216 slave.cpp:1527] flushing queued task 1 for executor 'default' of framework 20131120151328776267965257632340000  i1120 15:13:40.388317 1681801216 slave.cpp:1406] got registration for executor 'default' of framework 20131120151328776267965257632340000  i1120 15:13:40.388983 1681801216 slave.cpp:1527] flushing queued task 0 for executor 'default' of framework 20131120151328776267965257632340000  i1120 15:13:40.398084 1679654912 slave.cpp:1406] got registration for executor 'default' of framework 20131120151328776267965257632340000  i1120 15:13:40.399344 1679654912 slave.cpp:1527] flushing queued task 2 for executor 'default' of framework 20131120151328776267965257632340000  registered executor on vkone.local  i1120 15:13:40.491843 1679654912 slave.cpp:1740] handling status update taskrunning (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000 from executor(1)@172.25.133.171:52577  i1120 15:13:40.492202 1679654912 statusupdatemanager.cpp:305] received status update taskrunning (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000  i1120 15:13:40.492424 1679654912 statusupdatemanager.cpp:356] forwarding status update taskrunning (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000 to master@172.25.133.171:52576  registered executor on vkone.local  i1120 15:13:40.492671 1682337792 master.cpp:1452] status update taskrunning (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000 from slave(3)@172.25.133.171:52576  i1120 15:13:40.492735 1682337792 slave.cpp:1865] sending acknowledgement for status update taskrunning (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000 to executor(1)@172.25.133.171:52577  status update: task 1 is in state taskrunning  i1120 15:13:40.502235 1679654912 statusupdatemanager.cpp:380] received status update acknowledgement (uuid: f04b18523669444a906f3675f784c14f) for task 1 of framework 20131120151328776267965257632340000  registered executor on vkone.local  i1120 15:13:40.531292 1679654912 slave.cpp:1740] handling status update taskrunning (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000 from executor(1)@172.25.133.171:52579  i1120 15:13:40.532091 1680728064 statusupdatemanager.cpp:305] received status update taskrunning (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000  i1120 15:13:40.532305 1680728064 statusupdatemanager.cpp:356] forwarding status update taskrunning (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000 to master@172.25.133.171:52576  i1120 15:13:40.532776 1682874368 slave.cpp:1865] sending acknowledgement for status update taskrunning (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000 to executor(1)@172.25.133.171:52579  i1120 15:13:40.532951 1681801216 master.cpp:1452] status update taskrunning (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000 from slave(1)@172.25.133.171:52576  status update: task 2 is in state taskrunning  i1120 15:13:40.538895 1682874368 statusupdatemanager.cpp:380] received status update acknowledgement (uuid: c19b6a5a19ce46138a5a08fe807ff27c) for task 2 of framework 20131120151328776267965257632340000  i1120 15:13:40.541267 1682874368 slave.cpp:1740] handling status update taskrunning (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of framework 20131120151328776267965257632340000 from executor(1)@172.25.133.171:52578  i1120 15:13:40.541555 1682874368 statusupdatemanager.cpp:305] received status update taskrunning (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of framework 20131120151328776267965257632340000  i1120 15:13:40.541725 1682874368 statusupdatemanager.cpp:356] forwarding status update taskrunning (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of framework 20131120151328776267965257632340000 to master@172.25.133.171:52576  i1120 15:13:40.542196 1682874368 master.cpp:1452] status update taskrunning (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of framework 20131120151328776267965257632340000 from slave(2)@172.25.133.171:52576  i1120 15:13:40.542251 1682874368 slave.cpp:1865] sending acknowledgement for status update taskrunning (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of framework 20131120151328776267965257632340000 to executor(1)@172.25.133.171:52578  status update: task 0 is in state taskrunning  i1120 15:13:40.545537 1682874368 statusupdate_manager.cpp:380] received status update acknowledgement (uuid: c218b0c3d77c49018570391c330ba117) for task 0 of...,8,train
MESOS-920,Set GLOG_drop_log_memory=false in environment prior to logging initialization.,"we've observed issues where the masters are slow to respond. two perf traces collected while the masters were slow to respond:       25.84%  [kernel]                [k] defaultsendipimasksequencephys   20.44%  [kernel]                [k] nativewritemsrsafe    4.54%  [kernel]                [k] rawspinlock    2.95%  libc2.5.so             [.] intmalloc    1.82%  libc2.5.so             [.] malloc    1.55%  [kernel]                [k] apictimerinterrupt    1.36%  libc2.5.so             [.] intfree         29.03%  [kernel]                [k] defaultsendipimasksequencephys    9.64%  [kernel]                [k] rawspinlock    7.38%  [kernel]                [k] nativewritemsrsafe    2.43%  libc2.5.so             [.] intmalloc    2.05%  libc2.5.so             [.] intfree    1.67%  [kernel]                [k] apictimerinterrupt    1.58%  libc2.5.so             [.] malloc      these have been found to be attributed to the posixfadvise calls made by glog. we can disable these via the environment:      glogdefinebool(droplogmemory, true, ""drop inmemory buffers of log contents. ""                   ""logs can grow very quickly and they are rarely read before they ""                   ""need to be evicted from memory. instead, drop them from memory ""                   ""as soon as they are flushed to disk."");              if (flagsdroplogmemory)       }      we should set glogdroplog_memory=false prior to making our call to google::initgooglelogging, to avoid others running into this issue.",2,train
MESOS-934,'Logging and Debugging' document is out-of-date.,the following is no longer correct:  http:/mesos.apache.org/documentation/latest/logginganddebugging/    we should either delete this document or re write it entirely.,1,train
MESOS-976,SlaveRecoveryTest/1.SchedulerFailover is flaky,"[==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from slaverecoverytest/1, where typeparam = mesos::internal::slave::cgroupsisolator  [ run      ] slaverecoverytest/1.schedulerfailover  i0206 20:18:31.525116 56447 master.cpp:239] master id: 2014020620:18:3117401213545556656447 hostname: smfdbkq03sr4.devel.twitter.com  i0206 20:18:31.525295 56481 master.cpp:321] master started on 10.37.184.103:55566  i0206 20:18:31.525315 56481 master.cpp:324] master only allowing authenticated frameworks to register!  i0206 20:18:31.527093 56481 master.cpp:756] the newly elected leader is master@10.37.184.103:55566  i0206 20:18:31.527122 56481 master.cpp:764] elected as the leading master!  i0206 20:18:31.530642 56473 slave.cpp:112] slave started on 9)@10.37.184.103:55566  i0206 20:18:31.530802 56473 slave.cpp:212] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 20:18:31.531203 56473 slave.cpp:240] slave hostname: smfdbkq03sr4.devel.twitter.com  i0206 20:18:31.531221 56473 slave.cpp:241] slave checkpoint: true  i0206 20:18:31.531991 56482 cgroupsisolator.cpp:225] using /tmp/mesostestcgroup as cgroups hierarchy root  i0206 20:18:31.532470 56478 state.cpp:33] recovering state from '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta'  i0206 20:18:31.532698 56469 statusupdatemanager.cpp:188] recovering status update manager  i0206 20:18:31.533962 56472 sched.cpp:265] authenticating with master master@10.37.184.103:55566  i0206 20:18:31.534102 56472 sched.cpp:234] detecting new master  i0206 20:18:31.534124 56484 authenticatee.hpp:124] creating new client sasl connection  i0206 20:18:31.534299 56473 master.cpp:2317] authenticating framework at scheduler(9)@10.37.184.103:55566  i0206 20:18:31.534459 56461 authenticator.hpp:140] creating new server sasl connection  i0206 20:18:31.534572 56466 authenticatee.hpp:212] received sasl authentication mechanisms: crammd5  i0206 20:18:31.534595 56466 authenticatee.hpp:238] attempting to authenticate with mechanism 'crammd5'  i0206 20:18:31.534667 56474 authenticator.hpp:243] received sasl authentication start  i0206 20:18:31.534732 56474 authenticator.hpp:325] authentication requires more steps  i0206 20:18:31.534814 56468 authenticatee.hpp:258] received sasl authentication step  i0206 20:18:31.534946 56466 authenticator.hpp:271] received sasl authentication step  i0206 20:18:31.535007 56466 authenticator.hpp:317] authentication success  i0206 20:18:31.535084 56471 authenticatee.hpp:298] authentication success  i0206 20:18:31.535107 56461 master.cpp:2357] successfully authenticated framework at scheduler(9)@10.37.184.103:55566  i0206 20:18:31.535392 56476 sched.cpp:339] successfully authenticated with master master@10.37.184.103:55566  i0206 20:18:31.535512 56465 master.cpp:812] received registration request from scheduler(9)@10.37.184.103:55566  i0206 20:18:31.535570 56465 master.cpp:830] registering framework 2014020620:18:31174012135455566564470000 at scheduler(9)@10.37.184.103:55566  i0206 20:18:31.535856 56465 hierarchicalallocatorprocess.hpp:332] added framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.537802 56482 cgroupsisolator.cpp:840] recovering isolator  i0206 20:18:31.538462 56472 slave.cpp:2760] finished recovery  i0206 20:18:31.538910 56472 slave.cpp:508] new master detected at master@10.37.184.103:55566  i0206 20:18:31.539036 56478 statusupdatemanager.cpp:162] new master detected at master@10.37.184.103:55566  i0206 20:18:31.539223 56464 master.cpp:1834] attempting to register slave on smfdbkq03sr4.devel.twitter.com at slave(9)@10.37.184.103:55566  i0206 20:18:31.539271 56472 slave.cpp:533] detecting new master  i0206 20:18:31.539330 56464 master.cpp:2804] adding slave 2014020620:18:31174012135455566564470 at smfdbkq03sr4.devel.twitter.com with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 20:18:31.539454 56472 slave.cpp:551] registered with master master@10.37.184.103:55566; given slave id 2014020620:18:31174012135455566564470  i0206 20:18:31.539620 56472 slave.cpp:564] checkpointing slaveinfo to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/slave.info'  i0206 20:18:31.539834 56475 hierarchicalallocatorprocess.hpp:445] added slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0206 20:18:31.540341 56472 master.cpp:2272] sending 1 offers to framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.543433 56472 master.cpp:1568] processing reply for offers: [ 2014020620:18:31174012135455566564470 ] on slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com) for framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.543642 56472 master.hpp:411] adding task d045a0bd2ed2410abd1f5bd9219896e3 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com)  i0206 20:18:31.543781 56472 master.cpp:2441] launching task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com)  i0206 20:18:31.544002 56484 slave.cpp:736] got assigned task d045a0bd2ed2410abd1f5bd9219896e3 for framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.544097 56484 slave.cpp:2899] checkpointing frameworkinfo to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/framework.info'  i0206 20:18:31.544272 56484 slave.cpp:2906] checkpointing framework pid 'scheduler(9)@10.37.184.103:55566' to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/framework.pid'  i0206 20:18:31.544617 56484 slave.cpp:845] launching task d045a0bd2ed2410abd1f5bd9219896e3 for framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.546721 56484 slave.cpp:3169] checkpointing executorinfo to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/executor.info'  i0206 20:18:31.547317 56484 slave.cpp:3257] checkpointing taskinfo to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/runs/9adabe165d8445c9bc831a72a6d1c986/tasks/d045a0bd2ed2410abd1f5bd9219896e3/task.info'  i0206 20:18:31.547514 56484 slave.cpp:955] queuing task 'd045a0bd2ed2410abd1f5bd9219896e3' for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework '2014020620:18:31174012135455566564470000  i0206 20:18:31.547590 56481 cgroupsisolator.cpp:517] launching d045a0bd2ed2410abd1f5bd9219896e3 (/home/vinod/mesos/build/src/mesosexecutor) in /tmp/slaverecoverytest1schedulerfailover7dc2n1/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/runs/9adabe165d8445c9bc831a72a6d1c986 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] for framework 2014020620:18:31174012135455566564470000 in cgroup mesostest/framework2014020620:18:31174012135455566564470000executord045a0bd2ed2410abd1f5bd9219896e3tag9adabe165d8445c9bc831a72a6d1c986  i0206 20:18:31.548408 56481 cgroupsisolator.cpp:717] changing cgroup controls for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 20:18:31.548833 56481 cgroupsisolator.cpp:1007] updated 'cpu.shares' to 2048 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.549294 56481 cgroupsisolator.cpp:1117] updated 'memory.softlimitinbytes' to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.550107 56481 cgroupsisolator.cpp:1147] updated 'memory.limitinbytes' to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.550571 56481 cgroupsisolator.cpp:1174] started listening for oom events for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.551553 56481 cgroupsisolator.cpp:569] forked executor at = 56671  checkpointing executor's forked pid 56671 to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/runs/9adabe165d8445c9bc831a72a6d1c986/pids/forked.pid'  i0206 20:18:31.552222 56472 slave.cpp:2098] monitoring executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 forked at pid 56671  fetching resources into '/tmp/slaverecoverytest1schedulerfailover7dc2n1/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/runs/9adabe165d8445c9bc831a72a6d1c986'  i0206 20:18:31.604012 56472 slave.cpp:1431] got registration for executor 'd045a0bd2ed2410abd1f5bd9219896e3' of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.604167 56472 slave.cpp:1516] checkpointing executor pid 'executor(1)@10.37.184.103:46181' to '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/executors/d045a0bd2ed2410abd1f5bd9219896e3/runs/9adabe165d8445c9bc831a72a6d1c986/pids/libprocess.pid'  i0206 20:18:31.605183 56472 slave.cpp:1552] flushing queued task d045a0bd2ed2410abd1f5bd9219896e3 for executor 'd045a0bd2ed2410abd1f5bd9219896e3' of framework 2014020620:18:31174012135455566564470000  registered executor on smfdbkq03sr4.devel.twitter.com  starting task d045a0bd2ed2410abd1f5bd9219896e3  sh c 'sleep 1000'  forked command at 56712  i0206 20:18:31.613098 56481 slave.cpp:1765] handling status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 from executor(1)@10.37.184.103:46181  i0206 20:18:31.613628 56469 statusupdatemanager.cpp:314] received status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.614006 56469 statusupdatemanager.hpp:342] checkpointing update for status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.795529 56469 statusupdatemanager.cpp:367] forwarding status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 to master@10.37.184.103:55566  i0206 20:18:31.795992 56480 slave.cpp:1890] sending acknowledgement for status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 to executor(1)@10.37.184.103:46181  i0206 20:18:31.796131 56471 master.cpp:2020] status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 from slave(9)@10.37.184.103:55566  i0206 20:18:31.797099 56483 statusupdatemanager.cpp:392] received status update acknowledgement (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.797165 56483 statusupdatemanager.hpp:342] checkpointing ack for status update taskrunning (uuid: fc151a46751b4c4bb0481727752f34e3) for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.882767 56481 slave.cpp:394] slave terminating  i0206 20:18:31.883112 56481 master.cpp:641] slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com) disconnected  i0206 20:18:31.883200 56476 hierarchicalallocatorprocess.hpp:484] slave 2014020620:18:31174012135455566564470 disconnected  i0206 20:18:31.888206 56473 sched.cpp:265] authenticating with master master@10.37.184.103:55566  i0206 20:18:31.888473 56473 sched.cpp:234] detecting new master  i0206 20:18:31.888556 56469 authenticatee.hpp:124] creating new client sasl connection  i0206 20:18:31.888978 56484 master.cpp:2317] authenticating framework at scheduler(10)@10.37.184.103:55566  i0206 20:18:31.889348 56469 authenticator.hpp:140] creating new server sasl connection  i0206 20:18:31.889925 56469 authenticatee.hpp:212] received sasl authentication mechanisms: crammd5  i0206 20:18:31.889989 56469 authenticatee.hpp:238] attempting to authenticate with mechanism 'crammd5'  i0206 20:18:31.890059 56469 authenticator.hpp:243] received sasl authentication start  i0206 20:18:31.890233 56469 authenticator.hpp:325] authentication requires more steps  i0206 20:18:31.890399 56468 authenticatee.hpp:258] received sasl authentication step  i0206 20:18:31.890554 56484 authenticator.hpp:271] received sasl authentication step  i0206 20:18:31.890630 56484 authenticator.hpp:317] authentication success  i0206 20:18:31.890728 56470 authenticatee.hpp:298] authentication success  i0206 20:18:31.890748 56484 master.cpp:2357] successfully authenticated framework at scheduler(10)@10.37.184.103:55566  i0206 20:18:31.892210 56469 sched.cpp:339] successfully authenticated with master master@10.37.184.103:55566  i0206 20:18:31.892410 56473 master.cpp:900] reregistering framework 2014020620:18:31174012135455566564470000 at scheduler(10)@10.37.184.103:55566  i0206 20:18:31.892460 56473 master.cpp:926] framework 2014020620:18:31174012135455566564470000 failed over  w0206 20:18:31.892691 56465 master.cpp:1048] ignoring deactivate framework message for framework 2014020620:18:31174012135455566564470000 from 'scheduler(9)@10.37.184.103:55566' because it is not from the registered framework 'scheduler(10)@10.37.184.103:55566'  i0206 20:18:31.897049 56466 slave.cpp:112] slave started on 10)@10.37.184.103:55566  i0206 20:18:31.897207 56466 slave.cpp:212] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 20:18:31.897536 56466 slave.cpp:240] slave hostname: smfdbkq03sr4.devel.twitter.com  i0206 20:18:31.897554 56466 slave.cpp:241] slave checkpoint: true  i0206 20:18:31.898388 56463 cgroupsisolator.cpp:225] using /tmp/mesostestcgroup as cgroups hierarchy root  i0206 20:18:31.898936 56472 state.cpp:33] recovering state from '/tmp/slaverecoverytest1schedulerfailover7dc2n1/meta'  i0206 20:18:31.901702 56465 slave.cpp:2828] recovering framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.901759 56465 slave.cpp:3020] recovering executor 'd045a0bd2ed2410abd1f5bd9219896e3' of framework 2014020620:18:31174012135455566564470000  i0206 20:18:31.902716 56464 statusupdatemanager.cpp:188] recovering status update manager  i0206 20:18:31.902884 56464 statusupdatemanager.cpp:196] recovering executor 'd045a0bd2ed2410abd1f5bd9219896e3' of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.475915 56463 cgroupsisolator.cpp:840] recovering isolator  i0206 20:18:34.476066 56463 cgroupsisolator.cpp:847] recovering executor 'd045a0bd2ed2410abd1f5bd9219896e3' of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.477478 56463 cgroupsisolator.cpp:1174] started listening for oom events for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.478728 56463 slave.cpp:2700] sending reconnect request to executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 at executor(1)@10.37.184.103:46181  i0206 20:18:34.480114 56476 slave.cpp:1597] reregistering executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.480566 56476 cgroupsisolator.cpp:717] changing cgroup controls for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 20:18:34.481370 56476 cgroupsisolator.cpp:1007] updated 'cpu.shares' to 2048 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.481827 56476 cgroupsisolator.cpp:1117] updated 'memory.softlimitinbytes' to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  reregistered executor on smfdbkq03sr4.devel.twitter.com  i0206 20:18:34.489497 56471 slave.cpp:1713] cleaning up unreregistered executors  i0206 20:18:34.489588 56471 slave.cpp:2760] finished recovery  i0206 20:18:34.490048 56463 slave.cpp:508] new master detected at master@10.37.184.103:55566  i0206 20:18:34.490257 56475 statusupdatemanager.cpp:162] new master detected at master@10.37.184.103:55566  i0206 20:18:34.490357 56463 slave.cpp:533] detecting new master  w0206 20:18:34.490603 56480 master.cpp:1878] slave at slave(10)@10.37.184.103:55566 (smfdbkq03sr4.devel.twitter.com) is being allowed to reregister with an already in use id (2014020620:18:31174012135455566564470)  i0206 20:18:34.490927 56479 slave.cpp:601] reregistered with master master@10.37.184.103:55566  i0206 20:18:34.491322 56461 hierarchicalallocatorprocess.hpp:498] slave 2014020620:18:31174012135455566564470 reconnected  i0206 20:18:34.491421 56468 slave.cpp:1312] updating framework 2014020620:18:31174012135455566564470000 pid to scheduler(10)@10.37.184.103:55566  i0206 20:18:34.491444 56480 master.cpp:1673] asked to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.491488 56468 slave.cpp:1320] checkpointing framework pid 'scheduler(10)@10.37.184.103:55566' to '/tmp/slaverecoverytest1schedulerfailover_7dc2n1/meta/slaves/2014020620:18:31174012135455566564470/frameworks/2014020620:18:31174012135455566564470000/framework.pid'  i0206 20:18:34.491497 56480 master.cpp:1707] telling slave 2014020620:18:31174012135455566564470 (smfdbkq03sr4.devel.twitter.com) to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:31174012135455566564470000  i0206 20:18:34.491657 56468 slave.cpp:1013] asked to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 2014020620:18:3117401213545556656447 0000  shutting down  killing process tree at pid 56712...",1,train
MESOS-988,ExamplesTest.PythonFramework is flaky,looks like a segfault during shutdown.      [ run      ] examplestest.pythonframework  using temporary directory '/tmp/examplestestpythonframeworkrz4yaf'  warning: logging before initgooglelogging() is written to stderr  i0211 21:14:47.861803 21045 process.cpp:1591] libprocess is initialized on 67.195.138.9:53443 for 8 cpus  i0211 21:14:47.861884 21045 logging.cpp:140] logging to stderr  i0211 21:14:47.862761 21045 master.cpp:240] master id: 2014021121:14:471600888995344321045 hostname: vesta.apache.org  i0211 21:14:47.862897 21054 master.cpp:322] master started on 67.195.138.9:53443  i0211 21:14:47.862908 21054 master.cpp:325] master only allowing authenticated frameworks to register!  i0211 21:14:47.864362 21053 master.cpp:86] no whitelist given. advertising offers for all slaves  i0211 21:14:47.864506 21055 slave.cpp:112] slave started on 1)@67.195.138.9:53443  i0211 21:14:47.864522 21059 slave.cpp:112] slave started on 2)@67.195.138.9:53443  i0211 21:14:47.864749 21055 slave.cpp:212] slave resources: cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.864778 21059 slave.cpp:212] slave resources: cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.864819 21055 slave.cpp:240] slave hostname: vesta.apache.org  i0211 21:14:47.864827 21055 slave.cpp:241] slave checkpoint: true  i0211 21:14:47.864850 21059 slave.cpp:240] slave hostname: vesta.apache.org  i0211 21:14:47.864858 21059 slave.cpp:241] slave checkpoint: true  i0211 21:14:47.865329 21055 master.cpp:760] the newly elected leader is master@67.195.138.9:53443 with id 2014021121:14:471600888995344321045  i0211 21:14:47.865350 21055 master.cpp:770] elected as the leading master!  i0211 21:14:47.865399 21055 state.cpp:33] recovering state from '/tmp/mesosz8v6cu/1/meta'  i0211 21:14:47.865407 21059 state.cpp:33] recovering state from '/tmp/mesosz8v6cu/0/meta'  i0211 21:14:47.865502 21052 hierarchicalallocatorprocess.hpp:302] initializing hierarchical allocator process with master : master@67.195.138.9:53443  i0211 21:14:47.865540 21054 statusupdatemanager.cpp:188] recovering status update manager  i0211 21:14:47.865619 21053 processisolator.cpp:319] recovering isolator  i0211 21:14:47.865674 21057 statusupdatemanager.cpp:188] recovering status update manager  i0211 21:14:47.865699 21059 slave.cpp:2760] finished recovery  i0211 21:14:47.865733 21053 processisolator.cpp:319] recovering isolator  i0211 21:14:47.865789 21053 slave.cpp:2760] finished recovery  i0211 21:14:47.865921 21059 slave.cpp:508] new master detected at master@67.195.138.9:53443  i0211 21:14:47.865958 21053 statusupdatemanager.cpp:162] new master detected at master@67.195.138.9:53443  i0211 21:14:47.865978 21059 slave.cpp:533] detecting new master  i0211 21:14:47.866019 21053 slave.cpp:508] new master detected at master@67.195.138.9:53443  i0211 21:14:47.866063 21053 slave.cpp:533] detecting new master  i0211 21:14:47.866070 21055 statusupdatemanager.cpp:162] new master detected at master@67.195.138.9:53443  i0211 21:14:47.866077 21059 master.cpp:1840] attempting to register slave on vesta.apache.org at slave(2)@67.195.138.9:53443  i0211 21:14:47.866092 21059 master.cpp:2810] adding slave 2014021121:14:4716008889953443210450 at vesta.apache.org with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.866216 21059 master.cpp:1840] attempting to register slave on vesta.apache.org at slave(1)@67.195.138.9:53443  i0211 21:14:47.866225 21053 slave.cpp:551] registered with master master@67.195.138.9:53443; given slave id 2014021121:14:4716008889953443210450  i0211 21:14:47.866228 21059 master.cpp:2810] adding slave 2014021121:14:4716008889953443210451 at vesta.apache.org with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.866278 21055 hierarchicalallocatorprocess.hpp:445] added slave 2014021121:14:4716008889953443210450 (vesta.apache.org) with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] (and cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] available)  i0211 21:14:47.866297 21059 slave.cpp:551] registered with master master@67.195.138.9:53443; given slave id 2014021121:14:4716008889953443210451  i0211 21:14:47.866327 21055 hierarchicalallocatorprocess.hpp:708] performed allocation for slave 2014021121:14:4716008889953443210450 in 11us  i0211 21:14:47.866330 21053 slave.cpp:564] checkpointing slaveinfo to '/tmp/mesosz8v6cu/1/meta/slaves/2014021121:14:4716008889953443210450/slave.info'  i0211 21:14:47.866400 21059 slave.cpp:564] checkpointing slaveinfo to '/tmp/mesosz8v6cu/0/meta/slaves/2014021121:14:4716008889953443210451/slave.info'  i0211 21:14:47.866399 21055 hierarchicalallocatorprocess.hpp:445] added slave 2014021121:14:4716008889953443210451 (vesta.apache.org) with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] (and cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] available)  i0211 21:14:47.866423 21055 hierarchicalallocatorprocess.hpp:708] performed allocation for slave 2014021121:14:4716008889953443210451 in 2505ns  i0211 21:14:47.866636 21059 slave.cpp:112] slave started on 3)@67.195.138.9:53443  i0211 21:14:47.866727 21059 slave.cpp:212] slave resources: cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.866766 21059 slave.cpp:240] slave hostname: vesta.apache.org  i0211 21:14:47.866772 21059 slave.cpp:241] slave checkpoint: true  i0211 21:14:47.867300 21052 state.cpp:33] recovering state from '/tmp/mesosz8v6cu/2/meta'  i0211 21:14:47.867368 21052 statusupdatemanager.cpp:188] recovering status update manager  i0211 21:14:47.867419 21055 processisolator.cpp:319] recovering isolator  i0211 21:14:47.867544 21052 slave.cpp:2760] finished recovery  i0211 21:14:47.867729 21052 slave.cpp:508] new master detected at master@67.195.138.9:53443  i0211 21:14:47.867770 21054 statusupdatemanager.cpp:162] new master detected at master@67.195.138.9:53443  i0211 21:14:47.867777 21052 slave.cpp:533] detecting new master  i0211 21:14:47.867815 21055 master.cpp:1840] attempting to register slave on vesta.apache.org at slave(3)@67.195.138.9:53443  i0211 21:14:47.867827 21055 master.cpp:2810] adding slave 2014021121:14:4716008889953443210452 at vesta.apache.org with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000]  i0211 21:14:47.867885 21052 slave.cpp:551] registered with master master@67.195.138.9:53443; given slave id 2014021121:14:4716008889953443210452  i0211 21:14:47.867961 21055 hierarchicalallocatorprocess.hpp:445] added slave 2014021121:14:4716008889953443210452 (vesta.apache.org) with cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] (and cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] available)  i0211 21:14:47.867985 21052 slave.cpp:564] checkpointing slaveinfo to '/tmp/mesosz8v6cu/2/meta/slaves/2014021121:14:4716008889953443210452/slave.info'  i0211 21:14:47.867987 21055 hierarchicalallocatorprocess.hpp:708] performed allocation for slave 2014021121:14:4716008889953443210452 in 3308ns  i0211 21:14:47.868468 21045 sched.cpp:121] version: 0.18.0  i0211 21:14:47.868633 21055 sched.cpp:217] new master detected at master@67.195.138.9:53443  i0211 21:14:47.868651 21055 sched.cpp:268] authenticating with master master@67.195.138.9:53443  i0211 21:14:47.868696 21055 sched.cpp:237] detecting new master  i0211 21:14:47.868708 21054 authenticatee.hpp:100] initializing client sasl  i0211 21:14:47.869549 21054 authenticatee.hpp:124] creating new client sasl connection  i0211 21:14:47.869633 21055 master.cpp:2323] authenticating framework at scheduler(1)@67.195.138.9:53443  i0211 21:14:47.869818 21059 authenticator.hpp:83] initializing server sasl  i0211 21:14:47.870029 21059 auxprop.cpp:45] initialized inmemory auxiliary property plugin  i0211 21:14:47.870040 21059 authenticator.hpp:140] creating new server sasl connection  i0211 21:14:47.870144 21057 authenticatee.hpp:212] received sasl authentication mechanisms: crammd5  i0211 21:14:47.870174 21057 authenticatee.hpp:238] attempting to authenticate with mechanism 'crammd5'  i0211 21:14:47.870203 21057 authenticator.hpp:243] received sasl authentication start  i0211 21:14:47.870256 21057 authenticator.hpp:325] authentication requires more steps  i0211 21:14:47.870282 21057 authenticatee.hpp:258] received sasl authentication step  i0211 21:14:47.870348 21057 authenticator.hpp:271] received sasl authentication step  i0211 21:14:47.870376 21057 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'vesta.apache.org' server fqdn: 'vesta.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0211 21:14:47.870384 21057 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0211 21:14:47.870396 21057 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0211 21:14:47.870405 21057 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'vesta.apache.org' server fqdn: 'vesta.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0211 21:14:47.870411 21057 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0211 21:14:47.870415 21057 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0211 21:14:47.870425 21057 authenticator.hpp:317] authentication success  i0211 21:14:47.870445 21057 master.cpp:2363] successfully authenticated framework at scheduler(1)@67.195.138.9:53443  i0211 21:14:47.870448 21055 authenticatee.hpp:298] authentication success  i0211 21:14:47.870492 21055 sched.cpp:342] successfully authenticated with master master@67.195.138.9:53443  i0211 21:14:47.870538 21057 master.cpp:818] received registration request from scheduler(1)@67.195.138.9:53443  i0211 21:14:47.870590 21057 master.cpp:836] registering framework 2014021121:14:4716008889953443210450000 at scheduler(1)@67.195.138.9:53443  i0211 21:14:47.870661 21055 sched.cpp:391] framework registered with 2014021121:14:4716008889953443210450000  i0211 21:14:47.870661 21057 hierarchicalallocatorprocess.hpp:332] added framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.870707 21057 hierarchicalallocatorprocess.hpp:752] offering cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210450 to framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.870798 21057 hierarchicalallocatorprocess.hpp:752] offering cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210451 to framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.870869 21057 hierarchicalallocatorprocess.hpp:752] offering cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210452 to framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.870894 21055 sched.cpp:405] scheduler::registered took 222149ns  i0211 21:14:47.871038 21057 hierarchicalallocatorprocess.hpp:688] performed allocation for 3 slaves in 351098ns  i0211 21:14:47.871106 21058 master.hpp:439] adding offer 2014021121:14:4716008889953443210450 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210452 (vesta.apache.org)  i0211 21:14:47.871215 21058 master.hpp:439] adding offer 2014021121:14:4716008889953443210451 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210451 (vesta.apache.org)  i0211 21:14:47.871296 21058 master.hpp:439] adding offer 2014021121:14:4716008889953443210452 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210450 (vesta.apache.org)  i0211 21:14:47.871333 21058 master.cpp:2278] sending 3 offers to framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.873667 21055 sched.cpp:525] scheduler::resourceoffers took 2.150843ms  i0211 21:14:47.873884 21053 master.hpp:449] removing offer 2014021121:14:4716008889953443210450 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210452 (vesta.apache.org)  i0211 21:14:47.873934 21053 master.cpp:1574] processing reply for offers: [ 2014021121:14:4716008889953443210450 ] on slave 2014021121:14:4716008889953443210452 (vesta.apache.org) for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874035 21053 master.hpp:411] adding task 0 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210452 (vesta.apache.org)  i0211 21:14:47.874059 21053 master.cpp:2447] launching task 0 of framework 2014021121:14:4716008889953443210450000 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210452 (vesta.apache.org)  i0211 21:14:47.874150 21059 slave.cpp:736] got assigned task 0 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874200 21058 hierarchicalallocatorprocess.hpp:547] framework 2014021121:14:4716008889953443210450000 left cpus():7; mem():6929; disk():1.38501e06; ports():[3100032000] unused on slave 2014021121:14:4716008889953443210452  i0211 21:14:47.874250 21053 master.hpp:449] removing offer 2014021121:14:4716008889953443210451 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210451 (vesta.apache.org)  i0211 21:14:47.874307 21053 master.cpp:1574] processing reply for offers: [ 2014021121:14:4716008889953443210451 ] on slave 2014021121:14:4716008889953443210451 (vesta.apache.org) for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874322 21058 hierarchicalallocatorprocess.hpp:590] framework 2014021121:14:4716008889953443210450000 filtered slave 2014021121:14:4716008889953443210452 for 5secs  i0211 21:14:47.874354 21059 slave.cpp:845] launching task 0 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874404 21053 master.hpp:411] adding task 1 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210451 (vesta.apache.org)  i0211 21:14:47.874428 21053 master.cpp:2447] launching task 1 of framework 2014021121:14:4716008889953443210450000 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210451 (vesta.apache.org)  i0211 21:14:47.874479 21058 slave.cpp:736] got assigned task 1 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874586 21053 master.hpp:449] removing offer 2014021121:14:4716008889953443210452 with resources cpus():8; mem():6961; disk():1.38501e06; ports():[3100032000] on slave 2014021121:14:4716008889953443210450 (vesta.apache.org)  i0211 21:14:47.874646 21053 master.cpp:1574] processing reply for offers: [ 2014021121:14:4716008889953443210452 ] on slave 2014021121:14:4716008889953443210450 (vesta.apache.org) for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874690 21058 slave.cpp:845] launching task 1 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.874694 21053 master.hpp:411] adding task 2 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210450 (vesta.apache.org)  i0211 21:14:47.874716 21053 master.cpp:2447] launching task 2 of framework 2014021121:14:4716008889953443210450000 with resources cpus():1; mem():32 on slave 2014021121:14:4716008889953443210450 (vesta.apache.org)  i0211 21:14:47.874820 21053 hierarchicalallocatorprocess.hpp:547] framework 2014021121:14:4716008889953443210450000 left cpus():7; mem():6929; disk():1.38501e06; ports():[3100032000] unused on slave 2014021121:14:4716008889953443210451  i0211 21:14:47.874892 21053 hierarchicalallocatorprocess.hpp:590] framework 2014021121:14:4716008889953443210450000 filtered slave 2014021121:14:4716008889953443210451 for 5secs  i0211 21:14:47.874922 21053 hierarchicalallocatorprocess.hpp:547] framework 2014021121:14:4716008889953443210450000 left cpus():7; mem():6929; disk():1.38501e06; ports():[3100032000] unused on slave 2014021121:14:4716008889953443210450  i0211 21:14:47.874980 21053 hierarchicalallocatorprocess.hpp:590] framework 2014021121:14:4716008889953443210450000 filtered slave 2014021121:14:4716008889953443210450 for 5secs  i0211 21:14:47.875012 21053 slave.cpp:736] got assigned task 2 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.875151 21053 slave.cpp:845] launching task 2 for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.875527 21059 slave.cpp:955] queuing task '0' for executor default of framework '2014021121:14:4716008889953443210450000  i0211 21:14:47.875608 21059 processisolator.cpp:102] launching default (/home/hudson/jenkinsslave/workspace/mesostrunkubuntubuildinsrcsetjavahome/src/examples/python/testexecutor) in /tmp/mesosz8v6cu/2/slaves/2014021121:14:4716008889953443210452/frameworks/2014021121:14:4716008889953443210450000/executors/default/runs/02cdf8bd07574a408e77af60bb202d71 with resources cpus():1; mem():32' for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.876787 21054 slave.cpp:469] successfully attached file '/tmp/mesosz8v6cu/2/slaves/2014021121:14:4716008889953443210452/frameworks/2014021121:14:4716008889953443210450000/executors/default/runs/02cdf8bd07574a408e77af60bb202d71'  i0211 21:14:47.876852 21059 processisolator.cpp:165] forked executor at 21061  i0211 21:14:47.876940 21058 slave.cpp:955] queuing task '1' for executor default of framework '2014021121:14:4716008889953443210450000  i0211 21:14:47.877095 21057 processisolator.cpp:102] launching default (/home/hudson/jenkinsslave/workspace/mesostrunkubuntubuildinsrcsetjavahome/src/examples/python/testexecutor) in /tmp/mesosz8v6cu/0/slaves/2014021121:14:4716008889953443210451/frameworks/2014021121:14:4716008889953443210450000/executors/default/runs/568b657d839d483faff14872fbfc27dc with resources cpus():1; mem():32' for framework 2014021121:14:4716008889953443210450000  i0211 21:14:47.877102 21052 slave.cpp:469] successfully attached file '/tmp/mesosz8v6cu/0/slaves/2014021121:14:4716008889953443210451/frameworks/2014021121:14:4716008889953443210450000/executors/default/runs/568b657d839d483faff14872fbfc27dc'  i0211 21:14:47.878783 21057 processisolator.cpp:165] forked executor at 21062  i0211 21:14:47.879032 21053 slave.cpp:955] queuing task '2' for executor default of framework '2014021121:14:4716008889953443210450000  i0211 21:14:47.879192 21054 slave.cpp:2098] monitoring executor default of framework 2014021121:14:4716008889953443210450000 forked at pid 21062  i0211 21:14:47.879192 21058 slave.cpp:469] successfully attached file '/tmp/mesosz8v6cu/1/slaves/2014021121:14:4716008889953443210450/frameworks/2014021121:14:4716008889953443210450000/executors/default/runs/a7c4170af40b449381b30ea8c70e3977'  i0211 21:14:47.879166 21052 processisolator.cpp:102] launching default (/home/hudson/jenkinsslave/workspace/mesostrunkubuntubuildinsrcset java_home/src/examples/p...,3,train
MESOS-998,Slave should wait until Containerizer::update() completes successfully,container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes.,5,train
MESOS-999,Slave should wait() and start executor registration timeout after launch ,the current code will start launch a container and wait on it before the launch is complete. we should do this only after the container has successfully launched. likewise for the executor registration timeout.,3,train
MESOS-1010,Python extension build is broken if gflags-dev is installed,"in my environment mesos build from master results in broken python api module mesos.so:    nekto0n@yadarkstar ~/workspace/mesos/src/python $ pythonpath=build/lib.linuxx86642.7/ python c ""import mesos""  traceback (most recent call last):    file ""/"", line 1, in /  importerror: /home/nekto0n/workspace/mesos/src/python/build/lib.linuxx86642.7/mesos.so: undefined symbol: zn6google14flagregistererc1epkcs2s2s2pvs3    unmangled version of symbol looks like this:    google::flagregisterer::flagregisterer(char const, char const, char const, char const, void, void)    during ./configure step glog finds gflags development files and starts using them, thus implicitly adding dependency on libgflags.so. this breaks python extensions module and perhaps can break other mesos subsystems when moved to hosts without gflags installed.    this task is done when the examplestest.pythonframework test will pass on a system with gflags installed.",3,train
MESOS-1013,ExamplesTest.JavaLog is flaky,"the examplestest.javalog test framework is flaky, possibly related to a race condition between mutexes.    [ run      ] examplestest.javalog  using temporary directory '/tmp/examplestestjavalogwbweb9'  feb 18, 2014 12:10:57 pm testlog main  info: starting a local zookeeper server  ...  f0218 12:10:58.575036 17450 coordinator.cpp:394] check failed: !missing not expecting local replica to be missing position 3 after the writing is done   check failure stack trace:   tests/script.cpp:81: failure  failed  javalogtest.sh terminated with signal 'aborted'  [  failed  ] examplestest.javalog (2166 ms)      full logs attached.",2,train
MESOS-1081,Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds.,"master should not deactivate an authenticated framework/slave upon receiving a new authenticatemessage unless new authentication succeeds. as it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an authenticatemessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to reauthenticate and reregister. this could be used in a dos attack.  but how should we handle the scenario when the actual authenticated framework/slave sends an authenticatemessage that fails authentication?",1,train
MESOS-1114,Authorize task/executor launches,nan,8,train
MESOS-1119,Allocator should make an allocation decision per slave instead of per framework/role.,"currently the allocator::allocate() code loops through roles and frameworks (based on drf sort) and allocates all slaves resources to the first framework.    this logic should be a bit inversed. instead, the slave should go through each slave, allocate it a role/framework and update the drf shares.",2,train
MESOS-1120,HTTP auth for CLI,integrate http auth into the cli programs,3,train
MESOS-1127,Implement the protobufs for the scheduler API,"the default scheduler/executor interface and implementation in mesos have a few drawbacks:    (1) the interface is fairly highlevel which makes it hard to do certain things, for example, handle events (callbacks) in batch. this can have a big impact on the performance of schedulers (for example, writing task updates that need to be persisted).    (2) the implementation requires writing a lot of boilerplate jni and native python wrappers when adding additional api components.    the plan is to provide a lowerlevel api that can easily be used to implement the higherlevel api that is currently provided. this will also open the door to more easily building nativelanguage mesos libraries (i.e., not needing the c shim layer) and building new higherlevel abstractions on top of the lowerlevel api.",8,train
MESOS-1143,Add a TASK_ERROR task status.,"during task validation we drop tasks that have errors and send task_lost status updates. in most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",2,train
MESOS-1148,Add support for rate limiting slave removal,"to safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send task_lost messages for tasks on those slaves.  ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact.",3,train
MESOS-1195,systemd.slice + cgroup enablement fails in multiple ways. ,"when attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator:     i0407 12:39:28.035354 14916 containerizer.cpp:180] using isolation: cgroups/cpu,cgroups/mem  failed to create a containerizer: could not create isolator cgroups/cpu: failed to create isolator: the cpu subsystem is comounted at /sys/fs/cgroup/cpu with other subsytems     details   /sys/fs/cgroup  total 0  drwxrxrx. 12 root root 280 mar 18 08:47 .  drwxrxrx.  6 root root   0 mar 18 08:47 ..  drwxrxrx.  2 root root   0 mar 18 08:47 blkio  lrwxrwxrwx.  1 root root  11 mar 18 08:47 cpu > cpu,cpuacct  lrwxrwxrwx.  1 root root  11 mar 18 08:47 cpuacct > cpu,cpuacct  drwxrxrx.  2 root root   0 mar 18 08:47 cpu,cpuacct  drwxrxrx.  2 root root   0 mar 18 08:47 cpuset  drwxrxrx.  2 root root   0 mar 18 08:47 devices  drwxrxrx.  2 root root   0 mar 18 08:47 freezer  drwxrxrx.  2 root root   0 mar 18 08:47 hugetlb  drwxrxrx.  3 root root   0 apr  3 11:26 memory  drwxrxrx.  2 root root   0 mar 18 08:47 netcls  drwxrxrx.  2 root root   0 mar 18 08:47 perfevent  drwxrxr x.  4 root root   0 mar 18 08:47 systemd  ",3,train
MESOS-1199,"Subprocess is ""slow"" -> gated by process::reap poll interval","subprocess uses process::reap to wait on the subprocess pid and set the exit status. however, process::reap polls with a one second interval resulting in a delay up to the interval duration before the status future is set.    this means if you need to wait for the subprocess to complete you get hit with e(delay) = 0.5 seconds, independent of the execution time. for example, the mesoscontainerizer uses mesosfetcher in a subprocess to fetch the executor during launch. at twitter we fetch a local file, i.e., a very fast operation, but the launch is blocked until the mesosfetcher pid is reaped  > adding 0 to 1 seconds for every launch!    the problem is even worse with a chain of short subprocesses because after the first subprocess completes you'll be synchronized with the reap interval and you'll see nearly the full interval before notification, i.e., 10 subprocesses each of << 1 second duration with take ~10 seconds!    this has become particularly apparent in some new tests i'm working on where test durations are now greatly extended with each taking several seconds.",1,train
MESOS-1219,Master should disallow frameworks that reconnect after failover timeout.,"when a scheduler reconnects after the failover timeout has exceeded, the framework id is usually reused because the scheduler doesn't know that the timeout exceeded and it is actually handled as a new framework.    the /framework/:framework_id route of the web ui doesn't handle those cases very well because its key is reused. it only shows the terminated one.    would it make sense to ignore the provided framework id when a scheduler reconnects to a terminated framework and generate a new id to make sure it's unique?",2,train
MESOS-1236,stout's os module uses a mix of Try<Nothing> and bool returns,stout's os module should use try/ for return values throughout.,2,train
MESOS-1237,stout's os::ls should return a Try<>,stout's os::ls returns a list that can be empty   instead it should return a try/ to be consistent.,2,train
MESOS-1303,"ExamplesTest.{TestFramework, NoExecutorFramework} flaky","i'm having trouble reproducing this but i did observe it once on my osx system:      [==========] running 2 tests from 1 test case.  [] global test environment setup.  [] 2 tests from examplestest  [ run      ] examplestest.testframework  ../../src/tests/script.cpp:81: failure  failed  testframeworktest.sh terminated with signal 'abort trap: 6'  [  failed  ] examplestest.testframework (953 ms)  [ run      ] examplestest.noexecutorframework  [       ok ] examplestest.noexecutorframework (10162 ms)  [] 2 tests from examplestest (11115 ms total)    [] global test environment teardown  [==========] 2 tests from 1 test case ran. (11121 ms total)  [  passed  ] 1 test.  [  failed  ] 1 test, listed below:  [  failed  ] examplestest.testframework      when investigating a failed make check for https:/reviews.apache.org/r/20971/    [] 6 tests from examplestest  [ run      ] examplestest.testframework  [       ok ] examplestest.testframework (8643 ms)  [ run      ] examplestest.noexecutorframework  tests/script.cpp:81: failure  failed  noexecutorframework_test.sh terminated with signal 'aborted'  [  failed  ] examplestest.noexecutorframework (7220 ms)  [ run      ] examplestest.javaframework  [       ok ] examplestest.javaframework (11181 ms)  [ run      ] examplestest.javaexception  [       ok ] examplestest.javaexception (5624 ms)  [ run      ] examplestest.javalog  [       ok ] examplestest.javalog (6472 ms)  [ run      ] examplestest.pythonframework  [       ok ] examplestest.pythonframework (14467 ms)  [] 6 tests from examplestest (53607 ms total)  ",1,train
MESOS-1307,Authorize offer allocations,when frameworks register or reregister they should authorize their roles.    split register framework / reregister framework.   ,8,train
MESOS-1316,Implement decent unit test coverage for the mesos-fetcher tool,"there are current no tests that cover the mesosfetcher tool itself, and hence bugs like mesos1313 have accidentally slipped though.",2,train
MESOS-1332,Improve Master and Slave metric names,"as we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.    it may also be worth considering changing some existing counter style metrics to gauges.  ",3,train
MESOS-1339,"Add ""per-framework-principal"" counters for all messages from a scheduler on Master",framework::principal is used identify one or more frameworks. if multiple frameworks use the same principal they'll have one counter showing their combined message count.,3,train
MESOS-1344,Add flags support for JSON,nan,2,train
MESOS-1347,GarbageCollectorIntegrationTest.DiskUsage is flaky.,from jenkins:  https:/builds.apache.org/job/mesosubuntudistcheck/79/consolefull      [ run      ] garbagecollectorintegrationtest.diskusage  using temporary directory '/tmp/garbagecollectorintegrationtestdiskusagepu3ym7'  i0507 03:27:38.775058  5758 leveldb.cpp:174] opened db in 44.343989ms  i0507 03:27:38.787498  5758 leveldb.cpp:181] compacted db in 12.411065ms  i0507 03:27:38.787533  5758 leveldb.cpp:196] created db iterator in 4008ns  i0507 03:27:38.787545  5758 leveldb.cpp:202] seeked to beginning of db in 598ns  i0507 03:27:38.787552  5758 leveldb.cpp:271] iterated through 0 keys in the db in 173ns  i0507 03:27:38.787564  5758 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0507 03:27:38.787858  5777 recover.cpp:425] starting replica recovery  i0507 03:27:38.788352  5793 master.cpp:267] master 20140507032738453759884584625758 (hemera.apache.org) started on 140.211.11.27:58462  i0507 03:27:38.788377  5793 master.cpp:304] master only allowing authenticated frameworks to register  i0507 03:27:38.788383  5793 master.cpp:309] master only allowing authenticated slaves to register  i0507 03:27:38.788389  5793 credentials.hpp:35] loading credentials for authentication  i0507 03:27:38.789064  5779 recover.cpp:451] replica is in empty status  w0507 03:27:38.789115  5793 credentials.hpp:48] failed to stat credentials file 'file:/tmp/garbagecollectorintegrationtestdiskusagepu3ym7/credentials': no such file or directory  i0507 03:27:38.789489  5779 master.cpp:104] no whitelist given. advertising offers for all slaves  i0507 03:27:38.789531  5778 hierarchicalallocatorprocess.hpp:301] initializing hierarchical allocator process with master : master@140.211.11.27:58462  i0507 03:27:38.791007  5788 replica.cpp:638] replica in empty status received a broadcasted recover request  i0507 03:27:38.791177  5780 master.cpp:921] the newly elected leader is master@140.211.11.27:58462 with id 20140507032738453759884584625758  i0507 03:27:38.791198  5780 master.cpp:931] elected as the leading master!  i0507 03:27:38.791205  5780 master.cpp:752] recovering from registrar  i0507 03:27:38.791251  5796 recover.cpp:188] received a recover response from a replica in empty status  i0507 03:27:38.791323  5797 registrar.cpp:313] recovering registrar  i0507 03:27:38.792137  5795 recover.cpp:542] updating replica status to starting  i0507 03:27:38.807531  5781 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 15.124092ms  i0507 03:27:38.807559  5781 replica.cpp:320] persisted replica status to starting  i0507 03:27:38.807621  5781 recover.cpp:451] replica is in starting status  i0507 03:27:38.809319  5799 replica.cpp:638] replica in starting status received a broadcasted recover request  i0507 03:27:38.809983  5795 recover.cpp:188] received a recover response from a replica in starting status  i0507 03:27:38.811204  5778 recover.cpp:542] updating replica status to voting  i0507 03:27:38.827595  5795 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 16.011355ms  i0507 03:27:38.827627  5795 replica.cpp:320] persisted replica status to voting  i0507 03:27:38.827683  5795 recover.cpp:556] successfully joined the paxos group  i0507 03:27:38.827775  5795 recover.cpp:440] recover process terminated  i0507 03:27:38.828966  5780 log.cpp:656] attempting to start the writer  i0507 03:27:38.831114  5782 replica.cpp:474] replica received implicit promise request with proposal 1  i0507 03:27:38.847708  5782 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 16.573137ms  i0507 03:27:38.847739  5782 replica.cpp:342] persisted promised to 1  i0507 03:27:38.848141  5797 coordinator.cpp:230] coordinator attemping to fill missing position  i0507 03:27:38.849684  5790 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0507 03:27:38.863777  5790 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 14.076775ms  i0507 03:27:38.863801  5790 replica.cpp:676] persisted action at 0  i0507 03:27:38.864915  5798 replica.cpp:508] replica received write request for position 0  i0507 03:27:38.864949  5798 leveldb.cpp:436] reading position from leveldb took 11807ns  i0507 03:27:38.879945  5798 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 14.978446ms  i0507 03:27:38.879976  5798 replica.cpp:676] persisted action at 0  i0507 03:27:38.880491  5797 replica.cpp:655] replica received learned notice for position 0  i0507 03:27:38.895969  5797 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 15.459949ms  i0507 03:27:38.895992  5797 replica.cpp:676] persisted action at 0  i0507 03:27:38.896003  5797 replica.cpp:661] replica learned nop action at position 0  i0507 03:27:38.896411  5783 log.cpp:672] writer started with ending position 0  i0507 03:27:38.898058  5798 leveldb.cpp:436] reading position from leveldb took 11910ns  i0507 03:27:38.899749  5777 registrar.cpp:346] successfully fetched the registry (0b)  i0507 03:27:38.899766  5777 registrar.cpp:422] attempting to update the 'registry'  i0507 03:27:38.901458  5791 log.cpp:680] attempting to append 137 bytes to the log  i0507 03:27:38.901666  5780 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0507 03:27:38.902773  5783 replica.cpp:508] replica received write request for position 1  i0507 03:27:38.916127  5783 leveldb.cpp:341] persisting action (156 bytes) to leveldb took 13.225715ms  i0507 03:27:38.916152  5783 replica.cpp:676] persisted action at 1  i0507 03:27:38.916534  5790 replica.cpp:655] replica received learned notice for position 1  i0507 03:27:38.928203  5790 leveldb.cpp:341] persisting action (158 bytes) to leveldb took 11.652434ms  i0507 03:27:38.928225  5790 replica.cpp:676] persisted action at 1  i0507 03:27:38.928236  5790 replica.cpp:661] replica learned append action at position 1  i0507 03:27:38.928546  5790 registrar.cpp:479] successfully updated 'registry'  i0507 03:27:38.928642  5790 registrar.cpp:372] successfully recovered registrar  i0507 03:27:38.929044  5783 master.cpp:779] recovered 0 slaves from the registry (99b) ; allowing 10mins for slaves to reregister  i0507 03:27:38.929502  5799 log.cpp:699] attempting to truncate the log to 1  i0507 03:27:38.929888  5797 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0507 03:27:38.930161  5781 replica.cpp:508] replica received write request for position 2  i0507 03:27:38.932977  5789 slave.cpp:140] slave started on 56)@140.211.11.27:58462  i0507 03:27:38.932991  5789 credentials.hpp:35] loading credentials for authentication  w0507 03:27:38.933567  5789 credentials.hpp:48] failed to stat credentials file 'file:/tmp/garbagecollectorintegrationtestdiskusagea9pxks/credential': no such file or directory  i0507 03:27:38.933585  5789 slave.cpp:230] slave using credential for: testprincipal  i0507 03:27:38.933765  5789 slave.cpp:243] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0507 03:27:38.933854  5789 slave.cpp:271] slave hostname: hemera.apache.org  i0507 03:27:38.933863  5789 slave.cpp:272] slave checkpoint: false  i0507 03:27:38.934239  5778 state.cpp:33] recovering state from '/tmp/garbagecollectorintegrationtestdiskusagea9pxks/meta'  i0507 03:27:38.934960  5792 statusupdatemanager.cpp:193] recovering status update manager  i0507 03:27:38.935123  5779 slave.cpp:2945] finished recovery  i0507 03:27:38.936998  5779 slave.cpp:526] new master detected at master@140.211.11.27:58462  i0507 03:27:38.937021  5779 slave.cpp:586] authenticating with master master@140.211.11.27:58462  i0507 03:27:38.937077  5798 statusupdatemanager.cpp:167] new master detected at master@140.211.11.27:58462  i0507 03:27:38.937306 5779 slave.cpp:559] detecting new master  i0507 03:27:38.937335  5800 authenticatee.hpp:128] creating new client sasl connection  i0507 03:27:38.938030  5778 master.cpp:2798] authenticating slave(56)@140.211.11.27:58462  i0507 03:27:38.938742 5783 authenticator.hpp:148] creating new server sasl connection  i0507 03:27:38.939312  5786 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0507 03:27:38.939340  5786 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0507 03:27:38.939390  5786 authenticator.hpp:254] received sasl authentication start  i0507 03:27:38.939553  5786 authenticator.hpp:342] authentication requires more steps  i0507 03:27:38.939592  5786 authenticatee.hpp:265] received sasl authentication step  i0507 03:27:38.939715  5786 authenticator.hpp:282] received sasl authentication step  i0507 03:27:38.939803  5786 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0507 03:27:38.939821 5786 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0507 03:27:38.939831  5786 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0507 03:27:38.939841  5786 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0507 03:27:38.939851 5786 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0507 03:27:38.939857  5786 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0507 03:27:38.939870  5786 authenticator.hpp:334] authentication success  i0507 03:27:38.939937  5786 authenticatee.hpp:305] authentication success  i0507 03:27:38.940016  5778 master.cpp:2838] successfully authenticated slave(56)@140.211.11.27:58462  i0507 03:27:38.940449 5799 slave.cpp:643] successfully authenticated with master master@140.211.11.27:58462  i0507 03:27:38.940513 5799 slave.cpp:872] will retry registration in 5.176207635secs if necessary  i0507 03:27:38.940625  5794 master.cpp:2134] registering slave at slave(56)@140.211.11.27:58462 (hemera.apache.org) with id 201405070327384537598845846257580  i0507 03:27:38.940800 5796 registrar.cpp:422] attempting to update the 'registry'  i0507 03:27:38.940850  5781 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 10.659152ms  i0507 03:27:38.940871  5781 replica.cpp:676] persisted action at 2  i0507 03:27:38.941843  5788 replica.cpp:655] replica received learned notice for position 2  i0507 03:27:38.953193  5788 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 11.291343ms  i0507 03:27:38.953258  5788 leveldb.cpp:399] deleting ~1 keys from leveldb took 33725ns  i0507 03:27:38.953274  5788 replica.cpp:676] persisted action at 2  i0507 03:27:38.953282  5788 replica.cpp:661] replica learned truncate action at position 2  i0507 03:27:38.953541  5797 log.cpp:680] attempting to append 330 bytes to the log  i0507 03:27:38.953614  5797 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0507 03:27:38.954731  5789 replica.cpp:508] replica received write request for position 3  i0507 03:27:38.965240  5789 leveldb.cpp:341] persisting action (349 bytes) to leveldb took 10.489719ms  i0507 03:27:38.965261  5789 replica.cpp:676] persisted action at 3  i0507 03:27:38.966253  5780 replica.cpp:655] replica received learned notice for position 3  i0507 03:27:38.977375  5780 leveldb.cpp:341] persisting action (351 bytes) to leveldb took 11.098798ms  i0507 03:27:38.977408  5780 replica.cpp:676] persisted action at 3  i0507 03:27:38.977421  5780 replica.cpp:661] replica learned append action at position 3  i0507 03:27:38.977859  5792 registrar.cpp:479] successfully updated 'registry'  i0507 03:27:38.977926  5780 log.cpp:699] attempting to truncate the log to 3  i0507 03:27:38.978060  5792 master.cpp:2174] registered slave 201405070327384537598845846257580 at slave(56)@140.211.11.27:58462 (hemera.apache.org)  i0507 03:27:38.978112  5792 master.cpp:3283] adding slave 201405070327384537598845846257580 at slave(56)@140.211.11.27:58462 (hemera.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0507 03:27:38.978134  5784 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0507 03:27:38.978508  5785 slave.cpp:676] registered with master master@140.211.11.27:58462; given slave id 201405070327384537598845846257580  i0507 03:27:38.978631 5786 hierarchicalallocatorprocess.hpp:444] added slave 201405070327384537598845846257580 (hemera.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0507 03:27:38.978677  5786 hierarchicalallocatorprocess.hpp:707] performed allocation for slave 201405070327384537598845846257580 in 5421ns  i0507 03:27:38.979872  5796 replica.cpp:508] replica received write request for position 4  i0507 03:27:38.982084  5758 sched.cpp:121] version: 0.19.0  i0507 03:27:38.982213  5789 sched.cpp:217] new master detected at master@140.211.11.27:58462  i0507 03:27:38.982228  5789 sched.cpp:268] authenticating with master master@140.211.11.27:58462  i0507 03:27:38.982347  5788 authenticatee.hpp:128] creating new client sasl connection  i0507 03:27:38.982676  5788 master.cpp:2798] authenticating scheduler(59)@140.211.11.27:58462  i0507 03:27:38.983100  5788 authenticator.hpp:148] creating new server sasl connection  i0507 03:27:38.983294  5788 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0507 03:27:38.983312  5788 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0507 03:27:38.983360  5788 authenticator.hpp:254] received sasl authentication start  i0507 03:27:38.983505  5788 authenticator.hpp:342] authentication requires more steps  i0507 03:27:38.984220  5782 authenticatee.hpp:265] received sasl authentication step  i0507 03:27:38.984275  5782 authenticator.hpp:282] received sasl authentication step  i0507 03:27:38.984315  5782 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0507 03:27:38.984347 5782 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0507 03:27:38.984359  5782 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0507 03:27:38.984370  5782 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0507 03:27:38.984377 5782 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0507 03:27:38.984383  5782 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0507 03:27:38.984397  5782 authenticator.hpp:334] authentication success  i0507 03:27:38.984429  5782 authenticatee.hpp:305] authentication success  i0507 03:27:38.984469  5795 master.cpp:2838] successfully authenticated scheduler(59)@140.211.11.27:58462  i0507 03:27:38.985110  5782 sched.cpp:342] successfully authenticated with master master@140.211.11.27:58462  i0507 03:27:38.985133  5782 sched.cpp:461] sending registration request to master@140.211.11.27:58462  i0507 03:27:38.985326 5795 master.cpp:980] received registration request from scheduler(59)@140.211.11.27:58462  i0507 03:27:38.985357  5795 master.cpp:998] registering framework 201405070327384537598845846257580000 at scheduler(59)@140.211.11.27:58462  i0507 03:27:38.985424  5795 sched.cpp:392] framework registered with 201405070327384537598845846257580000  i0507 03:27:38.985471  5792 hierarchicalallocatorprocess.hpp:331] added framework 201405070327384537598845846257580000  i0507 03:27:38.985610  5795 sched.cpp:406] scheduler::registered took 36702ns  i0507 03:27:38.985646  5792 hierarchicalallocatorprocess.hpp:751] offering cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201405070327384537598845846257580 to framework 201405070327384537598845846257580000  i0507 03:27:38.985954  5792 hierarchicalallocatorprocess.hpp:687] performed allocation for 1 slaves in 330895ns  i0507 03:27:38.986001  5789 master.hpp:612] adding offer 201405070327384537598845846257580 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201405070327384537598845846257580 (hemera.apache.org)  i0507 03:27:38.986090  5789 master.cpp:2747] sending 1 offers to framework 201405070327384537598845846257580000  i0507 03:27:38.986548  5792 sched.cpp:529] scheduler::resourceoffers took 162873ns  i0507 03:27:38.986721  5792 master.hpp:622] removing offer 201405070327384537598845846257580 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201405070327384537598845846257580 (hemera.apache.org)  i0507 03:27:38.986781  5792 master.cpp:1812] processing reply for offers: [ 201405070327384537598845846257580 ] on slave 201405070327384537598845846257580 at slave(56)@140.211.11.27:58462 (hemera.apache.org) for framework 201405070327384537598845846257580000  i0507 03:27:38.986843  5792 master.hpp:584] adding task 0 with resources cpus():2; mem():1024 on slave 201405070327384537598845846257580 (hemera.apache.org)  i0507 03:27:38.986876  5792 master.cpp:2922] launching task 0 of framework 201405070327384537598845846257580000 with resources cpus():2; mem():1024 on slave 201405070327384537598845846257580 at slave(56)@140.211.11.27:58462 (hemera.apache.org)  i0507 03:27:38.986981  5795 slave.cpp:906] got assigned task 0 for framework 201405070327384537598845846257580000  i0507 03:27:38.987180  5795 slave.cpp:1016] launching task 0 for framework 201405070327384537598845846257580000  i0507 03:27:38.987203  5787 hierarchicalallocatorprocess.hpp:546] framework 201405070327384537598845846257580000 left disk():1024; ports():[3100032000] unused on slave 201405070327384537598845846257580  i0507 03:27:38.987287  5787 hierarchicalallocatorprocess.hpp:589] framework 201405070327384537598845846257580000 filtered slave 201405070327384537598845846257580 for 5secs  i0507 03:27:38.991395  5795 exec.cpp:131] version: 0.19.0  i0507 03:27:38.991497  5779 exec.cpp:181] executor started at: executor(27)@140.211.11.27:58462 with pid 5758  i0507 03:27:38.991510  5795 slave.cpp:1126] queuing task '0' for executor default of framework '201405070327384537598845846257580000  i0507 03:27:38.991566  5795 slave.cpp:487] successfully attached file '/tmp/garbagecollectorintegrationtestdiskusagea9pxks/slaves/201405070327384537598845846257580/frameworks/201405070327384537598845846257580000/executors/default/runs/de776bec28224bbcbefceec40eb5f674'  i0507 03:27:38.991595  5795 slave.cpp:2283] monitoring executor 'default' of framework '201405070327384537598845846257580000' in container 'de776bec28224bbcbefceec40eb5f674'  i0507 03:27:38.991778  5795 slave.cpp:1599] got registration for executor 'default' of framework 201405070327384537598845846257580000  i0507 03:27:38.991874  5795 slave.cpp:1718] flushing queued task 0 for executor 'default' of framework 201405070327384537598845846257580000  i0507 03:27:38.991935  5780 exec.cpp:205] executor registered on slave 201405070327384537598845846257580  i0507 03:27:38.993419  5796 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 13.489998ms  i0507 03:27:38.993449  5796 replica.cpp:676] persisted action at 4  i0507 03:27:38.994510  5777 replica.cpp:655] replica received learned notice for position 4  i0507 03:27:...,2,train
MESOS-1358,Show when the leading master was elected in the webui,this would be nice to have during debugging.,1,train
MESOS-1365,SlaveRecoveryTest/0.MultipleFrameworks is flaky,"gtestrepeat=1 gtestshuffle gtestbreakonfailure      [ run      ] slaverecoverytest/0.multipleframeworks  warning: logging before initgooglelogging() is written to stderr  i0513 15:42:05.931761  4320 exec.cpp:131] version: 0.19.0  i0513 15:42:05.936698  4340 exec.cpp:205] executor registered on slave 201405131542041684287951872130620  registered executor on artoo  starting task 51991f97f5fd4905ad0f02668083af7c  forked command at 4367  sh c 'sleep 1000'  warning: logging before initgooglelogging() is written to stderr  i0513 15:42:06.915061  4408 exec.cpp:131] version: 0.19.0  i0513 15:42:06.931149  4435 exec.cpp:205] executor registered on slave 201405131542041684287951872130620  registered executor on artoo  starting task eaf5d8d63a6c4ee184c1fae20fb1df83  sh c 'sleep 1000'  forked command at 4439  i0513 15:42:06.998332  4340 exec.cpp:251] received reconnect request from slave 201405131542041684287951872130620  i0513 15:42:06.998414  4436 exec.cpp:251] received reconnect request from slave 201405131542041684287951872130620  i0513 15:42:07.006350  4437 exec.cpp:228] executor reregistered on slave 201405131542041684287951872130620  reregistered executor on artoo  i0513 15:42:07.027039  4337 exec.cpp:378] executor asked to shutdown  shutting down  sending sigterm to process tree at pid 4367  killing the following process trees:  [    4367 sh c sleep 1000    \ 4368 sleep 1000   ]  ../../src/tests/slaverecoverytests.cpp:2807: failure  value of: status1.get().state()    actual: taskfailed  expected: taskkilled    program received signal sigsegv, segmentation fault.  testing::unittest::addtestpartresult (this=0x154dac0 /, resulttype=testing::testpartresult::kfatalfailure, filename=0xeb6b6c ""../../src/tests/slaverecoverytests.cpp"", linenumber=2807, message=..., osstacktrace=...) at gmock1.6.0/gtest/src/gtest.cc:3795  3795          staticcast/(null) = 1;  (gdb) bt  #0  testing::unittest::addtestpartresult (this=0x154dac0 /, resulttype=testing::testpartresult::kfatalfailure, filename=0xeb6b6c ""../../src/tests/slaverecoverytests.cpp"", linenumber=2807, message=..., osstacktrace=...) at gmock1.6.0/gtest/src/gtest.cc:3795  #1  0x0000000000df98b9 in testing::internal::asserthelper::operator= (this=0x7fffffffb860, message=...) at gmock1.6.0/gtest/src/gtest.cc:356  #2  0x0000000000cdfa57 in slaverecoverytestmultipleframeworkstest/::testbody (this=0x1954db0) at ../../src/tests/slaverecoverytests.cpp:2807  #3  0x0000000000e22583 in testing::internal::handlesehexceptionsinmethodifsupported/ (object=0x1954db0, method=&virtual testing::test::testbody(), location=0xed0af0 ""the test body"") at gmock1.6.0/gtest/src/gtest.cc:2090  #4  0x0000000000e12467 in testing::internal::handleexceptionsinmethodifsupported/ (object=0x1954db0, method=&virtual testing::test::testbody(), location=0xed0af0 ""the test body"") at gmock1.6.0/gtest/src/gtest.cc:2126  #5  0x0000000000e010d5 in testing::test::run (this=0x1954db0) at gmock1.6.0/gtest/src/gtest.cc:2161  #6  0x0000000000e01ceb in testing::testinfo::run (this=0x158cf80) at gmock1.6.0/gtest/src/gtest.cc:2338  #7  0x0000000000e02387 in testing::testcase::run (this=0x158a880) at gmock1.6.0/gtest/src/gtest.cc:2445  #8  0x0000000000e079ed in testing::internal::unittestimpl::runalltests (this=0x1558b40) at gmock1.6.0/gtest/src/gtest.cc:4237  #9  0x0000000000e1ec83 in testing::internal::handlesehexceptionsinmethodifsupported/ (object=0x1558b40, method=(bool (testing::internal::unittestimpl::)(testing::internal::unittestimpl  const)) 0xe07700 /,       location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock1.6.0/gtest/src/gtest.cc:2090  #10 0x0000000000e14217 in testing::internal::handleexceptionsinmethodifsupported/ (object=0x1558b40, method=(bool (testing::internal::unittestimpl::)(testing::internal::unittestimpl  const)) 0xe07700 /,       location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock1.6.0/gtest/src/gtest.cc:2126  #11 0x0000000000e076d7 in testing::unittest::run (this=0x154dac0 /) at gmock1.6.0/gtest/src/gtest.cc:3872  #12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107  (gdb) frame 2  #2  0x0000000000cdfa57 in slaverecoverytestmultipleframeworkstest/::testbody (this=0x1954db0) at ../../src/tests/slaverecoverytests.cpp:2807  2807      asserteq(taskkilled, status1.get().state());  (gdb) p status1  $1 = }, /}}  (gdb) p status1.get()  $2 = (const mesos::taskstatus &) @0x7fffdc5bf5f0: , /}, static ktaskidfieldnumber = 1, static kstatefieldnumber = 2, static kmessagefieldnumber = 4,     static kdatafieldnumber = 3, static kslaveidfieldnumber = 5, static ktimestampfieldnumber = 6, unknownfields = , taskid = 0x7fffdc5ce9a0, message = 0x7fffdc5f5880, data = 0x154b4b0 /, slaveid = 0x7fffdc59c4f0, timestamp = 1429688582.046252,     state = 3, cachedsize = 0, hasbits = , static defaultinstance = 0x0}  (gdb) p status1.get().state()  $3 = mesos::taskfailed  (gdb) list  2802      / kill task 1.  2803      driver1.killtask(task1.taskid());  2804  2805      / wait for taskkilled update.  2806      awaitready(status1);  2807      asserteq(taskkilled, status1.get().state());  2808  2809      / kill task 2.  2810      driver2.killtask(task2.task_id());  2811  ",1,train
MESOS-1371,Expose libprocess queue length from scheduler driver to metrics endpoint,we expose the master's event queue length and we should do the same for the scheduler driver.,1,train
MESOS-1373,Keep track of the principals for authenticated pids in Master.,need to add a 'principal' field to frameworkinfo and verify if the framework has the claimed principal during registration.,3,train
MESOS-1374,Verify static libprocess scheduler port works with Mesos Master,nan,5,train
MESOS-1392,Failure when znode is removed before we can read its contents.,looks like the following can occur when a znode goes away right before we can read it's contents:      i0520 16:33:45.721727 29155 group.cpp:382] trying to create path '/home/mesos/test/master' in zookeeper  i0520 16:33:48.600837 29155 detector.cpp:134] detected a new leader: (id='2617')  i0520 16:33:48.601428 29147 group.cpp:655] trying to get '/home/mesos/test/master/info0000002617' in zookeeper  failed to detect a master: failed to get data for ephemeral node '/home/mesos/test/master/info0000002617' in zookeeper: no node  slave exit status: 1  ,3,train
MESOS-1393,Write parser for perf output.,1. should support output from pid and cgroup targets.  2. should support output for the same events from >= 1 cgroup  3. should return as perfstatistics protobuf.  ,3,train
MESOS-1394,Test different versions of perf,test across different kernel versions (at least 2.6.xx and 3.x) and across different distributions.    test input flags and parsing output.,3,train
MESOS-1395,Test perf isolator for slave roll forward/roll back,"test that changes to add/remove perf isolator will be handled through slave recovery, e.g., containers started without the perf isolator continue to report resource statistics and containers started with the perf isolator will include perf statistics.",2,train
MESOS-1396,Introduce a PerfStatistics protobuf,"field names from `perf list` normalized to convert hyphens to underscores and down cased. start with just the hardware and software events, not raw hardware, breakpoints or tracepoints,     all fields should be optional. include as an optional field to resourcestatistics.",2,train
MESOS-1397,Rename ResourceStatistics for containers,rename containerstatistics which includes optional resourcestatistics and optional perfstatistics.,8,train
MESOS-1398,Document perf isolator flags,"document interval, duration and the event flags. document event name normalization for the protobuf.",1,train
MESOS-1410,Keep terminal unacknowledged tasks in the master's state.,"once we are sending acknowledgments through the master as per mesos1409, we need to keep terminal tasks that are unacknowledged in the master's memory.    this will allow us to identify these tasks to frameworks when we haven't yet forwarded them an update. without this, we're susceptible to mesos1389.",5,train
MESOS-1424,Mesos tests should not rely on echo,triggered by mesos1413 i would like to propose changing our tests to not rely on echo but to use printf instead.    this seems to be useful as echo is introducing an extra linefeed after the supplied string whereas printf does not. the n switch preventing that extra linefeed is unfortunately not portable   it is not supported by the builtin echo of the bsd / osx /bin/sh.  ,1,train
MESOS-1425,LogZooKeeperTest.WriteRead test is flaky,"  [ run      ] logzookeepertest.writeread  i0527 23:23:48.286031  1352 zookeepertestserver.cpp:158] started zookeepertestserver on port 39446  i0527 23:23:48.293916  1352 logtests.cpp:1945] using temporary directory '/tmp/logzookeepertestwritereadvyty8g'  i0527 23:23:48.296430  1352 leveldb.cpp:176] opened db in 2.459713ms  i0527 23:23:48.296740  1352 leveldb.cpp:183] compacted db in 286843ns  i0527 23:23:48.296761  1352 leveldb.cpp:198] created db iterator in 3083ns  i0527 23:23:48.296772  1352 leveldb.cpp:204] seeked to beginning of db in 4541ns  i0527 23:23:48.296777  1352 leveldb.cpp:273] iterated through 0 keys in the db in 87ns  i0527 23:23:48.296788  1352 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0527 23:23:48.297499  1383 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 505340ns  i0527 23:23:48.297513  1383 replica.cpp:320] persisted replica status to voting  i0527 23:23:48.299492  1352 leveldb.cpp:176] opened db in 1.73582ms  i0527 23:23:48.299773  1352 leveldb.cpp:183] compacted db in 263937ns  i0527 23:23:48.299793  1352 leveldb.cpp:198] created db iterator in 7494ns  i0527 23:23:48.299806  1352 leveldb.cpp:204] seeked to beginning of db in 235ns  i0527 23:23:48.299813  1352 leveldb.cpp:273] iterated through 0 keys in the db in 93ns  i0527 23:23:48.299821  1352 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0527 23:23:48.300503  1380 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 492309ns  i0527 23:23:48.300516  1380 replica.cpp:320] persisted replica status to voting  i0527 23:23:48.302500  1352 leveldb.cpp:176] opened db in 1.793829ms  i0527 23:23:48.303642  1352 leveldb.cpp:183] compacted db in 1.123929ms  i0527 23:23:48.303669  1352 leveldb.cpp:198] created db iterator in 5865ns  i0527 23:23:48.303689  1352 leveldb.cpp:204] seeked to beginning of db in 8811ns  i0527 23:23:48.303705  1352 leveldb.cpp:273] iterated through 1 keys in the db in 9545ns  i0527 23:23:48.303715  1352 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  20140527 23:23:48,303:1352(0x2b1173a29700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140527 23:23:48,303:1352(0x2b1173a29700):zooinfo@logenv@716: client environment:host.name=minerva  20140527 23:23:48,303:1352(0x2b1173a29700):zooinfo@logenv@723: client environment:os.name=linux  20140527 23:23:48,303:1352(0x2b1173a29700):zooinfo@logenv@724: client environment:os.arch=3.2.057generic  20140527 23:23:48,303:1352(0x2b1173a29700):zooinfo@logenv@725: client environment:os.version=#87ubuntu smp tue nov 12 21:35:10 utc 2013  20140527 23:23:48,303:1352(0x2b1173e2b700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@716: client environment:host.name=minerva  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@723: client environment:os.name=linux  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@724: client environment:os.arch=3.2.057generic  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@725: client environment:os.version=#87ubuntu smp tue nov 12 21:35:10 utc 2013  20140527 23:23:48,304:1352(0x2b1173a29700):zooinfo@logenv@733: client environment:user.name=(null)  i0527 23:23:48.303988  1380 log.cpp:238] attempting to join replica to zookeeper group  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@733: client environment:user.name=(null)  20140527 23:23:48,304:1352(0x2b1173a29700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  i0527 23:23:48.304198  1385 recover.cpp:425] starting replica recovery  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140527 23:23:48,304:1352(0x2b1173a29700):zooinfo@logenv@753: client environment:user.dir=/tmp/logzookeepertestwritereadvyty8g  20140527 23:23:48,304:1352(0x2b1173a29700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:39446 sessiontimeout=5000 watcher=0x2b11708e98d0 sessionid=0 sessionpasswd=/ context=0x2b118002f4e0 flags=0  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@logenv@753: client environment:user.dir=/tmp/logzookeepertestwritereadvyty8g  i0527 23:23:48.304352  1385 recover.cpp:451] replica is in voting status  20140527 23:23:48,304:1352(0x2b1173e2b700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:39446 sessiontimeout=5000 watcher=0x2b11708e98d0 sessionid=0 sessionpasswd=/ context=0x2b1198015ca0 flags=0  i0527 23:23:48.304417  1385 recover.cpp:440] recover process terminated  20140527 23:23:48,304:1352(0x2b12897b8700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:39446]  20140527 23:23:48,304:1352(0x2b12891b5700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:39446]  i0527 23:23:48.311262  1352 leveldb.cpp:176] opened db in 7.261703ms  20140527 23:23:48,311:1352(0x2b12897b8700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:39446], sessionid=0x1463fff34bd0000, negotiated timeout=6000  i0527 23:23:48.312379  1381 group.cpp:310] group process ((614)@67.195.138.8:35151) connected to zookeeper  i0527 23:23:48.312407  1381 group.cpp:784] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0527 23:23:48.312417  1381 group.cpp:382] trying to create path '/log' in zookeeper  i0527 23:23:48.312422  1352 leveldb.cpp:183] compacted db in 1.119843ms  i0527 23:23:48.312505  1352 leveldb.cpp:198] created db iterator in 3901ns  i0527 23:23:48.312526  1352 leveldb.cpp:204] seeked to beginning of db in 7398ns  i0527 23:23:48.312541  1352 leveldb.cpp:273] iterated through 1 keys in the db in 6345ns  i0527 23:23:48.312553  1352 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@716: client environment:host.name=minerva  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@723: client environment:os.name=linux  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@724: client environment:os.arch=3.2.057generic  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@725: client environment:os.version=#87ubuntu smp tue nov 12 21:35:10 utc 2013  20140527 23:23:48,312:1352(0x2b1173627700):zooinfo@logenv@733: client environment:user.name=(null)  20140527 23:23:48,312:1352(0x2b12891b5700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:39446], sessionid=0x1463fff34bd0001, negotiated timeout=6000  20140527 23:23:48,313:1352(0x2b1173627700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140527 23:23:48,313:1352(0x2b1173627700):zooinfo@logenv@753: client environment:user.dir=/tmp/logzookeepertestwritereadvyty8g  20140527 23:23:48,313:1352(0x2b1173627700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:39446 sessiontimeout=5000 watcher=0x2b11708e98d0 sessionid=0 sessionpasswd=/ context=0x2b119001fd20 flags=0  i0527 23:23:48.313247  1380 group.cpp:310] group process ((616)@67.195.138.8:35151) connected to zookeeper  i0527 23:23:48.313266  1380 group.cpp:784] syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)  i0527 23:23:48.313273  1380 group.cpp:382] trying to create path '/log' in zookeeper  20140527 23:23:48,313:1352(0x2b12889b0700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:39446]  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@716: client environment:host.name=minerva  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@723: client environment:os.name=linux  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@724: client environment:os.arch=3.2.057generic  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@725: client environment:os.version=#87ubuntu smp tue nov 12 21:35:10 utc 2013  i0527 23:23:48.313436  1387 log.cpp:238] attempting to join replica to zookeeper group  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@733: client environment:user.name=(null)  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@logenv@753: client environment:user.dir=/tmp/logzookeepertestwritereadvyty8g  20140527 23:23:48,313:1352(0x2b1173828700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:39446 sessiontimeout=5000 watcher=0x2b11708e98d0 sessionid=0 sessionpasswd=/ context=0x2b1190011ea0 flags=0  i0527 23:23:48.313601  1387 recover.cpp:425] starting replica recovery  i0527 23:23:48.313721  1382 recover.cpp:451] replica is in voting status  i0527 23:23:48.313794  1382 recover.cpp:440] recover process terminated  20140527 23:23:48,313:1352(0x2b1288bb1700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:39446]  i0527 23:23:48.313973  1383 log.cpp:656] attempting to start the writer  20140527 23:23:48,315:1352(0x2b12889b0700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:39446], sessionid=0x1463fff34bd0002, negotiated timeout=6000  i0527 23:23:48.315682  1387 group.cpp:310] group process ((619)@67.195.138.8:35151) connected to zookeeper  20140527 23:23:48,315:1352(0x2b1288bb1700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:39446], sessionid=0x1463fff34bd0003, negotiated timeout=6000  i0527 23:23:48.315709  1387 group.cpp:784] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0527 23:23:48.315738  1387 group.cpp:382] trying to create path '/log' in zookeeper  i0527 23:23:48.315964  1386 group.cpp:310] group process ((621)@67.195.138.8:35151) connected to zookeeper  i0527 23:23:48.315981  1386 group.cpp:784] syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)  i0527 23:23:48.315989  1386 group.cpp:382] trying to create path '/log' in zookeeper  i0527 23:23:48.317881  1385 network.hpp:423] zookeeper group memberships changed  i0527 23:23:48.317937  1381 group.cpp:655] trying to get '/log/0000000000' in zookeeper  i0527 23:23:48.318205  1382 network.hpp:423] zookeeper group memberships changed  i0527 23:23:48.318317  1383 group.cpp:655] trying to get '/log/0000000000' in zookeeper  i0527 23:23:48.319154  1382 network.hpp:461] zookeeper group pids:   i0527 23:23:48.319541  1386 network.hpp:461] zookeeper group pids:   i0527 23:23:48.319851  1381 replica.cpp:474] replica received implicit promise request with proposal 1  i0527 23:23:48.319905  1387 replica.cpp:474] replica received implicit promise request with proposal 1  i0527 23:23:48.319907  1384 network.hpp:423] zookeeper group memberships changed  i0527 23:23:48.320091  1385 group.cpp:655] trying to get '/log/0000000000' in zookeeper  i0527 23:23:48.320384  1383 network.hpp:423] zookeeper group memberships changed  i0527 23:23:48.320441  1381 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 568396ns  i0527 23:23:48.320456  1384 group.cpp:655] trying to get '/log/0000000000' in zookeeper  i0527 23:23:48.320461  1381 replica.cpp:342] persisted promised to 1  i0527 23:23:48.320446  1387 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 516015ns  i0527 23:23:48.320497  1387 replica.cpp:342] persisted promised to 1  i0527 23:23:48.320814  1383 coordinator.cpp:230] coordinator attemping to fill missing position  i0527 23:23:48.321050  1384 group.cpp:655] trying to get '/log/0000000001' in zookeeper  i0527 23:23:48.321063  1385 group.cpp:655] trying to get '/log/0000000001' in zookeeper  i0527 23:23:48.321341  1387 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0527 23:23:48.321375  1381 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0527 23:23:48.321506  1387 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 89us  i0527 23:23:48.321530  1387 replica.cpp:676] persisted action at 0  i0527 23:23:48.321584  1381 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 122910ns  i0527 23:23:48.321602  1381 replica.cpp:676] persisted action at 0  i0527 23:23:48.321775  1383 network.hpp:461] zookeeper group pids:   i0527 23:23:48.321961  1381 replica.cpp:508] replica received write request for position 0  i0527 23:23:48.321984  1381 leveldb.cpp:438] reading position from leveldb took 7813ns  i0527 23:23:48.322064  1380 network.hpp:461] zookeeper group pids:   i0527 23:23:48.322073  1381 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 78683ns  i0527 23:23:48.322077  1383 replica.cpp:508] replica received write request for position 0  i0527 23:23:48.322084  1381 replica.cpp:676] persisted action at 0  i0527 23:23:48.322111  1383 leveldb.cpp:438] reading position from leveldb took 17416ns  i0527 23:23:48.322330  1383 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 157199ns  i0527 23:23:48.322345  1383 replica.cpp:676] persisted action at 0  i0527 23:23:48.322522  1386 replica.cpp:655] replica received learned notice for position 0  i0527 23:23:48.322523  1382 replica.cpp:655] replica received learned notice for position 0  i0527 23:23:48.322638  1386 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 86907ns  i0527 23:23:48.322661  1386 replica.cpp:676] persisted action at 0  i0527 23:23:48.322670  1386 replica.cpp:661] replica learned nop action at position 0  i0527 23:23:48.322682  1382 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 85031ns  i0527 23:23:48.322693  1382 replica.cpp:676] persisted action at 0  i0527 23:23:48.322700  1382 replica.cpp:661] replica learned nop action at position 0  i0527 23:23:48.322790  1380 log.cpp:672] writer started with ending position 0  i0527 23:23:48.322898  1380 log.cpp:680] attempting to append 11 bytes to the log  i0527 23:23:48.322978  1383 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0527 23:23:48.323122  1380 replica.cpp:508] replica received write request for position 1  i0527 23:23:48.323158  1381 replica.cpp:508] replica received write request for position 1  i0527 23:23:48.323202  1380 leveldb.cpp:343] persisting action (27 bytes) to leveldb took 66527ns  i0527 23:23:48.323215  1380 replica.cpp:676] persisted action at 1  i0527 23:23:48.323238  1381 leveldb.cpp:343] persisting action (27 bytes) to leveldb took 67074ns  i0527 23:23:48.323252  1381 replica.cpp:676] persisted action at 1  i0527 23:23:48.323354  1380 replica.cpp:655] replica received learned notice for position 1  i0527 23:23:48.323362  1382 replica.cpp:655] replica received learned notice for position 1  i0527 23:23:48.323443  1380 leveldb.cpp:343] persisting action (29 bytes) to leveldb took 77398ns  i0527 23:23:48.323461  1380 replica.cpp:676] persisted action at 1  i0527 23:23:48.323463  1382 leveldb.cpp:343] persisting action (29 bytes) to leveldb took 90567ns  i0527 23:23:48.323467  1380 replica.cpp:661] replica learned append action at position 1  i0527 23:23:48.323477  1382 replica.cpp:676] persisted action at 1  i0527 23:23:48.323484  1382 replica.cpp:661] replica learned append action at position 1  i0527 23:23:48.323729  1380 leveldb.cpp:438] reading position from leveldb took 7224ns  20140527 23:23:48,324:1352(0x2b1173c2a700):zooinfo@zookeeperclose@2505: closing zookeeper sessionid=0x1463fff34bd0003 to [127.0.0.1:39446]    20140527 23:23:48,324:1352(0x2b117301ff80):zooinfo@zookeeperclose@2505: closing zookeeper sessionid=0x1463fff34bd0002 to [127.0.0.1:39446]    i0527 23:23:48.326591  1386 network.hpp:423] zookeeper group memberships changed  i0527 23:23:48.326690  1382 group.cpp:655] trying to get '/log/0000000000' in zookeeper  i0527 23:23:48.327450  1384 network.hpp:461] zookeeper group pids:   20140527 23:23:48,446:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:23:51,782:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:23:55,118:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  i0527 23:23:57.002908  1381 network.hpp:423] zookeeper group memberships changed  i0527 23:23:57.003042  1381 network.hpp:461] zookeeper group pids:   20140527 23:23:58,455:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:01,791:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:05,127:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:08,464:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:11,800:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:15,136:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:18,473:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:21,809:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:25,146:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:28,482:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:31,818:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:35,155:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140527 23:24:38,491:1352(0x2b12bc401700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:51020] zk retcode=4, errno=111(connection refused): server refused to accept the client  201405 27 23:24:41,827:1352(0x2b12bc401700):zooerror@handlesocketerror_msg@1697: socket [127.0.0.1:51020] zk retc...",1,train
MESOS-1443,Create a protobuf for framework rate limit configuration and load it as JSON through master flags,nan,2,train
MESOS-1444,Integrate rate limiter into the master,nan,5,train
MESOS-1445,Add new tests for framework rate limiting,nan,3,train
MESOS-1459,Build failure: Ubuntu 13.10/clang due to missing virtual destructor,"in file included from launcher/main.cpp:19:  in file included from ./launcher/launcher.hpp:24:  in file included from ../3rdparty/libprocess/include/process/future.hpp:23:  ../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::operation' that is abstract but has nonvirtual destructor [werror,wdeletenonvirtualdtor]      delete t;        /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/bits/sharedptrbase.h:456:8: note: in instantiation of member function 'process::owned/::data::~data' requested here                delete p;                  /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/bits/sharedptrbase.h:768:24: note: in instantiation of function template specialization 'std::sharedcount/::sharedcount/::data >' requested here          : mptr(p), mrefcount(p)                           /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/bits/sharedptrbase.h:919:4: note: in instantiation of function template specialization 'std::sharedptr/::data, 2>::sharedptr/::data>' requested here            sharedptr(p).swap(this);              ../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::shared_ptr/::data, 2>::reset/::data>' requested here      data.reset(new data(t));             ./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::owned/::owned' requested here    add(process::owned/(new t()));          launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add/' requested here    launcher::add/();    ^  1 error generated.",1,train
MESOS-1466,Race between executor exited event and launch task can cause overcommit of resources,the following sequence of events can cause an overcommit    > launch task is called for a task whose executor is already running    > executor's resources are not accounted for on the master    > executor exits and the event is enqueued behind launch tasks on the master    > master sends the task to the slave which needs to commit for resources for task and the (new) executor.    > master processes the executor exited event and re offers the executor's resources causing an overcommit of resources.,8,train
MESOS-1469,No output from review bot on timeout,"when the mesos review build times out, likely due to a long running failing test, we have no output to debug. we should find a way to stream the output from the build instead of waiting for the build to finish.",1,train
MESOS-1471,Document replicated log design/internals,"the replicated log could benefit from some documentation. in particular, how does it work? what do operators need to know? possibly there is some overlap with our future maintenance documentation in mesos 1470.    i believe  has some unpublished work that could be leveraged here!",5,train
MESOS-1472,Improve child exit if slave dies during executor launch in MC,when restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the mesoscontainerizer.    the forked child correctly detects this however rather than abort it should safely log and then exit non zero cleanly.,1,train
MESOS-1518,Update Rate Limiting Design doc to reflect the latest changes, usage   design    implementation notes,2,train
MESOS-1527,Choose containerizer at runtime,"currently you have to choose the containerizer at mesosslave start time via the  isolation option.  i'd like to be able to specify the containerizer in the request to launch the job. this could be specified by a new ""provider"" field in the containerinfo proto buf.",3,train
MESOS-1529,Handle a network partition between Master and Slave,"if a network partition occurs between a master and slave, the master will remove the slave (as it fails health check) and mark the tasks being run there as lost. however, the slave is not aware that it has been removed so the tasks will continue to run.    (to clarify a little bit: neither the master nor the slave receives 'exited' event, indicating that the connection between the master and slave is not closed).    there are at least two possible approaches to solving this issue:    1. introduce a health check from slave to master so they have a consistent view of a network partition. we may still see this issue should a one way connection error occur.    2. be less aggressive about marking tasks and slaves as lost. wait until the slave reappears and reconcile then. we'd still need to mark slaves and tasks as potentially lost (zombie state) but maybe the scheduler can make a more intelligent decision.",5,train
MESOS-1545,SlaveRecoveryTest/0.MultipleFrameworks is flaky,"  [ run      ] slaverecoverytest/0.multipleframeworks  using temporary directory '/tmp/slaverecoverytest0multipleframeworks6djqxr'  i0626 00:04:39.557339  5450 leveldb.cpp:176] opened db in 179.857593ms  i0626 00:04:39.565433  5450 leveldb.cpp:183] compacted db in 8.071041ms  i0626 00:04:39.565457  5450 leveldb.cpp:198] created db iterator in 4065ns  i0626 00:04:39.565466  5450 leveldb.cpp:204] seeked to beginning of db in 596ns  i0626 00:04:39.565474  5450 leveldb.cpp:273] iterated through 0 keys in the db in 396ns  i0626 00:04:39.565490  5450 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0626 00:04:39.565827  5476 recover.cpp:425] starting replica recovery  i0626 00:04:39.566033  5474 recover.cpp:451] replica is in empty status  i0626 00:04:39.566504  5474 replica.cpp:638] replica in empty status received a broadcasted recover request  i0626 00:04:39.566686  5477 recover.cpp:188] received a recover response from a replica in empty status  i0626 00:04:39.566905  5472 recover.cpp:542] updating replica status to starting  i0626 00:04:39.568307  5471 master.cpp:288] master 201406260004391032504131554235450 (juno.apache.org) started on 67.195.138.61:55423  i0626 00:04:39.568332  5471 master.cpp:325] master only allowing authenticated frameworks to register  i0626 00:04:39.568339  5471 master.cpp:330] master only allowing authenticated slaves to register  i0626 00:04:39.568348  5471 credentials.hpp:35] loading credentials for authentication from '/tmp/slaverecoverytest0multipleframeworks6djqxr/credentials'  i0626 00:04:39.568461  5471 master.cpp:356] authorization enabled  i0626 00:04:39.568739  5478 master.cpp:122] no whitelist given. advertising offers for all slaves  i0626 00:04:39.568814  5475 hierarchicalallocatorprocess.hpp:301] initializing hierarchical allocator process with master : master@67.195.138.61:55423  i0626 00:04:39.569206  5478 master.cpp:1122] the newly elected leader is master@67.195.138.61:55423 with id 201406260004391032504131554235450  i0626 00:04:39.569223  5478 master.cpp:1135] elected as the leading master!  i0626 00:04:39.569231  5478 master.cpp:953] recovering from registrar  i0626 00:04:39.569286  5475 registrar.cpp:313] recovering registrar  i0626 00:04:39.600639  5477 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 33.682136ms  i0626 00:04:39.600661  5477 replica.cpp:320] persisted replica status to starting  i0626 00:04:39.600790  5476 recover.cpp:451] replica is in starting status  i0626 00:04:39.601184  5474 replica.cpp:638] replica in starting status received a broadcasted recover request  i0626 00:04:39.601274  5477 recover.cpp:188] received a recover response from a replica in starting status  i0626 00:04:39.601465  5471 recover.cpp:542] updating replica status to voting  i0626 00:04:39.610605  5471 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 9.076262ms  i0626 00:04:39.610638  5471 replica.cpp:320] persisted replica status to voting  i0626 00:04:39.610683  5471 recover.cpp:556] successfully joined the paxos group  i0626 00:04:39.610780  5471 recover.cpp:440] recover process terminated  i0626 00:04:39.610946  5474 log.cpp:656] attempting to start the writer  i0626 00:04:39.611486  5475 replica.cpp:474] replica received implicit promise request with proposal 1  i0626 00:04:39.618924  5475 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 7.418789ms  i0626 00:04:39.618942  5475 replica.cpp:342] persisted promised to 1  i0626 00:04:39.619220  5476 coordinator.cpp:230] coordinator attemping to fill missing position  i0626 00:04:39.619763  5476 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0626 00:04:39.627267  5476 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 7.485492ms  i0626 00:04:39.627295  5476 replica.cpp:676] persisted action at 0  i0626 00:04:39.627822  5473 replica.cpp:508] replica received write request for position 0  i0626 00:04:39.627861  5473 leveldb.cpp:438] reading position from leveldb took 17132ns  i0626 00:04:39.635592  5473 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 7.714322ms  i0626 00:04:39.635612  5473 replica.cpp:676] persisted action at 0  i0626 00:04:39.635797  5473 replica.cpp:655] replica received learned notice for position 0  i0626 00:04:39.643941  5473 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 8.129347ms  i0626 00:04:39.643960  5473 replica.cpp:676] persisted action at 0  i0626 00:04:39.643970  5473 replica.cpp:661] replica learned nop action at position 0  i0626 00:04:39.644207  5473 log.cpp:672] writer started with ending position 0  i0626 00:04:39.644625  5471 leveldb.cpp:438] reading position from leveldb took 9128ns  i0626 00:04:39.646010  5476 registrar.cpp:346] successfully fetched the registry (0b)  i0626 00:04:39.646044  5476 registrar.cpp:422] attempting to update the 'registry'  i0626 00:04:39.647274  5471 log.cpp:680] attempting to append 136 bytes to the log  i0626 00:04:39.647337  5471 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0626 00:04:39.647687  5476 replica.cpp:508] replica received write request for position 1  i0626 00:04:39.655206  5476 leveldb.cpp:343] persisting action (155 bytes) to leveldb took 7.499736ms  i0626 00:04:39.655225  5476 replica.cpp:676] persisted action at 1  i0626 00:04:39.655467  5476 replica.cpp:655] replica received learned notice for position 1  i0626 00:04:39.663534  5476 leveldb.cpp:343] persisting action (157 bytes) to leveldb took 8.054929ms  i0626 00:04:39.663554  5476 replica.cpp:676] persisted action at 1  i0626 00:04:39.663563  5476 replica.cpp:661] replica learned append action at position 1  i0626 00:04:39.663890  5478 registrar.cpp:479] successfully updated 'registry'  i0626 00:04:39.663947  5478 registrar.cpp:372] successfully recovered registrar  i0626 00:04:39.663969  5476 log.cpp:699] attempting to truncate the log to 1  i0626 00:04:39.664044  5478 master.cpp:980] recovered 0 slaves from the registry (98b) ; allowing 10mins for slaves to reregister  i0626 00:04:39.664057  5476 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0626 00:04:39.664341  5476 replica.cpp:508] replica received write request for position 2  i0626 00:04:39.664681  5450 containerizer.cpp:124] using isolation: posix/cpu,posix/mem  i0626 00:04:39.666721  5471 slave.cpp:168] slave started on 173)@67.195.138.61:55423  i0626 00:04:39.666741  5471 credentials.hpp:35] loading credentials for authentication from '/tmp/slaverecoverytest0multipleframeworksg6obtk/credential'  i0626 00:04:39.666806  5471 slave.cpp:268] slave using credential for: testprincipal  i0626 00:04:39.666936  5471 slave.cpp:281] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0626 00:04:39.667000  5471 slave.cpp:326] slave hostname: juno.apache.org  i0626 00:04:39.667009  5471 slave.cpp:327] slave checkpoint: true  i0626 00:04:39.667572  5478 state.cpp:33] recovering state from '/tmp/slaverecoverytest0multipleframeworksg6obtk/meta'  i0626 00:04:39.667703  5475 statusupdatemanager.cpp:193] recovering status update manager  i0626 00:04:39.667840  5475 containerizer.cpp:287] recovering containerizer  i0626 00:04:39.668478  5471 slave.cpp:3128] finished recovery  i0626 00:04:39.668712  5471 slave.cpp:601] new master detected at master@67.195.138.61:55423  i0626 00:04:39.668738  5471 slave.cpp:677] authenticating with master master@67.195.138.61:55423  i0626 00:04:39.668802  5471 slave.cpp:650] detecting new master  i0626 00:04:39.668861  5471 statusupdatemanager.cpp:167] new master detected at master@67.195.138.61:55423  i0626 00:04:39.668916  5471 authenticatee.hpp:128] creating new client sasl connection  i0626 00:04:39.669087  5471 master.cpp:3499] authenticating slave(173)@67.195.138.61:55423  i0626 00:04:39.669203  5471 authenticator.hpp:156] creating new server sasl connection  i0626 00:04:39.669340  5471 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0626 00:04:39.669359  5471 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0626 00:04:39.669386  5471 authenticator.hpp:262] received sasl authentication start  i0626 00:04:39.669414  5471 authenticator.hpp:384] authentication requires more steps  i0626 00:04:39.669457  5471 authenticatee.hpp:265] received sasl authentication step  i0626 00:04:39.669514  5471 authenticator.hpp:290] received sasl authentication step  i0626 00:04:39.669534  5471 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'juno.apache.org' server fqdn: 'juno.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0626 00:04:39.669543  5471 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0626 00:04:39.669567  5471 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0626 00:04:39.669580  5471 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'juno.apache.org' server fqdn: 'juno.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0626 00:04:39.669589  5471 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0626 00:04:39.669594  5471 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0626 00:04:39.669606  5471 authenticator.hpp:376] authentication success  i0626 00:04:39.669641  5471 authenticatee.hpp:305] authentication success  i0626 00:04:39.669669  5471 master.cpp:3539] successfully authenticated principal 'testprincipal' at slave(173)@67.195.138.61:55423  i0626 00:04:39.669761  5450 sched.cpp:139] version: 0.20.0  i0626 00:04:39.669764  5478 slave.cpp:734] successfully authenticated with master master@67.195.138.61:55423  i0626 00:04:39.669826  5478 slave.cpp:972] will retry registration in 3.190666ms if necessary  i0626 00:04:39.669950  5471 master.cpp:2781] registering slave at slave(173)@67.195.138.61:55423 (juno.apache.org) with id 2014062600043910325041315542354500  i0626 00:04:39.669960  5475 sched.cpp:235] new master detected at master@67.195.138.61:55423  i0626 00:04:39.669977  5475 sched.cpp:285] authenticating with master master@67.195.138.61:55423  i0626 00:04:39.670073  5471 registrar.cpp:422] attempting to update the 'registry'  i0626 00:04:39.670114  5475 authenticatee.hpp:128] creating new client sasl connection  i0626 00:04:39.670263  5475 master.cpp:3499] authenticating schedulere66c50d227904d20bc77a57af0e1780b@67.195.138.61:55423  i0626 00:04:39.670361  5474 authenticator.hpp:156] creating new server sasl connection  i0626 00:04:39.670506  5475 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0626 00:04:39.670526  5475 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0626 00:04:39.670559  5475 authenticator.hpp:262] received sasl authentication start  i0626 00:04:39.670590  5475 authenticator.hpp:384] authentication requires more steps  i0626 00:04:39.670619  5475 authenticatee.hpp:265] received sasl authentication step  i0626 00:04:39.670650  5475 authenticator.hpp:290] received sasl authentication step  i0626 00:04:39.670670  5475 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'juno.apache.org' server fqdn: 'juno.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0626 00:04:39.670677  5475 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0626 00:04:39.670687  5475 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0626 00:04:39.670697  5475 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'juno.apache.org' server fqdn: 'juno.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0626 00:04:39.670706  5475 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0626 00:04:39.670712  5475 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0626 00:04:39.670723  5475 authenticator.hpp:376] authentication success  i0626 00:04:39.670749  5475 authenticatee.hpp:305] authentication success  i0626 00:04:39.670773  5475 master.cpp:3539] successfully authenticated principal 'testprincipal' at schedulere66c50d227904d20bc77a57af0e1780b@67.195.138.61:55423  i0626 00:04:39.670845  5475 sched.cpp:359] successfully authenticated with master master@67.195.138.61:55423  i0626 00:04:39.670858  5475 sched.cpp:478] sending registration request to master@67.195.138.61:55423  i0626 00:04:39.670899  5475 master.cpp:1241] received registration request from schedulere66c50d227904d20bc77a57af0e1780b@67.195.138.61:55423  i0626 00:04:39.670922  5475 master.cpp:1201] authorizing framework principal 'testprincipal' to receive offers for role ''  i0626 00:04:39.671052  5475 master.cpp:1300] registering framework 2014062600043910325041315542354500000 at schedulere66c50d227904d20bc77a57af0e1780b@67.195.138.61:55423  i0626 00:04:39.671159  5474 sched.cpp:409] framework registered with 2014062600043910325041315542354500000  i0626 00:04:39.671185  5474 sched.cpp:423] scheduler::registered took 10223ns  i0626 00:04:39.671226  5474 hierarchicalallocatorprocess.hpp:331] added framework 2014062600043910325041315542354500000  i0626 00:04:39.671241  5474 hierarchicalallocatorprocess.hpp:724] no resources available to allocate!  i0626 00:04:39.671247  5474 hierarchicalallocatorprocess.hpp:686] performed allocation for 0 slaves in 8574ns  i0626 00:04:39.671879  5476 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 7.48781ms  i0626 00:04:39.671900  5476 replica.cpp:676] persisted action at 2  i0626 00:04:39.672164  5471 replica.cpp:655] replica received learned notice for position 2  i0626 00:04:39.674092  5472 slave.cpp:972] will retry registration in 25.467893ms if necessary  i0626 00:04:39.674108  5476 master.cpp:2769] ignoring register slave message from slave(173)@67.195.138.61:55423 (juno.apache.org) as admission is already in progress  i0626 00:04:39.680193  5471 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 8.01285ms  i0626 00:04:39.680223  5471 leveldb.cpp:401] deleting ~1 keys from leveldb took 11393ns  i0626 00:04:39.680234  5471 replica.cpp:676] persisted action at 2  i0626 00:04:39.680245  5471 replica.cpp:661] replica learned truncate action at position 2  i0626 00:04:39.680585  5472 log.cpp:680] attempting to append 326 bytes to the log  i0626 00:04:39.680670  5477 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0626 00:04:39.680953  5474 replica.cpp:508] replica received write request for position 3  i0626 00:04:39.688521  5474 leveldb.cpp:343] persisting action (345 bytes) to leveldb took 7.548316ms  i0626 00:04:39.688542  5474 replica.cpp:676] persisted action at 3  i0626 00:04:39.688750  5474 replica.cpp:655] replica received learned notice for position 3  i0626 00:04:39.696851  5474 leveldb.cpp:343] persisting action (347 bytes) to leveldb took 8.088289ms  i0626 00:04:39.696869  5474 replica.cpp:676] persisted action at 3  i0626 00:04:39.696878  5474 replica.cpp:661] replica learned append action at position 3  i0626 00:04:39.697268  5474 registrar.cpp:479] successfully updated 'registry'  i0626 00:04:39.697350  5474 log.cpp:699] attempting to truncate the log to 3  i0626 00:04:39.697412  5474 master.cpp:2821] registered slave 2014062600043910325041315542354500 at slave(173)@67.195.138.61:55423 (juno.apache.org)  i0626 00:04:39.697423  5474 master.cpp:3967] adding slave 2014062600043910325041315542354500 at slave(173)@67.195.138.61:55423 (juno.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0626 00:04:39.697535  5474 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0626 00:04:39.697618  5474 slave.cpp:768] registered with master master@67.195.138.61:55423; given slave id 2014062600043910325041315542354500  i0626 00:04:39.697754  5474 slave.cpp:781] checkpointing slaveinfo to '/tmp/slaverecoverytest0multipleframeworksg6obtk/meta/slaves/2014062600043910325041315542354500/slave.info'  i0626 00:04:39.697762  5471 hierarchicalallocatorprocess.hpp:444] added slave 2014062600043910325041315542354500 (juno.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0626 00:04:39.697845  5471 hierarchicalallocatorprocess.hpp:750] offering cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2014062600043910325041315542354500 to framework 2014062600043910325041315542354500000  i0626 00:04:39.697854  5474 slave.cpp:2325] received ping from slaveobserver(142)@67.195.138.61:55423  i0626 00:04:39.698040  5471 hierarchicalallocatorprocess.hpp:706] performed allocation for slave 2014062600043910325041315542354500 in 231333ns  i0626 00:04:39.698051  5474 replica.cpp:508] replica received write request for position 4  i0626 00:04:39.698118  5471 master.hpp:794] adding offer 2014062600043910325041315542354500 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2014062600043910325041315542354500 (juno.apache.org)  i0626 00:04:39.698170  5471 master.cpp:3446] sending 1 offers to framework 2014062600043910325041315542354500000  i0626 00:04:39.698318  5471 sched.cpp:546] scheduler::resourceoffers took 24371ns  i0626 00:04:39.699718  5477 master.hpp:804] removing offer 2014062600043910325041315542354500 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2014062600043910325041315542354500 (juno.apache.org)  i0626 00:04:39.699787  5477 master.cpp:2125] processing reply for offers: [ 2014062600043910325041315542354500 ] on slave 2014062600043910325041315542354500 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 2014062600043910325041315542354500000  i0626 00:04:39.699812  5477 master.cpp:2211] authorizing framework principal 'testprincipal' to launch task 897522cc4ec54904aed000b6b8c41028 as user 'jenkins'  i0626 00:04:39.700160  5477 master.hpp:766] adding task 897522cc4ec54904aed000b6b8c41028 with resources cpus():1; mem():512 on slave 2014062600043910325041315542354500 (juno.apache.org)  i0626 00:04:39.700188  5477 master.cpp:2277] launching task 897522cc4ec54904aed000b6b8c41028 of framework 2014062600043910325041315542354500000 with resources cpus():1; mem():512 on slave 2014062600043910325041315542354500 at slave(173)@67.195.138.61:55423 (juno.apache.org)  i0626 00:04:39.700392  5471 slave.cpp:1003] got assigned task 897522cc4ec54904aed000b6b8c41028 for framework 2014062600043910325041315542354500000  i0626 00:04:39.700479  5477 hierarchicalallocatorprocess.hpp:546] framework 2014062600043910325041315542354500000 left cpus():1; mem():512; disk():1024; ports( ):[3100032000] unused on slave 2014062600043910325041315542354500  i0626 00:04:39.700505  5471 slave.cpp:3400] checkpointing frameworkinfo to '/tmp/slaverecoverytest0multipleframeworksg6obtk/meta/slaves/2014062600043910325041315542354500/frameworks/2014062600043910325041315542354500000/framework.info'  i0626 00:04:39.700597  5477 hierarchicalallocatorprocess.hpp:588] framework 2014062600043910325041315542354500000 filtered slave 2014062600043910325041315542354500 for 5secs  i0626 00:04:39.700686  5471 slave.cpp:3407] checkpointing framework pid 'schedulere66c50d227904d20bc77 a57af0e1780b@67.195.138.61:55423' to '...",1,train
MESOS-1559,Allow jenkins build machine to dump stack traces of all threads when timeout,"many of the time, when jenkins build times out, we know that some test freezes at some place. however, most of the time, it's very hard to reproduce the deadlock on dev machines.    i would be cool if we can dump the stack traces of all threads when jenkins build times out. some command like the following:      echo thread apply all bt > tmp; gdb attach `pgrep ltmesostests` < tmp  ",5,train
MESOS-1567,Add logging of the user uid when receiving SIGTERM.,"we currently do not log the user id when receiving a sigterm, this makes debugging a bit difficult. it's easy to get this information through sigaction.",1,train
MESOS-1571,Signal escalation timeout is not configurable,"even though the executor shutdown grace period is set to a larger interval, the signal escalation timeout will still be 3 seconds. it should either be configurable or dependent on executorshutdowngrace_period.    thoughts?",2,train
MESOS-1578,Improve framework rate limiting by imposing the max number of outstanding messages per framework principal, rate limits config takes a configurable capacity for each principal.   to ensure that master maintain the message order of a framework it's important that master sends an frameworkerrormessage back to the scheduler to ask it to abort.,5,train
MESOS-1586,"Isolate system directories, e.g., per-container /tmp","ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.    1) we should include any such files in disk usage and quota.  2) we should make these ""shared"" directories private, i.e., each container has their own.  3) we should make the lifetime of any such files the same as the executor work directory.",3,train
MESOS-1587,Report disk usage from MesosContainerizer,we should report disk usage for the executor work directory from mesoscontainerizer and include in the resourcestatistics protobuf.,5,train
MESOS-1590,Allow LoadGeneratorFramework to read password from a file,it currently just reads the flag as the value of the password.,1,train
MESOS-1592,Design inverse resource offer support,"an ""inverse"" resource offer means that mesos is requesting resources back from the framework, possibly within some time interval.    this can be leveraged initially to provide more automated cluster maintenance, by offering schedulers the opportunity to move tasks to compensate for planned maintenance. operators can set a time limit on how long to wait for schedulers to relocate tasks before the tasks are forcibly terminated.    inverse resource offers have many other potential uses, as it opens the opportunity for the allocator to attempt to move tasks in the cluster through the cooperation of the framework, possibly providing better oversubscription, fairness, etc.",5,train
MESOS-1594,SlaveRecoveryTest/0.ReconcileKillTask is flaky,"observed this on jenkins.      [ run      ] slaverecoverytest/0.reconcilekilltask  using temporary directory '/tmp/slaverecoverytest0reconcilekilltask3zj6dg'  i0714 15:08:43.915114 27216 leveldb.cpp:176] opened db in 474.695188ms  i0714 15:08:43.933645 27216 leveldb.cpp:183] compacted db in 18.068942ms  i0714 15:08:43.934129 27216 leveldb.cpp:198] created db iterator in 7860ns  i0714 15:08:43.934439 27216 leveldb.cpp:204] seeked to beginning of db in 2560ns  i0714 15:08:43.934779 27216 leveldb.cpp:273] iterated through 0 keys in the db in 1400ns  i0714 15:08:43.935098 27216 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0714 15:08:43.936027 27238 recover.cpp:425] starting replica recovery  i0714 15:08:43.936225 27238 recover.cpp:451] replica is in empty status  i0714 15:08:43.936867 27238 replica.cpp:638] replica in empty status received a broadcasted recover request  i0714 15:08:43.937049 27238 recover.cpp:188] received a recover response from a replica in empty status  i0714 15:08:43.937232 27238 recover.cpp:542] updating replica status to starting  i0714 15:08:43.945600 27235 master.cpp:288] master 20140714150843168428795585027216 (quantal) started on 127.0.1.1:55850  i0714 15:08:43.945643 27235 master.cpp:325] master only allowing authenticated frameworks to register  i0714 15:08:43.945651 27235 master.cpp:330] master only allowing authenticated slaves to register  i0714 15:08:43.945658 27235 credentials.hpp:36] loading credentials for authentication from '/tmp/slaverecoverytest0reconcilekilltask3zj6dg/credentials'  i0714 15:08:43.945808 27235 master.cpp:359] authorization enabled  i0714 15:08:43.946369 27235 hierarchicalallocatorprocess.hpp:301] initializing hierarchical allocator process with master : master@127.0.1.1:55850  i0714 15:08:43.946419 27235 master.cpp:122] no whitelist given. advertising offers for all slaves  i0714 15:08:43.946614 27235 master.cpp:1128] the newly elected leader is master@127.0.1.1:55850 with id 20140714150843168428795585027216  i0714 15:08:43.946630 27235 master.cpp:1141] elected as the leading master!  i0714 15:08:43.946637 27235 master.cpp:959] recovering from registrar  i0714 15:08:43.946707 27235 registrar.cpp:313] recovering registrar  i0714 15:08:43.957895 27238 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 20.529301ms  i0714 15:08:43.957978 27238 replica.cpp:320] persisted replica status to starting  i0714 15:08:43.958142 27238 recover.cpp:451] replica is in starting status  i0714 15:08:43.958664 27238 replica.cpp:638] replica in starting status received a broadcasted recover request  i0714 15:08:43.958762 27238 recover.cpp:188] received a recover response from a replica in starting status  i0714 15:08:43.958945 27238 recover.cpp:542] updating replica status to voting  i0714 15:08:43.975685 27238 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 16.646136ms  i0714 15:08:43.976367 27238 replica.cpp:320] persisted replica status to voting  i0714 15:08:43.976824 27241 recover.cpp:556] successfully joined the paxos group  i0714 15:08:43.977072 27242 recover.cpp:440] recover process terminated  i0714 15:08:43.980590 27236 log.cpp:656] attempting to start the writer  i0714 15:08:43.981385 27236 replica.cpp:474] replica received implicit promise request with proposal 1  i0714 15:08:43.999141 27236 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 17.705787ms  i0714 15:08:43.999222 27236 replica.cpp:342] persisted promised to 1  i0714 15:08:44.004451 27240 coordinator.cpp:230] coordinator attemping to fill missing position  i0714 15:08:44.004914 27240 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0714 15:08:44.021456 27240 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 16.499775ms  i0714 15:08:44.021533 27240 replica.cpp:676] persisted action at 0  i0714 15:08:44.022006 27240 replica.cpp:508] replica received write request for position 0  i0714 15:08:44.022043 27240 leveldb.cpp:438] reading position from leveldb took 21376ns  i0714 15:08:44.035969 27240 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 13.885907ms  i0714 15:08:44.036365 27240 replica.cpp:676] persisted action at 0  i0714 15:08:44.040156 27238 replica.cpp:655] replica received learned notice for position 0  i0714 15:08:44.058082 27238 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 17.860707ms  i0714 15:08:44.058161 27238 replica.cpp:676] persisted action at 0  i0714 15:08:44.058176 27238 replica.cpp:661] replica learned nop action at position 0  i0714 15:08:44.058526 27238 log.cpp:672] writer started with ending position 0  i0714 15:08:44.058872 27238 leveldb.cpp:438] reading position from leveldb took 25660ns  i0714 15:08:44.060556 27238 registrar.cpp:346] successfully fetched the registry (0b)  i0714 15:08:44.060845 27238 registrar.cpp:422] attempting to update the 'registry'  i0714 15:08:44.062304 27238 log.cpp:680] attempting to append 120 bytes to the log  i0714 15:08:44.062866 27236 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0714 15:08:44.063154 27236 replica.cpp:508] replica received write request for position 1  i0714 15:08:44.082813 27236 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 19.61683ms  i0714 15:08:44.082890 27236 replica.cpp:676] persisted action at 1  i0714 15:08:44.083256 27236 replica.cpp:655] replica received learned notice for position 1  i0714 15:08:44.097398 27236 leveldb.cpp:343] persisting action (139 bytes) to leveldb took 14.104796ms  i0714 15:08:44.097475 27236 replica.cpp:676] persisted action at 1  i0714 15:08:44.097488 27236 replica.cpp:661] replica learned append action at position 1  i0714 15:08:44.098569 27236 registrar.cpp:479] successfully updated 'registry'  i0714 15:08:44.098906 27240 log.cpp:699] attempting to truncate the log to 1  i0714 15:08:44.099608 27240 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0714 15:08:44.100005 27240 replica.cpp:508] replica received write request for position 2  i0714 15:08:44.100566 27236 registrar.cpp:372] successfully recovered registrar  i0714 15:08:44.101227 27239 master.cpp:986] recovered 0 slaves from the registry (84b) ; allowing 10mins for slaves to reregister  i0714 15:08:44.118376 27240 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 18.329495ms  i0714 15:08:44.118455 27240 replica.cpp:676] persisted action at 2  i0714 15:08:44.122258 27242 replica.cpp:655] replica received learned notice for position 2  i0714 15:08:44.137336 27242 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 15.023553ms  i0714 15:08:44.137460 27242 leveldb.cpp:401] deleting ~1 keys from leveldb took 55049ns  i0714 15:08:44.137480 27242 replica.cpp:676] persisted action at 2  i0714 15:08:44.137492 27242 replica.cpp:661] replica learned truncate action at position 2  i0714 15:08:44.143729 27216 containerizer.cpp:124] using isolation: posix/cpu,posix/mem  i0714 15:08:44.145934 27242 slave.cpp:168] slave started on 43)@127.0.1.1:55850  i0714 15:08:44.145953 27242 credentials.hpp:84] loading credential for authentication from '/tmp/slaverecoverytest0reconcilekilltaskzl9dut/credential'  i0714 15:08:44.146040 27242 slave.cpp:266] slave using credential for: testprincipal  i0714 15:08:44.146136 27242 slave.cpp:279] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0714 15:08:44.146198 27242 slave.cpp:324] slave hostname: quantal  i0714 15:08:44.146209 27242 slave.cpp:325] slave checkpoint: true  i0714 15:08:44.146708 27242 state.cpp:33] recovering state from '/tmp/slaverecoverytest0reconcilekilltaskzl9dut/meta'  i0714 15:08:44.146824 27242 statusupdatemanager.cpp:193] recovering status update manager  i0714 15:08:44.146901 27242 containerizer.cpp:287] recovering containerizer  i0714 15:08:44.147228 27242 slave.cpp:3126] finished recovery  i0714 15:08:44.147531 27242 slave.cpp:599] new master detected at master@127.0.1.1:55850  i0714 15:08:44.147562 27242 slave.cpp:675] authenticating with master master@127.0.1.1:55850  i0714 15:08:44.147614 27242 slave.cpp:648] detecting new master  i0714 15:08:44.147652 27242 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:55850  i0714 15:08:44.147691 27242 authenticatee.hpp:128] creating new client sasl connection  i0714 15:08:44.148533 27235 master.cpp:3507] authenticating slave(43)@127.0.1.1:55850  i0714 15:08:44.148666 27235 authenticator.hpp:156] creating new server sasl connection  i0714 15:08:44.149054 27242 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0714 15:08:44.149447 27242 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0714 15:08:44.149917 27236 authenticator.hpp:262] received sasl authentication start  i0714 15:08:44.149974 27236 authenticator.hpp:384] authentication requires more steps  i0714 15:08:44.150208 27242 authenticatee.hpp:265] received sasl authentication step  i0714 15:08:44.150720 27239 authenticator.hpp:290] received sasl authentication step  i0714 15:08:44.150749 27239 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'quantal' server fqdn: 'quantal' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0714 15:08:44.150758 27239 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0714 15:08:44.150771 27239 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0714 15:08:44.150781 27239 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'quantal' server fqdn: 'quantal' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0714 15:08:44.150787 27239 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0714 15:08:44.150792 27239 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0714 15:08:44.150804 27239 authenticator.hpp:376] authentication success  i0714 15:08:44.150848 27239 master.cpp:3547] successfully authenticated principal 'testprincipal' at slave(43)@127.0.1.1:55850  i0714 15:08:44.157696 27242 authenticatee.hpp:305] authentication success  i0714 15:08:44.158855 27242 slave.cpp:732] successfully authenticated with master master@127.0.1.1:55850  i0714 15:08:44.158936 27242 slave.cpp:970] will retry registration in 10.352612ms if necessary  i0714 15:08:44.161813 27216 sched.cpp:139] version: 0.20.0  i0714 15:08:44.162608 27236 sched.cpp:235] new master detected at master@127.0.1.1:55850  i0714 15:08:44.162637 27236 sched.cpp:285] authenticating with master master@127.0.1.1:55850  i0714 15:08:44.162747 27236 authenticatee.hpp:128] creating new client sasl connection  i0714 15:08:44.163506 27239 master.cpp:2789] registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 201407141508431684287955850272160  i0714 15:08:44.164086 27238 registrar.cpp:422] attempting to update the 'registry'  i0714 15:08:44.165694 27238 log.cpp:680] attempting to append 295 bytes to the log  i0714 15:08:44.166231 27240 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0714 15:08:44.166517 27240 replica.cpp:508] replica received write request for position 3  i0714 15:08:44.167199 27239 master.cpp:3507] authenticating scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850  i0714 15:08:44.167867 27241 authenticator.hpp:156] creating new server sasl connection  i0714 15:08:44.168058 27241 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0714 15:08:44.168081 27241 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0714 15:08:44.168107 27241 authenticator.hpp:262] received sasl authentication start  i0714 15:08:44.168149 27241 authenticator.hpp:384] authentication requires more steps  i0714 15:08:44.168176 27241 authenticatee.hpp:265] received sasl authentication step  i0714 15:08:44.168215 27241 authenticator.hpp:290] received sasl authentication step  i0714 15:08:44.168233 27241 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'quantal' server fqdn: 'quantal' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0714 15:08:44.168793 27241 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0714 15:08:44.168820 27241 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0714 15:08:44.168834 27241 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'quantal' server fqdn: 'quantal' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0714 15:08:44.168840 27241 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0714 15:08:44.168845 27241 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0714 15:08:44.168858 27241 authenticator.hpp:376] authentication success  i0714 15:08:44.168895 27241 authenticatee.hpp:305] authentication success  i0714 15:08:44.168970 27241 sched.cpp:359] successfully authenticated with master master@127.0.1.1:55850  i0714 15:08:44.168987 27241 sched.cpp:478] sending registration request to master@127.0.1.1:55850  i0714 15:08:44.169426 27239 master.cpp:1239] queuing up registration request from scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850 because authentication is still in progress  i0714 15:08:44.169958 27239 master.cpp:3547] successfully authenticated principal 'testprincipal' at scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850  i0714 15:08:44.170440 27241 slave.cpp:970] will retry registration in 8.76707ms if necessary  i0714 15:08:44.175359 27239 master.cpp:2777] ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress  i0714 15:08:44.175916 27239 master.cpp:1247] received registration request from scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850  i0714 15:08:44.176298 27239 master.cpp:1207] authorizing framework principal 'testprincipal' to receive offers for role ''  i0714 15:08:44.176858 27239 master.cpp:1306] registering framework 201407141508431684287955850272160000 at scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850  i0714 15:08:44.177408 27236 sched.cpp:409] framework registered with 201407141508431684287955850272160000  i0714 15:08:44.177443 27236 sched.cpp:423] scheduler::registered took 12527ns  i0714 15:08:44.177727 27241 hierarchicalallocatorprocess.hpp:331] added framework 201407141508431684287955850272160000  i0714 15:08:44.177747 27241 hierarchicalallocatorprocess.hpp:724] no resources available to allocate!  i0714 15:08:44.177753 27241 hierarchicalallocatorprocess.hpp:686] performed allocation for 0 slaves in 8120ns  i0714 15:08:44.179908 27241 slave.cpp:970] will retry registration in 66.781028ms if necessary  i0714 15:08:44.180007 27241 master.cpp:2777] ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress  i0714 15:08:44.183082 27240 leveldb.cpp:343] persisting action (314 bytes) to leveldb took 16.533189ms  i0714 15:08:44.183125 27240 replica.cpp:676] persisted action at 3  i0714 15:08:44.183465 27240 replica.cpp:655] replica received learned notice for position 3  i0714 15:08:44.203276 27240 leveldb.cpp:343] persisting action (316 bytes) to leveldb took 19.768951ms  i0714 15:08:44.203376 27240 replica.cpp:676] persisted action at 3  i0714 15:08:44.203392 27240 replica.cpp:661] replica learned append action at position 3  i0714 15:08:44.204033 27240 registrar.cpp:479] successfully updated 'registry'  i0714 15:08:44.204138 27240 log.cpp:699] attempting to truncate the log to 3  i0714 15:08:44.204221 27240 master.cpp:2829] registered slave 201407141508431684287955850272160 at slave(43)@127.0.1.1:55850 (quantal)  i0714 15:08:44.204241 27240 master.cpp:3975] adding slave 201407141508431684287955850272160 at slave(43)@127.0.1.1:55850 (quantal) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0714 15:08:44.204387 27240 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0714 15:08:44.204489 27240 slave.cpp:766] registered with master master@127.0.1.1:55850; given slave id 201407141508431684287955850272160  i0714 15:08:44.204745 27240 slave.cpp:779] checkpointing slaveinfo to '/tmp/slaverecoverytest0reconcilekilltaskzl9dut/meta/slaves/201407141508431684287955850272160/slave.info'  i0714 15:08:44.204954 27240 hierarchicalallocatorprocess.hpp:444] added slave 201407141508431684287955850272160 (quantal) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0714 15:08:44.205023 27240 hierarchicalallocatorprocess.hpp:750] offering cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201407141508431684287955850272160 to framework 201407141508431684287955850272160000  i0714 15:08:44.205122 27240 hierarchicalallocatorprocess.hpp:706] performed allocation for slave 201407141508431684287955850272160 in 131192ns  i0714 15:08:44.205189 27240 slave.cpp:2323] received ping from slaveobserver(32)@127.0.1.1:55850  i0714 15:08:44.205258 27240 master.hpp:801] adding offer 201407141508431684287955850272160 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201407141508431684287955850272160 (quantal)  i0714 15:08:44.205303 27240 master.cpp:3454] sending 1 offers to framework 201407141508431684287955850272160000  i0714 15:08:44.205469 27240 sched.cpp:546] scheduler::resourceoffers took 23591ns  i0714 15:08:44.206351 27241 replica.cpp:508] replica received write request for position 4  i0714 15:08:44.208353 27237 master.hpp:811] removing offer 201407141508431684287955850272160 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201407141508431684287955850272160 (quantal)  i0714 15:08:44.208436 27237 master.cpp:2133] processing reply for offers: [ 201407141508431684287955850272160 ] on slave 201407141508431684287955850272160 at slave(43)@127.0.1.1:55850 (quantal) for framework 201407141508431684287955850272160000  i0714 15:08:44.208472 27237 master.cpp:2219] authorizing framework principal 'testprincipal' to launch task 4a6783aa8d0746e383992a5d047f0021 as user 'jenkins'  i0714 15:08:44.208909 27237 master.hpp:773] adding task 4a6783aa8d0746e383992a5d047f0021 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201407141508431684287955850272160 (quantal)  i0714 15:08:44.208947 27237 master.cpp:2285] launching task 4a6783aa8d0746e383992a5d047f0021 of framework 201407141508431684287955850272160000 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 201407141508431684287955850272160 at slave(43)@127.0.1.1:55850 (quantal)  i0714 15:08:44.209090 27237 slave.cpp:1001] got assigned task 4a6783aa8d0746e383992a5d047f0021 for framework 201407141508431684287955850272160000  i0714 15:08:44.209190 27237 slave.cpp:3398] checkpointing frameworkinfo to '/tmp/slaverecoverytest0reconcilekilltaskzl9dut/meta/slaves/201407141508431684287955850272160/frameworks/201407141508431684287955850272160000/framework.info'  i0714 15:08:44.209413 27237 slave.cpp:3405] checkpointing framework pid 'scheduler225679c4a9fd41199debc7712eba37e1@127.0.1.1:55850' to '/tmp/slaverecoverytest0reconcilekilltask_zl9dut/meta/slaves/201407141508431684287955850272160/frameworks/20140714150843168428795585027216 0000/framework.pid'  i0714 15:08...",1,train
MESOS-1605,Cleanup stout build setup,"while investigating stout build setup for making it installable, i came across some discrepancies.    stout tests are included in libprocess's makefile instead of stout makefile.    stout's 3rd party dependencies (e.g., picojson) live in libprocess's 3rdparty directory instead of living in stout's (non existent) 3rd party directory.    it would be nice to fix these issues before making stout installable.",3,train
MESOS-1615,Create design document for Optimistic Offers,"as a first step toward optimistic offers, take the description from the epic and build an implementation design doc that can be shared for comments.    note: the links to the working group notes and design doc are located in the mesos 1607.",8,train
MESOS-1620,Reconciliation does not send back tasks pending validation / authorization.,"per vinod's feedback on https:/reviews.apache.org/r/23542/, we do not send back taskstaging for those tasks that are pending in the master (validation / authorization still in progress).    for both implicit and explicit task reconciliation, the master could reply with taskstaging for these tasks, as this provides additional information to the framework.",3,train
MESOS-1624,Apache Jenkins build fails due to -lsnappy is set when building leveldb,"the failed build: https:/builds.apache.org/job/mesostrunkubuntubuildoutofsrcsetjavahome/2261/consolefull    gzip d c ../../3rdparty/leveldb.tar.gz | patch d leveldb p1 <../../3rdparty/leveldb.patch  touch leveldbstamp  cd leveldb && \            make  cc=""gcc"" cxx=""g"" opt=""g g2 o2 wnounusedlocaltypedefs std=c11 fpic""  make[5]: entering directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/mesos0.20.0/build/3rdparty/leveldb'  g pthread lsnappy shared wl,soname wl,/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/mesos0.20.0/build/3rdparty/leveldb/libleveldb.so.1 i. i./include fnobuiltinmemcmp pthread doslinux dleveldbplatformposix dsnappy g g2 o2 wnounusedlocaltypedefs std=c11 fpic fpic db/builder.cc db/c.cc db/dbimpl.cc db/dbiter.cc db/dbformat.cc db/filename.cc db/logreader.cc db/logwriter.cc db/memtable.cc db/repair.cc db/tablecache.cc db/versionedit.cc db/versionset.cc db/writebatch.cc table/block.cc table/blockbuilder.cc table/filterblock.cc table/format.cc table/iterator.cc table/merger.cc table/table.cc table/tablebuilder.cc table/twoleveliterator.cc util/arena.cc util/bloom.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/envposix.cc util/filterpolicy.cc util/hash.cc util/histogram.cc util/logging.cc util/options.cc util/status.cc  port/portposix.cc o libleveldb.so.1.4  ln fs libleveldb.so.1.4 libleveldb.so  ln fs libleveldb.so.1.4 libleveldb.so.1  g i. i./include fnobuiltinmemcmp pthread doslinux dleveldbplatformposix dsnappy g g2 o2 wnounusedlocaltypedefs std=c11 fpic c db/builder.cc o db/builder.o        /bin/bash ../libtool  tag=cxx   mode=link g pthread g g2 o2 wnounusedlocaltypedefs std=c11   o mesoslocal local/mesoslocalmain.o libmesos.la lsasl2 lcurl lz  lrt  libtool: link: g pthread g g2 o2 wnounusedlocaltypedefs std=c11 o .libs/mesoslocal local/mesoslocalmain.o  ./.libs/libmesos.so lsasl2 /usr/lib/x8664linuxgnu/libcurl.so lz lrt pthread wl,rpath wl,/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/mesos0.20.0/inst/lib  ./.libs/libmesos.so: undefined reference to `snappy::rawcompress(char const, unsigned long, char, unsigned long)'  ./.libs/libmesos.so: undefined reference to `snappy::rawuncompress(char const, unsigned long, char)'  ./.libs/libmesos.so: undefined reference to `snappy::getuncompressedlength(char const, unsigned long, unsigned long )'  ./.libs/libmesos.so: undefined reference to `snappy::maxcompressedlength(unsigned long)'  ",1,train
MESOS-1627,Installed protobuf header files include wrong path to mesos header file,"playing with installed mesos headers, realized that we expect users to include the path to mesos directory (e.g., /usr/local/include/mesos) even though it is on the system path. this is because scheduler.pb.h etc include ""mesos.pb.h"" instead of ""mesos/mesos.pb.h"".",2,train
MESOS-1629,GLOG Initialized twice if the Framework Scheduler also uses GLOG,  could not create logging file: no such file or directory  could not create a loggingfile 20140722 205220.31450!f0722 20:52:20.494424 31450 utilities.cc:317] check failed: !isgooglelogginginitialized() you called initgooglelogging() twice!   check failure stack trace:       @           0x4399ce  google::logmessage::fail()      @           0x43991d  google::logmessage::sendtolog()      @           0x43932e  google::logmessage::flush()      @           0x43c0e5  google::logmessagefatal::~logmessagefatal()      @           0x44089f  google::gloginternalnamespace::initgoogleloggingutilities()      @           0x43c409  google::initgooglelogging()      @     0x7f0bdd43b55c  mesos::internal::logging::initialize()      @     0x7f0bdcf9564d  mesos::scheduler::mesosprocess::mesosprocess()      @     0x7f0bdcf92de0  mesos::scheduler::mesos::mesos()      @           0x421483  heron::mesos::scheduler::scheduler()      @           0x4305dc  main      @     0x7f0bd97159c4  libcstart_main      @           0x420869  (unknown)  aborted  ,2,train
MESOS-1645,0.20.0 Release,"i would like to volunteer to be the release manager for 0.20.0, which will be releasing the following major features:     docker support in mesos (mesos1524)     container level network monitoring for mesos containerizer (mesos1228)     authorization (mesos1342)     framework rate limiting (mesos1306)     enable building against installed thirdparty dependencies (mesos 1071)    i would like to track blockers for the release on this ticket.",5,train
MESOS-1649,Network isolator should tolerate slave crashes while doing isolate/cleanup.,"a slave may crash while we are installing/removing filters. the slave recovery for the network isolator should tolerate those partially installed filters. also, we want to avoid leaking a filter on host eth0 and host lo.    the current code cannot tolerate that, thus may cause the following error:      failed to perform recovery: collect failed: failed to recover container d409a1002afb497c864ffe3002cf65d9 with pid 50405: no ephemeral ports found  to remedy this do as follows:  step 1: rm  f /var/lib/mesos/meta/slaves/latest         this ensures slave doesn't recover old live executors.  step 2: restart the slave.  ",3,train
MESOS-1664,Inform framework when rate limiting is active,"when we ratelimit messages from a framework, we should let them know so they can proactively backoff to avoid putting extra pressure on the master.",3,train
MESOS-1666,Set maximum executors per slave to avoid overcommit of ephemeral ports,"with network isolation, we statically assign ephemeral port ranges. as such there is a upper bound on the number of containers each slave can support.    we should avoid sending offers for slaves that have hit that limit as any tasks will fail to launch and will be lost. ",1,train
MESOS-1668,Handle a temporary one-way master --> slave socket closure.,"in mesos1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:     master and slave connected operating normally.   temporary oneway network failure, masterslave link breaks.   master marks slave as disconnected.   network restored and health checking continues normally, slave is not removed as a result. slave does not attempt to reregister since it is receiving pings once again.   slave remains disconnected according to the master, and the slave does not try to reregister. bad!    we were originally thinking of using a failover timeout in the master to remove these slaves that don't reregister. however, it can be dangerous when zookeeper issues are preventing the slave from reregistering with the master; we do not want to remove a ton of slaves in this situation.    rather, when the slave is health checking correctly but does not reregister within a timeout, we could send a registration request from the master to the slave, telling the slave that it must reregister. this message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master.",2,train
MESOS-1671,Expose executor metrics for slave.,expose the following metrics:    slave/executorsregistering  slave/executorsrunning  slave/executorsterminating  slave/executorsterminated,2,train
MESOS-1672,Add filter to allocator resourcesRecovered method,the allocator already allows filters to be added when resources are unused. it is useful to also allow the same behaviour in resourcesrecovered.,2,train
MESOS-1673,The value of MASTER_PING_TIMEOUT is non-deterministic,"right now, it is declared as follows:    const duration masterpingtimeout =    master::slavepingtimeout   master::maxslavepingtimeouts      since static initialization order in c is undefined, masterpingtimeout's value is non deterministic. we've already observed that in tests (where masterping_timeout == 0).",1,train
MESOS-1674,Kill private_resources and treat 'ephemeral_ports' as a resource.,"as the first step to solve mesos1654, we need to kill privateresources in slaveinfo and add a 'ephemeralports' resource.    for now, the slave and the port mapping isolator will simply ignore the 'ephemeral_ports' resource in executorinfo and taskinfo, and make allocation by itself. we will revisit this once the overcommit race (mesos1466) is fixed.",3,train
MESOS-1676,ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky,"  [ run      ] zookeepermastercontenderdetectortest.masterdetectortimedoutsession  i0806 01:18:37.648684 17458 zookeepertestserver.cpp:158] started zookeepertestserver on port 42069  20140806 01:18:37,650:17458(0x2b4679ca5700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:37,650:17458(0x2b4679ca5700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:37,650:17458(0x2b4679ca5700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:37,650:17458(0x2b4679ca5700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:37,650:17458(0x2b4679ca5700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:37,651:17458(0x2b4679ca5700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:37,651:17458(0x2b4679ca5700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:37,651:17458(0x2b4679ca5700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:37,651:17458(0x2b4679ca5700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x1682db0 flags=0  20140806 01:18:37,656:17458(0x2b468638b700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:42069]  20140806 01:18:37,669:17458(0x2b468638b700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:42069], sessionid=0x147aa6601cf0000, negotiated timeout=6000  i0806 01:18:37.671725 17486 group.cpp:313] group process (group(37)@127.0.1.1:55561) connected to zookeeper  i0806 01:18:37.671758 17486 group.cpp:787] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0806 01:18:37.671771 17486 group.cpp:385] trying to create path '/mesos' in zookeeper  20140806 01:18:39,101:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:42,441:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  i0806 01:18:42.656673 17481 contender.cpp:131] joining the zk group  i0806 01:18:42.662484 17484 contender.cpp:247] new candidate (id='0') has entered the contest for leadership  i0806 01:18:42.663754 17481 detector.cpp:138] detected a new leader: (id='0')  i0806 01:18:42.663884 17481 group.cpp:658] trying to get '/mesos/info0000000000' in zookeeper  i0806 01:18:42.664788 17483 detector.cpp:426] a new leading master (upid=@128.150.152.0:10000) is detected  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:42,666:17458(0x2b4679ea6700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x15c00f0 flags=0  20140806 01:18:42,668:17458(0x2b4686d91700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:42069]  20140806 01:18:42,672:17458(0x2b4686d91700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:42069], sessionid=0x147aa6601cf0001, negotiated timeout=6000  i0806 01:18:42.673542 17485 group.cpp:313] group process (group(38)@127.0.1.1:55561) connected to zookeeper  i0806 01:18:42.673570 17485 group.cpp:787] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0806 01:18:42.673580 17485 group.cpp:385] trying to create path '/mesos' in zookeeper  20140806 01:18:46,796:17458(0x2b468638b700):zoowarn@zookeeperinterest@1557: exceeded deadline by 2131ms  20140806 01:18:46,796:17458(0x2b468638b700):zooerror@handlesocketerrormsg@1643: socket [127.0.0.1:42069] zk retcode=7, errno=110(connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms)  20140806 01:18:46,796:17458(0x2b468638b700):zoowarn@zookeeperinterest@1557: exceeded deadline by 2131ms  20140806 01:18:46,796:17458(0x2b4686d91700):zoowarn@zookeeperinterest@1557: exceeded deadline by 2115ms  20140806 01:18:46,796:17458(0x2b4686d91700):zooerror@handlesocketerrormsg@1643: socket [127.0.0.1:42069] zk retcode=7, errno=110(connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms)  20140806 01:18:46,796:17458(0x2b4686d91700):zoowarn@zookeeperinterest@1557: exceeded deadline by 2115ms  20140806 01:18:46,799:17458(0x2b4687394700):zoowarn@zookeeperinterest@1557: exceeded deadline by 1025ms  20140806 01:18:46,800:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  i0806 01:18:46.806895 17486 group.cpp:418] lost connection to zookeeper, attempting to reconnect ...  i0806 01:18:46.807857 17479 group.cpp:418] lost connection to zookeeper, attempting to reconnect ...  i0806 01:18:47.669064 17482 contender.cpp:131] joining the zk group  20140806 01:18:47,669:17458(0x2b4686d91700):zoowarn@zookeeperinterest@1557: exceeded deadline by 2989ms  20140806 01:18:47,669:17458(0x2b4686d91700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:42069]  20140806 01:18:47,671:17458(0x2b4686d91700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:42069], sessionid=0x147aa6601cf0001, negotiated timeout=6000  i0806 01:18:47.682868 17485 contender.cpp:247] new candidate (id='1') has entered the contest for leadership  i0806 01:18:47.683404 17482 group.cpp:313] group process (group(38)@127.0.1.1:55561) reconnected to zookeeper  i0806 01:18:47.683445 17482 group.cpp:787] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0806 01:18:47.685998 17482 detector.cpp:138] detected a new leader: (id='0')  i0806 01:18:47.686142 17482 group.cpp:658] trying to get '/mesos/info0000000000' in zookeeper  i0806 01:18:47.687289 17479 detector.cpp:426] a new leading master (upid=@128.150.152.0:10000) is detected  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:47,687:17458(0x2b467a2a8700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x2b467c0421c0 flags=0  20140806 01:18:47,699:17458(0x2b4687de6700):zooinfo@checkevents@1703: initiated connection to server [127.0.0.1:42069]  20140806 01:18:47,712:17458(0x2b4687de6700):zooinfo@checkevents@1750: session establishment complete on server [127.0.0.1:42069], sessionid=0x147aa6601cf0002, negotiated timeout=6000  i0806 01:18:47.712846 17479 group.cpp:313] group process (group(39)@127.0.1.1:55561) connected to zookeeper  i0806 01:18:47.712873 17479 group.cpp:787] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0806 01:18:47.712882 17479 group.cpp:385] trying to create path '/mesos' in zookeeper  i0806 01:18:47.714648 17479 detector.cpp:138] detected a new leader: (id='0')  i0806 01:18:47.714759 17479 group.cpp:658] trying to get '/mesos/info0000000000' in zookeeper  i0806 01:18:47.716130 17479 detector.cpp:426] a new leading master (upid=@128.150.152.0:10000) is detected  20140806 01:18:47,718:17458(0x2b4686d91700):zooerror@handlesocketerrormsg@1721: socket [127.0.0.1:42069] zk retcode=4, errno=112(host is down): failed while receiving a server response  i0806 01:18:47.718889 17479 group.cpp:418] lost connection to zookeeper, attempting to reconnect ...  20140806 01:18:47,720:17458(0x2b4687de6700):zooerror@handlesocketerrormsg@1721: socket [127.0.0.1:42069] zk retcode=4, errno=112(host is down): failed while receiving a server response  i0806 01:18:47.720788 17484 group.cpp:418] lost connection to zookeeper, attempting to reconnect ...  i0806 01:18:47.724663 17458 zookeepertestserver.cpp:122] shutdown zookeepertestserver on port 42069  20140806 01:18:48,798:17458(0x2b468638b700):zoowarn@zookeeperinterest@1557: exceeded deadline by 4133ms  20140806 01:18:48,798:17458(0x2b468638b700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:49,720:17458(0x2b4686d91700):zoowarn@zookeeperinterest@1557: exceeded deadline by 33ms  20140806 01:18:49,721:17458(0x2b4686d91700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:49,722:17458(0x2b4687de6700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:50,136:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:50,800:17458(0x2b468638b700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:51,723:17458(0x2b4686d91700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:51,723:17458(0x2b4687de6700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:52,801:17458(0x2b468638b700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  w0806 01:18:52.842553 17481 group.cpp:456] timed out waiting to reconnect to zookeeper. forcing zookeeper session (sessionid=147aa6601cf0000) expiration  i0806 01:18:52.842911 17481 group.cpp:472] zookeeper session expired  i0806 01:18:52.843468 17485 detector.cpp:126] the current leader (id=0) is lost  i0806 01:18:52.843483 17485 detector.cpp:138] detected a new leader: none  i0806 01:18:52.843618 17485 contender.cpp:196] membership cancelled: 0  20140806 01:18:52,843:17458(0x2b4679aa4700):zooinfo@zookeeperclose@2522: freeing zookeeper resources for sessionid=0x147aa6601cf0000    20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:52,844:17458(0x2b4679aa4700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x1349ad0 flags=0  20140806 01:18:52,844:17458(0x2b468698f700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:53,473:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  w0806 01:18:53.720684 17480 group.cpp:456] timed out waiting to reconnect to zookeeper. forcing zookeeper session (sessionid=147aa6601cf0001) expiration  i0806 01:18:53.721132 17480 group.cpp:472] zookeeper session expired  i0806 01:18:53.721516 17479 detector.cpp:126] the current leader (id=0) is lost  i0806 01:18:53.721534 17479 detector.cpp:138] detected a new leader: none  i0806 01:18:53.721696 17479 contender.cpp:196] membership cancelled: 1  20140806 01:18:53,721:17458(0x2b46798a3700):zooinfo@zookeeperclose@2522: freeing zookeeper resources for sessionid=0x147aa6601cf0001    20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:53,722:17458(0x2b46798a3700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x16a0550 flags=0  20140806 01:18:53,723:17458(0x2b4686f92700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:53,726:17458(0x2b4687de6700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  w0806 01:18:53.730258 17479 group.cpp:456] timed out waiting to reconnect to zookeeper. forcing zookeeper session (sessionid=147aa6601cf0002) expiration  i0806 01:18:53.730736 17479 group.cpp:472] zookeeper session expired  i0806 01:18:53.731081 17481 detector.cpp:126] the current leader (id=0) is lost  i0806 01:18:53.731132 17481 detector.cpp:138] detected a new leader: none  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@zookeeperclose@2522: freeing zookeeper resources for sessionid=0x147aa6601cf0002    20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@716: client environment:host.name=lucid  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@723: client environment:os.name=linux  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@724: client environment:os.arch=2.6.3264generic  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@725: client environment:os.version=#128ubuntu smp tue jul 15 08:32:40 utc 2014  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@733: client environment:user.name=(null)  20140806 01:18:53,731:17458(0x2b46796a2700):zooinfo@logenv@741: client environment:user.home=/home/jenkins  20140806 01:18:53,732:17458(0x2b46796a2700):zooinfo@logenv@753: client environment:user.dir=/var/jenkins/workspace/mesosubuntu10.04gcc/src  20140806 01:18:53,732:17458(0x2b46796a2700):zooinfo@zookeeperinit@786: initiating client connection, host=127.0.0.1:42069 sessiontimeout=5000 watcher=0x2b467450bc00 sessionid=0 sessionpasswd=/ context=0x2b467c035f30 flags=0  20140806 01:18:53,733:17458(0x2b4687be5700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:54,512:17458(0x2b468698f700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:55,393:17458(0x2b4686f92700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:55,403:17458(0x2b4687be5700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:56,301:17458(0x2b468698f700):zoowarn@zookeeperinterest@1557: exceeded deadline by 122ms  20140806 01:18:56,302:17458(0x2b468698f700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:56,809:17458(0x2b4687394700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:36197] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:57,939:17458(0x2b4686f92700):zoowarn@zookeeperinterest@1557: exceeded deadline by 879ms  20140806 01:18:57,940:17458(0x2b4686f92700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  20140806 01:18:57,940:17458(0x2b4687be5700):zoowarn@zookeeperinterest@1557: exceeded deadline by 870ms  20140806 01:18:57,940:17458(0x2b4687be5700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:42069] zk retcode=4, errno=111(connection refused): server refused to accept the client  tests/mastercontenderdetectortests.cpp:574: failure  failed to wait 10secs for leaderreconnecting  20140806 01:18:57,941:17458(0x2b46794a0120):zooinfo@zookeeperclose@2522: freeing zookeeper resources for sessionid=0    i0806 01:18:57.949972 17458 contender.cpp:186] now cancelling the membership: 1  201408 06 01:1...",1,train
MESOS-1677,AllocatorTest.FrameworkReregistersFirst is flaky.,"  gmock warning:  uninteresting mock function call  taking default action specified at:  ../../../mesos/src/tests/mesos.hpp:566:      function call: resourcesrecovered(@0x7f38f40043e8 20140806190304208117018636159245110000, @0x7f38f40043c8 20140806190304208117018636159245110, @0x7f38f40043b0 )  ",2,train
MESOS-1683,Create user doc for framework rate limiting feature,create a markdown doc under /docs,2,train
MESOS-1690,Expose metric for container destroy failures,increment counter when container destroy fails.,3,train
MESOS-1694,Future::failure should return a const string&,nan,1,train
MESOS-1695,"The stats.json endpoint on the slave exposes ""registered"" as a string.","the slave is currently exposing a string value for the ""registered"" statistic, this should be a number:      slave:5051/stats.json        should be a pretty straightforward fix, looks like this first originated back in 2013:      commit b8291304e1523eb67ea8dc5f195cdb0d8e7d8348  author: vinod kone /  date:   wed jul 3 12:37:36 2013 0700        added a ""registered"" key/value pair to slave's stats.json.        review: https:/reviews.apache.org/r/12256    diff git a/src/slave/http.cpp b/src/slave/http.cpp  index dc2955f..dd51516 100644   a/src/slave/http.cpp   b/src/slave/http.cpp  @@  281,6 281,8 @@ future/ slave::http::stats(const request& request)     object.values[""losttasks""] = slave.stats.tasks[tasklost];     object.values[""validstatusupdates""] = slave.stats.validstatusupdates;     object.values[""invalidstatusupdates""] = slave.stats.invalidstatusupdates;    object.values[""registered""] = slave.master ? ""1"" : ""0"";         return ok(object, request.query.get(""jsonp""));   }  ",1,train
MESOS-1696,Improve reconciliation between master and slave.,"as we update the master to keep tasks in memory until they are both terminal and acknowledged (mesos1410), the lifetime of tasks in mesos will look as follows:      master           slave   {}               {}                {}  / master receives task t, nonterminal. forwards to slave.                / slave receives task t, nonterminal.                / task becomes terminal on slave. update forwarded.                / master receives update, forwards to framework.   {}               / master receives ack, forwards to slave.   {}               {}  / slave receives ack.      in the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. at any point in the above lifecycle, the slave's reregistration message can reach the master.    note the following properties:    (1) the master may have a nonterminal task, not present in the slave's reregistration message.  (2) the master may have a nonterminal task, present in the slave's reregistration message but in a different state.  (3) the slave's reregistration message may contain a terminal unacknowledged task unknown to the master.    in the current master / slave https:/github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#l3146 code, the master assumes that case (1) is because a launch task message was dropped, and it sends tasklost. we've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!    after chatting with , we're considering updating the reconciliation to occur as follows:       slave sends all tasks that are not both terminal and acknowledged, during reregistration. this is the same as before.     if the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. this can be piggybacked on the reregistration message.     the slave will send tasklost if the task is not known to it. preferably in a retried manner, unless we update socket closure on the slave to force a re registration.",3,train
MESOS-1698,make check segfaults,observed this on apache ci: https:/builds.apache.org/job/mesostrunkubuntubuildoutofsrcsetjavahome/2331/consolefull    it looks like the segfault happens before any tests are run. so i suspect somewhere in the setup phase of the tests.      mv f .deps/teststimetests.tpo .deps/teststimetests.po  /bin/bash ./libtool  tag=cxx   mode=link g  g g2 o2 wnounusedlocaltypedefs std=c11   o tests testsdecodertests.o testsencodertests.o testshttptests.o testsiotests.o testsmain.o testsmutextests.o testsmetricstests.o testsownedtests.o testsprocesstests.o testsqueuetests.o testsreaptests.o testssequencetests.o testssharedtests.o testsstatisticstests.o testssubprocesstests.o testssystemtests.o teststimeseriestests.o teststimetests.o 3rdparty/libgmock.la libprocess.la 3rdparty/glog0.3.3/libglog.la 3rdparty/libryhttpparser.la 3rdparty/libev4.15/libev.la lz  lrt  libtool: link: g g g2 o2 wnounusedlocaltypedefs std=c11 o tests testsdecodertests.o testsencodertests.o testshttptests.o testsiotests.o testsmain.o testsmutextests.o testsmetricstests.o testsownedtests.o testsprocesstests.o testsqueuetests.o testsreaptests.o testssequencetests.o testssharedtests.o testsstatisticstests.o testssubprocesstests.o testssystemtests.o teststimeseriestests.o teststimetests.o  3rdparty/.libs/libgmock.a ./.libs/libprocess.a /home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess/3rdparty/glog0.3.3/.libs/libglog.a /home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess/3rdparty/libev4.15/.libs/libev.a 3rdparty/glog0.3.3/.libs/libglog.a lpthread 3rdparty/.libs/libryhttpparser.a 3rdparty/libev4.15/.libs/libev.a lm lz lrt  make[5]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess'  make  checklocal  make[5]: entering directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess'  ./tests  note: google test filter =   [==========] running 0 tests from 0 test cases.  [==========] 0 tests from 0 test cases ran. (0 ms total)  [  passed  ] 0 tests.      you have 3 disabled tests    make[5]:  [checklocal] segmentation fault  make[5]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess'  make[4]:  [checkam] error 2  make[4]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess'  make[3]:  [checkrecursive] error 1  make[3]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty/libprocess'  make[2]:  [checkrecursive] error 1  make[2]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty'  make[1]:  [check] error 2  make[1]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcsetjavahome/build/3rdparty'  make:  [checkrecursive] error 1  build step 'execute shell' marked build as failure  sending e mails to: dev@mesos.apache.org benjamin.hindman@gmail.com dhamon@twitter.com yujie.jay@gmail.com  finished: failure  ,2,train
MESOS-1702,Add document for network monitoring.,the doc should tell the user how to use the new network monitoring feature.,2,train
MESOS-1703,better error message when replicated log hasn't been initialized,"aurora uses the mesos replicated log.      if you don't run ""mesoslog initialize"" before starting aurora you'll get info messages in your aurora log:      i0814 15:18:38.346638 25141 replica.cpp:633] replica in empty status received a broadcasted recover request   i0814 15:18:38.346796 25132 recover.cpp:220] received a recover response from a replica in empty status      it is has been deemed too dangerous to automatically run mesoslog initialize for the user (see aurora243).     it would be helpful if that error message was made more friendly and at the error level.  the message could explain what the user should do and the implications of doing so.  links to the docs would be helpful.    see http:/wilderness.apache.org/channels/?f=aurora/201408 14#1408055261 for context",1,train
MESOS-1705,SubprocessTest.Status sometimes flakes out,"it's a pretty rare event, but happened more then once.      [ run      ] subprocesstest.status   aborted at 1408023909 (unix time) try ""date  d @1408023909"" if you are using gnu date   pc: @       0x35700094b1 (unknown)   sigterm (@0x3e8000041d8) received by pid 16872 (tid 0x7fa9ea426780) from pid 16856; stack trace:       @       0x3570435cb0 (unknown)      @       0x35700094b1 (unknown)      @       0x3570009d9f (unknown)      @       0x357000e726 (unknown)      @       0x3570015185 (unknown)      @           0x5ead42 process::childmain()      @           0x5ece8d std::functionhandler/::minvoke()      @           0x5eac9c process::defaultclone()      @           0x5ebbd4 process::subprocess()      @           0x55a229 process::subprocess()      @           0x55a846 process::subprocess()      @           0x54224c subprocessteststatustest::testbody()      @     0x7fa9ea460323 (unknown)      @     0x7fa9ea455b67 (unknown)      @     0x7fa9ea455c0e (unknown)      @     0x7fa9ea455d15 (unknown)      @     0x7fa9ea4593a8 (unknown)      @     0x7fa9ea459647 (unknown)      @           0x422466 main      @       0x3570421d65 (unknown)      @           0x4260bd (unknown)  [       ok ] subprocesstest.status (153 ms)",2,train
MESOS-1712,Automate disallowing of commits mixing mesos/libprocess/stout,"for various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. typically, it is up to the reviewee/reviewer to catch this.     it wold be nice to automate this via the pre commit hook .",2,train
MESOS-1715,The slave does not send pending tasks during re-registration.,"in what looks like an oversight, the pending tasks and executors in the slave (framework::pending) are not sent in the re registration message.    for tasks, this can lead to spurious task_lost notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",3,train
MESOS-1717,The slave does not show pending tasks in the JSON endpoints.,the slave does not show pending tasks in the /state.json endpoint.    this is a bit tricky to add since we rely on knowing the executor directory.,1,train
MESOS-1718,Command executor can overcommit the slave.,"currently we give a small amount of resources to the command executor, in addition to resources used by the command task:    https:/github.com/apache/mesos/blob/0.20.0rc1/src/slave/slave.cpp#l2448    executorinfo slave::getexecutorinfo(      const frameworkid& frameworkid,      const taskinfo& task)        this leads to an overcommit of the slave. ideally, for command tasks we can ""transfer"" all of the task resources to the executor at the slave / isolation level.",3,train
MESOS-1727,"Configure fails with ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'","i followed the ""getting started"" documentation and did:    $ git clone http:/gitwipus.apache.org/repos/asf/mesos.git; cd mesos  $ ./bootstrap  $ mkdir build; cd build  $ ../configure    which aborts with    ....  ....  checking whether we are using the gnu c compiler... (cached) yes  checking whether gcc accepts  g... (cached) yes  checking for gcc option to accept iso c89... (cached) none needed  checking dependency style of gcc... (cached) gcc3  ../configure: line 18439: syntax error near unexpected token `protobufprefix,'  ../configure: line 18439: `  pkgcheckmodules(protobufprefix,'  ",2,train
MESOS-1728,Libprocess: report bind parameters on failure,"when you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to bind call that failed.",1,train
MESOS-1733,"Change the stout path utility to declare a single, variadic 'join' function instead of several separate declarations of various discrete arities",nan,5,train
MESOS-1739,Allow slave reconfiguration on restart,"make it so that either via a slave restart or a out of process ""reconfigure"" ping, the attributes and resources of a slave can be updated to be a superset of what they used to be.",3,train
MESOS-1748,MasterZooKeeperTest.LostZooKeeperCluster is flaky,  tests/mastertests.cpp:1795: failure  failed to wait 10secs for slaveregisteredmessage      should have placed the futuremessage that attempts to capture this messages before the slave starts...,1,train
MESOS-1749,SlaveRecoveryTest.ShutdownSlave is flaky,"  [ run      ] slaverecoverytest/0.shutdownslave  using temporary directory '/tmp/slaverecoverytest0shutdownslave3o5eps'  i0828 21:21:46.206990 27625 leveldb.cpp:176] opened db in 24.461837ms  i0828 21:21:46.213706 27625 leveldb.cpp:183] compacted db in 6.021499ms  i0828 21:21:46.214047 27625 leveldb.cpp:198] created db iterator in 5566ns  i0828 21:21:46.214313 27625 leveldb.cpp:204] seeked to beginning of db in 1433ns  i0828 21:21:46.214515 27625 leveldb.cpp:273] iterated through 0 keys in the db in 723ns  i0828 21:21:46.214826 27625 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0828 21:21:46.215409 27642 recover.cpp:425] starting replica recovery  i0828 21:21:46.215718 27642 recover.cpp:451] replica is in empty status  i0828 21:21:46.216264 27642 replica.cpp:638] replica in empty status received a broadcasted recover request  i0828 21:21:46.216557 27642 recover.cpp:188] received a recover response from a replica in empty status  i0828 21:21:46.216917 27642 recover.cpp:542] updating replica status to starting  i0828 21:21:46.221271 27645 master.cpp:286] master 20140828212146168428794561327625 (saucy) started on 127.0.1.1:45613  i0828 21:21:46.221812 27645 master.cpp:332] master only allowing authenticated frameworks to register  i0828 21:21:46.222038 27645 master.cpp:337] master only allowing authenticated slaves to register  i0828 21:21:46.222250 27645 credentials.hpp:36] loading credentials for authentication from '/tmp/slaverecoverytest0shutdownslave3o5eps/credentials'  i0828 21:21:46.222585 27645 master.cpp:366] authorization enabled  i0828 21:21:46.222885 27642 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 5.596969ms  i0828 21:21:46.223085 27642 replica.cpp:320] persisted replica status to starting  i0828 21:21:46.223424 27642 recover.cpp:451] replica is in starting status  i0828 21:21:46.223933 27642 replica.cpp:638] replica in starting status received a broadcasted recover request  i0828 21:21:46.224984 27642 recover.cpp:188] received a recover response from a replica in starting status  i0828 21:21:46.225385 27642 recover.cpp:542] updating replica status to voting  i0828 21:21:46.224750 27646 master.cpp:1205] the newly elected leader is master@127.0.1.1:45613 with id 20140828212146168428794561327625  i0828 21:21:46.226132 27646 master.cpp:1218] elected as the leading master!  i0828 21:21:46.226349 27646 master.cpp:1036] recovering from registrar  i0828 21:21:46.226637 27646 registrar.cpp:313] recovering registrar  i0828 21:21:46.224473 27641 master.cpp:120] no whitelist given. advertising offers for all slaves  i0828 21:21:46.224431 27645 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@127.0.1.1:45613  i0828 21:21:46.240932 27642 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 15.182422ms  i0828 21:21:46.241453 27642 replica.cpp:320] persisted replica status to voting  i0828 21:21:46.241926 27643 recover.cpp:556] successfully joined the paxos group  i0828 21:21:46.242228 27642 recover.cpp:440] recover process terminated  i0828 21:21:46.242501 27645 log.cpp:656] attempting to start the writer  i0828 21:21:46.243247 27645 replica.cpp:474] replica received implicit promise request with proposal 1  i0828 21:21:46.253456 27645 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 9.95472ms  i0828 21:21:46.253955 27645 replica.cpp:342] persisted promised to 1  i0828 21:21:46.254518 27645 coordinator.cpp:230] coordinator attemping to fill missing position  i0828 21:21:46.255234 27641 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0828 21:21:46.263128 27641 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 7.484042ms  i0828 21:21:46.263536 27641 replica.cpp:676] persisted action at 0  i0828 21:21:46.263806 27641 replica.cpp:508] replica received write request for position 0  i0828 21:21:46.263834 27641 leveldb.cpp:438] reading position from leveldb took 14063ns  i0828 21:21:46.276149 27641 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 12.295476ms  i0828 21:21:46.276178 27641 replica.cpp:676] persisted action at 0  i0828 21:21:46.276319 27641 replica.cpp:655] replica received learned notice for position 0  i0828 21:21:46.285523 27641 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 9.185244ms  i0828 21:21:46.285552 27641 replica.cpp:676] persisted action at 0  i0828 21:21:46.285560 27641 replica.cpp:661] replica learned nop action at position 0  i0828 21:21:46.289685 27642 log.cpp:672] writer started with ending position 0  i0828 21:21:46.290166 27642 leveldb.cpp:438] reading position from leveldb took 14463ns  i0828 21:21:46.297260 27642 registrar.cpp:346] successfully fetched the registry (0b)  i0828 21:21:46.297622 27642 registrar.cpp:422] attempting to update the 'registry'  i0828 21:21:46.298893 27645 log.cpp:680] attempting to append 118 bytes to the log  i0828 21:21:46.299190 27645 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0828 21:21:46.299643 27645 replica.cpp:508] replica received write request for position 1  i0828 21:21:46.310351 27645 leveldb.cpp:343] persisting action (135 bytes) to leveldb took 10.349409ms  i0828 21:21:46.310577 27645 replica.cpp:676] persisted action at 1  i0828 21:21:46.311039 27645 replica.cpp:655] replica received learned notice for position 1  i0828 21:21:46.322127 27645 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 10.858061ms  i0828 21:21:46.322614 27645 replica.cpp:676] persisted action at 1  i0828 21:21:46.322875 27645 replica.cpp:661] replica learned append action at position 1  i0828 21:21:46.323480 27645 registrar.cpp:479] successfully updated 'registry'  i0828 21:21:46.323874 27645 registrar.cpp:372] successfully recovered registrar  i0828 21:21:46.323649 27639 log.cpp:699] attempting to truncate the log to 1  i0828 21:21:46.324465 27644 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0828 21:21:46.324988 27644 replica.cpp:508] replica received write request for position 2  i0828 21:21:46.325335 27643 master.cpp:1063] recovered 0 slaves from the registry (82b) ; allowing 10mins for slaves to reregister  i0828 21:21:46.335847 27644 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 10.651398ms  i0828 21:21:46.336320 27644 replica.cpp:676] persisted action at 2  i0828 21:21:46.336896 27644 replica.cpp:655] replica received learned notice for position 2  i0828 21:21:46.345854 27644 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 8.540555ms  i0828 21:21:46.346261 27644 leveldb.cpp:401] deleting ~1 keys from leveldb took 30183ns  i0828 21:21:46.346282 27644 replica.cpp:676] persisted action at 2  i0828 21:21:46.346315 27644 replica.cpp:661] replica learned truncate action at position 2  i0828 21:21:46.356840 27625 containerizer.cpp:89] using isolation: posix/cpu,posix/mem  i0828 21:21:46.361413 27644 slave.cpp:167] slave started on 48)@127.0.1.1:45613  i0828 21:21:46.361753 27644 credentials.hpp:84] loading credential for authentication from '/tmp/slaverecoverytest0shutdownslaveumhraw/credential'  i0828 21:21:46.362046 27644 slave.cpp:274] slave using credential for: testprincipal  i0828 21:21:46.362810 27644 slave.cpp:287] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0828 21:21:46.363088 27644 slave.cpp:315] slave hostname: saucy  i0828 21:21:46.363301 27644 slave.cpp:316] slave checkpoint: true  i0828 21:21:46.363986 27644 state.cpp:33] recovering state from '/tmp/slaverecoverytest0shutdownslaveumhraw/meta'  i0828 21:21:46.364308 27644 statusupdatemanager.cpp:193] recovering status update manager  i0828 21:21:46.364600 27644 containerizer.cpp:252] recovering containerizer  i0828 21:21:46.365325 27646 slave.cpp:3204] finished recovery  i0828 21:21:46.365839 27646 slave.cpp:598] new master detected at master@127.0.1.1:45613  i0828 21:21:46.366041 27646 slave.cpp:672] authenticating with master master@127.0.1.1:45613  i0828 21:21:46.366317 27646 slave.cpp:645] detecting new master  i0828 21:21:46.366569 27646 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:45613  i0828 21:21:46.366827 27646 authenticatee.hpp:128] creating new client sasl connection  i0828 21:21:46.367204 27646 master.cpp:3637] authenticating slave(48)@127.0.1.1:45613  i0828 21:21:46.367553 27646 authenticator.hpp:156] creating new server sasl connection  i0828 21:21:46.367857 27646 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0828 21:21:46.368031 27646 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0828 21:21:46.368228 27646 authenticator.hpp:262] received sasl authentication start  i0828 21:21:46.368444 27646 authenticator.hpp:384] authentication requires more steps  i0828 21:21:46.368648 27646 authenticatee.hpp:265] received sasl authentication step  i0828 21:21:46.368924 27646 authenticator.hpp:290] received sasl authentication step  i0828 21:21:46.369120 27646 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'saucy' server fqdn: 'saucy' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0828 21:21:46.369350 27646 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0828 21:21:46.369544 27646 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0828 21:21:46.369730 27646 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'saucy' server fqdn: 'saucy' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0828 21:21:46.369958 27646 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0828 21:21:46.370131 27646 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0828 21:21:46.370311 27646 authenticator.hpp:376] authentication success  i0828 21:21:46.370518 27646 authenticatee.hpp:305] authentication success  i0828 21:21:46.370637 27642 master.cpp:3677] successfully authenticated principal 'testprincipal' at slave(48)@127.0.1.1:45613  i0828 21:21:46.371772 27641 slave.cpp:729] successfully authenticated with master master@127.0.1.1:45613  i0828 21:21:46.371984 27641 slave.cpp:980] will retry registration in 15.311045ms if necessary  i0828 21:21:46.372643 27641 master.cpp:2836] registering slave at slave(48)@127.0.1.1:45613 (saucy) with id 201408282121461684287945613276250  i0828 21:21:46.373016 27641 registrar.cpp:422] attempting to update the 'registry'  i0828 21:21:46.374539 27641 log.cpp:680] attempting to append 289 bytes to the log  i0828 21:21:46.374876 27641 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0828 21:21:46.375296 27641 replica.cpp:508] replica received write request for position 3  i0828 21:21:46.376046 27625 sched.cpp:137] version: 0.21.0  i0828 21:21:46.376374 27646 sched.cpp:233] new master detected at master@127.0.1.1:45613  i0828 21:21:46.376595 27646 sched.cpp:283] authenticating with master master@127.0.1.1:45613  i0828 21:21:46.376857 27646 authenticatee.hpp:128] creating new client sasl connection  i0828 21:21:46.377234 27646 master.cpp:3637] authenticating schedulercb5a026423cc45d0bc4ca92fa5308158@127.0.1.1:45613  i0828 21:21:46.377496 27646 authenticator.hpp:156] creating new server sasl connection  i0828 21:21:46.377771 27646 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0828 21:21:46.377961 27646 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0828 21:21:46.378170 27646 authenticator.hpp:262] received sasl authentication start  i0828 21:21:46.378360 27646 authenticator.hpp:384] authentication requires more steps  i0828 21:21:46.378588 27639 authenticatee.hpp:265] received sasl authentication step  i0828 21:21:46.378789 27646 authenticator.hpp:290] received sasl authentication step  i0828 21:21:46.378942 27646 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'saucy' server fqdn: 'saucy' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0828 21:21:46.379091 27646 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0828 21:21:46.379298 27646 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0828 21:21:46.379539 27646 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'saucy' server fqdn: 'saucy' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0828 21:21:46.379720 27646 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0828 21:21:46.379935 27646 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0828 21:21:46.380089 27646 authenticator.hpp:376] authentication success  i0828 21:21:46.380306 27642 authenticatee.hpp:305] authentication success  i0828 21:21:46.382625 27642 sched.cpp:357] successfully authenticated with master master@127.0.1.1:45613  i0828 21:21:46.383031 27642 sched.cpp:476] sending registration request to master@127.0.1.1:45613  i0828 21:21:46.382928 27640 master.cpp:3677] successfully authenticated principal 'testprincipal' at schedulercb5a026423cc45d0bc4ca92fa5308158@127.0.1.1:45613  i0828 21:21:46.383651 27640 master.cpp:1324] received registration request from schedulercb5a026423cc45d0bc4ca92fa5308158@127.0.1.1:45613  i0828 21:21:46.383846 27640 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0828 21:21:46.384184 27640 master.cpp:1383] registering framework 201408282121461684287945613276250000 at schedulercb5a026423cc45d0bc4ca92fa5308158@127.0.1.1:45613  i0828 21:21:46.384464 27640 sched.cpp:407] framework registered with 201408282121461684287945613276250000  i0828 21:21:46.384764 27640 sched.cpp:421] scheduler::registered took 18266ns  i0828 21:21:46.384600 27644 hierarchicalallocatorprocess.hpp:329] added framework 201408282121461684287945613276250000  i0828 21:21:46.385171 27644 hierarchicalallocatorprocess.hpp:691] no resources available to allocate!  i0828 21:21:46.385330 27644 hierarchicalallocatorprocess.hpp:653] performed allocation for 0 slaves in 160171ns  i0828 21:21:46.386292 27641 leveldb.cpp:343] persisting action (308 bytes) to leveldb took 10.815384ms  i0828 21:21:46.386492 27641 replica.cpp:676] persisted action at 3  i0828 21:21:46.386844 27641 replica.cpp:655] replica received learned notice for position 3  i0828 21:21:46.387980 27643 slave.cpp:980] will retry registration in 19.851524ms if necessary  i0828 21:21:46.388140 27639 master.cpp:2824] ignoring register slave message from slave(48)@127.0.1.1:45613 (saucy) as admission is already in progress  i0828 21:21:46.396355 27641 leveldb.cpp:343] persisting action (310 bytes) to leveldb took 9.275034ms  i0828 21:21:46.396641 27641 replica.cpp:676] persisted action at 3  i0828 21:21:46.396837 27641 replica.cpp:661] replica learned append action at position 3  i0828 21:21:46.397405 27641 registrar.cpp:479] successfully updated 'registry'  i0828 21:21:46.397528 27645 log.cpp:699] attempting to truncate the log to 3  i0828 21:21:46.397878 27645 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0828 21:21:46.398239 27645 replica.cpp:508] replica received write request for position 4  i0828 21:21:46.398597 27641 master.cpp:2876] registered slave 201408282121461684287945613276250 at slave(48)@127.0.1.1:45613 (saucy)  i0828 21:21:46.398870 27641 master.cpp:4110] adding slave 201408282121461684287945613276250 at slave(48)@127.0.1.1:45613 (saucy) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0828 21:21:46.399178 27639 slave.cpp:763] registered with master master@127.0.1.1:45613; given slave id 201408282121461684287945613276250  i0828 21:21:46.399521 27639 slave.cpp:776] checkpointing slaveinfo to '/tmp/slaverecoverytest0shutdownslaveumhraw/meta/slaves/201408282121461684287945613276250/slave.info'  i0828 21:21:46.399961 27641 hierarchicalallocatorprocess.hpp:442] added slave 201408282121461684287945613276250 (saucy) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0828 21:21:46.400316 27641 hierarchicalallocatorprocess.hpp:728] offering cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201408282121461684287945613276250 to framework 201408282121461684287945613276250000  i0828 21:21:46.400158 27644 slave.cpp:2333] received ping from slaveobserver(45)@127.0.1.1:45613  i0828 21:21:46.400872 27639 master.hpp:857] adding offer 201408282121461684287945613276250 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201408282121461684287945613276250 (saucy)  i0828 21:21:46.401105 27639 master.cpp:3584] sending 1 offers to framework 201408282121461684287945613276250000  i0828 21:21:46.401448 27639 sched.cpp:544] scheduler::resourceoffers took 19056ns  i0828 21:21:46.401700 27641 hierarchicalallocatorprocess.hpp:673] performed allocation for slave 201408282121461684287945613276250 in 1.430159ms  i0828 21:21:46.403659 27644 master.hpp:867] removing offer 201408282121461684287945613276250 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201408282121461684287945613276250 (saucy)  i0828 21:21:46.403903 27644 master.cpp:2194] processing reply for offers: [ 201408282121461684287945613276250 ] on slave 201408282121461684287945613276250 at slave(48)@127.0.1.1:45613 (saucy) for framework 201408282121461684287945613276250000  i0828 21:21:46.404116 27644 master.cpp:2277] authorizing framework principal 'testprincipal' to launch task cf5afc1bc007435b8c36be8aa3659d3a as user 'jenkins'  i0828 21:21:46.404578 27644 master.hpp:829] adding task cf5afc1bc007435b8c36be8aa3659d3a with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201408282121461684287945613276250 (saucy)  i0828 21:21:46.404824 27644 master.cpp:2343] launching task cf5afc1bc007435b8c36be8aa3659d3a of framework 201408282121461684287945613276250000 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 201408282121461684287945613276250 at slave(48)@127.0.1.1:45613 (saucy)  i0828 21:21:46.405206 27644 slave.cpp:1011] got assigned task cf5afc1bc007435b8c36be8aa3659d3a for framework 201408282121461684287945613276250000  i0828 21:21:46.405462 27644 slave.cpp:3542] checkpointing frameworkinfo to '/tmp/slaverecoverytest0shutdownslaveumhraw/meta/slaves/201408282121461684287945613276250/frameworks/201408282121461684287945613276250000/framework.info'  i0828 21:21:46.405840 27644 slave.cpp:3549] checkpointing framework pid 'schedulercb5a026423cc45d0bc4ca92fa5308158@127.0.1.1:45613' to '/tmp/slaverecoverytest0shutdownslaveumhraw/meta/slaves/201408282121461684287945613276250/frameworks/201408282121461684287945613276250000/framework.pid'  i0828 21:21:46.406122 27645 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 7.684731ms  i0828 21:21:46.406288 27645 replica.cpp:676] persisted action at 4  i0828 21:21:46.406618 27645 replica.cpp:655] replica received learned notice for position 4  i0828 21:21:46.407562 27644 slave.cpp:1121] launching task cf5afc1bc007435b8c36be8aa3659d3a for framework 20140828212146168428794561327625 0000  i0828 21:21:46.409296 27644 slave.cpp:3858] checkpointing executorinfo to '/tmp/slaverecoverytest0shutdownslaveumhraw/meta/slaves/20...",2,train
MESOS-1751,"Request for ""stats.json"" cannot be fulfilled after stopping the framework ","request for ""stats.json"" to master from a test case doesn't work after calling frameworks' driver.stop(). however, it works for ""state.json"". i think the problem is related to stats() continuation stats(). the following test illustrates the issue:    testf(mastertest, requestafterdriverstop)    ",5,train
MESOS-1752,Allow variadic templates,"add variadic templates to the c11 configure check. once there, we can start using them in the code base.",1,train
MESOS-1758,Freezer failure leads to lost task during container destruction.,"in the past we've seen numerous issues around the freezer. lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:    (1) an oom occurs.  (2) no indication of oom in the kernel logs.  (3) the slave is unable to freeze the cgroup.  (4) the task is marked as lost.      i0903 16:46:24.956040 25469 mem.cpp:575] memory limit exceeded: requested: 15488mb maximum used: 15488mb    memory statistics:  cache 7958691840  rss 8281653248  mappedfile 9474048  pgpgin 4487861  pgpgout 522933  pgfault 2533780  pgmajfault 11  inactiveanon 0  activeanon 8281653248  inactivefile 7631708160  activefile 326852608  unevictable 0  hierarchicalmemorylimit 16240345088  totalcache 7958691840  totalrss 8281653248  totalmappedfile 9474048  totalpgpgin 4487861  totalpgpgout 522933  totalpgfault 2533780  totalpgmajfault 11  totalinactiveanon 0  totalactiveanon 8281653248  totalinactivefile 7631728640  totalactivefile 326852608  totalunevictable 0  i0903 16:46:24.956848 25469 containerizer.cpp:1041] container bbb9732ad6004c1bb326846338c608c3 has reached its limit for resource mem( ):1.62403e+10 and will be terminated  i0903 16:46:24.957427 25469 containerizer.cpp:909] destroying container 'bbb9732ad6004c1bb326846338c608c3'  i0903 16:46:24.958664 25481 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:34.959529 25488 cgroups.cpp:2209] thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:34.962070 25482 cgroups.cpp:1404] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3 after 1.710848ms  i0903 16:46:34.962658 25479 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:44.963349 25488 cgroups.cpp:2209] thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:44.965631 25472 cgroups.cpp:1404] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3 after 1.588224ms  i0903 16:46:44.966356 25472 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:54.967254 25488 cgroups.cpp:2209] thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:46:56.008447 25475 cgroups.cpp:1404] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3 after 2.15296ms  i0903 16:46:56.009071 25466 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:47:06.010329 25488 cgroups.cpp:2209] thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:47:06.012538 25467 cgroups.cpp:1404] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3 after 1.643008ms  i0903 16:47:06.013216 25467 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:47:12.516348 25480 slave.cpp:3030] current usage 9.57%. max allowed age: 5.630238827780799days  i0903 16:47:16.015192 25488 cgroups.cpp:2209] thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:47:16.017043 25486 cgroups.cpp:1404] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3 after 1.511168ms  i0903 16:47:16.017555 25480 cgroups.cpp:2192] freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732ad6004c1bb326846338c608c3  i0903 16:47:19.862746 25483 http.cpp:245] http request for '/slave(1)/stats.json'  e0903 16:47:24.960055 25472 slave.cpp:2557] termination of executor 'e' of framework '20110407000400000025630000' failed: failed to destroy container: discarded future  i0903 16:47:24.962054 25472 slave.cpp:2087] handling status update tasklost (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000 from @0.0.0.0:0  i0903 16:47:24.963470 25469 mem.cpp:293] updated 'memory.softlimitinbytes' to 128mb for container bbb9732ad6004c1bb326846338c608c3  i0903 16:47:24.963541 25471 cpushare.cpp:338] updated 'cpu.shares' to 256 (cpus 0.25) for container bbb9732ad6004c1bb326846338c608c3  i0903 16:47:24.964756 25471 cpushare.cpp:359] updated 'cpu.cfsperiodus' to 100ms and 'cpu.cfsquotaus' to 25ms (cpus 0.25) for container bbb9732ad6004c1bb326846338c608c3  i0903 16:47:43.406610 25476 statusupdatemanager.cpp:320] received status update tasklost (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000  i0903 16:47:43.406991 25476 statusupdatemanager.hpp:342] checkpointing update for status update tasklost (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000  i0903 16:47:43.410475 25476 statusupdatemanager.cpp:373] forwarding status update tasklost (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000 to master@/:5050  i0903 16:47:43.439923 25480 statusupdatemanager.cpp:398] received status update acknowledgement (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000  i0903 16:47:43.440115 25480 statusupdatemanager.hpp:342] checkpointing ack for status update task_lost (uuid: c0c1633b722140dc90a2660ef639f747) for task t of framework 20110407000400000025630000  i0903 16:47:43.443595 25480 slave.cpp:2709] cleaning up executor 'e' of framework 20110407000400000025630000      we should consider avoiding the freezer entirely in favor of a kill(2) loop. we don't have to wait for pid namespaces to remove the freezer dependency.    at the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup.",2,train
MESOS-1760,MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky,"observed this on apache ci: https:/builds.apache.org/job/mesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebui/2355/changes      [ run] masterauthorizationtest.frameworkremovedbeforereregistration  using temporary directory '/tmp/masterauthorizationtestframeworkremovedbeforereregistration0tw16z'  i0903 22:04:33.520237 25565 leveldb.cpp:176] opened db in 49.073821ms  i0903 22:04:33.538331 25565 leveldb.cpp:183] compacted db in 18.065051ms  i0903 22:04:33.538363 25565 leveldb.cpp:198] created db iterator in 4826ns  i0903 22:04:33.538377 25565 leveldb.cpp:204] seeked to beginning of db in 682ns  i0903 22:04:33.538385 25565 leveldb.cpp:273] iterated through 0 keys in the db in 312ns  i0903 22:04:33.538399 25565 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0903 22:04:33.538624 25593 recover.cpp:425] starting replica recovery  i0903 22:04:33.538707 25598 recover.cpp:451] replica is in empty status  i0903 22:04:33.540909 25590 master.cpp:286] master 201409032204334537598844412225565 (hemera.apache.org) started on 140.211.11.27:44122  i0903 22:04:33.540932 25590 master.cpp:332] master only allowing authenticated frameworks to register  i0903 22:04:33.540936 25590 master.cpp:337] master only allowing authenticated slaves to register  i0903 22:04:33.540941 25590 credentials.hpp:36] loading credentials for authentication from '/tmp/masterauthorizationtestframeworkremovedbeforereregistration0tw16z/credentials'  i0903 22:04:33.541337 25590 master.cpp:366] authorization enabled  i0903 22:04:33.541508 25597 replica.cpp:638] replica in empty status received a broadcasted recover request  i0903 22:04:33.542343 25582 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@140.211.11.27:44122  i0903 22:04:33.542445 25592 master.cpp:120] no whitelist given. advertising offers for all slaves  i0903 22:04:33.543175 25602 recover.cpp:188] received a recover response from a replica in empty status  i0903 22:04:33.543637 25587 recover.cpp:542] updating replica status to starting  i0903 22:04:33.544256 25579 master.cpp:1205] the newly elected leader is master@140.211.11.27:44122 with id 201409032204334537598844412225565  i0903 22:04:33.544275 25579 master.cpp:1218] elected as the leading master!  i0903 22:04:33.544282 25579 master.cpp:1036] recovering from registrar  i0903 22:04:33.544401 25579 registrar.cpp:313] recovering registrar  i0903 22:04:33.558487 25593 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 14.678563ms  i0903 22:04:33.558531 25593 replica.cpp:320] persisted replica status to starting  i0903 22:04:33.558653 25593 recover.cpp:451] replica is in starting status  i0903 22:04:33.559867 25588 replica.cpp:638] replica in starting status received a broadcasted recover request  i0903 22:04:33.560057 25602 recover.cpp:188] received a recover response from a replica in starting status  i0903 22:04:33.561280 25584 recover.cpp:542] updating replica status to voting  i0903 22:04:33.576900 25581 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 14.712427ms  i0903 22:04:33.576942 25581 replica.cpp:320] persisted replica status to voting  i0903 22:04:33.577018 25581 recover.cpp:556] successfully joined the paxos group  i0903 22:04:33.577108 25581 recover.cpp:440] recover process terminated  i0903 22:04:33.577401 25581 log.cpp:656] attempting to start the writer  i0903 22:04:33.578559 25589 replica.cpp:474] replica received implicit promise request with proposal 1  i0903 22:04:33.594611 25589 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 16.029152ms  i0903 22:04:33.594640 25589 replica.cpp:342] persisted promised to 1  i0903 22:04:33.595391 25584 coordinator.cpp:230] coordinator attemping to fill missing position  i0903 22:04:33.597512 25588 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0903 22:04:33.613037 25588 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 15.502568ms  i0903 22:04:33.613065 25588 replica.cpp:676] persisted action at 0  i0903 22:04:33.615435 25585 replica.cpp:508] replica received write request for position 0  i0903 22:04:33.615463 25585 leveldb.cpp:438] reading position from leveldb took 10743ns  i0903 22:04:33.630801 25585 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 15.320225ms  i0903 22:04:33.630852 25585 replica.cpp:676] persisted action at 0  i0903 22:04:33.631126 25585 replica.cpp:655] replica received learned notice for position 0  i0903 22:04:33.647801 25585 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 16.652951ms  i0903 22:04:33.647830 25585 replica.cpp:676] persisted action at 0  i0903 22:04:33.647842 25585 replica.cpp:661] replica learned nop action at position 0  i0903 22:04:33.648548 25583 log.cpp:672] writer started with ending position 0  i0903 22:04:33.649235 25583 leveldb.cpp:438] reading position from leveldb took 25209ns  i0903 22:04:33.650897 25591 registrar.cpp:346] successfully fetched the registry (0b)  i0903 22:04:33.650930 25591 registrar.cpp:422] attempting to update the 'registry'  i0903 22:04:33.652861 25601 log.cpp:680] attempting to append 138 bytes to the log  i0903 22:04:33.653097 25586 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0903 22:04:33.655225 25590 replica.cpp:508] replica received write request for position 1  i0903 22:04:33.669618 25590 leveldb.cpp:343] persisting action (157 bytes) to leveldb took 14.337486ms  i0903 22:04:33.669663 25590 replica.cpp:676] persisted action at 1  i0903 22:04:33.670045 25584 replica.cpp:655] replica received learned notice for position 1  i0903 22:04:34.414243 25584 leveldb.cpp:343] persisting action (159 bytes) to leveldb took 15.401247ms  i0903 22:04:34.414300 25584 replica.cpp:676] persisted action at 1  i0903 22:04:34.414316 25584 replica.cpp:661] replica learned append action at position 1  i0903 22:04:34.414937 25589 registrar.cpp:479] successfully updated 'registry'  i0903 22:04:34.415069 25585 log.cpp:699] attempting to truncate the log to 1  i0903 22:04:34.415194 25589 registrar.cpp:372] successfully recovered registrar  i0903 22:04:34.415284 25589 master.cpp:1063] recovered 0 slaves from the registry (100b) ; allowing 10mins for slaves to reregister  i0903 22:04:34.415362 25587 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0903 22:04:34.418926 25597 replica.cpp:508] replica received write request for position 2  i0903 22:04:34.434321 25597 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 15.368147ms  i0903 22:04:34.434352 25597 replica.cpp:676] persisted action at 2  i0903 22:04:34.435022 25582 replica.cpp:655] replica received learned notice for position 2  i0903 22:04:34.450331 25582 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 15.284486ms  i0903 22:04:34.450387 25582 leveldb.cpp:401] deleting ~1 keys from leveldb took 25774ns  i0903 22:04:34.450402 25582 replica.cpp:676] persisted action at 2  i0903 22:04:34.450412 25582 replica.cpp:661] replica learned truncate action at position 2  i0903 22:04:34.460691 25565 sched.cpp:137] version: 0.21.0  i0903 22:04:34.460927 25582 sched.cpp:233] new master detected at master@140.211.11.27:44122  i0903 22:04:34.460948 25582 sched.cpp:283] authenticating with master master@140.211.11.27:44122  i0903 22:04:34.461359 25582 authenticatee.hpp:128] creating new client sasl connection  i0903 22:04:34.461647 25582 master.cpp:3637] authenticating scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.461801 25598 authenticator.hpp:156] creating new server sasl connection  i0903 22:04:34.462172 25598 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0903 22:04:34.462185 25598 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0903 22:04:34.462257 25598 authenticator.hpp:262] received sasl authentication start  i0903 22:04:34.462323 25598 authenticator.hpp:384] authentication requires more steps  i0903 22:04:34.462345 25598 authenticatee.hpp:265] received sasl authentication step  i0903 22:04:34.462417 25598 authenticator.hpp:290] received sasl authentication step  i0903 22:04:34.462522 25598 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0903 22:04:34.462529 25598 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0903 22:04:34.462538 25598 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0903 22:04:34.462543 25598 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0903 22:04:34.462548 25598 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0903 22:04:34.462550 25598 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0903 22:04:34.462558 25598 authenticator.hpp:376] authentication success  i0903 22:04:34.462635 25598 master.cpp:3677] successfully authenticated principal 'testprincipal' at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.462687 25590 authenticatee.hpp:305] authentication success  i0903 22:04:34.463219 25588 sched.cpp:357] successfully authenticated with master master@140.211.11.27:44122  i0903 22:04:34.463243 25588 sched.cpp:476] sending registration request to master@140.211.11.27:44122  i0903 22:04:34.463307 25588 master.cpp:1324] received registration request from scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.463330 25588 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0903 22:04:34.463412 25588 master.cpp:1383] registering framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.463577 25598 sched.cpp:407] framework registered with 2014090322043345375988444122255650000  i0903 22:04:34.463728 25587 hierarchicalallocatorprocess.hpp:329] added framework 2014090322043345375988444122255650000  i0903 22:04:34.463739 25587 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0903 22:04:34.463743 25587 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 5016ns  i0903 22:04:34.463755 25598 sched.cpp:421] scheduler::registered took 165035ns  i0903 22:04:34.465558 25583 sched.cpp:227] scheduler::disconnected took 6254ns  i0903 22:04:34.465566 25583 sched.cpp:233] new master detected at master@140.211.11.27:44122  i0903 22:04:34.465575 25583 sched.cpp:283] authenticating with master master@140.211.11.27:44122  i0903 22:04:34.465642 25583 authenticatee.hpp:128] creating new client sasl connection  i0903 22:04:34.465790 25583 master.cpp:1680] deactivating framework 2014090322043345375988444122255650000  i0903 22:04:34.465850 25583 master.cpp:3637] authenticating scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.465879 25601 hierarchicalallocatorprocess.hpp:405] deactivated framework 2014090322043345375988444122255650000  i0903 22:04:34.466047 25600 authenticator.hpp:156] creating new server sasl connection  i0903 22:04:34.466315 25600 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0903 22:04:34.466326 25600 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0903 22:04:34.466346 25600 authenticator.hpp:262] received sasl authentication start  i0903 22:04:34.466418 25600 authenticator.hpp:384] authentication requires more steps  i0903 22:04:34.466436 25600 authenticatee.hpp:265] received sasl authentication step  i0903 22:04:34.466475 25600 authenticator.hpp:290] received sasl authentication step  i0903 22:04:34.466486 25600 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0903 22:04:34.466491 25600 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0903 22:04:34.466496 25600 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0903 22:04:34.466502 25600 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'hemera.apache.org' server fqdn: 'hemera.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0903 22:04:34.466506 25600 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0903 22:04:34.466509 25600 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0903 22:04:34.466516 25600 authenticator.hpp:376] authentication success  i0903 22:04:34.466596 25588 master.cpp:3677] successfully authenticated principal 'testprincipal' at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:34.466629 25597 authenticatee.hpp:305] authentication success  i0903 22:04:34.467062 25594 sched.cpp:357] successfully authenticated with master master@140.211.11.27:44122  i0903 22:04:34.467077 25594 sched.cpp:476] sending registration request to master@140.211.11.27:44122  i0903 22:04:34.467190 25588 master.cpp:1448] received reregistration request from framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:36.368134 25588 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0903 22:04:34.542999 25594 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0903 22:04:35.463639 25582 sched.cpp:476] sending registration request to master@140.211.11.27:44122  i0903 22:04:36.368185 25594 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 1.825177748secs  i0903 22:04:36.368302 25588 master.cpp:1448] received reregistration request from framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4@140.211.11.27:44122  i0903 22:04:36.368330 25588 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0903 22:04:36.368388 25582 sched.cpp:476] sending registration request to master@140.211.11.27:44122  : failure  mock function called more times than expected  returning default value.      function call: authorize(@0x2ba11964c1b0 40byte object /)      the mock function has no default action set, and its return type has no default value set.   aborted at 1409781876 (unix time) try ""date d @1409781876"" if you are using gnu date   i0903 22:04:36.368913 25598 sched.cpp:745] stopping framework '2014090322043345375988444122255650000'  pc: @     0x2ba117a990d5 (unknown)   sigabrt (@0x3ea000063dd) received by pid 25565 (tid 0x2ba11964d700) from pid 25565; stack trace:        @     0x2ba117854cb0 (unknown)      @     0x2ba117a990d5 (unknown)      @     0x2ba117a9c83b (unknown)      @           0x9cba9d testing::internal::googletestfailurereporter::reportfailure()      @           0x790091 testing::internal::functionmockerbase/::performdefaultaction()      @           0x790166 testing::internal::functionmockerbase/::untypedperformdefaultaction()      @           0x9c3daa testing::internal::untypedfunctionmockerbase::untypedinvokewith()      @           0x787279 mesos::internal::tests::mockauthorizer::authorize()      @     0x2ba1157c133d mesos::internal::master::master::validate()      @     0x2ba1157c2b7a mesos::internal::master::master::reregisterframework()      @     0x2ba1157e0038 protobufprocess/::handler2/()      @     0x2ba1157dde89 std::tr1::functionhandler/::minvoke()      @     0x2ba1157b15f7 mesos::internal::master::master::visit()      @     0x2ba1157bfa3e mesos::internal::master::master::visit()      @     0x2ba115caf5e7 process::processmanager::resume()      @     0x2ba115cb027c process::schedule()      @     0x2ba11784ce9a startthread      @     0x2ba117b5731d (unknown)  ",1,train
MESOS-1765,Use PID namespace to avoid freezing cgroup,"there is some known kernel issue when we freeze the whole cgroup upon oom. mesos probably can just use pid namespace so that we will only need to kill the ""init"" of the pid namespace, instead of freezing all the processes and killing them one by one. but i am not quite sure if this would break the existing code.",5,train
MESOS-1766,MasterAuthorizationTest.DuplicateRegistration test is flaky,"  [ run      ] masterauthorizationtest.duplicateregistration  using temporary directory '/tmp/masterauthorizationtestduplicateregistrationpvjg7m'  i0905 15:53:16.398993 25769 leveldb.cpp:176] opened db in 2.601036ms  i0905 15:53:16.399566 25769 leveldb.cpp:183] compacted db in 546216ns  i0905 15:53:16.399590 25769 leveldb.cpp:198] created db iterator in 2787ns  i0905 15:53:16.399605 25769 leveldb.cpp:204] seeked to beginning of db in 500ns  i0905 15:53:16.399617 25769 leveldb.cpp:273] iterated through 0 keys in the db in 185ns  i0905 15:53:16.399633 25769 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0905 15:53:16.399817 25786 recover.cpp:425] starting replica recovery  i0905 15:53:16.399952 25793 recover.cpp:451] replica is in empty status  i0905 15:53:16.400683 25795 replica.cpp:638] replica in empty status received a broadcasted recover request  i0905 15:53:16.400795 25787 recover.cpp:188] received a recover response from a replica in empty status  i0905 15:53:16.401005 25783 recover.cpp:542] updating replica status to starting  i0905 15:53:16.401470 25786 master.cpp:286] master 2014090515531631259205794918825769 (penates.apache.org) started on 67.195.81.186:49188  i0905 15:53:16.401521 25786 master.cpp:332] master only allowing authenticated frameworks to register  i0905 15:53:16.401533 25786 master.cpp:337] master only allowing authenticated slaves to register  i0905 15:53:16.401543 25786 credentials.hpp:36] loading credentials for authentication from '/tmp/masterauthorizationtestduplicateregistrationpvjg7m/credentials'  i0905 15:53:16.401558 25793 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 474683ns  i0905 15:53:16.401582 25793 replica.cpp:320] persisted replica status to starting  i0905 15:53:16.401667 25793 recover.cpp:451] replica is in starting status  i0905 15:53:16.401669 25786 master.cpp:366] authorization enabled  i0905 15:53:16.401898 25795 master.cpp:120] no whitelist given. advertising offers for all slaves  i0905 15:53:16.401936 25796 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@67.195.81.186:49188  i0905 15:53:16.402160 25784 replica.cpp:638] replica in starting status received a broadcasted recover request  i0905 15:53:16.402333 25790 master.cpp:1205] the newly elected leader is master@67.195.81.186:49188 with id 2014090515531631259205794918825769  i0905 15:53:16.402359 25790 master.cpp:1218] elected as the leading master!  i0905 15:53:16.402371 25790 master.cpp:1036] recovering from registrar  i0905 15:53:16.402472 25798 registrar.cpp:313] recovering registrar  i0905 15:53:16.402529 25791 recover.cpp:188] received a recover response from a replica in starting status  i0905 15:53:16.402782 25788 recover.cpp:542] updating replica status to voting  i0905 15:53:16.403002 25795 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 116403ns  i0905 15:53:16.403020 25795 replica.cpp:320] persisted replica status to voting  i0905 15:53:16.403081 25791 recover.cpp:556] successfully joined the paxos group  i0905 15:53:16.403197 25791 recover.cpp:440] recover process terminated  i0905 15:53:16.403388 25796 log.cpp:656] attempting to start the writer  i0905 15:53:16.403993 25784 replica.cpp:474] replica received implicit promise request with proposal 1  i0905 15:53:16.404147 25784 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 132156ns  i0905 15:53:16.404167 25784 replica.cpp:342] persisted promised to 1  i0905 15:53:16.404542 25795 coordinator.cpp:230] coordinator attemping to fill missing position  i0905 15:53:16.405498 25787 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0905 15:53:16.405868 25787 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 347231ns  i0905 15:53:16.405886 25787 replica.cpp:676] persisted action at 0  i0905 15:53:16.406553 25788 replica.cpp:508] replica received write request for position 0  i0905 15:53:16.406582 25788 leveldb.cpp:438] reading position from leveldb took 11402ns  i0905 15:53:16.529067 25788 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 535803ns  i0905 15:53:16.529088 25788 replica.cpp:676] persisted action at 0  i0905 15:53:16.529355 25784 replica.cpp:655] replica received learned notice for position 0  i0905 15:53:16.529784 25784 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 406036ns  i0905 15:53:16.529806 25784 replica.cpp:676] persisted action at 0  i0905 15:53:16.529817 25784 replica.cpp:661] replica learned nop action at position 0  i0905 15:53:16.530108 25783 log.cpp:672] writer started with ending position 0  i0905 15:53:16.530597 25792 leveldb.cpp:438] reading position from leveldb took 14594ns  i0905 15:53:16.532060 25787 registrar.cpp:346] successfully fetched the registry (0b)  i0905 15:53:16.532091 25787 registrar.cpp:422] attempting to update the 'registry'  i0905 15:53:16.533537 25785 log.cpp:680] attempting to append 140 bytes to the log  i0905 15:53:16.533596 25785 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0905 15:53:16.533998 25798 replica.cpp:508] replica received write request for position 1  i0905 15:53:16.534397 25798 leveldb.cpp:343] persisting action (159 bytes) to leveldb took 372452ns  i0905 15:53:16.534416 25798 replica.cpp:676] persisted action at 1  i0905 15:53:16.534808 25793 replica.cpp:655] replica received learned notice for position 1  i0905 15:53:16.534996 25793 leveldb.cpp:343] persisting action (161 bytes) to leveldb took 164609ns  i0905 15:53:16.535014 25793 replica.cpp:676] persisted action at 1  i0905 15:53:16.535025 25793 replica.cpp:661] replica learned append action at position 1  i0905 15:53:16.535368 25784 registrar.cpp:479] successfully updated 'registry'  i0905 15:53:16.535419 25784 registrar.cpp:372] successfully recovered registrar  i0905 15:53:16.535452 25785 log.cpp:699] attempting to truncate the log to 1  i0905 15:53:16.535555 25791 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0905 15:53:16.535553 25792 master.cpp:1063] recovered 0 slaves from the registry (102b) ; allowing 10mins for slaves to reregister  i0905 15:53:16.536038 25784 replica.cpp:508] replica received write request for position 2  i0905 15:53:16.536166 25784 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 101619ns  i0905 15:53:16.536185 25784 replica.cpp:676] persisted action at 2  i0905 15:53:16.536497 25791 replica.cpp:655] replica received learned notice for position 2  i0905 15:53:16.536633 25791 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 109281ns  i0905 15:53:16.536664 25791 leveldb.cpp:401] deleting ~1 keys from leveldb took 14164ns  i0905 15:53:16.536677 25791 replica.cpp:676] persisted action at 2  i0905 15:53:16.536689 25791 replica.cpp:661] replica learned truncate action at position 2  i0905 15:53:16.548408 25769 sched.cpp:137] version: 0.21.0  i0905 15:53:16.548627 25792 sched.cpp:233] new master detected at master@67.195.81.186:49188  i0905 15:53:16.548653 25792 sched.cpp:283] authenticating with master master@67.195.81.186:49188  i0905 15:53:16.548857 25797 authenticatee.hpp:128] creating new client sasl connection  i0905 15:53:16.548950 25797 master.cpp:3637] authenticating scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:16.549041 25797 authenticator.hpp:156] creating new server sasl connection  i0905 15:53:16.549120 25797 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0905 15:53:16.549141 25797 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0905 15:53:16.549180 25797 authenticator.hpp:262] received sasl authentication start  i0905 15:53:16.549229 25797 authenticator.hpp:384] authentication requires more steps  i0905 15:53:16.549268 25797 authenticatee.hpp:265] received sasl authentication step  i0905 15:53:16.549351 25787 authenticator.hpp:290] received sasl authentication step  i0905 15:53:16.549378 25787 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'penates.apache.org' server fqdn: 'penates.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0905 15:53:16.549391 25787 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0905 15:53:16.549403 25787 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0905 15:53:16.549415 25787 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'penates.apache.org' server fqdn: 'penates.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0905 15:53:16.549424 25787 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0905 15:53:16.549432 25787 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0905 15:53:16.549448 25787 authenticator.hpp:376] authentication success  i0905 15:53:16.549489 25787 authenticatee.hpp:305] authentication success  i0905 15:53:16.549525 25787 master.cpp:3677] successfully authenticated principal 'testprincipal' at scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:16.549669 25783 sched.cpp:357] successfully authenticated with master master@67.195.81.186:49188  i0905 15:53:16.549690 25783 sched.cpp:476] sending registration request to master@67.195.81.186:49188  i0905 15:53:16.549751 25787 master.cpp:1324] received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:16.549782 25787 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0905 15:53:16.551250 25791 sched.cpp:233] new master detected at master@67.195.81.186:49188  i0905 15:53:16.551273 25791 sched.cpp:283] authenticating with master master@67.195.81.186:49188  i0905 15:53:16.551357 25788 authenticatee.hpp:128] creating new client sasl connection  i0905 15:53:16.551456 25791 master.cpp:3637] authenticating scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:16.551553 25788 authenticator.hpp:156] creating new server sasl connection  i0905 15:53:16.551673 25786 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0905 15:53:16.551697 25786 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0905 15:53:16.551755 25792 authenticator.hpp:262] received sasl authentication start  i0905 15:53:16.551808 25792 authenticator.hpp:384] authentication requires more steps  i0905 15:53:16.551856 25792 authenticatee.hpp:265] received sasl authentication step  i0905 15:53:16.551920 25786 authenticator.hpp:290] received sasl authentication step  i0905 15:53:16.551949 25786 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'penates.apache.org' server fqdn: 'penates.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false  i0905 15:53:16.551966 25786 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0905 15:53:16.551985 25786 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0905 15:53:16.551997 25786 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'penates.apache.org' server fqdn: 'penates.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true  i0905 15:53:16.552006 25786 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0905 15:53:16.552014 25786 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0905 15:53:16.552031 25786 authenticator.hpp:376] authentication success  i0905 15:53:16.552081 25792 authenticatee.hpp:305] authentication success  i0905 15:53:16.552100 25786 master.cpp:3677] successfully authenticated principal 'testprincipal' at scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:16.552249 25792 sched.cpp:357] successfully authenticated with master master@67.195.81.186:49188  i0905 15:53:17.402861 25793 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0905 15:53:18.874348 25792 sched.cpp:476] sending registration request to master@67.195.81.186:49188  i0905 15:53:18.874364 25793 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 1.471501003secs  i0905 15:53:18.874420 25792 sched.cpp:476] sending registration request to master@67.195.81.186:49188  i0905 15:53:18.874451 25793 master.cpp:1324] received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:18.874480 25793 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  i0905 15:53:18.874565 25793 master.cpp:1324] received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5@67.195.81.186:49188  i0905 15:53:18.874588 25793 master.cpp:1284] authorizing framework principal 'testprincipal' to receive offers for role ''  : failure  mock function called more times than expected  returning default value.      function call: authorize(@0x2b9ed7fe9350 40byte object /)      the mock function has no default action set, and its return type has no default value set.   aborted at 1409932398 (unix time) try ""date  d @1409932398"" if you are using gnu date   pc: @     0x2b9ed6233f79 (unknown)   sigabrt (@0x95c000064a9) received by pid 25769 (tid 0x2b9ed7fea700) from pid 25769; stack trace:        @     0x2b9ed5fef340 (unknown)      @     0x2b9ed6233f79 (unknown)      @     0x2b9ed6237388 (unknown)      @           0x93a5ec testing::internal::googletestfailurereporter::reportfailure()      @           0x7296c5 testing::internal::functionmockerbase/::untypedperformdefaultaction()      @           0x933094 testing::internal::untypedfunctionmockerbase::untypedinvokewith()      @           0x71fbde mesos::internal::tests::mockauthorizer::authorize()      @     0x2b9ed4038caf mesos::internal::master::master::validate()      @     0x2b9ed4039763 mesos::internal::master::master::registerframework()      @     0x2b9ed40a0c0f protobufprocess/::handler1/()      @     0x2b9ed4050c57 std::functionhandler/::minvoke()      @     0x2b9ed407d202 protobufprocess/::visit()      @     0x2b9ed402af1a mesos::internal::master::master::visit()      @     0x2b9ed4037eb8 mesos::internal::master::master::visit()      @     0x2b9ed44cb792 process::processmanager::resume()      @     0x2b9ed44cba9c process::schedule()      @     0x2b9ed5fe7182 startthread      @     0x2b9ed62f830d (unknown)  ",2,train
MESOS-1771,introduce unique_ptr," add uniqueptr to the configure check   document use of uniqueptr in style guide   use when possible, use std::move when necessary   move raw pointers to owned to establish ownership   deprecate owned in favour of unique_ptr  ",1,train
MESOS-1777,Design persistent resources,nan,13,train
MESOS-1778,Provide an option to validate flag value in stout/flags. ,"currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. passing an optional lambda checker to flagbase::add() can be a possible solution.",3,train
MESOS-1782,AllocatorTest/0.FrameworkExited is flaky,  [ run      ] allocatortest/0.frameworkexited  using temporary directory '/tmp/allocatortest0frameworkexitedb6wzng'  i0909 08:02:35.116555 18112 leveldb.cpp:176] opened db in 31.64686ms  i0909 08:02:35.126065 18112 leveldb.cpp:183] compacted db in 9.449823ms  i0909 08:02:35.126118 18112 leveldb.cpp:198] created db iterator in 5858ns  i0909 08:02:35.126137 18112 leveldb.cpp:204] seeked to beginning of db in 1136ns  i0909 08:02:35.126150 18112 leveldb.cpp:273] iterated through 0 keys in the db in 560ns  i0909 08:02:35.126178 18112 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0909 08:02:35.126502 18133 recover.cpp:425] starting replica recovery  i0909 08:02:35.126601 18133 recover.cpp:451] replica is in empty status  i0909 08:02:35.127012 18133 replica.cpp:638] replica in empty status received a broadcasted recover request  i0909 08:02:35.127094 18133 recover.cpp:188] received a recover response from a replica in empty status  i0909 08:02:35.127223 18133 recover.cpp:542] updating replica status to starting  i0909 08:02:35.226631 18133 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 99.308134ms  i0909 08:02:35.226690 18133 replica.cpp:320] persisted replica status to starting  i0909 08:02:35.226812 18131 recover.cpp:451] replica is in starting status  i0909 08:02:35.227246 18131 replica.cpp:638] replica in starting status received a broadcasted recover request  i0909 08:02:35.227308 18131 recover.cpp:188] received a recover response from a replica in starting status  i0909 08:02:35.227409 18131 recover.cpp:542] updating replica status to voting  i0909 08:02:35.228540 18129 master.cpp:286] master 20140909080235168428794400518112 (precise) started on 127.0.1.1:44005  i0909 08:02:35.228593 18129 master.cpp:332] master only allowing authenticated frameworks to register  i0909 08:02:35.228607 18129 master.cpp:337] master only allowing authenticated slaves to register  i0909 08:02:35.228620 18129 credentials.hpp:36] loading credentials for authentication from '/tmp/allocatortest0frameworkexitedb6wzng/credentials'  i0909 08:02:35.228754 18129 master.cpp:366] authorization enabled  i0909 08:02:35.229560 18129 master.cpp:120] no whitelist given. advertising offers for all slaves  i0909 08:02:35.229933 18129 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@127.0.1.1:44005  i0909 08:02:35.230057 18127 master.cpp:1212] the newly elected leader is master@127.0.1.1:44005 with id 20140909080235168428794400518112  i0909 08:02:35.230129 18127 master.cpp:1225] elected as the leading master!  i0909 08:02:35.230144 18127 master.cpp:1043] recovering from registrar  i0909 08:02:35.230257 18127 registrar.cpp:313] recovering registrar  i0909 08:02:35.232461 18131 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 4.999384ms  i0909 08:02:35.232489 18131 replica.cpp:320] persisted replica status to voting  i0909 08:02:35.232544 18131 recover.cpp:556] successfully joined the paxos group  i0909 08:02:35.232611 18131 recover.cpp:440] recover process terminated  i0909 08:02:35.232727 18131 log.cpp:656] attempting to start the writer  i0909 08:02:35.233012 18131 replica.cpp:474] replica received implicit promise request with proposal 1  i0909 08:02:35.238785 18131 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 5.749504ms  i0909 08:02:35.238818 18131 replica.cpp:342] persisted promised to 1  i0909 08:02:35.244056 18131 coordinator.cpp:230] coordinator attemping to fill missing position  i0909 08:02:35.244580 18131 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0909 08:02:35.250143 18131 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 5.382351ms  i0909 08:02:35.250319 18131 replica.cpp:676] persisted action at 0  i0909 08:02:35.250901 18131 replica.cpp:508] replica received write request for position 0  i0909 08:02:35.251137 18131 leveldb.cpp:438] reading position from leveldb took 18689ns  i0909 08:02:35.256597 18131 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 5.274169ms  i0909 08:02:35.256764 18131 replica.cpp:676] persisted action at 0  i0909 08:02:35.263712 18126 replica.cpp:655] replica received learned notice for position 0  i0909 08:02:35.269613 18126 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 5.417225ms  i0909 08:02:35.351641 18126 replica.cpp:676] persisted action at 0  i0909 08:02:35.351655 18126 replica.cpp:661] replica learned nop action at position 0  i0909 08:02:35.351889 18126 log.cpp:672] writer started with ending position 0  i0909 08:02:35.352165 18126 leveldb.cpp:438] reading position from leveldb took 25215ns  i0909 08:02:35.353163 18126 registrar.cpp:346] successfully fetched the registry (0b)  i0909 08:02:35.353185 18126 registrar.cpp:422] attempting to update the 'registry'  i0909 08:02:35.354152 18126 log.cpp:680] attempting to append 120 bytes to the log  i0909 08:02:35.354195 18126 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0909 08:02:35.354416 18126 replica.cpp:508] replica received write request for position 1  i0909 08:02:35.351579 18127 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0909 08:02:35.354558 18127 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 2.984795ms  i0909 08:02:35.360254 18126 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 5.811986ms  i0909 08:02:35.360285 18126 replica.cpp:676] persisted action at 1  i0909 08:02:35.364126 18132 replica.cpp:655] replica received learned notice for position 1  i0909 08:02:35.369856 18132 leveldb.cpp:343] persisting action (139 bytes) to leveldb took 5.702756ms  i0909 08:02:35.369899 18132 replica.cpp:676] persisted action at 1  i0909 08:02:35.369910 18132 replica.cpp:661] replica learned append action at position 1  i0909 08:02:35.370209 18132 registrar.cpp:479] successfully updated 'registry'  i0909 08:02:35.370311 18132 registrar.cpp:372] successfully recovered registrar  i0909 08:02:35.370477 18132 log.cpp:699] attempting to truncate the log to 1  i0909 08:02:35.370553 18132 master.cpp:1070] recovered 0 slaves from the registry (84b) ; allowing 10mins for slaves to reregister  i0909 08:02:35.370594 18132 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0909 08:02:35.371201 18127 replica.cpp:508] replica received write request for position 2  i0909 08:02:35.376760 18127 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 5.264501ms  i0909 08:02:35.377105 18127 replica.cpp:676] persisted action at 2  i0909 08:02:35.377770 18127 replica.cpp:655] replica received learned notice for position 2  i0909 08:02:35.383363 18127 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 5.272769ms  i0909 08:02:35.383818 18127 leveldb.cpp:401] deleting ~1 keys from leveldb took 28148ns  i0909 08:02:35.384137 18127 replica.cpp:676] persisted action at 2  i0909 08:02:35.384399 18127 replica.cpp:661] replica learned truncate action at position 2  i0909 08:02:35.396512 18127 slave.cpp:167] slave started on 64)@127.0.1.1:44005  i0909 08:02:35.654770 18131 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0909 08:02:35.654847 18131 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 104933ns  i0909 08:02:35.654974 18127 credentials.hpp:84] loading credential for authentication from '/tmp/allocatortest0frameworkexitedxv9mk4/credential'  i0909 08:02:35.655097 18127 slave.cpp:274] slave using credential for: testprincipal  i0909 08:02:35.655203 18127 slave.cpp:287] slave resources: cpus():3; mem():1024; disk():25116; ports():[3100032000]  i0909 08:02:35.655274 18127 slave.cpp:315] slave hostname: precise  i0909 08:02:35.655285 18127 slave.cpp:316] slave checkpoint: false  i0909 08:02:35.655804 18127 state.cpp:33] recovering state from '/tmp/allocatortest0frameworkexitedxv9mk4/meta'  i0909 08:02:35.655913 18127 statusupdatemanager.cpp:193] recovering status update manager  i0909 08:02:35.656005 18127 slave.cpp:3202] finished recovery  i0909 08:02:35.656251 18127 slave.cpp:598] new master detected at master@127.0.1.1:44005  i0909 08:02:35.656285 18127 slave.cpp:672] authenticating with master master@127.0.1.1:44005  i0909 08:02:35.656325 18127 slave.cpp:645] detecting new master  i0909 08:02:35.656358 18127 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:44005  i0909 08:02:35.656389 18127 authenticatee.hpp:128] creating new client sasl connection  i0909 08:02:35.656563 18127 master.cpp:3653] authenticating slave(64)@127.0.1.1:44005  i0909 08:02:35.656651 18127 authenticator.hpp:156] creating new server sasl connection  i0909 08:02:35.656770 18127 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0909 08:02:35.656796 18127 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0909 08:02:35.656822 18127 authenticator.hpp:262] received sasl authentication start  i0909 08:02:35.656858 18127 authenticator.hpp:384] authentication requires more steps  i0909 08:02:35.656883 18127 authenticatee.hpp:265] received sasl authentication step  i0909 08:02:35.656924 18127 authenticator.hpp:290] received sasl authentication step  i0909 08:02:35.656960 18127 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'precise' server fqdn: 'precise' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0909 08:02:35.656971 18127 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0909 08:02:35.656982 18127 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0909 08:02:35.656997 18127 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'precise' server fqdn: 'precise' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0909 08:02:35.657004 18127 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0909 08:02:35.657008 18127 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0909 08:02:35.657019 18127 authenticator.hpp:376] authentication success  i0909 08:02:35.657047 18127 authenticatee.hpp:305] authentication success  i0909 08:02:35.657073 18127 master.cpp:3693] successfully authenticated principal 'testprincipal' at slave(64)@127.0.1.1:44005  i0909 08:02:35.657145 18127 slave.cpp:729] successfully authenticated with master master@127.0.1.1:44005  i0909 08:02:35.657183 18127 slave.cpp:980] will retry registration in 19.238717ms if necessary  i0909 08:02:35.657276 18128 master.cpp:2843] registering slave at slave(64)@127.0.1.1:44005 (precise) with id 201409090802351684287944005181120  i0909 08:02:35.657389 18128 registrar.cpp:422] attempting to update the 'registry'  i0909 08:02:35.658382 18130 log.cpp:680] attempting to append 295 bytes to the log  i0909 08:02:35.658432 18130 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0909 08:02:35.658635 18130 replica.cpp:508] replica received write request for position 3  i0909 08:02:35.660959 18112 sched.cpp:137] version: 0.21.0  i0909 08:02:35.661093 18126 sched.cpp:233] new master detected at master@127.0.1.1:44005  i0909 08:02:35.661111 18126 sched.cpp:283] authenticating with master master@127.0.1.1:44005  i0909 08:02:35.661175 18126 authenticatee.hpp:128] creating new client sasl connection  i0909 08:02:35.661306 18126 master.cpp:3653] authenticating schedulerfd92991870574fef923aed9d6fd355be@127.0.1.1:44005  i0909 08:02:35.661376 18126 authenticator.hpp:156] creating new server sasl connection  i0909 08:02:35.661466 18126 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0909 08:02:35.661483 18126 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0909 08:02:35.661504 18126 authenticator.hpp:262] received sasl authentication start  i0909 08:02:35.661530 18126 authenticator.hpp:384] authentication requires more steps  i0909 08:02:35.661552 18126 authenticatee.hpp:265] received sasl authentication step  i0909 08:02:35.661579 18126 authenticator.hpp:290] received sasl authentication step  i0909 08:02:35.661592 18126 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'precise' server fqdn: 'precise' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0909 08:02:35.661598 18126 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0909 08:02:35.661607 18126 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0909 08:02:35.661613 18126 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'precise' server fqdn: 'precise' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0909 08:02:35.661619 18126 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0909 08:02:35.661623 18126 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0909 08:02:35.661633 18126 authenticator.hpp:376] authentication success  i0909 08:02:35.661653 18126 authenticatee.hpp:305] authentication success  i0909 08:02:35.661672 18126 master.cpp:3693] successfully authenticated principal 'testprincipal' at schedulerfd92991870574fef923aed9d6fd355be@127.0.1.1:44005  i0909 08:02:35.661730 18126 sched.cpp:357] successfully authenticated with master master@127.0.1.1:44005  i0909 08:02:35.661741 18126 sched.cpp:476] sending registration request to master@127.0.1.1:44005  i0909 08:02:35.661782 18126 master.cpp:1331] received registration request from schedulerfd92991870574fef923aed9d6fd355be@127.0.1.1:44005  i0909 08:02:35.661798 18126 master.cpp:1291] authorizing framework principal 'testprincipal' to receive offers for role ''  i0909 08:02:35.661917 18126 master.cpp:1390] registering framework 201409090802351684287944005181120000 at schedulerfd92991870574fef923aed9d6fd355be@127.0.1.1:44005  i0909 08:02:35.662017 18126 sched.cpp:407] framework registered with 201409090802351684287944005181120000  i0909 08:02:35.662039 18126 sched.cpp:421] scheduler::registered took 9070ns  i0909 08:02:35.662119 18126 hierarchicalallocatorprocess.hpp:329] added framework 201409090802351684287944005181120000  i0909 08:02:35.662130 18126 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i0909 08:02:35.662135 18126 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 5558ns  i0909 08:02:35.672230 18130 leveldb.cpp:343] persisting action (314 bytes) to leveldb took 13.567526ms  i0909 08:02:35.672268 18130 replica.cpp:676] persisted action at 3  i0909 08:02:35.672483 18130 replica.cpp:655] replica received learned notice for position 3  i0909 08:02:35.677322 18132 slave.cpp:980] will retry registration in 14.890338ms if necessary  i0909 08:02:35.677399 18132 master.cpp:2831] ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress  i0909 08:02:35.680881 18130 leveldb.cpp:343] persisting action (316 bytes) to leveldb took 8.376798ms  i0909 08:02:35.680908 18130 replica.cpp:676] persisted action at 3  i0909 08:02:35.680917 18130 replica.cpp:661] replica learned append action at position 3  i0909 08:02:35.681252 18130 registrar.cpp:479] successfully updated 'registry'  i0909 08:02:35.681330 18130 log.cpp:699] attempting to truncate the log to 3  i0909 08:02:35.681385 18130 master.cpp:2883] registered slave 201409090802351684287944005181120 at slave(64)@127.0.1.1:44005 (precise)  i0909 08:02:35.681399 18130 master.cpp:4126] adding slave 201409090802351684287944005181120 at slave(64)@127.0.1.1:44005 (precise) with cpus():3; mem():1024; disk():25116; ports():[3100032000]  i0909 08:02:35.681504 18130 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0909 08:02:35.681570 18130 slave.cpp:763] registered with master master@127.0.1.1:44005; given slave id 201409090802351684287944005181120  i0909 08:02:35.681689 18130 slave.cpp:2329] received ping from slaveobserver(50)@127.0.1.1:44005  i0909 08:02:35.681753 18130 hierarchicalallocatorprocess.hpp:442] added slave 201409090802351684287944005181120 (precise) with cpus():3; mem():1024; disk():25116; ports():[3100032000] (and cpus():3; mem():1024; disk():25116; ports():[3100032000] available)  i0909 08:02:35.681808 18130 hierarchicalallocatorprocess.hpp:734] offering cpus():3; mem():1024; disk():25116; ports():[3100032000] on slave 201409090802351684287944005181120 to framework 201409090802351684287944005181120000  i0909 08:02:35.681892 18130 hierarchicalallocatorprocess.hpp:679] performed allocation for slave 201409090802351684287944005181120 in 109580ns  i0909 08:02:35.681968 18130 master.hpp:861] adding offer 201409090802351684287944005181120 with resources cpus():3; mem():1024; disk():25116; ports():[3100032000] on slave 201409090802351684287944005181120 (precise)  i0909 08:02:35.682014 18130 master.cpp:3600] sending 1 offers to framework 201409090802351684287944005181120000  i0909 08:02:35.682443 18130 sched.cpp:544] scheduler::resourceoffers took 254258ns  i0909 08:02:35.682633 18130 master.hpp:871] removing offer 201409090802351684287944005181120 with resources cpus():3; mem():1024; disk():25116; ports():[3100032000] on slave 201409090802351684287944005181120 (precise)  i0909 08:02:35.682684 18130 master.cpp:2201] processing reply for offers: [ 201409090802351684287944005181120 ] on slave 201409090802351684287944005181120 at slave(64)@127.0.1.1:44005 (precise) for framework 201409090802351684287944005181120000  i0909 08:02:35.682708 18130 master.cpp:2284] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  i0909 08:02:35.682971 18130 replica.cpp:508] replica received write request for position 4  i0909 08:02:35.683132 18132 master.hpp:833] adding task 0 with resources cpus():2; mem():512 on slave 201409090802351684287944005181120 (precise)  i0909 08:02:35.683159 18132 master.cpp:2350] launching task 0 of framework 201409090802351684287944005181120000 with resources cpus():2; mem():512 on slave 201409090802351684287944005181120 at slave(64)@127.0.1.1:44005 (precise)  i0909 08:02:35.683363 18132 slave.cpp:1011] got assigned task 0 for framework 201409090802351684287944005181120000  i0909 08:02:35.683580 18132 slave.cpp:1121] launching task 0 for framework 201409090802351684287944005181120000  i0909 08:02:35.684833 18133 hierarchicalallocatorprocess.hpp:563] recovered cpus():1; mem():512; disk():25116; ports():[3100032000] (total allocatable: cpus():1; mem():512; disk():25116; ports( ):[3100032000]) on slave 201409090802351684287944005181120 from framework 201409090802351684287944005181120000  i0909 08:02:35.684864 18133 hierarchicalallocatorprocess.hpp:599] framework 201409090802351684287944005181120000 filtered slave 201409090802351684287944005181120 for 5secs  i0909 08:02:35.686401 18132 exec.cpp:132] version: 0.21.0  i0909 08:02:35.686848 18128 exec.cpp:182] executor started at: executor(8)@127.0.1.1:44005 with pid 18112  i0909 08:02:35.687095 18132 slave.cpp:1231] queuing task '0' for executor executor1 of framework '201409090802351684287944005181120000  i0909 08:02:35.687302 18132 slave.cpp:552] successfully attached file '/tmp/allocatortest0frameworkexited_xv9mk4/slaves/201409090802351684287944005181120/frameworks/201409090802351684287944005181120000/executors/executor1/runs/c4458e43...,1,train
MESOS-1784,Design the semantics for updating FrameworkInfo,"currently, there is no easy way for frameworks to update their frameworkinfo., resulting in issues like mesos703 and mesos1218.    this ticket captures the design for doing frameworkinfo update without having to roll masters/slaves/tasks/executors.",3,train
MESOS-1790,"Add ""chown"" option to CommandInfo.URI","mesos fetcher always chown()s the extracted executor uris as the executor user but sometimes this is not desirable, e.g., ""setuid"" bit gets lost during chown() if slave/fetcher is running as root.     it would be nice to give frameworks the ability to skip the chown.",2,train
MESOS-1799,Reconciliation can send out-of-order updates.,"when a slave reregisters with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.    however, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.    as a result, outoforder updates are possible, e.g.    (1) slave has task t in taskfinished, with unacknowledged updates: [taskrunning, taskfinished].  (2) master fails over.  (3) new master reregisters the slave with t in taskfinished.  (4) reconciliation request arrives, master sends taskfinished.  (5) slave sends taskrunning to master, master sends task_running.    i think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. this means when the slave re registers, it should instead send the latest acknowledged state of each task.",3,train
MESOS-1807,Disallow executors with cpu only or memory only resources,"currently master allows executors to be launched with either only cpus or only memory but we shouldn't allow that.  this is because executor is an actual unix process that is launched by the slave. if an executor doesn't specify cpus, what should do the cpu limits be for that executor when there are no tasks running on it? if no cpu limits are set then it might starve other executors/tasks on the slave violating isolation guarantees. same goes with memory. moreover, the current containerizer/isolator code will throw failures when using such an executor, e.g., when the last task on the executor finishes and containerizer::update() is called with 0 cpus or 0 mem.",3,train
MESOS-1808,Expose RTT in container stats,"as we expose the bandwidth, so we should expose the rtt as a measure of latency each container is experiencing.    we can use ss to get the per socket statistics and filter and aggregate accordingly to get a measure of rtt.",3,train
MESOS-1811,Reconcile disconnected/deactivated semantics in the master code,"currently the master code treats a deactivated and disconnected slave similarly, by setting 'disconnected' variable in the slave struct. this causes us to disconnect() a slave in cases where we really only want to deactivate() the slave (e.g., authentication).    it would be nice to differentiate these semantics by adding a new variable ""active"" in the slave struct.    we might want to do the same with the framework struct for consistency.",3,train
MESOS-1813,Fail fast in example frameworks if task goes into unexpected state,"most of the example frameworks launch a bunch of tasks and exit if all of them reach finished state. but if there is a bug in the code resulting in task_lost, the framework waits forever. instead the framework should abort if an un expected task state is encountered.",1,train
MESOS-1814,Task attempted to use more offers than requested in example jave and python frameworks,"  [ run      ] examplestest.javaframework  using temporary directory '/tmp/examplestestjavaframework2pcfch'  enabling authentication for the framework  warning: logging before initgooglelogging() is written to stderr  i0917 23:14:35.199069 31510 process.cpp:1771] libprocess is initialized on 127.0.1.1:34609 for 8 cpus  i0917 23:14:35.199794 31510 logging.cpp:177] logging to stderr  i0917 23:14:35.225342 31510 leveldb.cpp:176] opened db in 22.197149ms  i0917 23:14:35.231133 31510 leveldb.cpp:183] compacted db in 5.601897ms  i0917 23:14:35.231498 31510 leveldb.cpp:198] created db iterator in 215441ns  i0917 23:14:35.231608 31510 leveldb.cpp:204] seeked to beginning of db in 11488ns  i0917 23:14:35.231722 31510 leveldb.cpp:273] iterated through 0 keys in the db in 14016ns  i0917 23:14:35.231917 31510 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0917 23:14:35.233129 31526 recover.cpp:425] starting replica recovery  i0917 23:14:35.233614 31526 recover.cpp:451] replica is in empty status  i0917 23:14:35.234994 31526 replica.cpp:638] replica in empty status received a broadcasted recover request  i0917 23:14:35.240116 31519 recover.cpp:188] received a recover response from a replica in empty status  i0917 23:14:35.240782 31519 recover.cpp:542] updating replica status to starting  i0917 23:14:35.242846 31524 master.cpp:286] master 20140917231435168428793460931503 (saucy) started on 127.0.1.1:34609  i0917 23:14:35.243191 31524 master.cpp:332] master only allowing authenticated frameworks to register  i0917 23:14:35.243288 31524 master.cpp:339] master allowing unauthenticated slaves to register  i0917 23:14:35.243399 31524 credentials.hpp:36] loading credentials for authentication from '/tmp/examplestestjavaframework2pcfch/credentials'  w0917 23:14:35.243588 31524 credentials.hpp:51] permissions on credentials file '/tmp/examplestestjavaframework2pcfch/credentials' are too open. it is recommended that your credentials file is not accessible by others.  i0917 23:14:35.243846 31524 master.cpp:366] authorization enabled  i0917 23:14:35.244882 31520 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@127.0.1.1:34609  i0917 23:14:35.245224 31520 master.cpp:120] no whitelist given. advertising offers for all slaves  i0917 23:14:35.246934 31524 master.cpp:1211] the newly elected leader is master@127.0.1.1:34609 with id 20140917231435168428793460931503  i0917 23:14:35.247234 31524 master.cpp:1224] elected as the leading master!  i0917 23:14:35.247336 31524 master.cpp:1042] recovering from registrar  i0917 23:14:35.247542 31526 registrar.cpp:313] recovering registrar  i0917 23:14:35.250555 31510 containerizer.cpp:89] using isolation: posix/cpu,posix/mem  i0917 23:14:35.252326 31510 containerizer.cpp:89] using isolation: posix/cpu,posix/mem  i0917 23:14:35.252821 31520 slave.cpp:169] slave started on 1)@127.0.1.1:34609  i0917 23:14:35.253552 31520 slave.cpp:289] slave resources: cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:35.253906 31520 slave.cpp:317] slave hostname: saucy  i0917 23:14:35.254004 31520 slave.cpp:318] slave checkpoint: true  i0917 23:14:35.254818 31520 state.cpp:33] recovering state from '/tmp/mesosw8snrw/0/meta'  i0917 23:14:35.255106 31519 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 13.99622ms  i0917 23:14:35.255235 31519 replica.cpp:320] persisted replica status to starting  i0917 23:14:35.255419 31519 recover.cpp:451] replica is in starting status  i0917 23:14:35.255834 31519 replica.cpp:638] replica in starting status received a broadcasted recover request  i0917 23:14:35.256000 31519 recover.cpp:188] received a recover response from a replica in starting status  i0917 23:14:35.256217 31519 recover.cpp:542] updating replica status to voting  i0917 23:14:35.256641 31520 statusupdatemanager.cpp:193] recovering status update manager  i0917 23:14:35.257064 31520 containerizer.cpp:252] recovering containerizer  i0917 23:14:35.257725 31520 slave.cpp:3220] finished recovery  i0917 23:14:35.258463 31520 slave.cpp:600] new master detected at master@127.0.1.1:34609  i0917 23:14:35.258769 31524 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:34609  i0917 23:14:35.258885 31520 slave.cpp:636] no credentials provided. attempting to register without authentication  i0917 23:14:35.259024 31520 slave.cpp:647] detecting new master  i0917 23:14:35.259863 31520 slave.cpp:169] slave started on 2)@127.0.1.1:34609  i0917 23:14:35.260288 31520 slave.cpp:289] slave resources: cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:35.260493 31520 slave.cpp:317] slave hostname: saucy  i0917 23:14:35.260588 31520 slave.cpp:318] slave checkpoint: true  i0917 23:14:35.265127 31510 containerizer.cpp:89] using isolation: posix/cpu,posix/mem  i0917 23:14:35.265877 31519 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 9.536278ms  i0917 23:14:35.265983 31519 replica.cpp:320] persisted replica status to voting  i0917 23:14:35.266324 31519 recover.cpp:556] successfully joined the paxos group  i0917 23:14:35.266511 31519 recover.cpp:440] recover process terminated  i0917 23:14:35.266978 31519 log.cpp:656] attempting to start the writer  i0917 23:14:35.268165 31523 replica.cpp:474] replica received implicit promise request with proposal 1  i0917 23:14:35.269850 31525 slave.cpp:169] slave started on 3)@127.0.1.1:34609  i0917 23:14:35.270365 31525 slave.cpp:289] slave resources: cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:35.270658 31525 slave.cpp:317] slave hostname: saucy  i0917 23:14:35.270781 31525 slave.cpp:318] slave checkpoint: true  i0917 23:14:35.271332 31525 state.cpp:33] recovering state from '/tmp/mesosw8snrw/2/meta'  i0917 23:14:35.271580 31522 statusupdatemanager.cpp:193] recovering status update manager  i0917 23:14:35.271838 31522 containerizer.cpp:252] recovering containerizer  i0917 23:14:35.272238 31525 slave.cpp:3220] finished recovery  i0917 23:14:35.273002 31525 slave.cpp:600] new master detected at master@127.0.1.1:34609  i0917 23:14:35.273252 31521 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:34609  i0917 23:14:35.273360 31525 slave.cpp:636] no credentials provided. attempting to register without authentication  i0917 23:14:35.273507 31525 slave.cpp:647] detecting new master  i0917 23:14:35.275413 31525 state.cpp:33] recovering state from '/tmp/mesosw8snrw/1/meta'  i0917 23:14:35.278506 31523 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 10.232514ms  i0917 23:14:35.278712 31523 replica.cpp:342] persisted promised to 1  i0917 23:14:35.279585 31523 coordinator.cpp:230] coordinator attemping to fill missing position  i0917 23:14:35.280400 31523 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0917 23:14:35.280900 31526 statusupdatemanager.cpp:193] recovering status update manager  i0917 23:14:35.281282 31519 containerizer.cpp:252] recovering containerizer  i0917 23:14:35.281615 31520 slave.cpp:3220] finished recovery  i0917 23:14:35.281891 31510 sched.cpp:137] version: 0.21.0  i0917 23:14:35.282306 31526 sched.cpp:233] new master detected at master@127.0.1.1:34609  i0917 23:14:35.282464 31526 sched.cpp:283] authenticating with master master@127.0.1.1:34609  i0917 23:14:35.282891 31526 authenticatee.hpp:104] initializing client sasl  i0917 23:14:35.284816 31526 authenticatee.hpp:128] creating new client sasl connection  i0917 23:14:35.285428 31519 master.cpp:873] dropping 'mesos.internal.authenticatemessage' message since not recovered yet  i0917 23:14:35.288007 31521 slave.cpp:600] new master detected at master@127.0.1.1:34609  i0917 23:14:35.288399 31521 slave.cpp:636] no credentials provided. attempting to register without authentication  i0917 23:14:35.288535 31521 slave.cpp:647] detecting new master  i0917 23:14:35.288501 31519 statusupdatemanager.cpp:167] new master detected at master@127.0.1.1:34609  i0917 23:14:35.289625 31523 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 8.997343ms  i0917 23:14:35.289784 31523 replica.cpp:676] persisted action at 0  i0917 23:14:35.292667 31521 replica.cpp:508] replica received write request for position 0  i0917 23:14:35.293112 31521 leveldb.cpp:438] reading position from leveldb took 325638ns  i0917 23:14:35.301774 31521 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 8.576338ms  i0917 23:14:35.301916 31521 replica.cpp:676] persisted action at 0  i0917 23:14:35.302289 31521 replica.cpp:655] replica received learned notice for position 0  i0917 23:14:35.310542 31521 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 8.087789ms  i0917 23:14:35.310675 31521 replica.cpp:676] persisted action at 0  i0917 23:14:35.310946 31521 replica.cpp:661] replica learned nop action at position 0  i0917 23:14:35.311254 31521 log.cpp:672] writer started with ending position 0  i0917 23:14:35.311957 31521 leveldb.cpp:438] reading position from leveldb took 35110ns  i0917 23:14:35.320283 31521 registrar.cpp:346] successfully fetched the registry (0b)  i0917 23:14:35.320513 31521 registrar.cpp:422] attempting to update the 'registry'  i0917 23:14:35.322226 31525 log.cpp:680] attempting to append 118 bytes to the log  i0917 23:14:35.322549 31525 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0917 23:14:35.322931 31525 replica.cpp:508] replica received write request for position 1  i0917 23:14:35.330169 31525 leveldb.cpp:343] persisting action (135 bytes) to leveldb took 7.133053ms  i0917 23:14:35.330340 31525 replica.cpp:676] persisted action at 1  i0917 23:14:35.330890 31525 replica.cpp:655] replica received learned notice for position 1  i0917 23:14:35.339218 31525 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 8.192024ms  i0917 23:14:35.339380 31525 replica.cpp:676] persisted action at 1  i0917 23:14:35.339715 31525 replica.cpp:661] replica learned append action at position 1  i0917 23:14:35.340615 31525 registrar.cpp:479] successfully updated 'registry'  i0917 23:14:35.340802 31525 registrar.cpp:372] successfully recovered registrar  i0917 23:14:35.341104 31525 log.cpp:699] attempting to truncate the log to 1  i0917 23:14:35.341351 31525 master.cpp:1069] recovered 0 slaves from the registry (82b) ; allowing 10mins for slaves to reregister  i0917 23:14:35.341527 31525 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0917 23:14:35.341964 31525 replica.cpp:508] replica received write request for position 2  i0917 23:14:35.352336 31525 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 10.213086ms  i0917 23:14:35.352494 31525 replica.cpp:676] persisted action at 2  i0917 23:14:35.356258 31523 replica.cpp:655] replica received learned notice for position 2  i0917 23:14:35.364992 31523 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 8.606522ms  i0917 23:14:35.365166 31523 leveldb.cpp:401] deleting 1 keys from leveldb took 48378ns  i0917 23:14:35.365404 31523 replica.cpp:676] persisted action at 2  i0917 23:14:35.365537 31523 replica.cpp:661] replica learned truncate action at position 2  i0917 23:14:35.568366 31523 slave.cpp:994] will retry registration in 423.208575ms if necessary  i0917 23:14:35.568840 31522 master.cpp:2870] registering slave at slave(3)@127.0.1.1:34609 (saucy) with id 201409172314351684287934609315030  i0917 23:14:35.569422 31522 registrar.cpp:422] attempting to update the 'registry'  i0917 23:14:35.572013 31522 log.cpp:680] attempting to append 289 bytes to the log  i0917 23:14:35.572273 31519 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0917 23:14:35.572816 31519 replica.cpp:508] replica received write request for position 3  i0917 23:14:35.579784 31519 leveldb.cpp:343] persisting action (308 bytes) to leveldb took 6.809365ms  i0917 23:14:35.579907 31519 replica.cpp:676] persisted action at 3  i0917 23:14:35.580512 31519 replica.cpp:655] replica received learned notice for position 3  i0917 23:14:35.588748 31519 leveldb.cpp:343] persisting action (310 bytes) to leveldb took 8.112519ms  i0917 23:14:35.588888 31519 replica.cpp:676] persisted action at 3  i0917 23:14:35.588985 31519 replica.cpp:661] replica learned append action at position 3  i0917 23:14:35.589754 31519 registrar.cpp:479] successfully updated 'registry'  i0917 23:14:35.590070 31519 master.cpp:2910] registered slave 201409172314351684287934609315030 at slave(3)@127.0.1.1:34609 (saucy)  i0917 23:14:35.590255 31519 master.cpp:4118] adding slave 201409172314351684287934609315030 at slave(3)@127.0.1.1:34609 (saucy) with cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:35.590831 31519 slave.cpp:765] registered with master master@127.0.1.1:34609; given slave id 201409172314351684287934609315030  i0917 23:14:35.589913 31523 log.cpp:699] attempting to truncate the log to 3  i0917 23:14:35.591414 31523 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0917 23:14:35.591815 31523 replica.cpp:508] replica received write request for position 4  i0917 23:14:35.591117 31521 hierarchicalallocatorprocess.hpp:442] added slave 201409172314351684287934609315030 (saucy) with cpus():1; mem():1001; disk():24988; ports():[3100032000] (and cpus():1; mem():1001; disk():24988; ports():[3100032000] available)  i0917 23:14:35.592293 31521 hierarchicalallocatorprocess.hpp:679] performed allocation for slave 201409172314351684287934609315030 in 64364ns  i0917 23:14:35.592953 31519 slave.cpp:778] checkpointing slaveinfo to '/tmp/mesosw8snrw/2/meta/slaves/201409172314351684287934609315030/slave.info'  i0917 23:14:35.593475 31519 slave.cpp:2347] received ping from slaveobserver(1)@127.0.1.1:34609  i0917 23:14:35.601356 31523 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 9.420461ms  i0917 23:14:35.601539 31523 replica.cpp:676] persisted action at 4  i0917 23:14:35.602325 31523 replica.cpp:655] replica received learned notice for position 4  i0917 23:14:35.610779 31523 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 8.34398ms  i0917 23:14:35.611114 31523 leveldb.cpp:401] deleting 2 keys from leveldb took 66521ns  i0917 23:14:35.611554 31523 replica.cpp:676] persisted action at 4  i0917 23:14:35.611690 31523 replica.cpp:661] replica learned truncate action at position 4  i0917 23:14:36.033941 31523 slave.cpp:994] will retry registration in 322.705631ms if necessary  i0917 23:14:36.034276 31521 master.cpp:2870] registering slave at slave(1)@127.0.1.1:34609 (saucy) with id 201409172314351684287934609315031  i0917 23:14:36.034536 31521 registrar.cpp:422] attempting to update the 'registry'  i0917 23:14:36.035889 31521 log.cpp:680] attempting to append 454 bytes to the log  i0917 23:14:36.036099 31524 coordinator.cpp:340] coordinator attempting to write append action at position 5  i0917 23:14:36.036416 31524 replica.cpp:508] replica received write request for position 5  i0917 23:14:36.046672 31524 leveldb.cpp:343] persisting action (473 bytes) to leveldb took 10.160627ms  i0917 23:14:36.047035 31524 replica.cpp:676] persisted action at 5  i0917 23:14:36.047613 31524 replica.cpp:655] replica received learned notice for position 5  i0917 23:14:36.053006 31524 leveldb.cpp:343] persisting action (475 bytes) to leveldb took 5.180742ms  i0917 23:14:36.053246 31524 replica.cpp:676] persisted action at 5  i0917 23:14:36.053678 31524 replica.cpp:661] replica learned append action at position 5  i0917 23:14:36.060384 31524 registrar.cpp:479] successfully updated 'registry'  i0917 23:14:36.061328 31524 master.cpp:2910] registered slave 201409172314351684287934609315031 at slave(1)@127.0.1.1:34609 (saucy)  i0917 23:14:36.061537 31524 master.cpp:4118] adding slave 201409172314351684287934609315031 at slave(1)@127.0.1.1:34609 (saucy) with cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:36.061982 31524 slave.cpp:765] registered with master master@127.0.1.1:34609; given slave id 201409172314351684287934609315031  i0917 23:14:36.062891 31524 slave.cpp:778] checkpointing slaveinfo to '/tmp/mesosw8snrw/0/meta/slaves/201409172314351684287934609315031/slave.info'  i0917 23:14:36.061050 31525 log.cpp:699] attempting to truncate the log to 5  i0917 23:14:36.063244 31525 coordinator.cpp:340] coordinator attempting to write truncate action at position 6  i0917 23:14:36.063746 31525 replica.cpp:508] replica received write request for position 6  i0917 23:14:36.062386 31520 hierarchicalallocatorprocess.hpp:442] added slave 201409172314351684287934609315031 (saucy) with cpus():1; mem():1001; disk():24988; ports():[3100032000] (and cpus():1; mem():1001; disk():24988; ports():[3100032000] available)  i0917 23:14:36.064352 31520 hierarchicalallocatorprocess.hpp:679] performed allocation for slave 201409172314351684287934609315031 in 35730ns  i0917 23:14:36.065166 31524 slave.cpp:2347] received ping from slaveobserver(2)@127.0.1.1:34609  i0917 23:14:36.070137 31525 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 6.242192ms  i0917 23:14:36.070355 31525 replica.cpp:676] persisted action at 6  i0917 23:14:36.071005 31525 replica.cpp:655] replica received learned notice for position 6  i0917 23:14:36.076560 31525 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 5.368532ms  i0917 23:14:36.077137 31525 leveldb.cpp:401] deleting ~2 keys from leveldb took 371245ns  i0917 23:14:36.077241 31525 replica.cpp:676] persisted action at 6  i0917 23:14:36.077345 31525 replica.cpp:661] replica learned truncate action at position 6  i0917 23:14:36.141270 31522 slave.cpp:994] will retry registration in 1.857205901secs if necessary  i0917 23:14:36.141644 31522 master.cpp:2870] registering slave at slave(2)@127.0.1.1:34609 (saucy) with id 201409172314351684287934609315032  i0917 23:14:36.141930 31522 registrar.cpp:422] attempting to update the 'registry'  i0917 23:14:36.143316 31521 log.cpp:680] attempting to append 619 bytes to the log  i0917 23:14:36.143646 31521 coordinator.cpp:340] coordinator attempting to write append action at position 7  i0917 23:14:36.143954 31521 replica.cpp:508] replica received write request for position 7  i0917 23:14:36.148875 31521 leveldb.cpp:343] persisting action (638 bytes) to leveldb took 4.787834ms  i0917 23:14:36.149085 31521 replica.cpp:676] persisted action at 7  i0917 23:14:36.149673 31521 replica.cpp:655] replica received learned notice for position 7  i0917 23:14:36.155232 31521 leveldb.cpp:343] persisting action (640 bytes) to leveldb took 5.472209ms  i0917 23:14:36.155522 31521 replica.cpp:676] persisted action at 7  i0917 23:14:36.155936 31521 replica.cpp:661] replica learned append action at position 7  i0917 23:14:36.156481 31521 registrar.cpp:479] successfully updated 'registry'  i0917 23:14:36.156663 31526 log.cpp:699] attempting to truncate the log to 7  i0917 23:14:36.156813 31526 coordinator.cpp:340] coordinator attempting to write truncate action at position 8  i0917 23:14:36.157155 31526 replica.cpp:508] replica received write request for position 8  i0917 23:14:36.157510 31520 master.cpp:2910] registered slave 201409172314351684287934609315032 at slave(2)@127.0.1.1:34609 (saucy)  i0917 23:14:36.157645 31520 master.cpp:4118] adding slave 201409172314351684287934609315032 at slave(2)@127.0.1.1:34609 (saucy) with cpus():1; mem():1001; disk():24988; ports():[3100032000]  i0917 23:14:36.157928 31520 slave.cpp:765] registered with master master@127.0.1.1:34609; given slave id 201409172314351684287934609315032  i0917 23:14:36.158304 31520 slave.cpp:778] checkpointing slaveinfo to '/tmp/mesosw8snrw/1/meta/slaves/20140917231435168428793460931503 2/slave.info'  i0...",2,train
MESOS-1815,Create a guide to becoming a committer,"we have a committer's guide, but the process by which one becomes a committer is unclear. we should set some guidelines and a process by which we can grow contributors into committers.",3,train
MESOS-1817,Completed tasks remains in TASK_RUNNING when framework is disconnected,"we have run into a problem that cause tasks which completes, when a framework is disconnected and has a failover time, to remain in a running state even though the tasks actually finishes. this hogs the cluster and gives users a inconsistent view of the cluster state. going to the slave, the task is finished. going to the master, the task is still in a nonterminal state. when the scheduler reattaches or the failover timeout expires, the tasks finishes correctly. the current workflow of this scheduler has a long failover timeout, but may on the other hand never reattach.    here is a test framework we have been able to reproduce the issue with: https:/gist.github.com/nqn/9b9b1de9123a6e836f54  it launches many shortlived tasks (1 second sleep) and when killing the framework instance, the master reports the tasks as running even after several minutes: http:/cl.ly/image/2r3719461e0t/screen%20shot%2020140910%20at%203.19.39%20pm.png    when clicking on one of the slaves where, for example, task 49 runs; the slave knows that it completed: http:/cl.ly/image/2p410l3m1o1n/screen%20shot%2020140910%20at%203.21.29%20pm.png    here is the log of a mesoslocal instance where i reproduced it: https:/gist.github.com/nqn/f7ee20601199d70787c0 (here task 10 to 19 are stuck in running state).  there is a lot of output, so here is a filtered log for task 10: https:/gist.github.com/nqn/a53e5ea05c5e41cd5a7d    the problem turn out to be an issue with the ackcycle of status updates:  if the framework disconnects (with a failover timeout set), the status update manage on the slaves will keep trying to send the front of status update stream to the master (which in turn forwards it to the framework). if the first status update after the disconnect is terminal, things work out fine; the master pick the terminal state up, removes the task and release the resources.  if, on the other hand, one nonterminal status is in the stream. the master will never know that the task finished (or failed) before the framework reconnects.    during a discussion on the dev mailing list (http:/mailarchives.apache.org/modmbox/mesosdev/201409.mbox/%3ccadkthhavr5mrq1s9hxw1bbxfalxwwxjutp7mv4y3wpbh=awg@mail.gmail.com%3e) we enumerated a couple of options to solve this problem.    first off, having two ackcycles: one between masters and slaves and one between masters and frameworks, would be ideal. we would be able to replay the statuses in order while keeping the master state current. however, this requires us to persist the master state in a replicated storage.    as a first pass, we can make sure that the tasks caught in a running state doesn't hog the cluster when completed and the framework being disconnected.    here is a proofofconcept to work out of: https:/github.com/nqn/mesos/tree/niklas/statusupdatedisconnect/    a new (optional) field have been added to the internal status update message:  https:/github.com/nqn/mesos/blob/niklas/statusupdatedisconnect/src/messages/messages.proto#l68    which makes it possible for the status update manager to set the field, if the latest status was terminal: https:/github.com/nqn/mesos/blob/niklas/statusupdatedisconnect/src/slave/statusupdatemanager.cpp#l501    i added a test which should highlight the issue as well:  https:/github.com/nqn/mesos/blob/niklas/statusupdatedisconnect/src/tests/faulttolerancetests.cpp#l2478    i would love some input on the approach before moving on.  there are rough edges in the poc which (of course) should be addressed before bringing it for up review.",2,train
MESOS-1830,Expose master stats differentiating between master-generated and slave-generated LOST tasks,the master exports a monotonically increasing counter of tasks transitioned to tasklost.  this loses fidelity of the source of the lost task.  a first step in exposing the source of lost tasks might be to just differentiate between tasklost transitions initiated by the master vs the slave (and maybe bad input from the scheduler).,5,train
MESOS-1844,AllocatorTest/0.SlaveLost is flaky,"  [ run      ] allocatortest/0.slavelost  using temporary directory '/tmp/allocatortest0slavelostz2oazw'  i0929 16:58:29.484141  3486 leveldb.cpp:176] opened db in 604109ns  i0929 16:58:29.484629  3486 leveldb.cpp:183] compacted db in 172697ns  i0929 16:58:29.484912  3486 leveldb.cpp:198] created db iterator in 6429ns  i0929 16:58:29.485133  3486 leveldb.cpp:204] seeked to beginning of db in 1618ns  i0929 16:58:29.485337  3486 leveldb.cpp:273] iterated through 0 keys in the db in 752ns  i0929 16:58:29.485595  3486 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0929 16:58:29.486017  3500 recover.cpp:425] starting replica recovery  i0929 16:58:29.486304  3500 recover.cpp:451] replica is in empty status  i0929 16:58:29.486793  3500 replica.cpp:638] replica in empty status received a broadcasted recover request  i0929 16:58:29.487205  3500 recover.cpp:188] received a recover response from a replica in empty status  i0929 16:58:29.487540  3500 recover.cpp:542] updating replica status to starting  i0929 16:58:29.487911  3500 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 36629ns  i0929 16:58:29.488173  3500 replica.cpp:320] persisted replica status to starting  i0929 16:58:29.488438  3500 recover.cpp:451] replica is in starting status  i0929 16:58:29.488891  3500 replica.cpp:638] replica in starting status received a broadcasted recover request  i0929 16:58:29.489187  3500 recover.cpp:188] received a recover response from a replica in starting status  i0929 16:58:29.489516  3500 recover.cpp:542] updating replica status to voting  i0929 16:58:29.489887  3502 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 32099ns  i0929 16:58:29.490124  3502 replica.cpp:320] persisted replica status to voting  i0929 16:58:29.490381  3500 recover.cpp:556] successfully joined the paxos group  i0929 16:58:29.490713  3500 recover.cpp:440] recover process terminated  i0929 16:58:29.493401  3506 master.cpp:312] master 201409291658292759502016556183486 (fedora20) started on 192.168.122.164:55618  i0929 16:58:29.493700  3506 master.cpp:358] master only allowing authenticated frameworks to register  i0929 16:58:29.493921  3506 master.cpp:363] master only allowing authenticated slaves to register  i0929 16:58:29.494123  3506 credentials.hpp:36] loading credentials for authentication from '/tmp/allocatortest0slavelostz2oazw/credentials'  i0929 16:58:29.494500  3506 master.cpp:392] authorization enabled  i0929 16:58:29.495249  3506 master.cpp:120] no whitelist given. advertising offers for all slaves  i0929 16:58:29.495728  3502 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@192.168.122.164:55618  i0929 16:58:29.496196  3506 master.cpp:1241] the newly elected leader is master@192.168.122.164:55618 with id 201409291658292759502016556183486  i0929 16:58:29.496469  3506 master.cpp:1254] elected as the leading master!  i0929 16:58:29.496713  3506 master.cpp:1072] recovering from registrar  i0929 16:58:29.497020  3506 registrar.cpp:312] recovering registrar  i0929 16:58:29.497486  3506 log.cpp:656] attempting to start the writer  i0929 16:58:29.498105  3506 replica.cpp:474] replica received implicit promise request with proposal 1  i0929 16:58:29.498373  3506 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 27145ns  i0929 16:58:29.498605  3506 replica.cpp:342] persisted promised to 1  i0929 16:58:29.500880  3500 coordinator.cpp:230] coordinator attemping to fill missing position  i0929 16:58:29.501404  3500 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i0929 16:58:29.501687  3500 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 57971ns  i0929 16:58:29.501935  3500 replica.cpp:676] persisted action at 0  i0929 16:58:29.504905  3507 replica.cpp:508] replica received write request for position 0  i0929 16:58:29.505130  3507 leveldb.cpp:438] reading position from leveldb took 18418ns  i0929 16:58:29.505377  3507 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 19998ns  i0929 16:58:29.505571  3507 replica.cpp:676] persisted action at 0  i0929 16:58:29.505957  3507 replica.cpp:655] replica received learned notice for position 0  i0929 16:58:29.506186  3507 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 21648ns  i0929 16:58:29.506433  3507 replica.cpp:676] persisted action at 0  i0929 16:58:29.506767  3507 replica.cpp:661] replica learned nop action at position 0  i0929 16:58:29.507199  3507 log.cpp:672] writer started with ending position 0  i0929 16:58:29.507730  3507 leveldb.cpp:438] reading position from leveldb took 11532ns  i0929 16:58:29.508915  3507 registrar.cpp:345] successfully fetched the registry (0b)  i0929 16:58:29.509230  3507 registrar.cpp:421] attempting to update the 'registry'  i0929 16:58:29.510516  3500 log.cpp:680] attempting to append 130 bytes to the log  i0929 16:58:29.510949  3500 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0929 16:58:29.511363  3500 replica.cpp:508] replica received write request for position 1  i0929 16:58:29.511697  3500 leveldb.cpp:343] persisting action (149 bytes) to leveldb took 66530ns  i0929 16:58:29.512039  3500 replica.cpp:676] persisted action at 1  i0929 16:58:29.512460  3500 replica.cpp:655] replica received learned notice for position 1  i0929 16:58:29.512778  3500 leveldb.cpp:343] persisting action (151 bytes) to leveldb took 24121ns  i0929 16:58:29.513013  3500 replica.cpp:676] persisted action at 1  i0929 16:58:29.513239  3500 replica.cpp:661] replica learned append action at position 1  i0929 16:58:29.513674  3500 log.cpp:699] attempting to truncate the log to 1  i0929 16:58:29.513954  3500 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0929 16:58:29.514385  3500 replica.cpp:508] replica received write request for position 2  i0929 16:58:29.514680  3500 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 65014ns  i0929 16:58:29.514991  3500 replica.cpp:676] persisted action at 2  i0929 16:58:29.516978  3501 replica.cpp:655] replica received learned notice for position 2  i0929 16:58:29.517319  3501 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 24103ns  i0929 16:58:29.517546  3501 leveldb.cpp:401] deleting 1 keys from leveldb took 16533ns  i0929 16:58:29.517801  3501 replica.cpp:676] persisted action at 2  i0929 16:58:29.518039  3501 replica.cpp:661] replica learned truncate action at position 2  i0929 16:58:29.518539  3507 registrar.cpp:478] successfully updated 'registry'  i0929 16:58:29.518885  3507 registrar.cpp:371] successfully recovered registrar  i0929 16:58:29.519201  3507 master.cpp:1099] recovered 0 slaves from the registry (94b) ; allowing 10mins for slaves to reregister  i0929 16:58:29.533073  3505 slave.cpp:169] slave started on 57)@192.168.122.164:55618  i0929 16:58:29.533500  3505 credentials.hpp:84] loading credential for authentication from '/tmp/allocatortest0slavelostxdxhfg/credential'  i0929 16:58:29.533834  3505 slave.cpp:276] slave using credential for: testprincipal  i0929 16:58:29.534168  3505 slave.cpp:289] slave resources: cpus():2; mem():1024; disk():752; ports():[3100032000]  i0929 16:58:29.534751  3505 slave.cpp:317] slave hostname: fedora20  i0929 16:58:29.534965  3505 slave.cpp:318] slave checkpoint: false  i0929 16:58:29.535557  3505 state.cpp:33] recovering state from '/tmp/allocatortest0slavelostxdxhfg/meta'  i0929 16:58:29.535951  3505 statusupdatemanager.cpp:193] recovering status update manager  i0929 16:58:29.536290  3505 slave.cpp:3271] finished recovery  i0929 16:58:29.536782  3505 slave.cpp:598] new master detected at master@192.168.122.164:55618  i0929 16:58:29.537122  3505 slave.cpp:672] authenticating with master master@192.168.122.164:55618  i0929 16:58:29.537492  3505 slave.cpp:645] detecting new master  i0929 16:58:29.537294  3506 statusupdatemanager.cpp:167] new master detected at master@192.168.122.164:55618  i0929 16:58:29.537642  3507 authenticatee.hpp:128] creating new client sasl connection  i0929 16:58:29.538769  3502 master.cpp:3737] authenticating slave(57)@192.168.122.164:55618  i0929 16:58:29.539091  3502 authenticator.hpp:156] creating new server sasl connection  i0929 16:58:29.539710  3503 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0929 16:58:29.539943  3503 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0929 16:58:29.540206  3502 authenticator.hpp:262] received sasl authentication start  i0929 16:58:29.540457  3502 authenticator.hpp:384] authentication requires more steps  i0929 16:58:29.540757  3502 authenticatee.hpp:265] received sasl authentication step  i0929 16:58:29.541121  3502 authenticator.hpp:290] received sasl authentication step  i0929 16:58:29.541368  3502 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'fedora20' server fqdn: 'fedora20' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0929 16:58:29.541599  3502 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0929 16:58:29.541874  3502 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0929 16:58:29.542129  3502 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'fedora20' server fqdn: 'fedora20' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0929 16:58:29.542333  3502 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0929 16:58:29.542553  3502 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0929 16:58:29.542785  3502 authenticator.hpp:376] authentication success  i0929 16:58:29.543047  3502 authenticatee.hpp:305] authentication success  i0929 16:58:29.543381  3502 slave.cpp:729] successfully authenticated with master master@192.168.122.164:55618  i0929 16:58:29.543707  3502 slave.cpp:992] will retry registration in 11.795692ms if necessary  i0929 16:58:29.543179  3503 master.cpp:3777] successfully authenticated principal 'testprincipal' at slave(57)@192.168.122.164:55618  i0929 16:58:29.544255  3503 master.cpp:2930] registering slave at slave(57)@192.168.122.164:55618 (fedora20) with id 2014092916582927595020165561834860  i0929 16:58:29.544587  3503 registrar.cpp:421] attempting to update the 'registry'  i0929 16:58:29.545816  3500 log.cpp:680] attempting to append 299 bytes to the log  i0929 16:58:29.546267  3500 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0929 16:58:29.546749  3500 replica.cpp:508] replica received write request for position 3  i0929 16:58:29.547030  3500 leveldb.cpp:343] persisting action (318 bytes) to leveldb took 31759ns  i0929 16:58:29.547236  3500 replica.cpp:676] persisted action at 3  i0929 16:58:29.548902  3506 replica.cpp:655] replica received learned notice for position 3  i0929 16:58:29.549139  3506 leveldb.cpp:343] persisting action (320 bytes) to leveldb took 25595ns  i0929 16:58:29.549343  3506 replica.cpp:676] persisted action at 3  i0929 16:58:29.549607  3506 replica.cpp:661] replica learned append action at position 3  i0929 16:58:29.550081  3506 log.cpp:699] attempting to truncate the log to 3  i0929 16:58:29.550497  3506 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0929 16:58:29.550943  3506 replica.cpp:508] replica received write request for position 4  i0929 16:58:29.551198  3506 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 20852ns  i0929 16:58:29.551409  3506 replica.cpp:676] persisted action at 4  i0929 16:58:29.551795  3506 replica.cpp:655] replica received learned notice for position 4  i0929 16:58:29.552094  3506 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 22182ns  i0929 16:58:29.552320  3506 leveldb.cpp:401] deleting 2 keys from leveldb took 18503ns  i0929 16:58:29.552525  3506 replica.cpp:676] persisted action at 4  i0929 16:58:29.552781  3506 replica.cpp:661] replica learned truncate action at position 4  i0929 16:58:29.550289  3503 registrar.cpp:478] successfully updated 'registry'  i0929 16:58:29.553553  3503 master.cpp:2970] registered slave 2014092916582927595020165561834860 at slave(57)@192.168.122.164:55618 (fedora20)  i0929 16:58:29.553807  3503 master.cpp:4180] adding slave 2014092916582927595020165561834860 at slave(57)@192.168.122.164:55618 (fedora20) with cpus():2; mem():1024; disk():752; ports():[3100032000]  i0929 16:58:29.554152  3503 slave.cpp:763] registered with master master@192.168.122.164:55618; given slave id 2014092916582927595020165561834860  i0929 16:58:29.554455  3503 slave.cpp:2345] received ping from slaveobserver(56)@192.168.122.164:55618  i0929 16:58:29.554707  3504 hierarchicalallocatorprocess.hpp:442] added slave 2014092916582927595020165561834860 (fedora20) with cpus():2; mem():1024; disk():752; ports():[3100032000] (and cpus():2; mem():1024; disk():752; ports():[3100032000] available)  i0929 16:58:29.555064  3504 hierarchicalallocatorprocess.hpp:679] performed allocation for slave 2014092916582927595020165561834860 in 13111ns  i0929 16:58:29.558220  3486 sched.cpp:137] version: 0.21.0  i0929 16:58:29.558821  3501 sched.cpp:233] new master detected at master@192.168.122.164:55618  i0929 16:58:29.559054  3501 sched.cpp:283] authenticating with master master@192.168.122.164:55618  i0929 16:58:29.559360  3501 authenticatee.hpp:128] creating new client sasl connection  i0929 16:58:29.560096  3501 master.cpp:3737] authenticating schedulerc8df3f3b2552476f9daf9aa2f012ad28@192.168.122.164:55618  i0929 16:58:29.560430  3501 authenticator.hpp:156] creating new server sasl connection  i0929 16:58:29.561141  3501 authenticatee.hpp:219] received sasl authentication mechanisms: crammd5  i0929 16:58:29.561465  3501 authenticatee.hpp:245] attempting to authenticate with mechanism 'crammd5'  i0929 16:58:29.561743  3501 authenticator.hpp:262] received sasl authentication start  i0929 16:58:29.562098  3501 authenticator.hpp:384] authentication requires more steps  i0929 16:58:29.562353  3501 authenticatee.hpp:265] received sasl authentication step  i0929 16:58:29.562721  3507 authenticator.hpp:290] received sasl authentication step  i0929 16:58:29.563022  3507 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'fedora20' server fqdn: 'fedora20' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0929 16:58:29.563254  3507 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i0929 16:58:29.563484  3507 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i0929 16:58:29.563736  3507 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'fedora20' server fqdn: 'fedora20' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0929 16:58:29.563976  3507 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0929 16:58:29.564188  3507 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0929 16:58:29.564415  3507 authenticator.hpp:376] authentication success  i0929 16:58:29.564673  3507 master.cpp:3777] successfully authenticated principal 'testprincipal' at schedulerc8df3f3b2552476f9daf9aa2f012ad28@192.168.122.164:55618  i0929 16:58:29.568681  3501 authenticatee.hpp:305] authentication success  i0929 16:58:29.569046  3501 sched.cpp:357] successfully authenticated with master master@192.168.122.164:55618  i0929 16:58:29.569286  3501 sched.cpp:476] sending registration request to master@192.168.122.164:55618  i0929 16:58:29.569581  3507 master.cpp:1360] received registration request from schedulerc8df3f3b2552476f9daf9aa2f012ad28@192.168.122.164:55618  i0929 16:58:29.569846  3507 master.cpp:1320] authorizing framework principal 'testprincipal' to receive offers for role ''  i0929 16:58:29.570219  3507 master.cpp:1419] registering framework 2014092916582927595020165561834860000 at schedulerc8df3f3b2552476f9daf9aa2f012ad28@192.168.122.164:55618  i0929 16:58:29.570543  3506 sched.cpp:407] framework registered with 2014092916582927595020165561834860000  i0929 16:58:29.570811  3506 sched.cpp:421] scheduler::registered took 13811ns  i0929 16:58:29.571135  3502 hierarchicalallocatorprocess.hpp:329] added framework 2014092916582927595020165561834860000  i0929 16:58:29.571393  3502 hierarchicalallocatorprocess.hpp:734] offering cpus():2; mem():1024; disk():752; ports():[3100032000] on slave 2014092916582927595020165561834860 to framework 2014092916582927595020165561834860000  i0929 16:58:29.571723  3502 hierarchicalallocatorprocess.hpp:659] performed allocation for 1 slaves in 368547ns  i0929 16:58:29.572125  3507 master.hpp:868] adding offer 2014092916582927595020165561834860 with resources cpus():2; mem():1024; disk():752; ports():[3100032000] on slave 2014092916582927595020165561834860 (fedora20)  i0929 16:58:29.572374  3507 master.cpp:3679] sending 1 offers to framework 2014092916582927595020165561834860000  i0929 16:58:29.572841  3503 sched.cpp:544] scheduler::resourceoffers took 114306ns  i0929 16:58:29.573197  3507 master.hpp:877] removing offer 2014092916582927595020165561834860 with resources cpus():2; mem():1024; disk():752; ports():[3100032000] on slave 2014092916582927595020165561834860 (fedora20)  i0929 16:58:29.573457  3507 master.cpp:2274] processing reply for offers: [ 2014092916582927595020165561834860 ] on slave 2014092916582927595020165561834860 at slave(57)@192.168.122.164:55618 (fedora20) for framework 2014092916582927595020165561834860000  w0929 16:58:29.573717  3507 master.cpp:1944] executor default for task 0 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0929 16:58:29.573953  3507 master.cpp:1955] executor default for task 0 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0929 16:58:29.574177  3507 master.cpp:2357] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  i0929 16:58:29.574745  3507 master.hpp:845] adding task 0 with resources cpus():2; mem():512 on slave 2014092916582927595020165561834860 (fedora20)  i0929 16:58:29.574992  3507 master.cpp:2423] launching task 0 of framework 2014092916582927595020165561834860000 with resources cpus():2; mem( ):512 on slave 2014092916582927595020165561834860 at slave(57)@192.168.122.164:55618 (fedora20)  i0929 16:58:29.575315  3503 slave.cpp:1023] got assigned task 0 for framework 2014092916582927595020165561834860000  i0929 16:58:29.575724  3503 slave.cpp:1133] launching task 0 for framework 2014092916582927595020165561834860000  i0929 16:58:29.578129  3503 exec.cpp:132] version: 0.21.0  i0929 16:58:29.578505  3504 exec.cpp:182] executor started at: executor(30)@192.168.122.164:55618 with pid 3486  i0929 16:58:29.578867  3503 slave.cpp:1246] queuing task '0' for executor default of framework '2014092916582927595020165561834860000  i0929 16:58:29.579144  3503 slave.cpp:554] successfully attached file '/tmp/allocatortest0slavelost_xdxhfg/slaves/2014092916582927595020165561834860/frameworks/2014092916582927595020165561834860000/executors/default/runs/b0de97597054476390f4889ddc3a8524'  i0929 16:58:29.579401  3503 slave.cpp:1756] got registration for executor 'default' of framework 2014092916582927595020165561834860000 fro...",1,train
MESOS-1853,Remove /proc and /sys remounts from port_mapping isolator,"/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",3,train
MESOS-1855,Mesos 0.20.1 doesn't compile,"the compilation of mesos 0.20.1 fails on ubuntu trusty with the following error     slave/containerizer/mesos/containerizer.cpp  fpic dpic o slave/containerizer/mesos/.libs/libmesosno3rdpartyla containerizer.o  in file included from ./linux/routing/filter/ip.hpp:36:0,                   from ./slave/containerizer/isolators/network/portmapping.hpp:42,                   from slave/containerizer/mesos/containerizer.cpp:44:  ./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: no such file or directory   #include ""linux/routing/filter/handle.hpp""                                             ^",1,train
MESOS-1856,Support specifying libnl3 install location.,"libnl_cflags uses a hard coded path in the configure script, instead of detecting the location.",2,train
MESOS-1858,Leaked file descriptors in StatusUpdateStream.,https:/github.com/apache/mesos/blob/master/src/slave/statusupdatemanager.hpp#l180    we should set cloexec for 'fd'.,1,train
MESOS-1862,Performance regression in the Master's http metrics.,"as part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:    https:/github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247    commit 0760b007ad65bc91e8cea377339978c78d36d247  author: benjamin mahler /  date:   thu sep 11 10:48:20 2014 0700        minor cleanups to the master code.        review: https:/reviews.apache.org/r/25566      rather than keeping a running count of allocated resources, we now compute resources ondemand. this was done in order to ignore terminal task's resources.    as a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.      $ time curl localhost:5050/health  real 0m0.004s  user 0m0.001s  sys 0m0.002s    $ time curl localhost:5050/stats.json > /dev/null  real 0m15.402s  user 0m0.001s  sys 0m0.003s    $ time curl localhost:5050/metrics/snapshot > /dev/null  real 0m6.059s  user 0m0.002s  sys 0m0.002s      perf top reveals some of the resource computation during a request to stats.json:    events: 36k cycles   10.53%  libc2.5.so             [.] intfree    9.90%  libc2.5.so             [.] malloc    8.56%  libmesos0.21.0.so  [.] std::rbtree/, std::less/, std::allocator/ >::    8.23%  libc2.5.so             [.] intmalloc    5.80%  libstdc.so.6.0.8      [.] std::rbtreeincrement(std::rbtreenodebase)    5.33%  [kernel]                [k] rawspinlock    3.13%  libstdc.so.6.0.8      [.] std::string::assign(std::string const&)    2.95%  libmesos0.21.0.so  [.] process::socketmanager::exited(process::processbase)    2.43%  libmesos0.21.0.so  [.] mesos::resource::mergefrom(mesos::resource const&)    1.88%  libmesos0.21.0.so  [.] mesos::internal::master::slave::used() const    1.48%  libstdc.so.6.0.8      [.] gnucxx::atomicadd(int volatile, int)    1.45%  [kernel]                [k] findbusiestgroup    1.41%  libc2.5.so             [.] free    1.38%  libmesos0.21.0.so  [.] mesos::valuerange::mergefrom(mesos::valuerange const&)    1.13%  libmesos0.21.0.so  [.] mesos::valuescalar::mergefrom(mesos::valuescalar const&)    1.12%  libmesos0.21.0.so  [.] mesos::resource::shareddtor()    1.07%  libstdc.so.6.0.8      [.] gnucxx::exchangeandadd(int volatile , int)    0.94%  libmesos0.21.0.so  [.] google::protobuf::unknownfieldset::mergefrom(google::protobuf::unknownfieldset const&)    0.92%  libstdc.so.6.0.8      [.] operator new(unsigned long)    0.88%  libmesos0.21.0.so  [.] mesos::valueranges::mergefrom(mesos::valueranges const&)    0.75%  libmesos0.21.0.so  [.] mesos::matches(mesos::resource const&, mesos::resource const&)  ",3,train
MESOS-1863,Split launch tasks and decline offers metrics,"both launchtasks() and declineoffers() scheduler driver calls end up in ""messageslaunchtasks"" metric on the master. it would be nice to split them for differentiating these two calls.",1,train
MESOS-1865,Redirect to the leader master when current master is not a leader,"some of the api endpoints, for example /master/tasks.json, will return bogus information if you query a nonleading master:       jq .  head n 10    [steven@anesthetize:~]% curl http:/master3.mesosvpcqa.otenv.com:5050/master/tasks.json  head n 10      this is very hard for end users to work around.  for example if i query ""which master is leading"" followed by ""leader: which tasks are running"" it is possible that the leader fails over in between, leaving me with an incorrect answer and no way to know that this happened.    in my opinion the api should return the correct response (by asking the current leader?) or an error (500 not the leader?) but it's unacceptable to return a successful wrong answer.  ",3,train
MESOS-1866,Race between ~Authenticator() and Authenticator::authenticate() can lead to schedulers/slaves to never get authenticated,"the master might get a duplicate authenticate() request while a previous authentication attempt is in progress. depending on what the authenticatorprocess is executing at the time, there are 2 possible race conditions which will cause scheduler/slave to continuously retry authentication but never succeed.    we have seen both the race conditions in a heavily loaded production cluster.    race1:    > an authenticate() event was dispatched to authenticatorprocess (master::authenticate() called authenticator::authenticate())    > a terminate() event was then injected into the front of the authenticatorprocess queue (duplicate master::authenticate() did authenticator) before the above authenticate() event was executed.    > due to the bug in libprocess, the future returned by master::authenticate() was never transitioned to discarded (master::authenticate() was never called).    > this caused all the subsequent authentication retries to be enqueued on the master waiting for master::authenticate() to be executed.    fix: transition the dispatched future to discarded if the libprocess is terminated (https:/reviews.apache.org/r/25945/)    race 2:    > an authenticate() event was dispatched to authenticatorprocess (master::authenticate() called authenticator::authenticate())    > authenticatorprocess::authenticate() executed and set promise.ondiscard(defer(self, self::discarded)). note: the internal promise of authenticatorprocess is discarded in authenticatorprocess::discarded()    > a terminate() event was then injected into the front of the authenticatorprocess queue (duplicate master::authenticate() did   authenticator) before the above discarded() event was executed)    > ~authenticatorprocess is destructed without ever discarding the internal promise (master::authenticate() was never called).     > this caused all the subsequent authentication retries to be enqueued on the master waiting for master::authenticate() to be executed.    fix: the fix here is to discard the internal promise when the authenticatorprocess is destructed.",2,train
MESOS-1869,UpdateFramework message might reach the slave before Reregistered message and get dropped,"in reregisterslave() we send 'slavereregisteredmessage' before we link the slave pid, which means a temporary socket will be created and used.    subsequently, after linking, we send the updateframeworkmessage, which creates and uses a persistent socket.    this might lead to outoforder delivery, resulting in updateframeworkmessage reaching the slave before the slavereregisteredmessage and getting dropped because the slave is not yet (re )registered.",1,train
MESOS-1875,os::killtree() incorrectly returns early if pid has terminated,"if groups == true and/or sessions == true then os::killtree() should continue to signal all processes in the process group and/or session, even if the leading pid has terminated.",2,train
MESOS-1901,Slave resources obtained from localhost:5051/state.json is not correct.,"the 'resources' field in slave is uninitialized.    also, seems that 'attributes' field in slave is redundant as we store slave info. ",2,train
MESOS-1903,Add backoff to framework re-registration retries,to avoid so many duplicate framework reregistration attempts (and thus offer rescinds) we should add backoff to reregistration retries.,3,train
MESOS-1913,Create libevent/SSL-backed Socket implementation,nan,13,train
MESOS-1941,Make executor's user owner of executor's cgroup directory,"currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/. this directory in current implementation is only writable by root user. this prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.    to enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.",3,train
MESOS-1943,Add event queue size metrics to scheduler driver,"in the master process, we expose metrics for event queue sizes for various event types. we should do the same for the scheduler driver process.",2,train
MESOS-1955,Specification for Executor and Task life cycles in Slave,"we should create a precise specification of what the mesos source code is supposed to be implementing wrt. the life cycle of executors and tasks. and in addition, we should document why certain design decisions have been made one way or another, to provide guidance for future code changes.    with such a source code independent specification, we could write unbiased regression and scale tests, which would be instrumental in maintaining high quality.    furthermore, this would make the source code more amenable.    why pick this particular area of the source code? shouldn't more of mesos have a thorough specification? probably so. but we need to start somewhere and this area seems to be a good choice, given both its intricacy and its importance.  ",5,train
MESOS-1964,0.21.0 release,mesos release 0.21.0 will include the following major feature(s):     provide state reconciliation for frameworks. https:/issues.apache.org/jira/browse/mesos1407    possible features to include:   isolation of system directories (/tmp) for mesos containers https:/issues.apache.org/jira/browse/mesos1586   expose reason for task_killed https:/issues.apache.org/jira/browse/mesos1930    this ticket will be used to track blockers to this release.  ,5,train
MESOS-1967,Test RoutingTest.INETSockets fails on some machine,"  [ run      ] routingtest.inetsockets  ../../../mesos/src/tests/routingtests.cpp:238: failure  infos: input data out of range  abort: (../../../mesos/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:92): try::get() but state == error: input data out of range aborted at 1414000937 (unix time) try ""date d @1414000937"" if you are using gnu date   pc: @     0x7f2c2d509fc5 giraise   sigabrt (@0x1b49000040b1) received by pid 16561 (tid 0x7f2c31031720) from pid 16561; stack trace:       @     0x7f2c2f0d4ca0 (unknown)      @     0x7f2c2d509fc5 giraise      @     0x7f2c2d50ba70 giabort      @           0x4cf782 abort()      @           0x4cf7bc abort()      @           0x99459e routingtestinetsocketstest::testbody()      @           0xa1c363 testing::internal::handleexceptionsinmethodifsupported/()      @           0xa13617 testing::test::run()      @           0xa136be testing::testinfo::run()      @           0xa137c5 testing::testcase::run()      @           0xa13a68 testing::internal::unittestimpl::runalltests()      @           0xa13cf7 testing::unittest::run()      @           0x49bc4b main      @     0x7f2c2d4f79f4 libcstartmain      @           0x4aad79 (unknown)  make[3]:   [checklocal] aborted  ",2,train
MESOS-1969,RBT only takes revision ranges as args for versions >= 0.6,the support/post reviews.py script doesn't differentiate between rbt versions although the calling conventions for passing revision ranges are different. ,1,train
MESOS-1970,slave and offer ids are indistinguishable in the logs,it is currently impossible to tell slave ids and offer ids apart when looking at logs. adding some differentiator will make log reading a little simpler.,1,train
MESOS-1972,Move TASK_LOST generations due to invalid tasks from scheduler driver to master,"as we move towards pure scheduler/executor clients, it is imperative that the scheduler driver doesn't do validation of tasks and generate task_lost messages itself. all that logic should live in the master. schedulers should reconcile dropped messages via reconciliation.  ",3,train
MESOS-1974,Refactor the C++ Resources abstraction for DiskInfo,"as we introduce diskinfo and reservation for resource. we need to change the c resources abstraction to properly deal with merge/split of resources with those additional fields.    also, the existing c 'resources' interfaces are poorly designed. some of them are confusing and unintuitive. some of them are overloaded with too many functionalities. for instance,      bool operator / resources::get(const resource& r) const;      this one assume resources is flattened, but it might not be.    as we start to introduce persistent disk resources (mesos1554), things will get more complicated. for example, one may want to get two types of 'disk()' functions: one returns the ephemeral disk bytes (with no disk info), one returns the total disk bytes (including ones that have disk info). we may wanna introduce a concept about resource that indicates that a resource cannot be merged or split (e.g., atomic?).    since we need to change this class anyway. i wanna take this chance to refactor it.",8,train
MESOS-1984,Documentation for Egress Control Limit,nan,1,train
MESOS-1989,Container network stats reported by the port mapping isolator is the reverse of the actual network stats.,"looks like the tx/rx network stats reported is the reverse of the actual network stats. the reason is because we simply get tx/rx data from veth on the host.    since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). therefore, we need to flip the data we got from veth.      [jyu@... ]$ sudo ip netns exec 24926 /sbin/ip s link show dev eth0  2: eth0: / mtu 1500 qdisc pfifofast state up mode default qlen 1000      link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff      rx: bytes  packets  errors  dropped overrun mcast         46030857691178 12561038581 0       0       0       0            tx: bytes  packets  errors  dropped carrier collsns       29792886058561 15036798198 0       0       0       0        [jyu@... ]$ ip s link show dev mesos24926  7412: mesos24926: / mtu 1500 qdisc pfifofast state up mode default qlen 1000      link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff      rx: bytes  packets  errors  dropped overrun mcast         29793066979551 15036894749 0       0       0       0            tx: bytes  packets  errors  dropped carrier collsns       46031126366116 12561113732 0       0       0       0  ",1,train
MESOS-2007,AllocatorTest/0.SlaveReregistersFirst is flaky,"  [ run      ] allocatortest/0.slavereregistersfirst  using temporary directory '/tmp/allocatortest0slavereregistersfirstype61d'  i1028 23:48:22.360447 31190 leveldb.cpp:176] opened db in 2.192575ms  i1028 23:48:22.361253 31190 leveldb.cpp:183] compacted db in 760753ns  i1028 23:48:22.361320 31190 leveldb.cpp:198] created db iterator in 22188ns  i1028 23:48:22.361340 31190 leveldb.cpp:204] seeked to beginning of db in 1950ns  i1028 23:48:22.361351 31190 leveldb.cpp:273] iterated through 0 keys in the db in 345ns  i1028 23:48:22.361403 31190 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1028 23:48:22.362185 31217 recover.cpp:437] starting replica recovery  i1028 23:48:22.362764 31219 recover.cpp:463] replica is in empty status  i1028 23:48:22.363955 31210 replica.cpp:638] replica in empty status received a broadcasted recover request  i1028 23:48:22.364320 31217 recover.cpp:188] received a recover response from a replica in empty status  i1028 23:48:22.364820 31211 recover.cpp:554] updating replica status to starting  i1028 23:48:22.365365 31215 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 418991ns  i1028 23:48:22.365391 31215 replica.cpp:320] persisted replica status to starting  i1028 23:48:22.365617 31217 recover.cpp:463] replica is in starting status  i1028 23:48:22.366328 31206 master.cpp:312] master 2014102823482231930294435004331190 (pietas.apache.org) started on 67.195.81.190:50043  i1028 23:48:22.366377 31206 master.cpp:358] master only allowing authenticated frameworks to register  i1028 23:48:22.366391 31206 master.cpp:363] master only allowing authenticated slaves to register  i1028 23:48:22.366402 31206 credentials.hpp:36] loading credentials for authentication from '/tmp/allocatortest0slavereregistersfirstype61d/credentials'  i1028 23:48:22.366708 31206 master.cpp:392] authorization enabled  i1028 23:48:22.366886 31209 replica.cpp:638] replica in starting status received a broadcasted recover request  i1028 23:48:22.367311 31208 master.cpp:120] no whitelist given. advertising offers for all slaves  i1028 23:48:22.367312 31207 recover.cpp:188] received a recover response from a replica in starting status  i1028 23:48:22.367686 31211 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@67.195.81.190:50043  i1028 23:48:22.367863 31212 recover.cpp:554] updating replica status to voting  i1028 23:48:22.368477 31218 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 375527ns  i1028 23:48:22.368505 31218 replica.cpp:320] persisted replica status to voting  i1028 23:48:22.368517 31204 master.cpp:1242] the newly elected leader is master@67.195.81.190:50043 with id 2014102823482231930294435004331190  i1028 23:48:22.368549 31204 master.cpp:1255] elected as the leading master!  i1028 23:48:22.368567 31204 master.cpp:1073] recovering from registrar  i1028 23:48:22.368621 31215 recover.cpp:568] successfully joined the paxos group  i1028 23:48:22.368716 31219 registrar.cpp:313] recovering registrar  i1028 23:48:22.369000 31215 recover.cpp:452] recover process terminated  i1028 23:48:22.369523 31208 log.cpp:656] attempting to start the writer  i1028 23:48:22.370909 31205 replica.cpp:474] replica received implicit promise request with proposal 1  i1028 23:48:22.371266 31205 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 325016ns  i1028 23:48:22.371290 31205 replica.cpp:342] persisted promised to 1  i1028 23:48:22.371979 31218 coordinator.cpp:230] coordinator attemping to fill missing position  i1028 23:48:22.373378 31210 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i1028 23:48:22.373746 31210 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 329018ns  i1028 23:48:22.373772 31210 replica.cpp:676] persisted action at 0  i1028 23:48:22.374897 31214 replica.cpp:508] replica received write request for position 0  i1028 23:48:22.374951 31214 leveldb.cpp:438] reading position from leveldb took 26002ns  i1028 23:48:22.375272 31214 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 289094ns  i1028 23:48:22.375298 31214 replica.cpp:676] persisted action at 0  i1028 23:48:22.375886 31204 replica.cpp:655] replica received learned notice for position 0  i1028 23:48:22.376258 31204 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 346650ns  i1028 23:48:22.376277 31204 replica.cpp:676] persisted action at 0  i1028 23:48:22.376298 31204 replica.cpp:661] replica learned nop action at position 0  i1028 23:48:22.376843 31215 log.cpp:672] writer started with ending position 0  i1028 23:48:22.378056 31205 leveldb.cpp:438] reading position from leveldb took 28265ns  i1028 23:48:22.380323 31217 registrar.cpp:346] successfully fetched the registry (0b) in 11.55584ms  i1028 23:48:22.380466 31217 registrar.cpp:445] applied 1 operations in 50632ns; attempting to update the 'registry'  i1028 23:48:22.382472 31217 log.cpp:680] attempting to append 139 bytes to the log  i1028 23:48:22.382715 31210 coordinator.cpp:340] coordinator attempting to write append action at position 1  i1028 23:48:22.383463 31210 replica.cpp:508] replica received write request for position 1  i1028 23:48:22.383857 31210 leveldb.cpp:343] persisting action (158 bytes) to leveldb took 363758ns  i1028 23:48:22.383875 31210 replica.cpp:676] persisted action at 1  i1028 23:48:22.384397 31218 replica.cpp:655] replica received learned notice for position 1  i1028 23:48:22.384840 31218 leveldb.cpp:343] persisting action (160 bytes) to leveldb took 420161ns  i1028 23:48:22.384862 31218 replica.cpp:676] persisted action at 1  i1028 23:48:22.384882 31218 replica.cpp:661] replica learned append action at position 1  i1028 23:48:22.385684 31211 registrar.cpp:490] successfully updated the 'registry' in 5.158144ms  i1028 23:48:22.385818 31211 registrar.cpp:376] successfully recovered registrar  i1028 23:48:22.385912 31214 log.cpp:699] attempting to truncate the log to 1  i1028 23:48:22.386101 31218 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i1028 23:48:22.386124 31211 master.cpp:1100] recovered 0 slaves from the registry (101b) ; allowing 10mins for slaves to reregister  i1028 23:48:22.387398 31209 replica.cpp:508] replica received write request for position 2  i1028 23:48:22.387758 31209 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 334969ns  i1028 23:48:22.387776 31209 replica.cpp:676] persisted action at 2  i1028 23:48:22.388272 31204 replica.cpp:655] replica received learned notice for position 2  i1028 23:48:22.388453 31204 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 159390ns  i1028 23:48:22.388501 31204 leveldb.cpp:401] deleting 1 keys from leveldb took 30409ns  i1028 23:48:22.388516 31204 replica.cpp:676] persisted action at 2  i1028 23:48:22.388531 31204 replica.cpp:661] replica learned truncate action at position 2  i1028 23:48:22.400737 31207 slave.cpp:169] slave started on 34)@67.195.81.190:50043  i1028 23:48:22.400786 31207 credentials.hpp:84] loading credential for authentication from '/tmp/allocatortest0slavereregistersfirstqppv21/credential'  i1028 23:48:22.400996 31207 slave.cpp:276] slave using credential for: testprincipal  i1028 23:48:22.401304 31207 slave.cpp:289] slave resources: cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000]  i1028 23:48:22.401413 31207 slave.cpp:318] slave hostname: pietas.apache.org  i1028 23:48:22.401520 31207 slave.cpp:319] slave checkpoint: false  w1028 23:48:22.401535 31207 slave.cpp:321] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i1028 23:48:22.402349 31207 state.cpp:33] recovering state from '/tmp/allocatortest0slavereregistersfirstqppv21/meta'  i1028 23:48:22.402678 31207 statusupdatemanager.cpp:197] recovering status update manager  i1028 23:48:22.403048 31211 slave.cpp:3456] finished recovery  i1028 23:48:22.403815 31215 slave.cpp:602] new master detected at master@67.195.81.190:50043  i1028 23:48:22.403852 31215 slave.cpp:665] authenticating with master master@67.195.81.190:50043  i1028 23:48:22.403875 31206 statusupdatemanager.cpp:171] pausing sending status updates  i1028 23:48:22.403961 31215 slave.cpp:638] detecting new master  i1028 23:48:22.404016 31211 authenticatee.hpp:133] creating new client sasl connection  i1028 23:48:22.404230 31204 master.cpp:3853] authenticating slave(34)@67.195.81.190:50043  i1028 23:48:22.404464 31205 authenticator.hpp:161] creating new server sasl connection  i1028 23:48:22.404613 31211 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1028 23:48:22.404649 31211 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1028 23:48:22.404734 31211 authenticator.hpp:267] received sasl authentication start  i1028 23:48:22.404783 31211 authenticator.hpp:389] authentication requires more steps  i1028 23:48:22.404898 31215 authenticatee.hpp:270] received sasl authentication step  i1028 23:48:22.404999 31215 authenticator.hpp:295] received sasl authentication step  i1028 23:48:22.405030 31215 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pietas.apache.org' server fqdn: 'pietas.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1028 23:48:22.405047 31215 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1028 23:48:22.405086 31215 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1028 23:48:22.405109 31215 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pietas.apache.org' server fqdn: 'pietas.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1028 23:48:22.405122 31215 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1028 23:48:22.405129 31215 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1028 23:48:22.405146 31215 authenticator.hpp:381] authentication success  i1028 23:48:22.405243 31213 authenticatee.hpp:310] authentication success  i1028 23:48:22.405253 31214 master.cpp:3893] successfully authenticated principal 'testprincipal' at slave(34)@67.195.81.190:50043  i1028 23:48:22.405505 31213 slave.cpp:722] successfully authenticated with master master@67.195.81.190:50043  i1028 23:48:22.405619 31213 slave.cpp:1050] will retry registration in 17.050994ms if necessary  i1028 23:48:22.405819 31215 master.cpp:3032] registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 2014102823482231930294435004331190s0  i1028 23:48:22.406262 31216 registrar.cpp:445] applied 1 operations in 52647ns; attempting to update the 'registry'  i1028 23:48:22.406697 31190 sched.cpp:137] version: 0.21.0  i1028 23:48:22.407083 31211 sched.cpp:233] new master detected at master@67.195.81.190:50043  i1028 23:48:22.407114 31211 sched.cpp:283] authenticating with master master@67.195.81.190:50043  i1028 23:48:22.407290 31214 authenticatee.hpp:133] creating new client sasl connection  i1028 23:48:22.407424 31214 master.cpp:3853] authenticating scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  i1028 23:48:22.407659 31207 authenticator.hpp:161] creating new server sasl connection  i1028 23:48:22.407757 31207 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1028 23:48:22.407774 31207 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1028 23:48:22.407830 31207 authenticator.hpp:267] received sasl authentication start  i1028 23:48:22.407868 31207 authenticator.hpp:389] authentication requires more steps  i1028 23:48:22.407927 31207 authenticatee.hpp:270] received sasl authentication step  i1028 23:48:22.408015 31212 authenticator.hpp:295] received sasl authentication step  i1028 23:48:22.408037 31212 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pietas.apache.org' server fqdn: 'pietas.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1028 23:48:22.408046 31212 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1028 23:48:22.408072 31212 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1028 23:48:22.408092 31212 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pietas.apache.org' server fqdn: 'pietas.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1028 23:48:22.408100 31212 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1028 23:48:22.408105 31212 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1028 23:48:22.408116 31212 authenticator.hpp:381] authentication success  i1028 23:48:22.408192 31210 authenticatee.hpp:310] authentication success  i1028 23:48:22.408210 31217 master.cpp:3893] successfully authenticated principal 'testprincipal' at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  i1028 23:48:22.408419 31210 sched.cpp:357] successfully authenticated with master master@67.195.81.190:50043  i1028 23:48:22.408460 31210 sched.cpp:476] sending registration request to master@67.195.81.190:50043  i1028 23:48:22.408568 31217 master.cpp:1362] received registration request for framework 'default' at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  i1028 23:48:22.408617 31217 master.cpp:1321] authorizing framework principal 'testprincipal' to receive offers for role ''  i1028 23:48:22.408937 31214 master.cpp:1426] registering framework 20141028234822319302944350043311900000 (default) at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  i1028 23:48:22.409265 31213 sched.cpp:407] framework registered with 20141028234822319302944350043311900000  i1028 23:48:22.409267 31212 hierarchicalallocatorprocess.hpp:329] added framework 20141028234822319302944350043311900000  i1028 23:48:22.409312 31212 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1028 23:48:22.409324 31215 log.cpp:680] attempting to append 316 bytes to the log  i1028 23:48:22.409333 31213 sched.cpp:421] scheduler::registered took 38591ns  i1028 23:48:22.409327 31212 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 24107ns  i1028 23:48:22.409518 31205 coordinator.cpp:340] coordinator attempting to write append action at position 3  i1028 23:48:22.410127 31206 replica.cpp:508] replica received write request for position 3  i1028 23:48:22.410706 31206 leveldb.cpp:343] persisting action (335 bytes) to leveldb took 554098ns  i1028 23:48:22.410725 31206 replica.cpp:676] persisted action at 3  i1028 23:48:22.411151 31217 replica.cpp:655] replica received learned notice for position 3  i1028 23:48:22.411499 31217 leveldb.cpp:343] persisting action (337 bytes) to leveldb took 326572ns  i1028 23:48:22.411519 31217 replica.cpp:676] persisted action at 3  i1028 23:48:22.411533 31217 replica.cpp:661] replica learned append action at position 3  i1028 23:48:22.412292 31219 registrar.cpp:490] successfully updated the 'registry' in 5.972992ms  i1028 23:48:22.412518 31218 log.cpp:699] attempting to truncate the log to 3  i1028 23:48:22.412621 31213 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i1028 23:48:22.412734 31219 slave.cpp:2522] received ping from slaveobserver(38)@67.195.81.190:50043  i1028 23:48:22.412787 31206 master.cpp:3086] registered slave 2014102823482231930294435004331190s0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000]  i1028 23:48:22.412858 31219 slave.cpp:756] registered with master master@67.195.81.190:50043; given slave id 2014102823482231930294435004331190s0  i1028 23:48:22.412994 31210 statusupdatemanager.cpp:178] resuming sending status updates  i1028 23:48:22.413014 31211 hierarchicalallocatorprocess.hpp:442] added slave 2014102823482231930294435004331190s0 (pietas.apache.org) with cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000] (and cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000] available)  i1028 23:48:22.413159 31211 hierarchicalallocatorprocess.hpp:734] offering cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000] on slave 2014102823482231930294435004331190s0 to framework 20141028234822319302944350043311900000  i1028 23:48:22.413290 31208 replica.cpp:508] replica received write request for position 4  i1028 23:48:22.413421 31211 hierarchicalallocatorprocess.hpp:679] performed allocation for slave 2014102823482231930294435004331190s0 in 346658ns  i1028 23:48:22.413650 31208 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 336067ns  i1028 23:48:22.413668 31208 replica.cpp:676] persisted action at 4  i1028 23:48:22.413797 31216 master.cpp:3795] sending 1 offers to framework 20141028234822319302944350043311900000 (default) at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  i1028 23:48:22.414077 31212 replica.cpp:655] replica received learned notice for position 4  i1028 23:48:22.414356 31212 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 260401ns  i1028 23:48:22.414403 31212 leveldb.cpp:401] deleting 2 keys from leveldb took 28541ns  i1028 23:48:22.414417 31212 replica.cpp:676] persisted action at 4  i1028 23:48:22.414446 31212 replica.cpp:661] replica learned truncate action at position 4  i1028 23:48:22.414422 31207 sched.cpp:544] scheduler::resourceoffers took 310278ns  i1028 23:48:22.415086 31214 master.cpp:2321] processing reply for offers: [ 2014102823482231930294435004331190o0 ] on slave 2014102823482231930294435004331190s0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028234822319302944350043311900000 (default) at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043  w1028 23:48:22.415163 31214 master.cpp:1969] executor default for task 0 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w1028 23:48:22.415186 31214 master.cpp:1980] executor default for task 0 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i1028 23:48:22.415256 31214 master.cpp:2417] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  i1028 23:48:22.416033 31219 master.hpp:877] adding task 0 with resources cpus():1; mem():500 on slave 2014102823482231930294435004331190s0 (pietas.apache.org)  i1028 23:48:22.416084 31219 master.cpp:2480] launching task 0 of framework 20141028234822319302944350043311900000 (default) at scheduler0aa33fc70d29487c80ebf933681f9c95@67.195.81.190:50043 with resources cpus():1; mem():500 on slave 2014102823482231930294435004331190s0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)  i1028 23:48:22.416317 31214 slave.cpp:1081] got assigned task 0 for framework 20141028234822319302944350043311900000  i1028 23:48:22.416679 31215 hierarchicalallocatorprocess.hpp:563] recovered cpus():1; mem():524; disk():3.70122e06; ports():[3100032000] (total allocatable: cpus():1; mem():524; disk():3.70122e+06; ports( ):[3100032000]) on slave 2014102823482231930294435004331190s0 from framework 20141028234822319302944350043311900000  i1028 23:48:22.416721 31215 hierarchicalallocatorprocess.hpp:599] framework 2014102823482231930294435004331190 0000 filtered slave 2...",2,train
MESOS-2008,MasterAuthorizationTest.DuplicateReregistration is flaky,  [ run      ] masterauthorizationtest.duplicatereregistration  using temporary directory '/tmp/masterauthorizationtestduplicatereregistrationdlomyx'  i1029 08:25:26.021766 32232 leveldb.cpp:176] opened db in 3.066621ms  i1029 08:25:26.022734 32232 leveldb.cpp:183] compacted db in 935019ns  i1029 08:25:26.022766 32232 leveldb.cpp:198] created db iterator in 4350ns  i1029 08:25:26.022785 32232 leveldb.cpp:204] seeked to beginning of db in 902ns  i1029 08:25:26.022799 32232 leveldb.cpp:273] iterated through 0 keys in the db in 387ns  i1029 08:25:26.022831 32232 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1029 08:25:26.023305 32248 recover.cpp:437] starting replica recovery  i1029 08:25:26.023598 32248 recover.cpp:463] replica is in empty status  i1029 08:25:26.025059 32260 replica.cpp:638] replica in empty status received a broadcasted recover request  i1029 08:25:26.025320 32247 recover.cpp:188] received a recover response from a replica in empty status  i1029 08:25:26.025585 32256 recover.cpp:554] updating replica status to starting  i1029 08:25:26.026546 32249 master.cpp:312] master 2014102908252631426977954069632232 (pomona.apache.org) started on 67.195.81.187:40696  i1029 08:25:26.026561 32261 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 694444ns  i1029 08:25:26.026592 32249 master.cpp:358] master only allowing authenticated frameworks to register  i1029 08:25:26.026592 32261 replica.cpp:320] persisted replica status to starting  i1029 08:25:26.026605 32249 master.cpp:363] master only allowing authenticated slaves to register  i1029 08:25:26.026639 32249 credentials.hpp:36] loading credentials for authentication from '/tmp/masterauthorizationtestduplicatereregistrationdlomyx/credentials'  i1029 08:25:26.026877 32249 master.cpp:392] authorization enabled  i1029 08:25:26.026901 32260 recover.cpp:463] replica is in starting status  i1029 08:25:26.027498 32261 master.cpp:120] no whitelist given. advertising offers for all slaves  i1029 08:25:26.027541 32248 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@67.195.81.187:40696  i1029 08:25:26.028055 32252 replica.cpp:638] replica in starting status received a broadcasted recover request  i1029 08:25:26.028451 32247 recover.cpp:188] received a recover response from a replica in starting status  i1029 08:25:26.028733 32249 master.cpp:1242] the newly elected leader is master@67.195.81.187:40696 with id 2014102908252631426977954069632232  i1029 08:25:26.028764 32249 master.cpp:1255] elected as the leading master!  i1029 08:25:26.028781 32249 master.cpp:1073] recovering from registrar  i1029 08:25:26.028904 32246 recover.cpp:554] updating replica status to voting  i1029 08:25:26.029163 32257 registrar.cpp:313] recovering registrar  i1029 08:25:26.029556 32251 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 485711ns  i1029 08:25:26.029588 32251 replica.cpp:320] persisted replica status to voting  i1029 08:25:26.029726 32253 recover.cpp:568] successfully joined the paxos group  i1029 08:25:26.029932 32253 recover.cpp:452] recover process terminated  i1029 08:25:26.030436 32250 log.cpp:656] attempting to start the writer  i1029 08:25:26.032152 32248 replica.cpp:474] replica received implicit promise request with proposal 1  i1029 08:25:26.032778 32248 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 597030ns  i1029 08:25:26.032807 32248 replica.cpp:342] persisted promised to 1  i1029 08:25:26.033481 32254 coordinator.cpp:230] coordinator attemping to fill missing position  i1029 08:25:26.035429 32247 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i1029 08:25:26.036154 32247 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 690208ns  i1029 08:25:26.036181 32247 replica.cpp:676] persisted action at 0  i1029 08:25:26.037344 32249 replica.cpp:508] replica received write request for position 0  i1029 08:25:26.037395 32249 leveldb.cpp:438] reading position from leveldb took 22607ns  i1029 08:25:26.038074 32249 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 647429ns  i1029 08:25:26.038105 32249 replica.cpp:676] persisted action at 0  i1029 08:25:26.038683 32247 replica.cpp:655] replica received learned notice for position 0  i1029 08:25:26.039378 32247 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 664911ns  i1029 08:25:26.039407 32247 replica.cpp:676] persisted action at 0  i1029 08:25:26.039433 32247 replica.cpp:661] replica learned nop action at position 0  i1029 08:25:26.040045 32252 log.cpp:672] writer started with ending position 0  i1029 08:25:26.041378 32251 leveldb.cpp:438] reading position from leveldb took 25625ns  i1029 08:25:26.044642 32246 registrar.cpp:346] successfully fetched the registry (0b) in 15.433984ms  i1029 08:25:26.044742 32246 registrar.cpp:445] applied 1 operations in 16444ns; attempting to update the 'registry'  i1029 08:25:26.047538 32256 log.cpp:680] attempting to append 139 bytes to the log  i1029 08:25:26.156330 32247 coordinator.cpp:340] coordinator attempting to write append action at position 1  i1029 08:25:26.158460 32261 replica.cpp:508] replica received write request for position 1  i1029 08:25:26.159277 32261 leveldb.cpp:343] persisting action (158 bytes) to leveldb took 782308ns  i1029 08:25:26.159328 32261 replica.cpp:676] persisted action at 1  i1029 08:25:26.160267 32255 replica.cpp:655] replica received learned notice for position 1  i1029 08:25:26.161070 32255 leveldb.cpp:343] persisting action (160 bytes) to leveldb took 750259ns  i1029 08:25:26.161100 32255 replica.cpp:676] persisted action at 1  i1029 08:25:26.161125 32255 replica.cpp:661] replica learned append action at position 1  i1029 08:25:26.162199 32253 registrar.cpp:490] successfully updated the 'registry' in 117.40416ms  i1029 08:25:26.162400 32253 registrar.cpp:376] successfully recovered registrar  i1029 08:25:26.162724 32249 master.cpp:1100] recovered 0 slaves from the registry (101b) ; allowing 10mins for slaves to reregister  i1029 08:25:26.162757 32253 log.cpp:699] attempting to truncate the log to 1  i1029 08:25:26.162919 32256 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i1029 08:25:26.163949 32250 replica.cpp:508] replica received write request for position 2  i1029 08:25:26.164589 32250 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 603175ns  i1029 08:25:26.164618 32250 replica.cpp:676] persisted action at 2  i1029 08:25:26.165385 32251 replica.cpp:655] replica received learned notice for position 2  i1029 08:25:26.166007 32251 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 594003ns  i1029 08:25:26.166056 32251 leveldb.cpp:401] deleting ~1 keys from leveldb took 23309ns  i1029 08:25:26.166077 32251 replica.cpp:676] persisted action at 2  i1029 08:25:26.166100 32251 replica.cpp:661] replica learned truncate action at position 2  i1029 08:25:26.178493 32232 sched.cpp:137] version: 0.21.0  i1029 08:25:26.179029 32256 sched.cpp:233] new master detected at master@67.195.81.187:40696  i1029 08:25:26.179078 32256 sched.cpp:283] authenticating with master master@67.195.81.187:40696  i1029 08:25:26.179424 32246 authenticatee.hpp:133] creating new client sasl connection  i1029 08:25:26.179678 32259 master.cpp:3853] authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.179970 32250 authenticator.hpp:161] creating new server sasl connection  i1029 08:25:26.180165 32250 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1029 08:25:26.180191 32250 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1029 08:25:26.180272 32250 authenticator.hpp:267] received sasl authentication start  i1029 08:25:26.180378 32250 authenticator.hpp:389] authentication requires more steps  i1029 08:25:26.180557 32260 authenticatee.hpp:270] received sasl authentication step  i1029 08:25:26.180704 32254 authenticator.hpp:295] received sasl authentication step  i1029 08:25:26.180737 32254 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1029 08:25:26.180748 32254 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1029 08:25:26.180780 32254 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1029 08:25:26.180804 32254 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1029 08:25:26.180816 32254 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1029 08:25:26.180824 32254 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1029 08:25:26.180841 32254 authenticator.hpp:381] authentication success  i1029 08:25:26.180937 32259 authenticatee.hpp:310] authentication success  i1029 08:25:26.180991 32260 master.cpp:3893] successfully authenticated principal 'testprincipal' at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.181422 32259 sched.cpp:357] successfully authenticated with master master@67.195.81.187:40696  i1029 08:25:26.181449 32259 sched.cpp:476] sending registration request to master@67.195.81.187:40696  i1029 08:25:26.181697 32260 master.cpp:1362] received registration request for framework 'default' at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.181758 32260 master.cpp:1321] authorizing framework principal 'testprincipal' to receive offers for role ''  i1029 08:25:26.182063 32260 master.cpp:1426] registering framework 20141029082526314269779540696322320000 (default) at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.182430 32248 hierarchicalallocatorprocess.hpp:329] added framework 20141029082526314269779540696322320000  i1029 08:25:26.182462 32248 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:26.182462 32261 sched.cpp:407] framework registered with 20141029082526314269779540696322320000  i1029 08:25:26.182473 32248 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 15372ns  i1029 08:25:26.182554 32261 sched.cpp:421] scheduler::registered took 60059ns  i1029 08:25:26.185515 32260 sched.cpp:227] scheduler::disconnected took 16607ns  i1029 08:25:26.185538 32260 sched.cpp:233] new master detected at master@67.195.81.187:40696  i1029 08:25:26.185567 32260 sched.cpp:283] authenticating with master master@67.195.81.187:40696  i1029 08:25:26.185783 32246 authenticatee.hpp:133] creating new client sasl connection  i1029 08:25:26.186218 32250 master.cpp:3853] authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.186456 32247 authenticator.hpp:161] creating new server sasl connection  i1029 08:25:26.186594 32250 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1029 08:25:26.186621 32250 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1029 08:25:26.186745 32259 authenticator.hpp:267] received sasl authentication start  i1029 08:25:26.186800 32259 authenticator.hpp:389] authentication requires more steps  i1029 08:25:26.186936 32260 authenticatee.hpp:270] received sasl authentication step  i1029 08:25:26.187062 32249 authenticator.hpp:295] received sasl authentication step  i1029 08:25:26.187095 32249 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1029 08:25:26.187108 32249 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1029 08:25:26.187137 32249 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1029 08:25:26.187162 32249 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1029 08:25:26.187175 32249 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1029 08:25:26.187182 32249 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1029 08:25:26.187199 32249 authenticator.hpp:381] authentication success  i1029 08:25:26.187327 32249 authenticatee.hpp:310] authentication success  i1029 08:25:26.187366 32260 master.cpp:3893] successfully authenticated principal 'testprincipal' at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:26.187631 32249 sched.cpp:357] successfully authenticated with master master@67.195.81.187:40696  i1029 08:25:26.187659 32249 sched.cpp:476] sending registration request to master@67.195.81.187:40696  i1029 08:25:27.028445 32251 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:28.045682 32251 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 1.017231941secs  i1029 08:25:28.045760 32249 sched.cpp:476] sending registration request to master@67.195.81.187:40696  i1029 08:25:28.045900 32253 master.cpp:1499] received reregistration request from framework 20141029082526314269779540696322320000 (default) at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:28.045989 32253 master.cpp:1321] authorizing framework principal 'testprincipal' to receive offers for role ''  i1029 08:25:28.046455 32253 master.cpp:1499] received reregistration request from framework 20141029082526314269779540696322320000 (default) at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:28.046529 32253 master.cpp:1321] authorizing framework principal 'testprincipal' to receive offers for role ''  i1029 08:25:28.050155 32247 sched.cpp:233] new master detected at master@67.195.81.187:40696  i1029 08:25:28.050217 32247 sched.cpp:283] authenticating with master master@67.195.81.187:40696  i1029 08:25:28.050405 32252 master.cpp:1552] reregistering framework 20141029082526314269779540696322320000 (default)  at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:28.050509 32253 authenticatee.hpp:133] creating new client sasl connection  i1029 08:25:28.050566 32252 master.cpp:1592] allowing framework 20141029082526314269779540696322320000 (default) at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696 to reregister with an already used id  i1029 08:25:28.051084 32257 sched.cpp:449] framework reregistered with 20141029082526314269779540696322320000  i1029 08:25:28.051151 32252 master.cpp:3853] authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:28.051167 32257 sched.cpp:463] scheduler::reregistered took 52801ns  i1029 08:25:28.051723 32261 authenticator.hpp:161] creating new server sasl connection  i1029 08:25:28.052042 32249 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1029 08:25:28.052077 32249 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1029 08:25:28.052170 32249 master.cpp:1534] dropping reregistration request of framework 20141029082526314269779540696322320000 (default) at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress  i1029 08:25:28.052218 32257 authenticator.hpp:267] received sasl authentication start  i1029 08:25:28.052325 32257 authenticator.hpp:389] authentication requires more steps  i1029 08:25:28.052428 32257 authenticatee.hpp:270] received sasl authentication step  i1029 08:25:28.052641 32246 authenticator.hpp:295] received sasl authentication step  i1029 08:25:28.052685 32246 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1029 08:25:28.052701 32246 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1029 08:25:28.052739 32246 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1029 08:25:28.052767 32246 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1029 08:25:28.052779 32246 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1029 08:25:28.052788 32246 auxprop.cpp:103] skipping auxiliary property ' cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1029 08:25:28.052804 32246 authenticator.hpp:381] authentication success  i1029 08:25:28.052947 32252 authenticatee.hpp:310] authentication success  i1029 08:25:28.053020 32246 master.cpp:3893] successfully authenticated principal 'testprincipal' at scheduler9ba6b80340b448b9bcef45a329f6b2a4@67.195.81.187:40696  i1029 08:25:28.053462 32247 sched.cpp:357] successfully authenticated with master master@67.195.81.187:40696  i1029 08:25:29.046855 32261 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:29.046880 32261 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 35632ns  i1029 08:25:30.047458 32253 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:30.047487 32253 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 43031ns  i1029 08:25:31.028373 32261 master.cpp:120] no whitelist given. advertising offers for all slaves  i1029 08:25:31.048673 32249 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:31.048702 32249 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 44769ns  i1029 08:25:32.049576 32259 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:32.049604 32259 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 51919ns  i1029 08:25:33.050864 32249 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:33.050896 32249 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 38019ns  i1029 08:25:34.051961 32251 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:34.051993 32251 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 64619ns  i1029 08:25:35.052196 32249 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:35.052223 32249 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 34475ns  i1029 08:25:36.029101 32259 master.cpp:120] no whitelist given. advertising offers for all slaves  i1029 08:25:36.053067 32249 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:36.053095 32249 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 38354ns  i1029 08:25:37.053506 32259 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1029 08:25:37.053536 32259 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 38249ns  tests/masterauthorizationtests.cpp:877: failure  failed to wait 10secs for frameworkreregisteredmessage  i1029 08:25:38.053241 32259 master.cpp:768] framework 2014...,2,train
MESOS-2017,"Segfault with ""Pure virtual method called"" when tests fail","the most recent one:      [ run      ] drfallocatortest.drfallocatorprocess  using temporary directory '/tmp/drfallocatortestdrfallocatorprocessbi905j'  i1030 05:55:06.934813 24459 leveldb.cpp:176] opened db in 3.175202ms  i1030 05:55:06.935925 24459 leveldb.cpp:183] compacted db in 1.077924ms  i1030 05:55:06.935976 24459 leveldb.cpp:198] created db iterator in 16460ns  i1030 05:55:06.935995 24459 leveldb.cpp:204] seeked to beginning of db in 2018ns  i1030 05:55:06.936005 24459 leveldb.cpp:273] iterated through 0 keys in the db in 335ns  i1030 05:55:06.936039 24459 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1030 05:55:06.936705 24480 recover.cpp:437] starting replica recovery  i1030 05:55:06.937023 24480 recover.cpp:463] replica is in empty status  i1030 05:55:06.938158 24475 replica.cpp:638] replica in empty status received a broadcasted recover request  i1030 05:55:06.938859 24482 recover.cpp:188] received a recover response from a replica in empty status  i1030 05:55:06.939486 24474 recover.cpp:554] updating replica status to starting  i1030 05:55:06.940249 24489 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 591981ns  i1030 05:55:06.940274 24489 replica.cpp:320] persisted replica status to starting  i1030 05:55:06.940752 24481 recover.cpp:463] replica is in starting status  i1030 05:55:06.940820 24489 master.cpp:312] master 2014103005550631426977954042924459 (pomona.apache.org) started on 67.195.81.187:40429  i1030 05:55:06.940871 24489 master.cpp:358] master only allowing authenticated frameworks to register  i1030 05:55:06.940891 24489 master.cpp:363] master only allowing authenticated slaves to register  i1030 05:55:06.940908 24489 credentials.hpp:36] loading credentials for authentication from '/tmp/drfallocatortestdrfallocatorprocessbi905j/credentials'  i1030 05:55:06.941215 24489 master.cpp:392] authorization enabled  i1030 05:55:06.941751 24475 master.cpp:120] no whitelist given. advertising offers for all slaves  i1030 05:55:06.942227 24474 replica.cpp:638] replica in starting status received a broadcasted recover request  i1030 05:55:06.942401 24476 hierarchicalallocatorprocess.hpp:299] initializing hierarchical allocator process with master : master@67.195.81.187:40429  i1030 05:55:06.942895 24483 recover.cpp:188] received a recover response from a replica in starting status  i1030 05:55:06.943035 24474 master.cpp:1242] the newly elected leader is master@67.195.81.187:40429 with id 2014103005550631426977954042924459  i1030 05:55:06.943063 24474 master.cpp:1255] elected as the leading master!  i1030 05:55:06.943079 24474 master.cpp:1073] recovering from registrar  i1030 05:55:06.943313 24480 registrar.cpp:313] recovering registrar  i1030 05:55:06.943455 24475 recover.cpp:554] updating replica status to voting  i1030 05:55:06.944144 24474 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 536365ns  i1030 05:55:06.944172 24474 replica.cpp:320] persisted replica status to voting  i1030 05:55:06.944355 24489 recover.cpp:568] successfully joined the paxos group  i1030 05:55:06.944576 24489 recover.cpp:452] recover process terminated  i1030 05:55:06.945155 24486 log.cpp:656] attempting to start the writer  i1030 05:55:06.947013 24473 replica.cpp:474] replica received implicit promise request with proposal 1  i1030 05:55:06.947854 24473 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 806463ns  i1030 05:55:06.947883 24473 replica.cpp:342] persisted promised to 1  i1030 05:55:06.948547 24481 coordinator.cpp:230] coordinator attemping to fill missing position  i1030 05:55:06.950269 24479 replica.cpp:375] replica received explicit promise request for position 0 with proposal 2  i1030 05:55:06.950933 24479 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 603843ns  i1030 05:55:06.950961 24479 replica.cpp:676] persisted action at 0  i1030 05:55:06.952180 24476 replica.cpp:508] replica received write request for position 0  i1030 05:55:06.952239 24476 leveldb.cpp:438] reading position from leveldb took 28437ns  i1030 05:55:06.952896 24476 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 623980ns  i1030 05:55:06.952926 24476 replica.cpp:676] persisted action at 0  i1030 05:55:06.953543 24485 replica.cpp:655] replica received learned notice for position 0  i1030 05:55:06.954082 24485 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 511807ns  i1030 05:55:06.954107 24485 replica.cpp:676] persisted action at 0  i1030 05:55:06.954128 24485 replica.cpp:661] replica learned nop action at position 0  i1030 05:55:06.954710 24473 log.cpp:672] writer started with ending position 0  i1030 05:55:06.956215 24478 leveldb.cpp:438] reading position from leveldb took 33085ns  i1030 05:55:06.959481 24475 registrar.cpp:346] successfully fetched the registry (0b) in 16.11904ms  i1030 05:55:06.959616 24475 registrar.cpp:445] applied 1 operations in 28239ns; attempting to update the 'registry'  i1030 05:55:06.962514 24487 log.cpp:680] attempting to append 139 bytes to the log  i1030 05:55:06.962646 24474 coordinator.cpp:340] coordinator attempting to write append action at position 1  i1030 05:55:06.964146 24486 replica.cpp:508] replica received write request for position 1  i1030 05:55:06.964962 24486 leveldb.cpp:343] persisting action (158 bytes) to leveldb took 743389ns  i1030 05:55:06.964993 24486 replica.cpp:676] persisted action at 1  i1030 05:55:06.965895 24473 replica.cpp:655] replica received learned notice for position 1  i1030 05:55:06.966531 24473 leveldb.cpp:343] persisting action (160 bytes) to leveldb took 607242ns  i1030 05:55:06.966555 24473 replica.cpp:676] persisted action at 1  i1030 05:55:06.966578 24473 replica.cpp:661] replica learned append action at position 1  i1030 05:55:06.967706 24481 registrar.cpp:490] successfully updated the 'registry' in 8.036096ms  i1030 05:55:06.967895 24481 registrar.cpp:376] successfully recovered registrar  i1030 05:55:06.967993 24482 log.cpp:699] attempting to truncate the log to 1  i1030 05:55:06.968258 24479 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i1030 05:55:06.968268 24475 master.cpp:1100] recovered 0 slaves from the registry (101b) ; allowing 10mins for slaves to reregister  i1030 05:55:06.969156 24476 replica.cpp:508] replica received write request for position 2  i1030 05:55:06.969678 24476 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 491913ns  i1030 05:55:06.969703 24476 replica.cpp:676] persisted action at 2  i1030 05:55:06.970459 24478 replica.cpp:655] replica received learned notice for position 2  i1030 05:55:06.971060 24478 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 573076ns  i1030 05:55:06.971124 24478 leveldb.cpp:401] deleting 1 keys from leveldb took 35339ns  i1030 05:55:06.971145 24478 replica.cpp:676] persisted action at 2  i1030 05:55:06.971168 24478 replica.cpp:661] replica learned truncate action at position 2  i1030 05:55:06.980211 24459 containerizer.cpp:100] using isolation: posix/cpu,posix/mem  i1030 05:55:06.984153 24473 slave.cpp:169] slave started on 203)@67.195.81.187:40429  i1030 05:55:07.055308 24473 credentials.hpp:84] loading credential for authentication from '/tmp/drfallocatortestdrfallocatorprocesswulx31/credential'  i1030 05:55:06.988750 24459 sched.cpp:137] version: 0.21.0  i1030 05:55:07.055521 24473 slave.cpp:276] slave using credential for: testprincipal  i1030 05:55:07.055726 24473 slave.cpp:289] slave resources: cpus():2; mem():1024; disk():0; ports():[3100032000]  i1030 05:55:07.055865 24473 slave.cpp:318] slave hostname: pomona.apache.org  i1030 05:55:07.055881 24473 slave.cpp:319] slave checkpoint: false  w1030 05:55:07.055889 24473 slave.cpp:321] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i1030 05:55:07.056172 24485 sched.cpp:233] new master detected at master@67.195.81.187:40429  i1030 05:55:07.056222 24485 sched.cpp:283] authenticating with master master@67.195.81.187:40429  i1030 05:55:07.056717 24485 state.cpp:33] recovering state from '/tmp/drfallocatortestdrfallocatorprocesswulx31/meta'  i1030 05:55:07.056851 24475 authenticatee.hpp:133] creating new client sasl connection  i1030 05:55:07.057003 24473 statusupdatemanager.cpp:197] recovering status update manager  i1030 05:55:07.057252 24488 master.cpp:3853] authenticating schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:07.057502 24489 containerizer.cpp:281] recovering containerizer  i1030 05:55:07.057524 24475 authenticator.hpp:161] creating new server sasl connection  i1030 05:55:07.057688 24475 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1030 05:55:07.057719 24475 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1030 05:55:07.057919 24481 authenticator.hpp:267] received sasl authentication start  i1030 05:55:07.057968 24481 authenticator.hpp:389] authentication requires more steps  i1030 05:55:07.058070 24473 authenticatee.hpp:270] received sasl authentication step  i1030 05:55:07.058199 24485 authenticator.hpp:295] received sasl authentication step  i1030 05:55:07.058223 24485 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1030 05:55:07.058233 24485 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1030 05:55:07.058259 24485 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1030 05:55:07.058290 24485 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1030 05:55:07.058302 24485 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1030 05:55:07.058307 24485 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1030 05:55:07.058320 24485 authenticator.hpp:381] authentication success  i1030 05:55:07.058467 24480 master.cpp:3893] successfully authenticated principal 'testprincipal' at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:07.058493 24485 slave.cpp:3456] finished recovery  i1030 05:55:07.058593 24478 authenticatee.hpp:310] authentication success  i1030 05:55:07.058838 24478 sched.cpp:357] successfully authenticated with master master@67.195.81.187:40429  i1030 05:55:07.058861 24478 sched.cpp:476] sending registration request to master@67.195.81.187:40429  i1030 05:55:07.058969 24475 slave.cpp:602] new master detected at master@67.195.81.187:40429  i1030 05:55:07.058969 24487 statusupdatemanager.cpp:171] pausing sending status updates  i1030 05:55:07.059026 24475 slave.cpp:665] authenticating with master master@67.195.81.187:40429  i1030 05:55:07.059061 24481 master.cpp:1362] received registration request for framework 'framework1' at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:07.059131 24481 master.cpp:1321] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i1030 05:55:07.059171 24475 slave.cpp:638] detecting new master  i1030 05:55:07.059214 24482 authenticatee.hpp:133] creating new client sasl connection  i1030 05:55:07.059550 24481 master.cpp:3853] authenticating slave(203)@67.195.81.187:40429  i1030 05:55:07.059787 24487 authenticator.hpp:161] creating new server sasl connection  i1030 05:55:07.059922 24481 master.cpp:1426] registering framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:07.059996 24474 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1030 05:55:07.060034 24474 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  i1030 05:55:07.060117 24474 authenticator.hpp:267] received sasl authentication start  i1030 05:55:07.060165 24474 authenticator.hpp:389] authentication requires more steps  i1030 05:55:07.060377 24476 hierarchicalallocatorprocess.hpp:329] added framework 20141030055506314269779540429244590000  i1030 05:55:07.060394 24488 sched.cpp:407] framework registered with 20141030055506314269779540429244590000  i1030 05:55:07.060403 24476 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1030 05:55:07.060431 24476 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 29857ns  i1030 05:55:07.060443 24488 sched.cpp:421] scheduler::registered took 19407ns  i1030 05:55:07.060545 24478 authenticatee.hpp:270] received sasl authentication step  i1030 05:55:07.060645 24478 authenticator.hpp:295] received sasl authentication step  i1030 05:55:07.060673 24478 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1030 05:55:07.060685 24478 auxprop.cpp:153] looking up auxiliary property 'userpassword'  i1030 05:55:07.060714 24478 auxprop.cpp:153] looking up auxiliary property 'cmusaslsecretcrammd5'  i1030 05:55:07.060740 24478 auxprop.cpp:81] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1030 05:55:07.060760 24478 auxprop.cpp:103] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1030 05:55:07.060770 24478 auxprop.cpp:103] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1030 05:55:07.060788 24478 authenticator.hpp:381] authentication success  i1030 05:55:07.060920 24474 authenticatee.hpp:310] authentication success  i1030 05:55:07.060945 24485 master.cpp:3893] successfully authenticated principal 'testprincipal' at slave(203)@67.195.81.187:40429  i1030 05:55:07.061388 24489 slave.cpp:722] successfully authenticated with master master@67.195.81.187:40429  i1030 05:55:07.061504 24489 slave.cpp:1050] will retry registration in 4.778336ms if necessary  i1030 05:55:07.061718 24480 master.cpp:3032] registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 2014103005550631426977954042924459s0  i1030 05:55:07.062119 24489 registrar.cpp:445] applied 1 operations in 53691ns; attempting to update the 'registry'  i1030 05:55:07.065182 24479 log.cpp:680] attempting to append 316 bytes to the log  i1030 05:55:07.065337 24487 coordinator.cpp:340] coordinator attempting to write append action at position 3  i1030 05:55:07.066359 24474 replica.cpp:508] replica received write request for position 3  i1030 05:55:07.066643 24474 leveldb.cpp:343] persisting action (335 bytes) to leveldb took 249579ns  i1030 05:55:07.066671 24474 replica.cpp:676] persisted action at 3  i../../src/tests/allocatortests.cpp:120: failure  failed to wait 10secs for offers1  1030 05:55:07.067101 24477 slave.cpp:1050] will retry registration in 24.08243ms if necessary  i1030 05:55:07.067140 24473 master.cpp:3020] ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress  i1030 05:55:07.067395 24488 replica.cpp:655] replica received learned notice for position 3  i1030 05:55:07.943416 24478 hierarchicalallocatorprocess.hpp:697] no resources available to allocate!  i1030 05:55:19.804687 24478 hierarchicalallocatorprocess.hpp:659] performed allocation for 0 slaves in 11.861261123secs  i1030 05:55:11.942713 24474 master.cpp:120] no whitelist given. advertising offers for all slaves  i1030 05:55:19.805850 24488 leveldb.cpp:343] persisting action (337 bytes) to leveldb took 1.067224ms  i1030 05:55:19.806012 24488 replica.cpp:676] persisted action at 3  ../../src/tests/allocatortests.cpp:115: failure  actual function call count doesn't match expectcall(sched1, resourceoffers(, ))...           expected: to be called once             actual: never called  unsatisfied and active  i1030 05:55:19.806144 24488 replica.cpp:661] replica learned append action at position 3  i1030 05:55:19.806695 24473 master.cpp:768] framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429 disconnected  i1030 05:55:19.806726 24473 master.cpp:1731] disconnecting framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:19.806751 24473 master.cpp:1747] deactivating framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:19.806967 24473 master.cpp:790] giving framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429 0ns to failover  ../../src/tests/allocatortests.cpp:94: failure  actual function call count doesn't match expectcall(allocator, slaveadded(, , ))...           expected: to be called once             actual: never called  unsatisfied and active  f1030 05:55:19.806967 24480 logging.cpp:57] raw: pure virtual method called  i1030 05:55:19.807348 24488 master.cpp:3665] framework failover timeout, removing framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429  i1030 05:55:19.807370 24488 master.cpp:4201] removing framework 20141030055506314269779540429244590000 (framework1) at schedulerc98e7aacd03f464aaa7561208600e196@67.195.81.187:40429   aborted at 1414648519 (unix time) try ""date d @1414648519"" if you are using gnu date   pc: @           0x91bc86 process::pid/::pid()   sigsegv (@0x0) received by pid 24459 (tid 0x2b86c919a700) from pid 0; stack trace:   i1030 05:55:19.808631 24489 registrar.cpp:490] successfully updated the 'registry' in 12.746377984secs      @     0x2b86c55fc340 (unknown)  i1030 05:55:19.808938 24473 log.cpp:699] attempting to truncate the log to 3      @     0x2b86c3327174  google::logmessage::fail()  i1030 05:55:19.809084 24481 coordinator.cpp:340] coordinator attempting to write truncate action at position 4      @           0x91bc86 process::pid/::pid()      @     0x2b86c332c868  google::rawlog()  i1030 05:55:19.810191 24479 replica.cpp:508] replica received write request for position 4  i1030 05:55:19.810899 24479 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 678090ns  i1030 05:55:19.810919 24479 replica.cpp:676] persisted action at 4      @           0x91bf24 process::process/::self()  i1030 05:55:19.811635 24485 replica.cpp:655] replica received learned notice for position 4  i1030 05:55:19.812180 24485 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 523927ns  i1030 05:55:19.812228 24485 leveldb.cpp:401] deleting 2 keys from leveldb took 29523ns  i1030 05:55:19.812242 24485 replica.cpp:676] persisted action at 4  i    @     0x2b86c29d2a36  cxapurevirtual  1030 05:55:19.812258 24485 replica.cpp:661] replica learned truncate action at position 4      @          0x1046936  testing::internal::untypedfunctionmockerbase::untypedinvokewith()  i1030 05:55:19.829655 24474 slave.cpp:1050] will retry registration in 31.785967ms if necessary      @           0x9c0633  testing::internal::functionmockerbase/::invokewith()      @           0x9b6152  testing::internal::functionmocker/::invoke()      @           0x9abdeb  mesos::internal::tests::mockallocatorpr...",5,train
MESOS-2029,Allow slave to checkpoint resources.,"the checkpointed resources are independent of the slave lifecycle. in other words, even if the slave host reboots, it'll still recover the checkpointed resources (unlike other checkpointed data). the slave needs to verify during startup that the checkpointed resources are compatible with the resources of the slave (specified using resources flag).",5,train
MESOS-2030,Maintain persistent disk resources in master memory.,"maintain an inmemory data structure to track persistent disk resources on each slave. update this data structure when slaves register/reregister/disconnect, etc.",3,train
MESOS-2031,Manage persistent directories on slave.,"whenever a slave sees a persistent disk resource (in executorinfo or taskinfo) that is new to it, it will create a persistent directory which is for tasks to store persistent data.    the slave needs to do the following after it's created:  1) symlink into the executor sandbox so that tasks/executor can see it  2) garbage collect it once it is released by the framework",5,train
MESOS-2032,Update Maintenance design to account for persistent resources.,"with persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.    this is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. for example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re replicate all of it's persistent data on the machine. rather, tolerating one unavailable replica for the maintenance window would be preferred.    i'd like to do a revisit of the design to ensure it works well for persistent resources as well.",13,train
MESOS-2033,Documentation for isolator filesystem/shared.,nan,1,train
MESOS-2034,Documentation for isolator namespaces/pid.,nan,1,train
MESOS-2035,Add reason to containerizer proto Termination,"when an isolator kills a task, the reason is unknown. as part of mesos 1830, the reason is set to a general one but ideally we would have the termination reason to pass through to the status update.",5,train
MESOS-2043,Framework auth fail with timeout error and never get authenticated,"i'm facing this issue in master as of https:/github.com/apache/mesos/commit/74ea59e144d131814c66972fb0cc14784d3503d4    as  mentioned in irc, this sounds similar to mesos1866. i'm running 1 master and 1 scheduler (aurora). the framework authentication fail due to time out:    error on mesos master:      i1104 19:37:17.741449  8329 master.cpp:3874] authenticating schedulerd2d4437bd3754467a583362152fe065a@schedulerip:8083  i1104 19:37:17.741585  8329 master.cpp:3885] using default crammd5 authenticator  i1104 19:37:17.742106  8336 authenticator.hpp:169] creating new server sasl connection  w1104 19:37:22.742959  8329 master.cpp:3953] authentication timed out  w1104 19:37:22.743548  8329 master.cpp:3930] failed to authenticate schedulerd2d4437bd3754467a583362152fe065a@schedulerip:8083: authentication discarded      scheduler error:    i1104 19:38:57.885486 49012 sched.cpp:283] authenticating with master master@masterip:port  i1104 19:38:57.885928 49002 authenticatee.hpp:133] creating new client sasl connection  i1104 19:38:57.890581 49007 authenticatee.hpp:224] received sasl authentication mechanisms: crammd5  i1104 19:38:57.890656 49007 authenticatee.hpp:250] attempting to authenticate with mechanism 'crammd5'  w1104 19:39:02.891196 49005 sched.cpp:378] authentication timed out  i1104 19:39:02.891850 49018 sched.cpp:338] failed to authenticate with master master@masterip:port: authentication discarded      looks like 2 instances scheduler20f88a5359454977b5af28f6c52d3c94 & schedulerd2d4437bd3754467a583362152fe065a of same framework is trying to authenticate and fail.    w1104 19:36:30.769420  8319 master.cpp:3930] failed to authenticate scheduler20f88a5359454977b5af28f6c52d3c94@schedulerip:8083: failed to communicate with authenticatee  i1104 19:36:42.701441  8328 master.cpp:3860] queuing up authentication request from schedulerd2d4437bd3754467a583362152fe065a@schedulerip:8083 because authentication is still in progress      restarting master and scheduler didn't fix it.     this particular issue happen with 1 master and 1 scheduler after mesos1866 is fixed.",5,train
MESOS-2044,Use one IP address per container for network isolation,"if there are enough ip addresses, either ipv4 or ipv6, we should use one ip address per container, instead of the ugly port range based solution. one problem with this is the ip address management, usually it is managed by a dhcp server, maybe we need to manage them in mesos master/slave.    also, maybe use macvlan instead of veth for better isolation.",40,train
MESOS-2051,Pull Metrics struct out of Master and Slave to improve readability,nan,2,train
MESOS-2052,RunState::recover should always recover 'completed',"runstate::recover() will return partial state if it cannot find or open the libprocess pid file. specifically, it does not recover the 'completed' flag.    however, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. this ensures that container recovery is not attempted later.    this was discovered when the linuxlauncher failed to recover because it was asked to recover two containers with the same forkedpid. investigation showed the executors both oom'ed before registering, i.e., no libprocess pid file was present. however, the containerizer had detected the oom, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeexecutor (which writes the completed sentinel file.)",1,train
MESOS-2055,MesosContainerizerExecuteTest.IoRedirection test is flaky,"observed this on asf ci:      [ run      ] mesoscontainerizerexecutetest.ioredirection  using temporary directory '/tmp/mesoscontainerizerexecutetestioredirectionpbbn8a'  i1108 00:34:25.820514 30391 containerizer.cpp:100] using isolation: posix/cpu,posix/mem  i1108 00:34:25.821048 30411 containerizer.cpp:424] starting container 'testcontainer' for executor 'executor' of framework ''  i1108 00:34:25.824015 30411 launcher.cpp:137] forked child with pid '4221' for container 'testcontainer'  i1108 00:34:25.825438 30408 containerizer.cpp:571] fetching uris for container 'testcontainer' using command '/home/jenkins/jenkinsslave/workspace/mesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebui/build/src/mesosfetcher'  i1108 00:34:25.984254 30419 containerizer.cpp:1117] executor for container 'testcontainer' has exited  i1108 00:34:25.984341 30419 containerizer.cpp:946] destroying container 'testcontainer'  ../../src/tests/containerizertests.cpp:487: failure  value of: (os::read(path::join(directory, ""stderr""))).get()    actual: ""i1108 00:34:25.872990  4224 logging.cpp:177] logging to stderr\nthis is stderr\n""  expected: errmsg + ""\n""  which is: ""this is stderr\n""  [  failed  ] mesoscontainerizerexecutetest.ioredirection (185 ms)  [] 1 test from mesoscontainerizerexecutetest (185 ms total)  ",1,train
MESOS-2056,Refactor fetcher code in preparation for fetcher cache,"refactor/rearrange fetcherrelated code so that cache functionality can be dropped in. one could do both together in one go. this is splitting up reviews into smaller chunks. it will not immediately be obvious how this change will be used later, but it will look betterfactored and still do the exact same thing as before. in particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher related code can be moved from the containerizer realm into fetcher.cpp.",1,train
MESOS-2057,Concurrency control for fetcher cache,"having added a uri flag to commandinfo messages (in mesos2069) that indicates caching, caching files downloaded by the fetcher in a repository, now ensure that when a uri is ""cached"", it is only ever downloaded once for the same user on the same slave as long as the slave keeps running.     this even holds if multiple tasks request the same uri concurrently. if multiple requests for the same uri occur, perform only one of them and reuse the result. make concurrent requests for the same uri wait for the one download.     different uris from different commandinfos can be downloaded concurrently.    no cache eviction, cleanup or failover will be handled for now. additional tickets will be filed for these enhancements. (so don't use this feature in production until the whole epic is complete.)    note that implementing this does not suffice for production use. this ticket contains the main part of the fetcher logic, though. see the epic mesos336 for the rest of the features that lead to a fully functional fetcher cache.    the proposed general approach is to keep all bookkeeping about what is in which stage of being fetched and where it resides in the slave's mesoscontainerizerprocess, so that all concurrent access is disambiguated and controlled by an ""actor"" (aka libprocess ""process"").    depends on mesos2056 and mesos2069.  ",8,train
MESOS-2058,Deprecate stats.json endpoints for Master and Slave,"with the introduction of the libprocess /metrics/snapshot endpoint, metrics are now duplicated in the master and slave between this and stats.json. we should deprecate the stats.json endpoints.    manual inspection of stats.json shows that all metrics are now covered by the new endpoint for master and slave.",1,train
MESOS-2061,Add InverseOffer protobuf message.,"inverseoffer was defined as part of the maintenance work in mesos1474, design doc here: https:/docs.google.com/document/d/16k0lvwpsgvoyxpsyxkmgcgbnmrlisnee4p fausojk/edit?usp=sharing      /    a request to return some resources occupied by a framework.   /  message inverseoffer       this ticket is to capture the addition of the inverseoffer protobuf to mesos.proto, the necessary api changes for event/call and the language bindings will be tracked separately.",3,train
MESOS-2062,Add InverseOffer to Event/Call API.,the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos: mesos 1474.    one way to add this is to tack it on to the offers event:      message offers   ,3,train
MESOS-2063,Add InverseOffer to C++ Scheduler API.,"the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos: mesos 1474.    one way to add these to the c scheduler api is to add a new callback:        virtual void inverseresourceoffers(        schedulerdriver  driver,        const std::vector/& inverseoffers) = 0;      libmesos compatibility will need to be figured out here.    we may want to leave the c binding untouched in favor of event/call, in order to not break api compatibility for schedulers.",5,train
MESOS-2064,Add InverseOffer to Java Scheduler API.,"the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos: mesos 1474.    one way to add these to the java scheduler api is to add a new callback:        void inverseresourceoffers(        schedulerdriver driver,        list/ inverseoffers);      jar / libmesos compatibility will need to be figured out here.    we may want to leave the java binding untouched in favor of event/call, in order to not break api compatibility for schedulers.",5,train
MESOS-2065,Add InverseOffer to Python Scheduler API.,"the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos: mesos 1474.    one way to add these to the python scheduler api is to add a new callback:        def inverseresourceoffers(self, driver, inverse_offers):      egg / libmesos compatibility will need to be figured out here.    we may want to leave the python binding untouched in favor of event/call, in order to not break api compatibility for schedulers.",5,train
MESOS-2066,Add optional 'Unavailability' to resource offers to provide maintenance awareness.,"in order to inform frameworks about upcoming maintenance on offered resources, per mesos 1474, we'd like to add an optional 'unavailability' information to offers:      message interval     message offer   ",3,train
MESOS-2067,Add HTTP API to the master for maintenance operations.,"based on mesos1474, we'd like to provide an http api on the master for the maintenance primitives in mesos.    for the mvp, we'll want something like this for manipulating the schedule:    /maintenance/schedule    get  returns the schedule, which will include the various maintenance windows.    post  create or update the schedule with a json blob (see below).    /maintenance/status    get  returns a list of machines and their maintenance mode.    /maintenance/start    post  transition a set of machines from draining into deactivated mode.    /maintenance/stop    post  transition a set of machines from deactivated into normal mode.    /maintenance/consensus < (not sure what the right name is.  matrix?  acceptance?)    get  returns the latest info on which frameworks have accepted or declined the maintenance schedule.    (note: the slashes in urls might not be supported yet.)    a schedule might look like:    ,            ,            ...          ],         ""unavailability"" :       },      ...    ]  }      there should be firewall settings such that only those with access to master can use these endpoints.",8,train
MESOS-2069,Basic fetcher cache functionality,add a flag to commandinfo uri protobufs that indicates that files downloaded by the fetcher shall be cached in a repository. to be followed by mesos2057 for concurrency control.    also see mesos336 for the overall goals for the fetcher cache.,8,train
MESOS-2070,Implement simple slave recovery behavior for fetcher cache,"clean the fetcher cache completely upon slave restart/recovery. this implements correct, albeit not ideal behavior. more efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",2,train
MESOS-2072,Fetcher cache eviction,"delete files from the fetcher cache so that a given cache size is never exceeded. succeed in doing so while concurrent downloads are on their way and new requests are pouring in.    idea: measure the size of each download before it begins, make enough room before the download. this means that only download mechanisms that divulge the size before the main download will be supported. afawk, those in use so far have this property.     the calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (the python script that performs fetcher caching for aurora does not seem to implement this. see https:/gist.github.com/zmanji/f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's cacheeviction() call succeeding, each perceiving the same free space being available.)    ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. then, as a fallback, direct download into the work directory will be used for some tasks. tbd how to pick which task gets treated how.     at first, only support copying of any downloaded files to the work directory for task execution. this isolates the task life cycle after starting a task from cache eviction considerations.     (later, we can add symbolic links that avoid copying. but then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is mesos 1667 ""extract from uri while downloading into work dir"").  ",8,train
MESOS-2074,Fetcher cache test fixture,"to accelerate providing good test coverage for the fetcher cache (mesos336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following:   whether to cache or not   whether make what has been downloaded executable or not   whether to extract from an archive or not    whether to download from a file system, http, or...    we can create a simple hhtp server in the test fixture to support the latter.    furthermore, the tests need to be robust wrt. varying numbers of statusupdate messages. an accumulating update message sink that reports the final state is needed.    all this has already been programmed in this patch, just needs to be rebased:  https:/reviews.apache.org/r/21316/",5,train
MESOS-2075,Add maintenance information to the replicated registry.,"to achieve faulttolerance for the maintenance primitives, we will need to add the maintenance information to the registry.    the registry currently stores all of the slave information, which is quite large (~ 17mb for 50,000 slaves from my testing), which results in a protobuf object that is extremely expensive to copy.    as far as i can tell, reads / writes to maintenance information is independent of reads / writes to the existing 'registry' information. so there are two approach here:    add maintenance information to 'maintenance' key:   # the advantage of this approach is that we don't further grow the large registry object.  # this approach assumes that writes to 'maintenance' are independent of writes to the 'registry'. if these writes are not independent, this approach requires that we add transactional support to the state abstraction.  # this approach requires adding compaction to logstorage.   # this approach likely requires some refactoring to the registrar.    add maintenance information to 'registry' key: (this is the chosen method.)  # the advantage of this approach is that it's the easiest to implement.  # this will further grow the single 'registry' object, but doesn't preclude it being split apart in the future.  # this approach may require using the diff support in logstorage and/or adding compression support to logstorage snapshots to deal with the increased size of the registry.",13,train
MESOS-2076,Implement maintenance primitives in the Master.,"the master will need to do a number of things to implement the maintenance primitives:    # for machines that have a maintenance window:  # disambiguate machines to agents.  # for unused resources, offers must be augmented with an unavailability.  # for used resources, inverse offers must be sent.  # for inverse offers:  # filter them before sending them again.  #  for declined inverse offers, do something with the reason (store or log).  # recover the maintenance information upon failover.    note: some amount of this logic will need to be placed in the allocator.",13,train
MESOS-2077,Ensure that TASK_LOSTs for a hard slave drain (SIGUSR1) include a Reason.,"for maintenance, sometimes operators will force the drain of a slave (via sigusr1), when deemed safe (e.g. non critical tasks running) and/or necessary (e.g. bad hardware).    to eliminate alerting noise, we'd like to add a 'reason' that expresses the forced drain of the slave, so that these are not considered to be a generic slave removal task_lost.",3,train
MESOS-2078,Scheduler driver may ACK status updates when the scheduler threw an exception," discovered that this can happen if the scheduler calls schedulerdriver#stop before or while handling scheduler#statusupdate.    in src/sched/sched.cpp:  the driver invokes statusupdate and later checks the aborted flag to determine whether to send an ack.      void statusupdate(        const upid& from,        const statusupdate& update,        const upid& pid)      ...      in src/java/jni/orgapachemesosmesosschedulerdriver.cpp:  the statusupdate implementation checks for an exception and invokes driver>abort().    void jnischeduler::statusupdate(schedulerdriver driver,                                  const taskstatus& status)        jvm>detachcurrentthread();  }      in src/sched/sched.cpp:  the abort() implementation exits early if status != driverrunning, and does not set the aborted flag.    status mesosschedulerdriver::abort()        check(process != null);      / we set the volatile aborted to true here to prevent any further    / messages from being processed in the schedulerprocess. however,    / if abort() is called from another thread as the schedulerprocess,    / there may be at most one additional message processed.    / todo(bmahler): use an atomic boolean.    process >aborted = true;      / dispatching here ensures that we still process the outstanding    / requests from  the scheduler, since those do proceed when    / aborted is true.    dispatch(process, &schedulerprocess::abort);      return status = driver_aborted;  }      as a result, the code will ack despite an exception being thrown.",3,train
MESOS-2080,Add master metrics for maintenance.,"we'll need metrics in order to gain visibility into the maintenance functionality. this will also allow operators to add alerting on these metrics, in particular:    # number of scheduled hosts.  # number of active windows.  # number of expired windows.  # number of successful drains.  # number of failed drains.    as an example of an alert guideline, we would want to know the number of expired windows as a gauge to ensure that it is not growing excessively. this allows alerting to catch when operators are not properly unscheduling maintenance once it is complete.",3,train
MESOS-2081,Add safety constraints for maintenance primitives.,"in order to ensure that the maintenance primitives can be used safely by operators, we want to put a few safety mechanisms in place. some ideas from the https:/docs.google.com/a/twitter.com/document/d/16k0lvwpsgvoyxpsyxkmgcgbnmrlisnee4pfausojk/:    # prevent bad schedules from being constructed: schedules with more than x% overlap in slaves are rejected.  # prevent bad maintenance from proceeding unchecked: if x% of the slaves are not being unscheduled, or are not re registering, cancel the schedule.    these will likely be configurable via flags.",8,train
MESOS-2082,Update the webui to include maintenance information.,"the simplest thing here would probably be to include another tab in the header for maintenance information.    we could also consider adding maintenance information inline to the slaves table. depending on how this is done, the maintenance tab could actually be a subset of the slaves table; only those slaves for which there is maintenance information.",5,train
MESOS-2083,Add documentation for maintenance primitives.,"we should provide some guiding documentation around the upcoming maintenance primitives in mesos.    specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in mesos. some guidance and recommendations for the latter two audiences will be necessary.",8,train
MESOS-2085,Add support encrypted and non-encrypted communication in parallel for cluster upgrade,"during cluster upgrade from nonencrypted to encrypted communication, we need to support an interim where:  1) a master can have connections to both encrypted and nonencrypted slaves  2) a slave that supports encrypted communication connects to a master that has not yet been upgraded.  3) frameworks are encrypted but the master has not been upgraded yet.  4) master has been upgraded but frameworks haven't.  5) a slave process has upgraded but running executor processes haven't.",13,train
MESOS-2097,Update Resource protobuf with DiskInfo,  message resource       optional diskinfo disk = 8;  }  ,1,train
MESOS-2098,Update task validation to be after task authorization.,so that we can simply the task validation because we no longer need to check with pendingtasks.,3,train
MESOS-2099,Support acquiring/releasing resources with DiskInfo in allocator.,"the allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with diskinfo). for example, when we release a persistent disk resource, we are changing the release with diskinfo to a resource with the diskinfo.",8,train
MESOS-2100,Implement master to slave protocol for persistent disk resources.,we need to do the following:  1) slave needs to send persisted resources when registering (or reregistering).  2) master needs to send total persisted resources to slave by either reusing runtask/updateframeworkinfo or introduce new type of messages (like updateresources).,8,train
MESOS-2101,Add the persistent resources release primitive to the framework API,we are thinking about introducing a release protobuf message which specifies persistent disk resources (w/ diskinfo) to release. the release message could be piggybacked on the launch/decline message.    this probably will overlap with the dynamic reservation work (mesos 2018).,3,train
MESOS-2103,Expose number of processes and threads in a container,"the cfs cpu statistics (cpusnrthrottled, cpusnrperiods, cpusthrottledtime) are difficult to interpret.  1) nrthrottled is the number of intervals where any throttling occurred  2) throttledtime is the aggregate time across all runnable tasks (tasks in the linux sense).    for example, in a typical 60 second sampling interval: nrperiods = 600, nrthrottled could be 60, i.e., 10% of intervals, but throttledtime could be much higher than (60/600)  60 = 6 seconds if there is more than one task that is runnable but throttled. each throttled task contributes to the total throttled time.    small test to demonstrate throttledtime > nrperiods  quotainterval:    5 x 'openssl speed' running with quota=100ms:    cat cpu.stat && sleep 1 && cat cpu.stat  nrperiods 3228  nrthrottled 1276  throttledtime 528843772540  nrperiods 3238  nrthrottled 1286  throttledtime 531668964667    all 10 intervals throttled (100%) for total time of 2.8 seconds in 1 second (""more than 100%"" of the time interval)      it would be helpful to expose the number of processes and tasks in the container cgroup. this would be at a very coarse granularity but would give some guidance.",2,train
MESOS-2104,Correct naming of cgroup memory statistics,"memrssbytes is not rss but is the total memory usage (memory.usageinbytes) of the cgroup, including file cache etc. actual rss is reported as memanonbytes. these, and others, should be consistently named.",3,train
MESOS-2108,Add configure flag or environment variable to enable SSL/libevent Socket,nan,1,train
MESOS-2110,Configurable Ping Timeouts,"after a series of pingfailures, the master considers the slave lost and calls shutdownslave, requiring such a slave that reconnects to kill its tasks and reregister as a new slaveid. on the other side, after a similar timeout, the slave will consider the master lost and try to detect a new master. these timeouts are currently hardcoded constants (5   15s), which may not be wellsuited for all scenarios.   some clusters may tolerate a longer slave process restart period, and wouldn't want tasks to be killed upon reconnect.   some clusters may have higherlatency networks (e.g. crossdatacenter, or for volunteer computing efforts), and would like to tolerate longer periods without communication.    we should provide flags/mechanisms on the master to control its tolerance for noncommunicative slaves, and (less importantly?) on the slave to tolerate missing masters.",8,train
MESOS-2119,Add Socket tests,add more socket specific tests to get coverage while doing libev to libevent (w and wo ssl) move,5,train
MESOS-2123,Document changes in C++ Resources API in CHANGELOG.,"with the refactor introduced in mesos 1974, we need to document those api changes in changelog.  ",2,train
MESOS-2127,killTask() should perform reconciliation for unknown tasks.,"currently, killtask uses its own reconciliation logic, which has diverged from the reconciletasks logic. specifically, when the task is unknown and a non strict registry is in use, killtask will not send task_lost whereas reconciletask will.    we should make these consistent.  ",3,train
MESOS-2128,Turning on cgroups_limit_swap effectively disables memory isolation,"our test runs show that enabling cgroupslimitswap effectively disables memory isolation altogether.    per: https:/access.redhat.com/documentation/enus/redhatenterpriselinux/6/html/resourcemanagementguide/secmemory.html    ""it is important to set the memory.limitinbytes parameter before setting the memory.memsw.limitinbytes parameter: attempting to do so in the reverse order results in an error. this is because memory.memsw.limitinbytes becomes available only after all memory limitations (previously set in memory.limitinbytes) are exhausted.""    looks like the flag sets ""memory.memsw.limitinbytes"" if true and ""memory.limitinbytes"" if false, but should always set ""memory.limitinbytes"" and in addition set ""memory.memsw.limitin_bytes"" if true. otherwise the limits won't be set and enforced.    see: https:/github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#l365  ",2,train
MESOS-2135,Support DiskInfo in C++ Resources,"we need to change the following functions:  1) addable  2) subtractable  3) validate    we probably shouldn't add two disk resources with the same persistence id because they must come from different ""namespaces"". we can add more checks in the validate functions (for protobufs).",3,train
MESOS-2136,Expose per-cgroup memory pressure,"the cgroup memory controller can provide information on the memory pressure of a cgroup. this is in the form of an event based notification where events of (low, medium, critical) are generated when the kernel makes specific actions to allocate memory. this signal is probably more informative than comparing memory usage to memory limit.  ",5,train
MESOS-2139,Enable the master to handle reservation operations,"master's _accept function currently only handles create and destroy operations which exist for persistent volumes. we need to handle the reserve and unreserve operations for dynamic reservations as well.    in addition, we need to add validate functions for the reservation operations.",5,train
MESOS-2144,Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread,occured on review bot review of: https:/reviews.apache.org/r/28262/#review62333    the review doesn't touch code related to the test (and doesn't break libprocess in general)    [ run      ] examplestest.lowlevelschedulerpthread  ../../src/tests/script.cpp:83: failure  failed  lowlevelschedulerpthreadtest.sh terminated with signal segmentation fault  [  failed  ] examplestest.lowlevelschedulerpthread (7561 ms)    the test ,8,train
MESOS-2147,Large number of connections slows statistics.json responses.,"we observed that in our production environment with network monitoring being turned on.    if there are many connections (> 10^4) in a container, getting socket information is expensive. it might take 1min to process all the socket information.    one of the reason is that the library we are using (libnl) is not so optimized. cong wang has already submitted a patch:  http:/lists.infradead.org/pipermail/libnl/2014 november/001715.html",2,train
MESOS-2157,Add /master/slaves and /master/frameworks/{framework}/tasks/{task} endpoints,"master/state.json exports the entire state of the cluster and can, for large clusters, become massive (tens of megabytes of json).  often, a client only need information about subsets of the entire state, for example all connected slaves, or information (registration info, tasks, etc) belonging to a particular framework.    we can partition state.json into many smaller endpoints, but for starters, being able to get slave information and tasks information per framework would be useful.",5,train
MESOS-2166, PerfEventIsolatorTest.ROOT_CGROUPS_Sample requires 'perf' to be installed,the perf::valid() relies on the 'perf' command being installed. this isn't always the case. configure should probably check for the perf command exists.,1,train
MESOS-2176,Hierarchical allocator inconsistently accounts for reserved resources. ,"looking through the allocator code for mesos2099, i see an issue with respect to accounting reserved resources in the sorters:    within hierarchicalallocatorprocess::allocate, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.    from git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. my guess is that this was never caught due to the lack of unittestability of the allocator (unnecessarily requires a master pid to use an allocator).    from my understanding, the two levels of the hierarchical sorter should have the following semantics:    # level 1 sorts across roles. only unreserved resources are shared across roles, and therefore the ""role sorter"" for level 1 should only account for the unreserved resource pool.  # level 2 sorts across frameworks, within a role. both unreserved and reserved resources are shared across frameworks within a role, and therefore the ""framework sorters"" for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources allocated inside the role.",5,train
MESOS-2182,Performance issue in libprocess SocketManager.,"noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.    after looking at some perf data, the top offender is:          12.02%  mesosmaster  libmesos0.21.0rc3.so  [.] std::rbtree/, std::less/, std::allocator/ >::erase(process::processbase const&)  ...       3.29%  mesosmaster  libmesos0.21.0rc3.so  [.] process::socketmanager::exited(process::processbase)      it appears that in the socketmanager, whenever an internal process exits, we loop over all the links unnecessarily:      void socketmanager::exited(processbase process)              }            linker>enqueue(new exitedevent(linkee));          }        }      }        links.erase(pid);    }  }      on clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! this is because, the master contains links from the master process to each slave. however, when a random ephemeral process terminates, we don't need to loop over each slave link.    while we hold this lock, the following calls will block:      class socketmanager      as a result, the slave observers and the master can block calling send()!    short term, we will try to fix this issue by removing the unnecessary looping. longer term, it would be nice to avoid all this locking when sending on independent sockets.",3,train
MESOS-2184,deprecate unused flag 'cgroups_subsystems',cgroups_subsystems is a slave flag that is no longer used and should be deprecated.,1,train
MESOS-2191,Add ContainerId to the TaskStatus message,"taskstatus provides the frameworks with certain information (executorid, slaveid, etc.) which is useful when collecting statistics about cluster performance; however, it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself. therefore it would be good to provide the framework scheduler with this information, adding a new field in the taskstatus message.    see comments for a use case.",3,train
MESOS-2199,Failing test: SlaveTest.ROOT_RunTaskWithCommandInfoWithUser,"appears that running the executor as nobody is not supported.     can you take a look?    executor log:    [root@hostname build]# cat /tmp/slavetestrootruntaskwithcommandinfowithusercxf1dy/slaves/2014121900520620811701866048711862s0/frameworks/20141219005206208117018660  487118620000/executors/1/runs/latest/std   sh: /home/idownes/workspace/mesos/build/src/mesosexecutor: permission denied      test output:    [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from slavetest  [ run      ] slavetest.rootruntaskwithcommandinfowithuser  ../../src/tests/slavetests.cpp:680: failure  value of: statusrunning.get().state()    actual: taskfailed  expected: taskrunning  ../../src/tests/slavetests.cpp:682: failure  failed to wait 10secs for statusfinished  ../../src/tests/slavetests.cpp:673: failure  actual function call count doesn't match expectcall(sched, statusupdate(&driver, ))...           expected: to be called twice             actual: called once  unsatisfied and active  [  failed  ] slavetest.rootruntaskwithcommandinfowithuser (10641 ms)  [] 1 test from slavetest (10641 ms total)    [] global test environment teardown  [==========] 1 test from 1 test case ran. (10658 ms total)  ",2,train
MESOS-2200,bogus docker images result in bad error message to scheduler,"when a scheduler specifies a bogus image in containerinfo mesos doesn't tell the scheduler that the docker pull failed or why.    this error is logged in the mesosslave log, but it isn't given to the scheduler (as far as i can tell):      e1218 23:50:55.406230  8123 slave.cpp:2730] container '8f70784c3e4040729ca29daed23f15ff' for executor 'thermos1418946354013xxxxxxcurl0f500cc41dd0a43388cbcd631cb588bb1' of framework '2014052221314517490045615050295120000' failed to start: failed to 'docker pull docker registry.example.com/doesntexist/hello1.1:latest': exit status = exited with status 1 stderr = 2014/12/18 23:50:55 error: image doesntexist/hello1.1 not found      if the docker image is not in the registry, the scheduler should give the user an error message.  if docker pull failed because of networking issues, it should be retried.  mesos should give the scheduler enough information to be able to make that decision.",2,train
MESOS-2201,ReplicaTest.Restore fails with leveldb greater than v1.7.,"i wanted to configure mesos with system provided leveldb libraries when i ran into this issue. apparently,  if one does ../configure withleveldb=/path/to/leveldb, compilation succeeds, however the ""replicatestrestore"" test fails with the following back trace:      [ run      ] replicatest.restore  using temporary directory '/tmp/replicatestrestoreizbbrr'  i1222 14:16:49.517500  2927 leveldb.cpp:176] opened db in 10.758917ms  i1222 14:16:49.526495  2927 leveldb.cpp:183] compacted db in 8.931146ms  i1222 14:16:49.526523  2927 leveldb.cpp:198] created db iterator in 5787ns  i1222 14:16:49.526531  2927 leveldb.cpp:204] seeked to beginning of db in 511ns  i1222 14:16:49.526535  2927 leveldb.cpp:273] iterated through 0 keys in the db in 197ns  i1222 14:16:49.526623  2927 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1222 14:16:49.530972  2945 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 3.084458ms  i1222 14:16:49.531008  2945 replica.cpp:320] persisted replica status to voting  i1222 14:16:49.541263  2927 leveldb.cpp:176] opened db in 9.980586ms  i1222 14:16:49.551636  2927 leveldb.cpp:183] compacted db in 10.348096ms  i1222 14:16:49.551683  2927 leveldb.cpp:198] created db iterator in 3405ns  i1222 14:16:49.551693  2927 leveldb.cpp:204] seeked to beginning of db in 3559ns  i1222 14:16:49.551728  2927 leveldb.cpp:273] iterated through 1 keys in the db in 29722ns  i1222 14:16:49.551751  2927 replica.cpp:741] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1222 14:16:49.551996  2947 replica.cpp:474] replica received implicit promise request with proposal 1  i1222 14:16:49.560921  2947 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 8.899591ms  i1222 14:16:49.560940  2947 replica.cpp:342] persisted promised to 1  i1222 14:16:49.561338  2943 replica.cpp:508] replica received write request for position 1  i1222 14:16:49.568677  2943 leveldb.cpp:343] persisting action (27 bytes) to leveldb took 7.287155ms  i1222 14:16:49.568692  2943 replica.cpp:676] persisted action at 1  i1222 14:16:49.569042  2942 leveldb.cpp:438] reading position from leveldb took 26339ns  f1222 14:16:49.569411  2927 replica.cpp:721] checksome(state): io error: lock /tmp/replicatestrestoreizbbrr/.log/lock: already held by process failed to recover the log   check failure stack trace:       @     0x7f7f6c53e688  google::logmessage::fail()      @     0x7f7f6c53e5e7  google::logmessage::sendtolog()      @     0x7f7f6c53dff8  google::logmessage::flush()      @     0x7f7f6c540d2c  google::logmessagefatal::logmessagefatal()      @           0x90a520  checkfatal::checkfatal()      @     0x7f7f6c400f4d  mesos::internal::log::replicaprocess::restore()      @     0x7f7f6c3fd763  mesos::internal::log::replicaprocess::replicaprocess()      @     0x7f7f6c401271  mesos::internal::log::replica::replica()      @           0xcd7ca3  replicatestrestoretest::testbody()      @          0x10934b2  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x108e584  testing::internal::handleexceptionsinmethodifsupported/()      @          0x10768fd  testing::test::run()      @          0x1077020  testing::testinfo::run()      @          0x10775a8  testing::testcase::run()      @          0x107c324  testing::internal::unittestimpl::runalltests()      @          0x1094348  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x108f2b7  testing::internal::handleexceptionsinmethodifsupported/()      @          0x107b1d4  testing::unittest::run()      @           0xd344a9  main      @     0x7f7f66fdfb45  libcstartmain      @           0x8f3549  (unknown)      @              (nil)  (unknown)  [2]    2927 abort (core dumped)  gloglogtostderr=1 gtestv=10 ./bin/mesostests.sh verbose      the bundled version of leveldb is v1.4. i tested version 1.5 and that seems to work.  however, v1.6 had some build issues and us unusable with mesos. the next version v1.7, allows mesos to compile fine but results in the above error.",3,train
MESOS-2205,Add user documentation for reservations,"add a user guide for reservations which describes basic usage of them, how acls are used to specify who can unreserve whose resources, and few advanced usage cases.",2,train
MESOS-2210,Disallow special characters in role.,"as we introduce persistent volumes in mesos 1524, we will use roles as directory names on the slave (https:/reviews.apache.org/r/28562/). as a result, the master should disallow special characters (like space and slash) in role.",2,train
MESOS-2215,"The Docker containerizer attempts to recover any task when checkpointing is enabled, not just docker tasks.","once the slave restarts and recovers the task, i see this error in the log for all tasks that were recovered every second or so.  note, these were not docker tasks:    w0113 16:01:00.790323 773142 monitor.cpp:213] failed to get resource usage for  container 7b729b89dc7e4d08af978cd1af560a21 for executor thermos1421085237813slipstreamprodagent38f7695141835415190d03f55dcc940dd of framework 2015010916171371535028250502907970000: failed to 'docker inspect mesos7b729b89dc7e4d08af978cd1af560a21': exit status = exited with status 1 stderr = error: no such image or container: mesos7b729b89dc7e4d08af978cd1af560a21  however the tasks themselves are still healthy and running.    the slave was launched with containerizers=mesos,docker      more info: it looks like the docker containerizer is a little too ambitious about recovering containers, again this was not a docker task:  i0113 15:59:59.476145 773142 docker.cpp:814] recovering container '7b729b89dc7e4d08af978cd1af560a21' for executor 'thermos1421085237813slipstreamprodagent38f7695141835415190d03f55dcc940dd' of framework 201501091617137153502825050290797 0000    looking into the source, it looks like the problem is that the composingcontainerize runs recover in parallel, but neither the docker containerizer nor mesos containerizer check if they should recover the task or not (because they were the ones that launched it).  perhaps this needs to be written into the checkpoint somewhere?",8,train
MESOS-2222,Add ACLs for the maintenance HTTP endpoints.,"in order to authorize the http endpoints for maintenance (to be added in mesos 2067), we will need to add an acl definition for performing maintenance operations.",3,train
MESOS-2225,FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky,"observed this on internal ci.      [ run      ] faulttolerancetest.reregisterframeworkexitedexecutor  using temporary directory '/tmp/faulttolerancetestreregisterframeworkexitedexecutorynprki'  i0114 18:50:51.461186  4720 leveldb.cpp:176] opened db in 4.866948ms  i0114 18:50:51.462057  4720 leveldb.cpp:183] compacted db in 472256ns  i0114 18:50:51.462514  4720 leveldb.cpp:198] created db iterator in 42905ns  i0114 18:50:51.462784  4720 leveldb.cpp:204] seeked to beginning of db in 21630ns  i0114 18:50:51.463068  4720 leveldb.cpp:273] iterated through 0 keys in the db in 19967ns  i0114 18:50:51.463485  4720 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0114 18:50:51.464555  4737 recover.cpp:449] starting replica recovery  i0114 18:50:51.465188  4737 recover.cpp:475] replica is in empty status  i0114 18:50:51.467324  4741 replica.cpp:641] replica in empty status received a broadcasted recover request  i0114 18:50:51.470118  4736 recover.cpp:195] received a recover response from a replica in empty status  i0114 18:50:51.475424  4739 recover.cpp:566] updating replica status to starting  i0114 18:50:51.476553  4739 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 107545ns  i0114 18:50:51.476862  4739 replica.cpp:323] persisted replica status to starting  i0114 18:50:51.477309  4739 recover.cpp:475] replica is in starting status  i0114 18:50:51.479109  4734 replica.cpp:641] replica in starting status received a broadcasted recover request  i0114 18:50:51.481274  4738 recover.cpp:195] received a recover response from a replica in starting status  i0114 18:50:51.482324  4738 recover.cpp:566] updating replica status to voting  i0114 18:50:51.482913  4738 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 66011ns  i0114 18:50:51.483186  4738 replica.cpp:323] persisted replica status to voting  i0114 18:50:51.483608  4738 recover.cpp:580] successfully joined the paxos group  i0114 18:50:51.484031  4738 recover.cpp:464] recover process terminated  i0114 18:50:51.554949  4734 master.cpp:262] master 201501141850512272962752570184720 (fedora19) started on 192.168.122.135:57018  i0114 18:50:51.555785  4734 master.cpp:308] master only allowing authenticated frameworks to register  i0114 18:50:51.556046  4734 master.cpp:313] master only allowing authenticated slaves to register  i0114 18:50:51.556426  4734 credentials.hpp:36] loading credentials for authentication from '/tmp/faulttolerancetestreregisterframeworkexitedexecutorynprki/credentials'  i0114 18:50:51.557003  4734 master.cpp:357] authorization enabled  i0114 18:50:51.558007  4737 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0114 18:50:51.558521  4741 whitelistwatcher.cpp:65] no whitelist given  i0114 18:50:51.562185  4734 master.cpp:1219] the newly elected leader is master@192.168.122.135:57018 with id 201501141850512272962752570184720  i0114 18:50:51.562680  4734 master.cpp:1232] elected as the leading master!  i0114 18:50:51.562950  4734 master.cpp:1050] recovering from registrar  i0114 18:50:51.564506  4736 registrar.cpp:313] recovering registrar  i0114 18:50:51.566162  4737 log.cpp:660] attempting to start the writer  i0114 18:50:51.568691  4741 replica.cpp:477] replica received implicit promise request with proposal 1  i0114 18:50:51.569154  4741 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 106885ns  i0114 18:50:51.569504  4741 replica.cpp:345] persisted promised to 1  i0114 18:50:51.573277  4740 coordinator.cpp:230] coordinator attemping to fill missing position  i0114 18:50:51.575623  4739 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0114 18:50:51.576133  4739 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 86360ns  i0114 18:50:51.576449  4739 replica.cpp:679] persisted action at 0  i0114 18:50:51.586966  4736 replica.cpp:511] replica received write request for position 0  i0114 18:50:51.587666  4736 leveldb.cpp:438] reading position from leveldb took 60621ns  i0114 18:50:51.588043  4736 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 81094ns  i0114 18:50:51.588374  4736 replica.cpp:679] persisted action at 0  i0114 18:50:51.589418  4736 replica.cpp:658] replica received learned notice for position 0  i0114 18:50:51.590428  4736 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 106648ns  i0114 18:50:51.590840  4736 replica.cpp:679] persisted action at 0  i0114 18:50:51.591104  4736 replica.cpp:664] replica learned nop action at position 0  i0114 18:50:51.592260  4734 log.cpp:676] writer started with ending position 0  i0114 18:50:51.594172  4739 leveldb.cpp:438] reading position from leveldb took 52163ns  i0114 18:50:51.600744  4736 registrar.cpp:346] successfully fetched the registry (0b) in 35968us  i0114 18:50:51.601646  4736 registrar.cpp:445] applied 1 operations in 184502ns; attempting to update the 'registry'  i0114 18:50:51.604329  4737 log.cpp:684] attempting to append 130 bytes to the log  i0114 18:50:51.604966  4737 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0114 18:50:51.606449  4737 replica.cpp:511] replica received write request for position 1  i0114 18:50:51.606937  4737 leveldb.cpp:343] persisting action (149 bytes) to leveldb took 84877ns  i0114 18:50:51.607199  4737 replica.cpp:679] persisted action at 1  i0114 18:50:51.611934  4741 replica.cpp:658] replica received learned notice for position 1  i0114 18:50:51.612423  4741 leveldb.cpp:343] persisting action (151 bytes) to leveldb took 113059ns  i0114 18:50:51.612794  4741 replica.cpp:679] persisted action at 1  i0114 18:50:51.613056  4741 replica.cpp:664] replica learned append action at position 1  i0114 18:50:51.614598  4741 log.cpp:703] attempting to truncate the log to 1  i0114 18:50:51.615157  4741 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0114 18:50:51.616458  4737 replica.cpp:511] replica received write request for position 2  i0114 18:50:51.616902  4737 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 71716ns  i0114 18:50:51.617168  4737 replica.cpp:679] persisted action at 2  i0114 18:50:51.618505  4740 replica.cpp:658] replica received learned notice for position 2  i0114 18:50:51.619031  4740 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 78481ns  i0114 18:50:51.619567  4740 leveldb.cpp:401] deleting 1 keys from leveldb took 59638ns  i0114 18:50:51.619832  4740 replica.cpp:679] persisted action at 2  i0114 18:50:51.620101  4740 replica.cpp:664] replica learned truncate action at position 2  i0114 18:50:51.621757  4736 registrar.cpp:490] successfully updated the 'registry' in 19.78496ms  i0114 18:50:51.622658  4736 registrar.cpp:376] successfully recovered registrar  i0114 18:50:51.623261  4736 master.cpp:1077] recovered 0 slaves from the registry (94b) ; allowing 10mins for slaves to reregister  i0114 18:50:51.670349  4739 slave.cpp:173] slave started on 115)@192.168.122.135:57018  i0114 18:50:51.671133  4739 credentials.hpp:84] loading credential for authentication from '/tmp/faulttolerancetestreregisterframeworkexitedexecutoronrvug/credential'  i0114 18:50:51.671685  4739 slave.cpp:282] slave using credential for: testprincipal  i0114 18:50:51.672245  4739 slave.cpp:300] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0114 18:50:51.673360  4739 slave.cpp:329] slave hostname: fedora19  i0114 18:50:51.673660  4739 slave.cpp:330] slave checkpoint: false  w0114 18:50:51.674052  4739 slave.cpp:332] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0114 18:50:51.677234  4737 state.cpp:33] recovering state from '/tmp/faulttolerancetestreregisterframeworkexitedexecutoronrvug/meta'  i0114 18:50:51.684973  4739 statusupdatemanager.cpp:197] recovering status update manager  i0114 18:50:51.687644  4739 slave.cpp:3519] finished recovery  i0114 18:50:51.688698  4737 slave.cpp:613] new master detected at master@192.168.122.135:57018  i0114 18:50:51.688902  4734 statusupdatemanager.cpp:171] pausing sending status updates  i0114 18:50:51.689482  4737 slave.cpp:676] authenticating with master master@192.168.122.135:57018  i0114 18:50:51.689910  4737 slave.cpp:681] using default crammd5 authenticatee  i0114 18:50:51.690577  4741 authenticatee.hpp:138] creating new client sasl connection  i0114 18:50:51.691453  4737 slave.cpp:649] detecting new master  i0114 18:50:51.691864  4741 master.cpp:4130] authenticating slave(115)@192.168.122.135:57018  i0114 18:50:51.692369  4741 master.cpp:4141] using default crammd5 authenticator  i0114 18:50:51.693208  4741 authenticator.hpp:170] creating new server sasl connection  i0114 18:50:51.694598  4738 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0114 18:50:51.694893  4738 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0114 18:50:51.695329  4741 authenticator.hpp:276] received sasl authentication start  i0114 18:50:51.695641  4741 authenticator.hpp:398] authentication requires more steps  i0114 18:50:51.696028  4736 authenticatee.hpp:275] received sasl authentication step  i0114 18:50:51.696486  4741 authenticator.hpp:304] received sasl authentication step  i0114 18:50:51.696753  4741 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0114 18:50:51.697041  4741 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0114 18:50:51.697343  4741 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0114 18:50:51.697685  4741 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0114 18:50:51.697998  4741 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0114 18:50:51.698251  4741 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0114 18:50:51.698580  4741 authenticator.hpp:390] authentication success  i0114 18:50:51.698927  4735 authenticatee.hpp:315] authentication success  i0114 18:50:51.705123  4741 master.cpp:4188] successfully authenticated principal 'testprincipal' at slave(115)@192.168.122.135:57018  i0114 18:50:51.705847  4720 sched.cpp:151] version: 0.22.0  i0114 18:50:51.707159  4736 sched.cpp:248] new master detected at master@192.168.122.135:57018  i0114 18:50:51.707523  4736 sched.cpp:304] authenticating with master master@192.168.122.135:57018  i0114 18:50:51.707792  4736 sched.cpp:311] using default crammd5 authenticatee  i0114 18:50:51.708412  4736 authenticatee.hpp:138] creating new client sasl connection  i0114 18:50:51.709316  4735 slave.cpp:747] successfully authenticated with master master@192.168.122.135:57018  i0114 18:50:51.709723  4737 master.cpp:4130] authenticating scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.710274  4737 master.cpp:4141] using default crammd5 authenticator  i0114 18:50:51.710739  4735 slave.cpp:1075] will retry registration in 17.028024ms if necessary  i0114 18:50:51.711304  4737 master.cpp:3276] registering slave at slave(115)@192.168.122.135:57018 (fedora19) with id 201501141850512272962752570184720s0  i0114 18:50:51.711459  4738 authenticator.hpp:170] creating new server sasl connection  i0114 18:50:51.713142  4739 registrar.cpp:445] applied 1 operations in 100530ns; attempting to update the 'registry'  i0114 18:50:51.713465  4738 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0114 18:50:51.715435  4738 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0114 18:50:51.715963  4740 authenticator.hpp:276] received sasl authentication start  i0114 18:50:51.716258  4740 authenticator.hpp:398] authentication requires more steps  i0114 18:50:51.716524  4740 authenticatee.hpp:275] received sasl authentication step  i0114 18:50:51.716784  4740 authenticator.hpp:304] received sasl authentication step  i0114 18:50:51.716979  4740 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0114 18:50:51.717139  4740 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0114 18:50:51.717315  4740 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0114 18:50:51.717542  4740 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0114 18:50:51.717703  4740 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0114 18:50:51.717864  4740 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0114 18:50:51.718040  4740 authenticator.hpp:390] authentication success  i0114 18:50:51.718292  4740 authenticatee.hpp:315] authentication success  i0114 18:50:51.718454  4738 master.cpp:4188] successfully authenticated principal 'testprincipal' at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.719012  4740 sched.cpp:392] successfully authenticated with master master@192.168.122.135:57018  i0114 18:50:51.719364  4740 sched.cpp:515] sending registration request to master@192.168.122.135:57018  i0114 18:50:51.719702  4740 sched.cpp:548] will retry registration in 746.539282ms if necessary  i0114 18:50:51.719902  4735 master.cpp:1417] received registration request for framework 'default' at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.720232  4735 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0114 18:50:51.722206  4735 master.cpp:1481] registering framework 2015011418505122729627525701847200000 (default) at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.720927  4737 log.cpp:684] attempting to append 300 bytes to the log  i0114 18:50:51.722924  4737 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0114 18:50:51.724269  4737 replica.cpp:511] replica received write request for position 3  i0114 18:50:51.724817  4737 leveldb.cpp:343] persisting action (319 bytes) to leveldb took 116638ns  i0114 18:50:51.728560  4737 replica.cpp:679] persisted action at 3  i0114 18:50:51.726066  4736 sched.cpp:442] framework registered with 2015011418505122729627525701847200000  i0114 18:50:51.728879  4736 sched.cpp:456] scheduler::registered took 34885ns  i0114 18:50:51.725520  4735 hierarchicalallocatorprocess.hpp:319] added framework 2015011418505122729627525701847200000  i0114 18:50:51.731864  4735 hierarchicalallocatorprocess.hpp:839] no resources available to allocate!  i0114 18:50:51.732038  4735 hierarchicalallocatorprocess.hpp:746] performed allocation for 0 slaves in 214728ns  i0114 18:50:51.733106  4738 replica.cpp:658] replica received learned notice for position 3  i0114 18:50:51.733340  4738 leveldb.cpp:343] persisting action (321 bytes) to leveldb took 83165ns  i0114 18:50:51.733538  4738 replica.cpp:679] persisted action at 3  i0114 18:50:51.733705  4738 replica.cpp:664] replica learned append action at position 3  i0114 18:50:51.735610  4738 registrar.cpp:490] successfully updated the 'registry' in 21.936128ms  i0114 18:50:51.735805  4739 log.cpp:703] attempting to truncate the log to 3  i0114 18:50:51.736445  4739 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0114 18:50:51.737664  4739 replica.cpp:511] replica received write request for position 4  i0114 18:50:51.738013  4739 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 72906ns  i0114 18:50:51.738255  4739 replica.cpp:679] persisted action at 4  i0114 18:50:51.743397  4734 replica.cpp:658] replica received learned notice for position 4  i0114 18:50:51.743628  4734 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 78832ns  i0114 18:50:51.743837  4734 leveldb.cpp:401] deleting 2 keys from leveldb took 63991ns  i0114 18:50:51.744004  4734 replica.cpp:679] persisted action at 4  i0114 18:50:51.744168  4734 replica.cpp:664] replica learned truncate action at position 4  i0114 18:50:51.745537  4738 master.cpp:3330] registered slave 201501141850512272962752570184720s0 at slave(115)@192.168.122.135:57018 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0114 18:50:51.745968  4734 hierarchicalallocatorprocess.hpp:453] added slave 201501141850512272962752570184720s0 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0114 18:50:51.746070  4735 slave.cpp:781] registered with master master@192.168.122.135:57018; given slave id 201501141850512272962752570184720s0  i0114 18:50:51.751437  4741 statusupdatemanager.cpp:178] resuming sending status updates  i0114 18:50:51.752428  4740 master.cpp:4072] sending 1 offers to framework 2015011418505122729627525701847200000 (default) at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.753764  4740 sched.cpp:605] scheduler::resourceoffers took 751714ns  i0114 18:50:51.754812  4740 master.cpp:2541] processing reply for offers: [ 201501141850512272962752570184720o0 ] on slave 201501141850512272962752570184720s0 at slave(115)@192.168.122.135:57018 (fedora19) for framework 2015011418505122729627525701847200000 (default) at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018  i0114 18:50:51.755040  4740 master.cpp:2647] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  w0114 18:50:51.756431  4741 master.cpp:2124] executor default for task 0 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0114 18:50:51.756652  4741 master.cpp:2136] executor default for task 0 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0114 18:50:51.757284  4741 master.hpp:766] adding task 0 with resources cpus():1; mem():16 on slave 201501141850512272962752570184720s0 (fedora19)  i0114 18:50:51.757733  4734 hierarchicalallocatorprocess.hpp:764] performed allocation for slave 201501141850512272962752570184720s0 in 9.535066ms  i0114 18:50:51.758117  4735 slave.cpp:2588] received ping from slaveobserver(95)@192.168.122.135:57018  i0114 18:50:51.758630  4741 master.cpp:2897] launching task 0 of framework 2015011418505122729627525701847200000 (default) at scheduler092fbbec093843558187fb92e5174c64@192.168.122.135:57018 with resources cpus():1; mem():16 on slave 201501141850512272962752570184720s0 at slave(115)@192.168.122.135:57018 (fedora19)  i0114 18:50:51.759526  4741 hierarchicalallocator_process.hpp:610] updated allocation of framework 2015011418505122729627525701847200000 on slave 201501141850512272962752570184720s0 from cpus():2; mem():1024; disk():1024; ports():[3100032000] to cpus():2; mem():1024; disk():1024; ports( ):[3100032000]  i0114 18:50:51.759796  4737 slave.cpp:1130] got assigned task 0 for framework 201501141850512272962752570184720 0000  i0114 18:50:51.761184  4737 slave.cpp:1245] launching task 0 for framework 20150...",2,train
MESOS-2226,HookTest.VerifySlaveLaunchExecutorHook is flaky,"observed this on internal ci      [ run      ] hooktest.verifyslavelaunchexecutorhook  using temporary directory '/tmp/hooktestverifyslavelaunchexecutorhookgjbgme'  i0114 18:51:34.659353  4720 leveldb.cpp:176] opened db in 1.255951ms  i0114 18:51:34.662112  4720 leveldb.cpp:183] compacted db in 596090ns  i0114 18:51:34.662364  4720 leveldb.cpp:198] created db iterator in 177877ns  i0114 18:51:34.662719  4720 leveldb.cpp:204] seeked to beginning of db in 19709ns  i0114 18:51:34.663010  4720 leveldb.cpp:273] iterated through 0 keys in the db in 18208ns  i0114 18:51:34.663312  4720 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0114 18:51:34.664266  4735 recover.cpp:449] starting replica recovery  i0114 18:51:34.664908  4735 recover.cpp:475] replica is in empty status  i0114 18:51:34.667842  4734 replica.cpp:641] replica in empty status received a broadcasted recover request  i0114 18:51:34.669117  4735 recover.cpp:195] received a recover response from a replica in empty status  i0114 18:51:34.677913  4735 recover.cpp:566] updating replica status to starting  i0114 18:51:34.683157  4735 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 137939ns  i0114 18:51:34.683507  4735 replica.cpp:323] persisted replica status to starting  i0114 18:51:34.684013  4735 recover.cpp:475] replica is in starting status  i0114 18:51:34.685554  4738 replica.cpp:641] replica in starting status received a broadcasted recover request  i0114 18:51:34.696512  4736 recover.cpp:195] received a recover response from a replica in starting status  i0114 18:51:34.700552  4735 recover.cpp:566] updating replica status to voting  i0114 18:51:34.701128  4735 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 115624ns  i0114 18:51:34.701478  4735 replica.cpp:323] persisted replica status to voting  i0114 18:51:34.701817  4735 recover.cpp:580] successfully joined the paxos group  i0114 18:51:34.702569  4735 recover.cpp:464] recover process terminated  i0114 18:51:34.716439  4736 master.cpp:262] master 201501141851342272962752570184720 (fedora19) started on 192.168.122.135:57018  i0114 18:51:34.716913  4736 master.cpp:308] master only allowing authenticated frameworks to register  i0114 18:51:34.717136  4736 master.cpp:313] master only allowing authenticated slaves to register  i0114 18:51:34.717488  4736 credentials.hpp:36] loading credentials for authentication from '/tmp/hooktestverifyslavelaunchexecutorhookgjbgme/credentials'  i0114 18:51:34.718077  4736 master.cpp:357] authorization enabled  i0114 18:51:34.719238  4738 whitelistwatcher.cpp:65] no whitelist given  i0114 18:51:34.719755  4737 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0114 18:51:34.722584  4736 master.cpp:1219] the newly elected leader is master@192.168.122.135:57018 with id 201501141851342272962752570184720  i0114 18:51:34.722865  4736 master.cpp:1232] elected as the leading master!  i0114 18:51:34.723310  4736 master.cpp:1050] recovering from registrar  i0114 18:51:34.723760  4734 registrar.cpp:313] recovering registrar  i0114 18:51:34.725229  4740 log.cpp:660] attempting to start the writer  i0114 18:51:34.727893  4739 replica.cpp:477] replica received implicit promise request with proposal 1  i0114 18:51:34.728425  4739 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 114781ns  i0114 18:51:34.728662  4739 replica.cpp:345] persisted promised to 1  i0114 18:51:34.731271  4741 coordinator.cpp:230] coordinator attemping to fill missing position  i0114 18:51:34.733223  4734 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0114 18:51:34.734076  4734 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 87441ns  i0114 18:51:34.734441  4734 replica.cpp:679] persisted action at 0  i0114 18:51:34.740272  4739 replica.cpp:511] replica received write request for position 0  i0114 18:51:34.740910  4739 leveldb.cpp:438] reading position from leveldb took 59846ns  i0114 18:51:34.741672  4739 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 189259ns  i0114 18:51:34.741919  4739 replica.cpp:679] persisted action at 0  i0114 18:51:34.743000  4739 replica.cpp:658] replica received learned notice for position 0  i0114 18:51:34.746844  4739 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 328487ns  i0114 18:51:34.747118  4739 replica.cpp:679] persisted action at 0  i0114 18:51:34.747553  4739 replica.cpp:664] replica learned nop action at position 0  i0114 18:51:34.751344  4737 log.cpp:676] writer started with ending position 0  i0114 18:51:34.753504  4734 leveldb.cpp:438] reading position from leveldb took 61183ns  i0114 18:51:34.762962  4737 registrar.cpp:346] successfully fetched the registry (0b) in 38.907904ms  i0114 18:51:34.763610  4737 registrar.cpp:445] applied 1 operations in 67206ns; attempting to update the 'registry'  i0114 18:51:34.766079  4736 log.cpp:684] attempting to append 130 bytes to the log  i0114 18:51:34.766769  4736 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0114 18:51:34.768215  4741 replica.cpp:511] replica received write request for position 1  i0114 18:51:34.768759  4741 leveldb.cpp:343] persisting action (149 bytes) to leveldb took 87970ns  i0114 18:51:34.768995  4741 replica.cpp:679] persisted action at 1  i0114 18:51:34.770691  4736 replica.cpp:658] replica received learned notice for position 1  i0114 18:51:34.771273  4736 leveldb.cpp:343] persisting action (151 bytes) to leveldb took 83590ns  i0114 18:51:34.771579  4736 replica.cpp:679] persisted action at 1  i0114 18:51:34.771917  4736 replica.cpp:664] replica learned append action at position 1  i0114 18:51:34.773252  4738 log.cpp:703] attempting to truncate the log to 1  i0114 18:51:34.773756  4735 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0114 18:51:34.775552  4736 replica.cpp:511] replica received write request for position 2  i0114 18:51:34.775846  4736 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 71503ns  i0114 18:51:34.776695  4736 replica.cpp:679] persisted action at 2  i0114 18:51:34.785259  4739 replica.cpp:658] replica received learned notice for position 2  i0114 18:51:34.786252  4737 registrar.cpp:490] successfully updated the 'registry' in 22.340864ms  i0114 18:51:34.787094  4737 registrar.cpp:376] successfully recovered registrar  i0114 18:51:34.787749  4737 master.cpp:1077] recovered 0 slaves from the registry (94b) ; allowing 10mins for slaves to reregister  i0114 18:51:34.787282  4739 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 707150ns  i0114 18:51:34.788692  4739 leveldb.cpp:401] deleting 1 keys from leveldb took 60262ns  i0114 18:51:34.789048  4739 replica.cpp:679] persisted action at 2  i0114 18:51:34.789329  4739 replica.cpp:664] replica learned truncate action at position 2  i0114 18:51:34.819548  4738 slave.cpp:173] slave started on 171)@192.168.122.135:57018  i0114 18:51:34.820530  4738 credentials.hpp:84] loading credential for authentication from '/tmp/hooktestverifyslavelaunchexecutorhookayxnqe/credential'  i0114 18:51:34.820952  4738 slave.cpp:282] slave using credential for: testprincipal  i0114 18:51:34.821516  4738 slave.cpp:300] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0114 18:51:34.822217  4738 slave.cpp:329] slave hostname: fedora19  i0114 18:51:34.822502  4738 slave.cpp:330] slave checkpoint: false  w0114 18:51:34.822857  4738 slave.cpp:332] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0114 18:51:34.824998  4737 state.cpp:33] recovering state from '/tmp/hooktestverifyslavelaunchexecutorhookayxnqe/meta'  i0114 18:51:34.834015  4738 statusupdatemanager.cpp:197] recovering status update manager  i0114 18:51:34.834810  4738 slave.cpp:3519] finished recovery  i0114 18:51:34.835906  4734 statusupdatemanager.cpp:171] pausing sending status updates  i0114 18:51:34.836423  4738 slave.cpp:613] new master detected at master@192.168.122.135:57018  i0114 18:51:34.836908  4738 slave.cpp:676] authenticating with master master@192.168.122.135:57018  i0114 18:51:34.837190  4738 slave.cpp:681] using default crammd5 authenticatee  i0114 18:51:34.837820  4737 authenticatee.hpp:138] creating new client sasl connection  i0114 18:51:34.838784  4738 slave.cpp:649] detecting new master  i0114 18:51:34.839306  4740 master.cpp:4130] authenticating slave(171)@192.168.122.135:57018  i0114 18:51:34.839957  4740 master.cpp:4141] using default crammd5 authenticator  i0114 18:51:34.841236  4740 authenticator.hpp:170] creating new server sasl connection  i0114 18:51:34.842681  4741 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0114 18:51:34.843118  4741 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0114 18:51:34.843581  4740 authenticator.hpp:276] received sasl authentication start  i0114 18:51:34.843962  4740 authenticator.hpp:398] authentication requires more steps  i0114 18:51:34.844357  4740 authenticatee.hpp:275] received sasl authentication step  i0114 18:51:34.844780  4740 authenticator.hpp:304] received sasl authentication step  i0114 18:51:34.845113  4740 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0114 18:51:34.845507  4740 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0114 18:51:34.845835  4740 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0114 18:51:34.846238  4740 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0114 18:51:34.846542  4740 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0114 18:51:34.846806  4740 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0114 18:51:34.847110  4740 authenticator.hpp:390] authentication success  i0114 18:51:34.847808  4734 authenticatee.hpp:315] authentication success  i0114 18:51:34.851029  4734 slave.cpp:747] successfully authenticated with master master@192.168.122.135:57018  i0114 18:51:34.851608  4737 master.cpp:4188] successfully authenticated principal 'testprincipal' at slave(171)@192.168.122.135:57018  i0114 18:51:34.854962  4720 sched.cpp:151] version: 0.22.0  i0114 18:51:34.856674  4734 slave.cpp:1075] will retry registration in 3.085482ms if necessary  i0114 18:51:34.857434  4739 sched.cpp:248] new master detected at master@192.168.122.135:57018  i0114 18:51:34.861433  4739 sched.cpp:304] authenticating with master master@192.168.122.135:57018  i0114 18:51:34.861693  4739 sched.cpp:311] using default crammd5 authenticatee  i0114 18:51:34.857795  4737 master.cpp:3276] registering slave at slave(171)@192.168.122.135:57018 (fedora19) with id 201501141851342272962752570184720s0  i0114 18:51:34.862951  4737 authenticatee.hpp:138] creating new client sasl connection  i0114 18:51:34.863919  4735 registrar.cpp:445] applied 1 operations in 120272ns; attempting to update the 'registry'  i0114 18:51:34.864645  4738 master.cpp:4130] authenticating schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.865033  4738 master.cpp:4141] using default crammd5 authenticator  i0114 18:51:34.866904  4738 authenticator.hpp:170] creating new server sasl connection  i0114 18:51:34.868840  4737 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0114 18:51:34.869125  4737 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0114 18:51:34.869523  4737 authenticator.hpp:276] received sasl authentication start  i0114 18:51:34.869835  4737 authenticator.hpp:398] authentication requires more steps  i0114 18:51:34.870213  4737 authenticatee.hpp:275] received sasl authentication step  i0114 18:51:34.870622  4737 authenticator.hpp:304] received sasl authentication step  i0114 18:51:34.870946  4737 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0114 18:51:34.871219  4737 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0114 18:51:34.871554  4737 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0114 18:51:34.871968  4737 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0114 18:51:34.872297  4737 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0114 18:51:34.872655  4737 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0114 18:51:34.873024  4737 authenticator.hpp:390] authentication success  i0114 18:51:34.873428  4737 authenticatee.hpp:315] authentication success  i0114 18:51:34.873632  4739 master.cpp:4188] successfully authenticated principal 'testprincipal' at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.875006  4740 sched.cpp:392] successfully authenticated with master master@192.168.122.135:57018  i0114 18:51:34.875319  4740 sched.cpp:515] sending registration request to master@192.168.122.135:57018  i0114 18:51:34.876200  4740 sched.cpp:548] will retry registration in 1.952991346secs if necessary  i0114 18:51:34.876729  4738 master.cpp:1417] received registration request for framework 'default' at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.877040  4738 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0114 18:51:34.878059  4738 master.cpp:1481] registering framework 2015011418513422729627525701847200000 (default) at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.878473  4739 log.cpp:684] attempting to append 300 bytes to the log  i0114 18:51:34.879464  4737 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0114 18:51:34.880116  4734 hierarchicalallocatorprocess.hpp:319] added framework 2015011418513422729627525701847200000  i0114 18:51:34.880470  4734 hierarchicalallocatorprocess.hpp:839] no resources available to allocate!  i0114 18:51:34.882331  4734 hierarchicalallocatorprocess.hpp:746] performed allocation for 0 slaves in 1.901284ms  i0114 18:51:34.884024  4741 sched.cpp:442] framework registered with 2015011418513422729627525701847200000  i0114 18:51:34.884454  4741 sched.cpp:456] scheduler::registered took 44320ns  i0114 18:51:34.881965  4737 replica.cpp:511] replica received write request for position 3  i0114 18:51:34.885218  4737 leveldb.cpp:343] persisting action (319 bytes) to leveldb took 134480ns  i0114 18:51:34.885716  4737 replica.cpp:679] persisted action at 3  i0114 18:51:34.886034  4739 slave.cpp:1075] will retry registration in 22.947772ms if necessary  i0114 18:51:34.886291  4740 master.cpp:3264] ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora19) as admission is already in progress  i0114 18:51:34.894690  4736 replica.cpp:658] replica received learned notice for position 3  i0114 18:51:34.898638  4736 leveldb.cpp:343] persisting action (321 bytes) to leveldb took 215501ns  i0114 18:51:34.899055  4736 replica.cpp:679] persisted action at 3  i0114 18:51:34.899416  4736 replica.cpp:664] replica learned append action at position 3  i0114 18:51:34.911782  4736 registrar.cpp:490] successfully updated the 'registry' in 46.176768ms  i0114 18:51:34.912286  4740 log.cpp:703] attempting to truncate the log to 3  i0114 18:51:34.913108  4740 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0114 18:51:34.915027  4736 master.cpp:3330] registered slave 201501141851342272962752570184720s0 at slave(171)@192.168.122.135:57018 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0114 18:51:34.915642  4735 hierarchicalallocatorprocess.hpp:453] added slave 201501141851342272962752570184720s0 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0114 18:51:34.917809  4735 hierarchicalallocatorprocess.hpp:764] performed allocation for slave 201501141851342272962752570184720s0 in 514027ns  i0114 18:51:34.916689  4738 replica.cpp:511] replica received write request for position 4  i0114 18:51:34.915784  4741 slave.cpp:781] registered with master master@192.168.122.135:57018; given slave id 201501141851342272962752570184720s0  i0114 18:51:34.919293  4741 slave.cpp:2588] received ping from slaveobserver(156)@192.168.122.135:57018  i0114 18:51:34.919775  4740 statusupdatemanager.cpp:178] resuming sending status updates  i0114 18:51:34.920374  4736 master.cpp:4072] sending 1 offers to framework 2015011418513422729627525701847200000 (default) at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.920569  4738 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 1.540136ms  i0114 18:51:34.921092  4738 replica.cpp:679] persisted action at 4  i0114 18:51:34.927111  4735 replica.cpp:658] replica received learned notice for position 4  i0114 18:51:34.927299  4734 sched.cpp:605] scheduler::resourceoffers took 1.335524ms  i0114 18:51:34.930418  4735 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 1.596377ms  i0114 18:51:34.930882  4735 leveldb.cpp:401] deleting 2 keys from leveldb took 67578ns  i0114 18:51:34.931115  4735 replica.cpp:679] persisted action at 4  i0114 18:51:34.931529  4735 replica.cpp:664] replica learned truncate action at position 4  i0114 18:51:34.930356  4734 master.cpp:2541] processing reply for offers: [ 201501141851342272962752570184720o0 ] on slave 201501141851342272962752570184720s0 at slave(171)@192.168.122.135:57018 (fedora19) for framework 2015011418513422729627525701847200000 (default) at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018  i0114 18:51:34.932834  4734 master.cpp:2647] authorizing framework principal 'testprincipal' to launch task 1 as user 'jenkins'  w0114 18:51:34.934442  4736 master.cpp:2124] executor default for task 1 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0114 18:51:34.934960  4736 master.cpp:2136] executor default for task 1 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0114 18:51:34.935878  4736 master.hpp:766] adding task 1 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 201501141851342272962752570184720s0 (fedora19)  i0114 18:51:34.939453  4738 hierarchicalallocator_process.hpp:610] updated allocation of framework 2015011418513422729627525701847200000 on slave 201501141851342272962752570184720s0 from cpus():2; mem():1024; disk():1024; ports():[3100032000] to cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0114 18:51:34.939950  4736 master.cpp:2897] launching task 1 of framework 2015011418513422729627525701847200000 (default) at schedulerc45273e46eb544eebf4571b353db648f@192.168.122.135:57018 with resources cpus():2; mem():1024; disk():1024; ports( ):[31000 32000] on slave 201...",3,train
MESOS-2228,SlaveTest.MesosExecutorGracefulShutdown is flaky,"observed this on internal ci      [ run      ] slavetest.mesosexecutorgracefulshutdown  using temporary directory '/tmp/slavetestmesosexecutorgracefulshutdownawdtvj'  i0124 08:14:04.399211  7926 leveldb.cpp:176] opened db in 27.364056ms  i0124 08:14:04.402632  7926 leveldb.cpp:183] compacted db in 3.357646ms  i0124 08:14:04.402691  7926 leveldb.cpp:198] created db iterator in 23822ns  i0124 08:14:04.402708  7926 leveldb.cpp:204] seeked to beginning of db in 1913ns  i0124 08:14:04.402716  7926 leveldb.cpp:273] iterated through 0 keys in the db in 458ns  i0124 08:14:04.402767  7926 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0124 08:14:04.403728  7951 recover.cpp:449] starting replica recovery  i0124 08:14:04.404011  7951 recover.cpp:475] replica is in empty status  i0124 08:14:04.407765  7950 replica.cpp:641] replica in empty status received a broadcasted recover request  i0124 08:14:04.408710  7951 recover.cpp:195] received a recover response from a replica in empty status  i0124 08:14:04.419666  7951 recover.cpp:566] updating replica status to starting  i0124 08:14:04.429719  7953 master.cpp:262] master 2015012408140416842879477877926 (utopic) started on 127.0.1.1:47787  i0124 08:14:04.429790  7953 master.cpp:308] master only allowing authenticated frameworks to register  i0124 08:14:04.429802  7953 master.cpp:313] master only allowing authenticated slaves to register  i0124 08:14:04.429826  7953 credentials.hpp:36] loading credentials for authentication from '/tmp/slavetestmesosexecutorgracefulshutdownawdtvj/credentials'  i0124 08:14:04.430277  7953 master.cpp:357] authorization enabled  i0124 08:14:04.432682  7953 master.cpp:1219] the newly elected leader is master@127.0.1.1:47787 with id 2015012408140416842879477877926  i0124 08:14:04.432816  7953 master.cpp:1232] elected as the leading master!  i0124 08:14:04.432894  7953 master.cpp:1050] recovering from registrar  i0124 08:14:04.433212  7950 registrar.cpp:313] recovering registrar  i0124 08:14:04.434226  7951 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 14.323302ms  i0124 08:14:04.434270  7951 replica.cpp:323] persisted replica status to starting  i0124 08:14:04.434489  7951 recover.cpp:475] replica is in starting status  i0124 08:14:04.436164  7951 replica.cpp:641] replica in starting status received a broadcasted recover request  i0124 08:14:04.439368  7947 recover.cpp:195] received a recover response from a replica in starting status  i0124 08:14:04.440626  7947 recover.cpp:566] updating replica status to voting  i0124 08:14:04.443667  7947 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 2.698664ms  i0124 08:14:04.443759  7947 replica.cpp:323] persisted replica status to voting  i0124 08:14:04.443925  7947 recover.cpp:580] successfully joined the paxos group  i0124 08:14:04.444160  7947 recover.cpp:464] recover process terminated  i0124 08:14:04.444543  7949 log.cpp:660] attempting to start the writer  i0124 08:14:04.446331  7949 replica.cpp:477] replica received implicit promise request with proposal 1  i0124 08:14:04.449329  7949 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 2.690453ms  i0124 08:14:04.449388  7949 replica.cpp:345] persisted promised to 1  i0124 08:14:04.450637  7947 coordinator.cpp:230] coordinator attemping to fill missing position  i0124 08:14:04.452271  7949 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0124 08:14:04.455124  7949 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 2.593522ms  i0124 08:14:04.455157  7949 replica.cpp:679] persisted action at 0  i0124 08:14:04.456594  7951 replica.cpp:511] replica received write request for position 0  i0124 08:14:04.456657  7951 leveldb.cpp:438] reading position from leveldb took 30358ns  i0124 08:14:04.464860  7951 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 8.164646ms  i0124 08:14:04.464903  7951 replica.cpp:679] persisted action at 0  i0124 08:14:04.465947  7949 replica.cpp:658] replica received learned notice for position 0  i0124 08:14:04.471567  7949 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 5.587838ms  i0124 08:14:04.471601  7949 replica.cpp:679] persisted action at 0  i0124 08:14:04.471622  7949 replica.cpp:664] replica learned nop action at position 0  i0124 08:14:04.472682  7951 log.cpp:676] writer started with ending position 0  i0124 08:14:04.473919  7951 leveldb.cpp:438] reading position from leveldb took 28676ns  i0124 08:14:04.491591  7951 registrar.cpp:346] successfully fetched the registry (0b) in 58.337024ms  i0124 08:14:04.491704  7951 registrar.cpp:445] applied 1 operations in 28163ns; attempting to update the 'registry'  i0124 08:14:04.493938  7953 log.cpp:684] attempting to append 118 bytes to the log  i0124 08:14:04.494122  7953 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0124 08:14:04.495069  7953 replica.cpp:511] replica received write request for position 1  i0124 08:14:04.500089  7953 leveldb.cpp:343] persisting action (135 bytes) to leveldb took 4.989356ms  i0124 08:14:04.500123  7953 replica.cpp:679] persisted action at 1  i0124 08:14:04.501271  7950 replica.cpp:658] replica received learned notice for position 1  i0124 08:14:04.505698  7950 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 4.396221ms  i0124 08:14:04.505734  7950 replica.cpp:679] persisted action at 1  i0124 08:14:04.505755  7950 replica.cpp:664] replica learned append action at position 1  i0124 08:14:04.507313  7950 registrar.cpp:490] successfully updated the 'registry' in 15.52896ms  i0124 08:14:04.507478  7953 log.cpp:703] attempting to truncate the log to 1  i0124 08:14:04.507848  7953 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0124 08:14:04.508743  7953 replica.cpp:511] replica received write request for position 2  i0124 08:14:04.509214  7950 registrar.cpp:376] successfully recovered registrar  i0124 08:14:04.509682  7946 master.cpp:1077] recovered 0 slaves from the registry (82b) ; allowing 10mins for slaves to reregister  i0124 08:14:04.514654  7953 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 5.880031ms  i0124 08:14:04.514689  7953 replica.cpp:679] persisted action at 2  i0124 08:14:04.515736  7953 replica.cpp:658] replica received learned notice for position 2  i0124 08:14:04.522014  7953 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 6.245138ms  i0124 08:14:04.522086  7953 leveldb.cpp:401] deleting 1 keys from leveldb took 37803ns  i0124 08:14:04.522107  7953 replica.cpp:679] persisted action at 2  i0124 08:14:04.522128  7953 replica.cpp:664] replica learned truncate action at position 2  i0124 08:14:04.531460  7926 containerizer.cpp:103] using isolation: posix/cpu,posix/mem  i0124 08:14:04.547194  7951 slave.cpp:173] slave started on 208)@127.0.1.1:47787  i0124 08:14:04.555682  7951 credentials.hpp:84] loading credential for authentication from '/tmp/slavetestmesosexecutorgracefulshutdownkb74xo/credential'  i0124 08:14:04.556622  7951 slave.cpp:282] slave using credential for: testprincipal  i0124 08:14:04.557052  7951 slave.cpp:300] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0124 08:14:04.557842  7951 slave.cpp:329] slave hostname: utopic  i0124 08:14:04.558091  7951 slave.cpp:330] slave checkpoint: false  w0124 08:14:04.558352  7951 slave.cpp:332] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0124 08:14:04.566864  7948 state.cpp:33] recovering state from '/tmp/slavetestmesosexecutorgracefulshutdownkb74xo/meta'  i0124 08:14:04.575711  7951 statusupdatemanager.cpp:197] recovering status update manager  i0124 08:14:04.575904  7951 containerizer.cpp:300] recovering containerizer  i0124 08:14:04.577112  7951 slave.cpp:3519] finished recovery  i0124 08:14:04.577374  7926 sched.cpp:151] version: 0.22.0  i0124 08:14:04.578663  7950 sched.cpp:248] new master detected at master@127.0.1.1:47787  i0124 08:14:04.578759  7950 sched.cpp:304] authenticating with master master@127.0.1.1:47787  i0124 08:14:04.578781  7950 sched.cpp:311] using default crammd5 authenticatee  i0124 08:14:04.579071  7950 authenticatee.hpp:138] creating new client sasl connection  i0124 08:14:04.579550  7947 master.cpp:4129] authenticating scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.579582  7947 master.cpp:4140] using default crammd5 authenticator  i0124 08:14:04.580031  7947 authenticator.hpp:170] creating new server sasl connection  i0124 08:14:04.580402  7947 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0124 08:14:04.580430  7947 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0124 08:14:04.580538  7947 authenticator.hpp:276] received sasl authentication start  i0124 08:14:04.580581  7947 authenticator.hpp:398] authentication requires more steps  i0124 08:14:04.580651  7947 authenticatee.hpp:275] received sasl authentication step  i0124 08:14:04.580746  7947 authenticator.hpp:304] received sasl authentication step  i0124 08:14:04.580837  7947 authenticator.hpp:390] authentication success  i0124 08:14:04.580940  7947 authenticatee.hpp:315] authentication success  i0124 08:14:04.581009  7947 master.cpp:4187] successfully authenticated principal 'testprincipal' at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.581328  7947 sched.cpp:392] successfully authenticated with master master@127.0.1.1:47787  i0124 08:14:04.581509  7947 master.cpp:1420] received registration request for framework 'default' at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.581585  7947 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0124 08:14:04.582033  7947 master.cpp:1484] registering framework 20150124081404168428794778779260000 (default) at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.582595  7947 hierarchicalallocatorprocess.hpp:319] added framework 20150124081404168428794778779260000  i0124 08:14:04.583051  7947 sched.cpp:442] framework registered with 20150124081404168428794778779260000  i0124 08:14:04.584087  7951 slave.cpp:613] new master detected at master@127.0.1.1:47787  i0124 08:14:04.584388  7951 slave.cpp:676] authenticating with master master@127.0.1.1:47787  i0124 08:14:04.584564  7951 slave.cpp:681] using default crammd5 authenticatee  i0124 08:14:04.584951  7951 slave.cpp:649] detecting new master  i0124 08:14:04.585219  7951 statusupdatemanager.cpp:171] pausing sending status updates  i0124 08:14:04.585604  7951 authenticatee.hpp:138] creating new client sasl connection  i0124 08:14:04.587666  7953 master.cpp:4129] authenticating slave(208)@127.0.1.1:47787  i0124 08:14:04.587702  7953 master.cpp:4140] using default crammd5 authenticator  i0124 08:14:04.588434  7953 authenticator.hpp:170] creating new server sasl connection  i0124 08:14:04.588764  7953 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0124 08:14:04.588790  7953 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0124 08:14:04.588896  7953 authenticator.hpp:276] received sasl authentication start  i0124 08:14:04.588935  7953 authenticator.hpp:398] authentication requires more steps  i0124 08:14:04.589005  7953 authenticatee.hpp:275] received sasl authentication step  i0124 08:14:04.589082  7953 authenticator.hpp:304] received sasl authentication step  i0124 08:14:04.589140  7953 authenticator.hpp:390] authentication success  i0124 08:14:04.589232  7953 authenticatee.hpp:315] authentication success  i0124 08:14:04.589300  7953 master.cpp:4187] successfully authenticated principal 'testprincipal' at slave(208)@127.0.1.1:47787  i0124 08:14:04.589587  7953 slave.cpp:747] successfully authenticated with master master@127.0.1.1:47787  i0124 08:14:04.589913  7953 master.cpp:3275] registering slave at slave(208)@127.0.1.1:47787 (utopic) with id 2015012408140416842879477877926s0  i0124 08:14:04.590322  7953 registrar.cpp:445] applied 1 operations in 60404ns; attempting to update the 'registry'  i0124 08:14:04.595336  7948 log.cpp:684] attempting to append 283 bytes to the log  i0124 08:14:04.595552  7948 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0124 08:14:04.596535  7948 replica.cpp:511] replica received write request for position 3  i0124 08:14:04.597846  7951 master.cpp:3263] ignoring register slave message from slave(208)@127.0.1.1:47787 (utopic) as admission is already in progress  i0124 08:14:04.602326  7948 leveldb.cpp:343] persisting action (302 bytes) to leveldb took 5.758211ms  i0124 08:14:04.602363  7948 replica.cpp:679] persisted action at 3  i0124 08:14:04.603492  7951 replica.cpp:658] replica received learned notice for position 3  i0124 08:14:04.608952  7951 leveldb.cpp:343] persisting action (304 bytes) to leveldb took 5.427195ms  i0124 08:14:04.608985  7951 replica.cpp:679] persisted action at 3  i0124 08:14:04.609007  7951 replica.cpp:664] replica learned append action at position 3  i0124 08:14:04.610643  7951 registrar.cpp:490] successfully updated the 'registry' in 20.258048ms  i0124 08:14:04.610800  7948 log.cpp:703] attempting to truncate the log to 3  i0124 08:14:04.611184  7948 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0124 08:14:04.612076  7948 replica.cpp:511] replica received write request for position 4  i0124 08:14:04.613061  7946 master.cpp:3329] registered slave 2015012408140416842879477877926s0 at slave(208)@127.0.1.1:47787 (utopic) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0124 08:14:04.613299  7946 hierarchicalallocatorprocess.hpp:453] added slave 2015012408140416842879477877926s0 (utopic) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0124 08:14:04.613688  7946 slave.cpp:781] registered with master master@127.0.1.1:47787; given slave id 2015012408140416842879477877926s0  i0124 08:14:04.614112  7946 master.cpp:4071] sending 1 offers to framework 20150124081404168428794778779260000 (default) at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.614228  7946 statusupdatemanager.cpp:178] resuming sending status updates  i0124 08:14:04.617481  7947 master.cpp:2677] processing accept call for offers: [ 2015012408140416842879477877926o0 ] on slave 2015012408140416842879477877926s0 at slave(208)@127.0.1.1:47787 (utopic) for framework 20150124081404168428794778779260000 (default) at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787  i0124 08:14:04.617535  7947 master.cpp:2513] authorizing framework principal 'testprincipal' to launch task 7c16772d4aed471981c4658a2cc22543 as user 'jenkins'  i0124 08:14:04.618736  7947 master.hpp:782] adding task 7c16772d4aed471981c4658a2cc22543 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2015012408140416842879477877926s0 (utopic)  i0124 08:14:04.618854  7947 master.cpp:2885] launching task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 (default) at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef@127.0.1.1:47787 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 2015012408140416842879477877926s0 at slave(208)@127.0.1.1:47787 (utopic)  i0124 08:14:04.619209  7947 slave.cpp:1130] got assigned task 7c16772d4aed471981c4658a2cc22543 for framework 20150124081404168428794778779260000  i0124 08:14:04.619472  7948 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 7.364828ms  i0124 08:14:04.619941  7948 replica.cpp:679] persisted action at 4  i0124 08:14:04.624851  7953 replica.cpp:658] replica received learned notice for position 4  i0124 08:14:04.625757  7947 slave.cpp:1245] launching task 7c16772d4aed471981c4658a2cc22543 for framework 20150124081404168428794778779260000  i0124 08:14:04.630590  7953 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 5.705336ms  i0124 08:14:04.630805  7953 leveldb.cpp:401] deleting 2 keys from leveldb took 51263ns  i0124 08:14:04.630828  7953 replica.cpp:679] persisted action at 4  i0124 08:14:04.630851  7953 replica.cpp:664] replica learned truncate action at position 4  i0124 08:14:04.633968  7947 slave.cpp:3921] launching executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 in work directory '/tmp/slavetestmesosexecutorgracefulshutdownkb74xo/slaves/2015012408140416842879477877926s0/frameworks/20150124081404168428794778779260000/executors/7c16772d4aed471981c4658a2cc22543/runs/53887a08f11d4a2fa659a715d9fcf3d2'  i0124 08:14:04.634963  7951 containerizer.cpp:445] starting container '53887a08f11d4a2fa659a715d9fcf3d2' for executor '7c16772d4aed471981c4658a2cc22543' of framework '20150124081404168428794778779260000'  w0124 08:14:04.636931  7951 containerizer.cpp:296] commandinfo.graceperiod flag is not set, using default value: 3secs  i0124 08:14:04.655591  7947 slave.cpp:1368] queuing task '7c16772d4aed471981c4658a2cc22543' for executor 7c16772d4aed471981c4658a2cc22543 of framework '20150124081404168428794778779260000  i0124 08:14:04.656992  7951 launcher.cpp:137] forked child with pid '11030' for container '53887a08f11d4a2fa659a715d9fcf3d2'  i0124 08:14:04.673646  7951 slave.cpp:2890] monitoring executor '7c16772d4aed471981c4658a2cc22543' of framework '20150124081404168428794778779260000' in container '53887a08f11d4a2fa659a715d9fcf3d2'  i0124 08:14:04.964946 11044 exec.cpp:147] version: 0.22.0  i0124 08:14:05.113059  7948 slave.cpp:1912] got registration for executor '7c16772d4aed471981c4658a2cc22543' of framework 20150124081404168428794778779260000 from executor(1)@127.0.1.1:49174  i0124 08:14:05.121086  7948 slave.cpp:2031] flushing queued task 7c16772d4aed471981c4658a2cc22543 for executor '7c16772d4aed471981c4658a2cc22543' of framework 20150124081404168428794778779260000  i0124 08:14:05.266849 11062 exec.cpp:221] executor registered on slave 2015012408140416842879477877926s0  shutdown timeout is set to 3secsregistered executor on utopic  starting task 7c16772d4aed471981c4658a2cc22543  forked command at 11067  sh c 'sleep 1000'  i0124 08:14:05.492084  7953 slave.cpp:2265] handling status update taskrunning (uuid: 54742a87ef024e72a19b83b0eeb62568) for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 from executor(1)@127.0.1.1:49174  i0124 08:14:05.492805  7953 statusupdatemanager.cpp:317] received status update taskrunning (uuid: 54742a87ef024e72a19b83b0eeb62568) for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000  i0124 08:14:05.493762  7953 slave.cpp:2508] forwarding the update taskrunning (uuid: 54742a87ef024e72a19b83b0eeb62568) for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 to master@127.0.1.1:47787  i0124 08:14:05.493948  7953 slave.cpp:2441] sending acknowledgement for status update taskrunning (uuid: 54742a87ef024e72a19b83b0eeb62568) for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 to executor(1)@127.0.1.1:49174  i0124 08:14:05.495378  7949 master.cpp:3652] forwarding status update taskrunning (uuid: 54742a87ef024e72a19b83b0eeb62568) for task 7c16772d4aed471981c4658a2cc22543 of framework 2015012408140416842879477877926 0000  i0124 08:14:05.495584  7949 master.cpp:3624]...",3,train
MESOS-2230,Update RateLimiter to allow the acquired future to be discarded,currently there is no way for the future returned by ratelimiter's acquire() to be discarded by the user of the limiter. this is useful in cases where the user is no longer interested in the permit. see mesos 1148 for an example use case.,3,train
MESOS-2232,Suppress MockAllocator::transformAllocation() warnings.,"after transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. commits leading to this behaviour:  dacc88292cc13d4b08fe8cda4df71110a96cb12a  5a02d5bdc75d3b1149dcda519016374be06ec6bd  corresponding reviews:  https:/reviews.apache.org/r/29083  https:/reviews.apache.org/r/29084    here is an example:    [ run ] masterallocatortest/0.frameworkreregistersfirst gmock warning: uninteresting mock function call  taking default action specified at: ../../../src/tests/mesos.hpp:719: function call: transformallocation(@0x7fd3bb5274d8 20150115185632167776480059671441860000, @0x7fd3bb5274f8 2015011518563216777648005967144186s0, @0x1119140e0 16byte object /) stack trace: [ ok ] masterallocatortest/0.frameworkreregistersfirst (204 ms)  ",3,train
MESOS-2233,Run ASF CI mesos builds inside docker,"there are several limitations to mesos projects current state of ci, which is run on builds.a.o    > only runs on ubuntu  > doesn't run any tests that deal with cgroups  > doesn't run any tests that need root permissions    now that asf ci supports docker (https:/issues.apache.org/jira/browse/builds 25), it would be great for the mesos project to use it.",5,train
MESOS-2241,DiskUsageCollectorTest.SymbolicLink test is flaky,observed this on a local machine running linux w/ sudo.      [ run      ] diskusagecollectortest.symboliclink  ../../src/tests/diskquotatests.cpp:138: failure  expected: (usage1.get()) /  [  failed  ] diskusagecollectortest.symboliclink (201 ms)  ,1,train
MESOS-2257,Version the Operator/Admin API,"as a consumer of the mesos http api, it is necessary for us to determine the current version of mesos so that we can parse the json documents returned correctly (since they change from version to version).     currently we're doing this by fetching state.json, parsing it and pulling out the version field. a more idiomatic way to do this would be to filter on the contenttype in the header itself.    to give a more concrete example, currently the json documents returned by the http api return the following headers:    http/1.1 200 ok  date: fri, 23 jan 2015 21:31:37 gmt  contentlength: 9352  contenttype: application/json      something like the following (e.g. for master/state.json) would be easy to switch upon:    http/1.1 200 ok  date: fri, 23 jan 2015 21:31:37 gmt  contentlength: 9352  contenttype: application/vnd.mesos.master.state.v0.20.1json; charset=utf8      the vnd prefix is typically used for vendor specific file types (see: http:/en.wikipedia.org/wiki/internetmediatype#prefix_vnd). charset=utf8 is required for json documents and is currently being omitted.    this contenttype would change for each document type, for example:    application/vnd.mesos.master.state.v0.20.1json; charset=utf8  application/vnd.mesos.master.stats.v0.20.1json; charset=utf8  application/vnd.mesos.slave.state.v0.20.1json; charset=utf8  application/vnd.mesos.slave.stats.v0.20.1json; charset=utf8      alternatively, the version could be appended as an extra field:    application/vnd.mesos.master.statejson; charset=utf8; version=v0.20.1  application/vnd.mesos.master.statsjson; charset=utf8; version=v0.20.1  application/vnd.mesos.slave.statejson; charset=utf8; version=v0.20.1  application/vnd.mesos.slave.stats+json; charset=utf8; version=v0.20.1      thanks!",13,train
MESOS-2273,"Add ""tests"" target to Makefile for building-but-not-running tests.","'make check' allows one to build and run the test suite. however, often we just want to build the tests.  currently, this is done by setting gtest_filter to an empty string.    it will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",1,train
MESOS-2275,Document header include rules in style guide,"we have several ways of sorting, grouping and ordering headers includes in mesos. we should agree on a rule set and do a style scan.",3,train
MESOS-2279,Future callbacks should be cleared once the future has transitioned.,"for example, when a future has transitioned into ready state, all ondiscard callbacks should be cleared to avoid potential cyclic dependency and memory leak. for instance:      promise/ promise;  future/ f = promise.future();  f.ondiscard(lambda::bind(&somefunc, f));  promise.set(nothing());      the above code has a cyclic dependency because f.data has a reference to the future inside an std::function which has a reference to f.data.",2,train
MESOS-2281,Deprecate plain text Credential format.,"currently two formats of credentials are supported: json        ""credentials"": [            and a new line file:    principal1 secret1  pricipal2 secret2      we should deprecate the new line format and remove support for the old format.",3,train
MESOS-2283,SlaveRecoveryTest.ReconcileKillTask is flaky.,"saw this on an internal ci:      [ run      ] slaverecoverytest/0.reconcilekilltask  using temporary directory '/tmp/slaverecoverytest0reconcilekilltaskd5wswg'  i0126 19:10:52.005317 13291 leveldb.cpp:176] opened db in 978670ns  i0126 19:10:52.006155 13291 leveldb.cpp:183] compacted db in 541346ns  i0126 19:10:52.006494 13291 leveldb.cpp:198] created db iterator in 24562ns  i0126 19:10:52.006798 13291 leveldb.cpp:204] seeked to beginning of db in 3254ns  i0126 19:10:52.007036 13291 leveldb.cpp:273] iterated through 0 keys in the db in 949ns  i0126 19:10:52.007369 13291 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0126 19:10:52.008362 13308 recover.cpp:449] starting replica recovery  i0126 19:10:52.009141 13308 recover.cpp:475] replica is in empty status  i0126 19:10:52.016494 13308 replica.cpp:641] replica in empty status received a broadcasted recover request  i0126 19:10:52.017333 13309 recover.cpp:195] received a recover response from a replica in empty status  i0126 19:10:52.018244 13309 recover.cpp:566] updating replica status to starting  i0126 19:10:52.019064 13305 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 113577ns  i0126 19:10:52.019487 13305 replica.cpp:323] persisted replica status to starting  i0126 19:10:52.019937 13309 recover.cpp:475] replica is in starting status  i0126 19:10:52.021492 13307 replica.cpp:641] replica in starting status received a broadcasted recover request  i0126 19:10:52.022665 13309 recover.cpp:195] received a recover response from a replica in starting status  i0126 19:10:52.027971 13312 recover.cpp:566] updating replica status to voting  i0126 19:10:52.028590 13312 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 78452ns  i0126 19:10:52.028869 13312 replica.cpp:323] persisted replica status to voting  i0126 19:10:52.029252 13312 recover.cpp:580] successfully joined the paxos group  i0126 19:10:52.030828 13307 recover.cpp:464] recover process terminated  i0126 19:10:52.049947 13306 master.cpp:262] master 2015012619105222729627523554513291 (fedora19) started on 192.168.122.135:35545  i0126 19:10:52.050499 13306 master.cpp:308] master only allowing authenticated frameworks to register  i0126 19:10:52.050765 13306 master.cpp:313] master only allowing authenticated slaves to register  i0126 19:10:52.051048 13306 credentials.hpp:36] loading credentials for authentication from '/tmp/slaverecoverytest0reconcilekilltaskd5wswg/credentials'  i0126 19:10:52.051589 13306 master.cpp:357] authorization enabled  i0126 19:10:52.052531 13305 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0126 19:10:52.052881 13311 whitelistwatcher.cpp:65] no whitelist given  i0126 19:10:52.055524 13306 master.cpp:1219] the newly elected leader is master@192.168.122.135:35545 with id 2015012619105222729627523554513291  i0126 19:10:52.056226 13306 master.cpp:1232] elected as the leading master!  i0126 19:10:52.056639 13306 master.cpp:1050] recovering from registrar  i0126 19:10:52.057045 13307 registrar.cpp:313] recovering registrar  i0126 19:10:52.058554 13312 log.cpp:660] attempting to start the writer  i0126 19:10:52.060868 13309 replica.cpp:477] replica received implicit promise request with proposal 1  i0126 19:10:52.061691 13309 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 91680ns  i0126 19:10:52.062261 13309 replica.cpp:345] persisted promised to 1  i0126 19:10:52.064559 13310 coordinator.cpp:230] coordinator attemping to fill missing position  i0126 19:10:52.069105 13311 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0126 19:10:52.069860 13311 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 94858ns  i0126 19:10:52.070350 13311 replica.cpp:679] persisted action at 0  i0126 19:10:52.080348 13305 replica.cpp:511] replica received write request for position 0  i0126 19:10:52.081153 13305 leveldb.cpp:438] reading position from leveldb took 62247ns  i0126 19:10:52.081676 13305 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 81487ns  i0126 19:10:52.082053 13305 replica.cpp:679] persisted action at 0  i0126 19:10:52.083566 13309 replica.cpp:658] replica received learned notice for position 0  i0126 19:10:52.085734 13309 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 283144ns  i0126 19:10:52.086067 13309 replica.cpp:679] persisted action at 0  i0126 19:10:52.086448 13309 replica.cpp:664] replica learned nop action at position 0  i0126 19:10:52.089784 13306 log.cpp:676] writer started with ending position 0  i0126 19:10:52.093415 13309 leveldb.cpp:438] reading position from leveldb took 66744ns  i0126 19:10:52.104814 13306 registrar.cpp:346] successfully fetched the registry (0b) in 47.451136ms  i0126 19:10:52.105731 13306 registrar.cpp:445] applied 1 operations in 42124ns; attempting to update the 'registry'  i0126 19:10:52.111935 13305 log.cpp:684] attempting to append 131 bytes to the log  i0126 19:10:52.112754 13305 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0126 19:10:52.114297 13308 replica.cpp:511] replica received write request for position 1  i0126 19:10:52.114908 13308 leveldb.cpp:343] persisting action (150 bytes) to leveldb took 98332ns  i0126 19:10:52.115387 13308 replica.cpp:679] persisted action at 1  i0126 19:10:52.117277 13305 replica.cpp:658] replica received learned notice for position 1  i0126 19:10:52.118142 13305 leveldb.cpp:343] persisting action (152 bytes) to leveldb took 227799ns  i0126 19:10:52.118621 13305 replica.cpp:679] persisted action at 1  i0126 19:10:52.118979 13305 replica.cpp:664] replica learned append action at position 1  i0126 19:10:52.121311 13305 registrar.cpp:490] successfully updated the 'registry' in 15.161088ms  i0126 19:10:52.121548 13311 log.cpp:703] attempting to truncate the log to 1  i0126 19:10:52.122697 13311 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0126 19:10:52.124316 13307 replica.cpp:511] replica received write request for position 2  i0126 19:10:52.124913 13307 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 87281ns  i0126 19:10:52.125334 13307 replica.cpp:679] persisted action at 2  i0126 19:10:52.127018 13311 replica.cpp:658] replica received learned notice for position 2  i0126 19:10:52.127835 13311 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 201050ns  i0126 19:10:52.128232 13311 leveldb.cpp:401] deleting 1 keys from leveldb took 78012ns  i0126 19:10:52.128835 13305 registrar.cpp:376] successfully recovered registrar  i0126 19:10:52.128551 13311 replica.cpp:679] persisted action at 2  i0126 19:10:52.130105 13311 replica.cpp:664] replica learned truncate action at position 2  i0126 19:10:52.131479 13312 master.cpp:1077] recovered 0 slaves from the registry (95b) ; allowing 10mins for slaves to reregister  i0126 19:10:52.143465 13291 containerizer.cpp:103] using isolation: posix/cpu,posix/mem  i0126 19:10:52.170471 13309 slave.cpp:173] slave started on 101)@192.168.122.135:35545  i0126 19:10:52.171723 13309 credentials.hpp:84] loading credential for authentication from '/tmp/slaverecoverytest0reconcilekilltaskqbguum/credential'  i0126 19:10:52.172286 13309 slave.cpp:282] slave using credential for: testprincipal  i0126 19:10:52.172821 13309 slave.cpp:300] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0126 19:10:52.173982 13309 slave.cpp:329] slave hostname: fedora19  i0126 19:10:52.174505 13309 slave.cpp:330] slave checkpoint: true  i0126 19:10:52.179308 13309 state.cpp:33] recovering state from '/tmp/slaverecoverytest0reconcilekilltaskqbguum/meta'  i0126 19:10:52.180075 13308 statusupdatemanager.cpp:197] recovering status update manager  i0126 19:10:52.180611 13308 containerizer.cpp:300] recovering containerizer  i0126 19:10:52.182473 13309 slave.cpp:3519] finished recovery  i0126 19:10:52.184403 13312 slave.cpp:613] new master detected at master@192.168.122.135:35545  i0126 19:10:52.184916 13312 slave.cpp:676] authenticating with master master@192.168.122.135:35545  i0126 19:10:52.185230 13312 slave.cpp:681] using default crammd5 authenticatee  i0126 19:10:52.185715 13312 slave.cpp:649] detecting new master  i0126 19:10:52.186420 13312 authenticatee.hpp:138] creating new client sasl connection  i0126 19:10:52.186002 13311 statusupdatemanager.cpp:171] pausing sending status updates  i0126 19:10:52.188293 13312 master.cpp:4129] authenticating slave(101)@192.168.122.135:35545  i0126 19:10:52.188748 13312 master.cpp:4140] using default crammd5 authenticator  i0126 19:10:52.189525 13312 authenticator.hpp:170] creating new server sasl connection  i0126 19:10:52.191082 13305 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0126 19:10:52.191550 13305 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0126 19:10:52.191990 13312 authenticator.hpp:276] received sasl authentication start  i0126 19:10:52.192365 13312 authenticator.hpp:398] authentication requires more steps  i0126 19:10:52.192800 13311 authenticatee.hpp:275] received sasl authentication step  i0126 19:10:52.193244 13312 authenticator.hpp:304] received sasl authentication step  i0126 19:10:52.193565 13312 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0126 19:10:52.193902 13312 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0126 19:10:52.194301 13312 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0126 19:10:52.195669 13312 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0126 19:10:52.196048 13312 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0126 19:10:52.196395 13312 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0126 19:10:52.196723 13312 authenticator.hpp:390] authentication success  i0126 19:10:52.197206 13305 authenticatee.hpp:315] authentication success  i0126 19:10:52.204121 13305 slave.cpp:747] successfully authenticated with master master@192.168.122.135:35545  i0126 19:10:52.204676 13310 master.cpp:4187] successfully authenticated principal 'testprincipal' at slave(101)@192.168.122.135:35545  i0126 19:10:52.205729 13305 slave.cpp:1075] will retry registration in 5.608661ms if necessary  i0126 19:10:52.206451 13310 master.cpp:3275] registering slave at slave(101)@192.168.122.135:35545 (fedora19) with id 2015012619105222729627523554513291s0  i0126 19:10:52.210019 13310 registrar.cpp:445] applied 1 operations in 235087ns; attempting to update the 'registry'  i0126 19:10:52.220736 13308 slave.cpp:1075] will retry registration in 9.28397ms if necessary  i0126 19:10:52.221309 13311 master.cpp:3263] ignoring register slave message from slave(101)@192.168.122.135:35545 (fedora19) as admission is already in progress  i0126 19:10:52.224818 13307 log.cpp:684] attempting to append 302 bytes to the log  i0126 19:10:52.225554 13307 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0126 19:10:52.227422 13305 replica.cpp:511] replica received write request for position 3  i0126 19:10:52.227969 13305 leveldb.cpp:343] persisting action (321 bytes) to leveldb took 100350ns  i0126 19:10:52.228276 13305 replica.cpp:679] persisted action at 3  i0126 19:10:52.232475 13312 replica.cpp:658] replica received learned notice for position 3  i0126 19:10:52.233280 13312 leveldb.cpp:343] persisting action (323 bytes) to leveldb took 546567ns  i0126 19:10:52.233726 13312 replica.cpp:679] persisted action at 3  i0126 19:10:52.234035 13312 replica.cpp:664] replica learned append action at position 3  i0126 19:10:52.236556 13310 registrar.cpp:490] successfully updated the 'registry' in 26.040064ms  i0126 19:10:52.237330 13305 log.cpp:703] attempting to truncate the log to 3  i0126 19:10:52.238056 13311 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0126 19:10:52.239594 13311 replica.cpp:511] replica received write request for position 4  i0126 19:10:52.240129 13311 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 92868ns  i0126 19:10:52.240458 13311 replica.cpp:679] persisted action at 4  i0126 19:10:52.241976 13308 replica.cpp:658] replica received learned notice for position 4  i0126 19:10:52.242645 13308 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 95635ns  i0126 19:10:52.242990 13308 leveldb.cpp:401] deleting 2 keys from leveldb took 58066ns  i0126 19:10:52.243337 13308 replica.cpp:679] persisted action at 4  i0126 19:10:52.243695 13308 replica.cpp:664] replica learned truncate action at position 4  i0126 19:10:52.245657 13291 sched.cpp:151] version: 0.22.0  i0126 19:10:52.247625 13305 master.cpp:3329] registered slave 2015012619105222729627523554513291s0 at slave(101)@192.168.122.135:35545 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0126 19:10:52.248942 13307 slave.cpp:781] registered with master master@192.168.122.135:35545; given slave id 2015012619105222729627523554513291s0  i0126 19:10:52.250396 13307 slave.cpp:797] checkpointing slaveinfo to '/tmp/slaverecoverytest0reconcilekilltaskqbguum/meta/slaves/2015012619105222729627523554513291s0/slave.info'  i0126 19:10:52.250731 13309 statusupdatemanager.cpp:178] resuming sending status updates  i0126 19:10:52.251765 13307 slave.cpp:2588] received ping from slaveobserver(99)@192.168.122.135:35545  i0126 19:10:52.247951 13310 hierarchicalallocatorprocess.hpp:453] added slave 2015012619105222729627523554513291s0 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0126 19:10:52.252810 13310 hierarchicalallocatorprocess.hpp:831] no resources available to allocate!  i0126 19:10:52.254365 13310 hierarchicalallocatorprocess.hpp:756] performed allocation for slave 2015012619105222729627523554513291s0 in 1.732701ms  i0126 19:10:52.254137 13307 sched.cpp:248] new master detected at master@192.168.122.135:35545  i0126 19:10:52.257863 13307 sched.cpp:304] authenticating with master master@192.168.122.135:35545  i0126 19:10:52.258249 13307 sched.cpp:311] using default crammd5 authenticatee  i0126 19:10:52.258908 13306 authenticatee.hpp:138] creating new client sasl connection  i0126 19:10:52.261397 13309 master.cpp:4129] authenticating scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.261776 13309 master.cpp:4140] using default crammd5 authenticator  i0126 19:10:52.264528 13309 authenticator.hpp:170] creating new server sasl connection  i0126 19:10:52.266248 13312 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0126 19:10:52.266749 13312 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0126 19:10:52.267143 13312 authenticator.hpp:276] received sasl authentication start  i0126 19:10:52.267525 13312 authenticator.hpp:398] authentication requires more steps  i0126 19:10:52.267917 13312 authenticatee.hpp:275] received sasl authentication step  i0126 19:10:52.268404 13312 authenticator.hpp:304] received sasl authentication step  i0126 19:10:52.268725 13312 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0126 19:10:52.269078 13312 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0126 19:10:52.269498 13312 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0126 19:10:52.269881 13312 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0126 19:10:52.270385 13312 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0126 19:10:52.271015 13312 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0126 19:10:52.271599 13312 authenticator.hpp:390] authentication success  i0126 19:10:52.272126 13312 authenticatee.hpp:315] authentication success  i0126 19:10:52.272415 13305 master.cpp:4187] successfully authenticated principal 'testprincipal' at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.273998 13307 sched.cpp:392] successfully authenticated with master master@192.168.122.135:35545  i0126 19:10:52.274415 13307 sched.cpp:515] sending registration request to master@192.168.122.135:35545  i0126 19:10:52.274842 13307 sched.cpp:548] will retry registration in 674.656506ms if necessary  i0126 19:10:52.275235 13305 master.cpp:1420] received registration request for framework 'default' at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.276017 13305 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0126 19:10:52.277027 13305 master.cpp:1484] registering framework 20150126191052227296275235545132910000 (default) at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.278285 13308 hierarchicalallocatorprocess.hpp:319] added framework 20150126191052227296275235545132910000  i0126 19:10:52.279575 13308 hierarchicalallocatorprocess.hpp:738] performed allocation for 1 slaves in 697902ns  i0126 19:10:52.287966 13305 master.cpp:4071] sending 1 offers to framework 20150126191052227296275235545132910000 (default) at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.288776 13307 sched.cpp:442] framework registered with 20150126191052227296275235545132910000  i0126 19:10:52.289373 13307 sched.cpp:456] scheduler::registered took 21674ns  i0126 19:10:52.289932 13307 sched.cpp:605] scheduler::resourceoffers took 76147ns  i0126 19:10:52.293220 13311 master.cpp:2677] processing accept call for offers: [ 2015012619105222729627523554513291o0 ] on slave 2015012619105222729627523554513291s0 at slave(101)@192.168.122.135:35545 (fedora19) for framework 20150126191052227296275235545132910000 (default) at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545  i0126 19:10:52.293586 13311 master.cpp:2513] authorizing framework principal 'testprincipal' to launch task 61eaeec3e8ca4e1582d6284c05c3bb6e as user 'jenkins'  i0126 19:10:52.295825 13311 master.hpp:782] adding task 61eaeec3e8ca4e1582d6284c05c3bb6e with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2015012619105222729627523554513291s0 (fedora19)  i0126 19:10:52.296272 13311 master.cpp:2885] launching task 61eaeec3e8ca4e1582d6284c05c3bb6e of framework 20150126191052227296275235545132910000 (default) at scheduler6da85b48b57f4202b630c45f8f652321@192.168.122.135:35545 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 2015012619105222729627523554513291s0 at slave(101)@192.168.122.135:35545 (fedora19)  i0126 19:10:52.296886 13309 slave.cpp:1130] got assigned task 61eaeec3e8ca4e1582d6284c05c3bb6e for framework 20150126191052227296275235545132910000  i0126 19:10:52.297324 13309 slave.cpp:3846] checkpointing frameworkinfo to '/tmp/slaverecoverytest0reconcilekilltask_qbguum/meta/slaves/2015012619105222729627523554513291s0/frameworks/2015012619105222...",1,train
MESOS-2289,Design doc for the HTTP API,this tracks the design of the http api.,13,train
MESOS-2290,Move all scheduler driver validations to master,"with http api, the scheduler driver will no longer exist and hence all the validations should move to the master.",3,train
MESOS-2293,Implement the scheduler endpoint on master,nan,8,train
MESOS-2294,Implement the Events stream on master for Call endpoint,nan,8,train
MESOS-2295,Implement the Call endpoint on Slave,nan,8,train
MESOS-2296,Implement the Events stream on slave for Call endpoint,nan,8,train
MESOS-2297,Add authentication support for HTTP API,"since most of the communication between mesos components will happen through http with the arrival of the https:/issues.apache.org/jira/browse/mesos 2288, it makes sense to use http standard mechanisms to authenticate this communication.",1,train
MESOS-2298,Provide a Java library for master detection,"when schedulers start interacting with mesos master via http endpoints, they need a way to detect masters.     mesos should provide a master detection java library to make this easy for frameworks.",5,train
MESOS-2302,FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.,"bad run:    [ run      ] faulttolerancetest.schedulerfailoverframeworkmessage  using temporary directory '/tmp/faulttolerancetestschedulerfailoverframeworkmessagef3jykr'  i0123 18:50:11.669674 15688 leveldb.cpp:176] opened db in 31.920683ms  i0123 18:50:11.678328 15688 leveldb.cpp:183] compacted db in 8.580569ms  i0123 18:50:11.678455 15688 leveldb.cpp:198] created db iterator in 38478ns  i0123 18:50:11.678478 15688 leveldb.cpp:204] seeked to beginning of db in 3057ns  i0123 18:50:11.678489 15688 leveldb.cpp:273] iterated through 0 keys in the db in 427ns  i0123 18:50:11.678539 15688 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0123 18:50:11.682271 15705 recover.cpp:449] starting replica recovery  i0123 18:50:11.682634 15705 recover.cpp:475] replica is in empty status  i0123 18:50:11.684389 15708 replica.cpp:641] replica in empty status received a broadcasted recover request  i0123 18:50:11.685132 15708 recover.cpp:195] received a recover response from a replica in empty status  i0123 18:50:11.689842 15708 recover.cpp:566] updating replica status to starting  i0123 18:50:11.702548 15708 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 12.484558ms  i0123 18:50:11.702615 15708 replica.cpp:323] persisted replica status to starting  i0123 18:50:11.703531 15708 recover.cpp:475] replica is in starting status  i0123 18:50:11.705080 15704 replica.cpp:641] replica in starting status received a broadcasted recover request  i0123 18:50:11.712587 15708 recover.cpp:195] received a recover response from a replica in starting status  i0123 18:50:11.722898 15708 recover.cpp:566] updating replica status to voting  i0123 18:50:11.725427 15703 master.cpp:262] master 20150123185011167773433752615688 (localhost.localdomain) started on 127.0.0.1:37526  w0123 18:50:11.725464 15703 master.cpp:266]     master bound to loopback interface! cannot communicate with remote schedulers or slaves. you might want to set 'ip' flag to a routable ip address.    i0123 18:50:11.725502 15703 master.cpp:308] master only allowing authenticated frameworks to register  i0123 18:50:11.725513 15703 master.cpp:313] master only allowing authenticated slaves to register  i0123 18:50:11.725543 15703 credentials.hpp:36] loading credentials for authentication from '/tmp/faulttolerancetestschedulerfailoverframeworkmessagef3jykr/credentials'  i0123 18:50:11.725774 15703 master.cpp:357] authorization enabled  i0123 18:50:11.728428 15707 whitelistwatcher.cpp:65] no whitelist given  i0123 18:50:11.729169 15707 master.cpp:1219] the newly elected leader is master@127.0.0.1:37526 with id 20150123185011167773433752615688  i0123 18:50:11.729200 15707 master.cpp:1232] elected as the leading master!  i0123 18:50:11.729223 15707 master.cpp:1050] recovering from registrar  i0123 18:50:11.729595 15706 registrar.cpp:313] recovering registrar  i0123 18:50:11.730715 15703 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0123 18:50:11.737431 15708 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 14.259597ms  i0123 18:50:11.737511 15708 replica.cpp:323] persisted replica status to voting  i0123 18:50:11.737768 15708 recover.cpp:580] successfully joined the paxos group  i0123 18:50:11.737977 15708 recover.cpp:464] recover process terminated  i0123 18:50:11.739083 15706 log.cpp:660] attempting to start the writer  i0123 18:50:11.741236 15706 replica.cpp:477] replica received implicit promise request with proposal 1  i0123 18:50:11.750435 15706 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 8.813783ms  i0123 18:50:11.750514 15706 replica.cpp:345] persisted promised to 1  i0123 18:50:11.752239 15708 coordinator.cpp:230] coordinator attemping to fill missing position  i0123 18:50:11.754176 15706 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0123 18:50:11.763464 15706 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 8.799822ms  i0123 18:50:11.763535 15706 replica.cpp:679] persisted action at 0  i0123 18:50:11.765697 15709 replica.cpp:511] replica received write request for position 0  i0123 18:50:11.766293 15709 leveldb.cpp:438] reading position from leveldb took 54028ns  i0123 18:50:11.776468 15709 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 9.789169ms  i0123 18:50:11.776561 15709 replica.cpp:679] persisted action at 0  i0123 18:50:11.777515 15709 replica.cpp:658] replica received learned notice for position 0  i0123 18:50:11.785459 15709 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 7.897242ms  i0123 18:50:11.785531 15709 replica.cpp:679] persisted action at 0  i0123 18:50:11.785565 15709 replica.cpp:664] replica learned nop action at position 0  i0123 18:50:11.786633 15709 log.cpp:676] writer started with ending position 0  i0123 18:50:11.788460 15709 leveldb.cpp:438] reading position from leveldb took 266087ns  i0123 18:50:11.801141 15709 registrar.cpp:346] successfully fetched the registry (0b) in 71.491072ms  i0123 18:50:11.801300 15709 registrar.cpp:445] applied 1 operations in 41795ns; attempting to update the 'registry'  i0123 18:50:11.805186 15707 log.cpp:684] attempting to append 136 bytes to the log  i0123 18:50:11.805454 15707 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0123 18:50:11.806677 15703 replica.cpp:511] replica received write request for position 1  i0123 18:50:11.815621 15703 leveldb.cpp:343] persisting action (155 bytes) to leveldb took 8.89177ms  i0123 18:50:11.815692 15703 replica.cpp:679] persisted action at 1  i0123 18:50:11.817358 15704 replica.cpp:658] replica received learned notice for position 1  i0123 18:50:11.825014 15704 leveldb.cpp:343] persisting action (157 bytes) to leveldb took 7.578558ms  i0123 18:50:11.825088 15704 replica.cpp:679] persisted action at 1  i0123 18:50:11.825124 15704 replica.cpp:664] replica learned append action at position 1  i0123 18:50:11.827008 15705 registrar.cpp:490] successfully updated the 'registry' in 25.629952ms  i0123 18:50:11.827143 15705 registrar.cpp:376] successfully recovered registrar  i0123 18:50:11.827517 15705 master.cpp:1077] recovered 0 slaves from the registry (98b) ; allowing 10mins for slaves to reregister  i0123 18:50:11.828515 15704 log.cpp:703] attempting to truncate the log to 1  i0123 18:50:11.829074 15704 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0123 18:50:11.830546 15709 replica.cpp:511] replica received write request for position 2  i0123 18:50:11.837752 15709 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 7.142431ms  i0123 18:50:11.837826 15709 replica.cpp:679] persisted action at 2  i0123 18:50:11.839334 15709 replica.cpp:658] replica received learned notice for position 2  i0123 18:50:11.847069 15709 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 7.116607ms  i0123 18:50:11.847214 15709 leveldb.cpp:401] deleting ~1 keys from leveldb took 74008ns  i0123 18:50:11.847241 15709 replica.cpp:679] persisted action at 2  i0123 18:50:11.847295 15709 replica.cpp:664] replica learned truncate action at position 2  i0123 18:50:11.870337 15710 slave.cpp:173] slave started on 94)@127.0.0.1:37526  w0123 18:50:11.870980 15710 slave.cpp:176]     slave bound to loopback interface! cannot communicate with remote master(s). you might want to set 'ip' flag to a routable ip address.    i0123 18:50:11.871412 15710 credentials.hpp:84] loading credential for authentication from '/tmp/faulttolerancetestschedulerfailoverframeworkmessagetb8rh3/credential'  i0123 18:50:11.871819 15710 slave.cpp:282] slave using credential for: testprincipal  i0123 18:50:11.873178 15710 slave.cpp:300] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0123 18:50:11.873620 15710 slave.cpp:329] slave hostname: localhost.localdomain  i0123 18:50:11.873837 15710 slave.cpp:330] slave checkpoint: false  w0123 18:50:11.874068 15710 slave.cpp:332] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0123 18:50:11.879103 15705 state.cpp:33] recovering state from '/tmp/faulttolerancetestschedulerfailoverframeworkmessagetb8rh3/meta'  w0123 18:50:11.882972 15688 sched.cpp:1246]     scheduler driver bound to loopback interface! cannot communicate with remote master(s). you might want to set 'libprocessip' environment variable to use a routable ip address.    i0123 18:50:11.884106 15709 statusupdatemanager.cpp:197] recovering status update manager  i0123 18:50:11.884703 15710 slave.cpp:3519] finished recovery  i0123 18:50:11.892076 15704 statusupdatemanager.cpp:171] pausing sending status updates  i0123 18:50:11.892590 15710 slave.cpp:613] new master detected at master@127.0.0.1:37526  i0123 18:50:11.892937 15710 slave.cpp:676] authenticating with master master@127.0.0.1:37526  i0123 18:50:11.893165 15710 slave.cpp:681] using default crammd5 authenticatee  i0123 18:50:11.893754 15708 authenticatee.hpp:138] creating new client sasl connection  i0123 18:50:11.894120 15708 master.cpp:4129] authenticating slave(94)@127.0.0.1:37526  i0123 18:50:11.894153 15708 master.cpp:4140] using default crammd5 authenticator  i0123 18:50:11.894628 15708 authenticator.hpp:170] creating new server sasl connection  i0123 18:50:11.894913 15708 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0123 18:50:11.894942 15708 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0123 18:50:11.895043 15708 authenticator.hpp:276] received sasl authentication start  i0123 18:50:11.895095 15708 authenticator.hpp:398] authentication requires more steps  i0123 18:50:11.895165 15708 authenticatee.hpp:275] received sasl authentication step  i0123 18:50:11.895261 15708 authenticator.hpp:304] received sasl authentication step  i0123 18:50:11.895292 15708 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'localhost.localdomain' server fqdn: 'localhost.localdomain' saslauxpropoverride: false saslauxpropauthzid: false   i0123 18:50:11.895305 15708 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0123 18:50:11.895354 15708 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0123 18:50:11.895881 15710 slave.cpp:649] detecting new master  i0123 18:50:11.898449 15708 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'localhost.localdomain' server fqdn: 'localhost.localdomain' saslauxpropoverride: false saslauxpropauthzid: true   i0123 18:50:11.899024 15708 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0123 18:50:11.899106 15708 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0123 18:50:11.899190 15708 authenticator.hpp:390] authentication success  i0123 18:50:11.899569 15706 authenticatee.hpp:315] authentication success  i0123 18:50:11.902299 15706 slave.cpp:747] successfully authenticated with master master@127.0.0.1:37526  i0123 18:50:11.902847 15706 slave.cpp:1075] will retry registration in 19.809649ms if necessary  i0123 18:50:11.903264 15705 master.cpp:3214] queuing up registration request from slave(94)@127.0.0.1:37526 because authentication is still in progress  i0123 18:50:11.903497 15705 master.cpp:4187] successfully authenticated principal 'testprincipal' at slave(94)@127.0.0.1:37526  i0123 18:50:11.903940 15705 master.cpp:3275] registering slave at slave(94)@127.0.0.1:37526 (localhost.localdomain) with id 20150123185011167773433752615688s0  i0123 18:50:11.904398 15705 registrar.cpp:445] applied 1 operations in 63679ns; attempting to update the 'registry'  i0123 18:50:11.917883 15688 sched.cpp:151] version: 0.22.0  i0123 18:50:11.919347 15703 log.cpp:684] attempting to append 315 bytes to the log  i0123 18:50:11.921039 15703 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0123 18:50:11.919992 15706 sched.cpp:248] new master detected at master@127.0.0.1:37526  i0123 18:50:11.921352 15706 sched.cpp:304] authenticating with master master@127.0.0.1:37526  i0123 18:50:11.921408 15706 sched.cpp:311] using default crammd5 authenticatee  i0123 18:50:11.921773 15706 authenticatee.hpp:138] creating new client sasl connection  i0123 18:50:11.922266 15706 master.cpp:4129] authenticating scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.922301 15706 master.cpp:4140] using default crammd5 authenticator  i0123 18:50:11.923928 15703 replica.cpp:511] replica received write request for position 3  i0123 18:50:11.924285 15707 authenticator.hpp:170] creating new server sasl connection  i0123 18:50:11.925091 15707 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0123 18:50:11.925122 15707 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0123 18:50:11.925194 15707 authenticator.hpp:276] received sasl authentication start  i0123 18:50:11.925257 15707 authenticator.hpp:398] authentication requires more steps  i0123 18:50:11.925325 15707 authenticatee.hpp:275] received sasl authentication step  i0123 18:50:11.925442 15707 authenticator.hpp:304] received sasl authentication step  i0123 18:50:11.925473 15707 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'localhost.localdomain' server fqdn: 'localhost.localdomain' saslauxpropoverride: false saslauxpropauthzid: false   i0123 18:50:11.925487 15707 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0123 18:50:11.925532 15707 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0123 18:50:11.925559 15707 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'localhost.localdomain' server fqdn: 'localhost.localdomain' saslauxpropoverride: false saslauxpropauthzid: true   i0123 18:50:11.925571 15707 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0123 18:50:11.925580 15707 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0123 18:50:11.925595 15707 authenticator.hpp:390] authentication success  i0123 18:50:11.925695 15707 authenticatee.hpp:315] authentication success  i0123 18:50:11.925792 15707 master.cpp:4187] successfully authenticated principal 'testprincipal' at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.926127 15707 sched.cpp:392] successfully authenticated with master master@127.0.0.1:37526  i0123 18:50:11.926154 15707 sched.cpp:515] sending registration request to master@127.0.0.1:37526  i0123 18:50:11.926215 15707 sched.cpp:548] will retry registration in 866.81063ms if necessary  i0123 18:50:11.926640 15707 master.cpp:1420] received registration request for framework 'default' at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.926960 15707 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0123 18:50:11.927691 15707 master.cpp:1484] registering framework 201501231850111677734337526156880000 (default) at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.928292 15708 hierarchicalallocatorprocess.hpp:319] added framework 201501231850111677734337526156880000  i0123 18:50:11.928326 15708 hierarchicalallocatorprocess.hpp:839] no resources available to allocate!  i0123 18:50:11.928340 15708 hierarchicalallocatorprocess.hpp:746] performed allocation for 0 slaves in 21080ns  i0123 18:50:11.934458 15707 sched.cpp:442] framework registered with 201501231850111677734337526156880000  i0123 18:50:11.934927 15707 sched.cpp:456] scheduler::registered took 112885ns  i0123 18:50:11.935747 15709 slave.cpp:1075] will retry registration in 19.609252ms if necessary  i0123 18:50:11.935981 15709 master.cpp:3263] ignoring register slave message from slave(94)@127.0.0.1:37526 (localhost.localdomain) as admission is already in progress  i0123 18:50:11.938997 15703 leveldb.cpp:343] persisting action (334 bytes) to leveldb took 10.171709ms  i0123 18:50:11.939049 15703 replica.cpp:679] persisted action at 3  i0123 18:50:11.940630 15709 replica.cpp:658] replica received learned notice for position 3  i0123 18:50:11.945473 15709 leveldb.cpp:343] persisting action (336 bytes) to leveldb took 4.804742ms  i0123 18:50:11.945521 15709 replica.cpp:679] persisted action at 3  i0123 18:50:11.945550 15709 replica.cpp:664] replica learned append action at position 3  i0123 18:50:11.947105 15709 registrar.cpp:490] successfully updated the 'registry' in 42.637056ms  i0123 18:50:11.948020 15703 master.cpp:3329] registered slave 20150123185011167773433752615688s0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0123 18:50:11.948318 15703 hierarchicalallocatorprocess.hpp:453] added slave 20150123185011167773433752615688s0 (localhost.localdomain) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0123 18:50:11.948719 15703 hierarchicalallocatorprocess.hpp:764] performed allocation for slave 20150123185011167773433752615688s0 in 355831ns  i0123 18:50:11.948813 15703 slave.cpp:781] registered with master master@127.0.0.1:37526; given slave id 20150123185011167773433752615688s0  i0123 18:50:11.948969 15703 slave.cpp:2588] received ping from slaveobserver(92)@127.0.0.1:37526  i0123 18:50:11.949324 15703 master.cpp:4071] sending 1 offers to framework 201501231850111677734337526156880000 (default) at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.949571 15706 statusupdatemanager.cpp:178] resuming sending status updates  i0123 18:50:11.950023 15709 log.cpp:703] attempting to truncate the log to 3  i0123 18:50:11.950810 15705 sched.cpp:605] scheduler::resourceoffers took 135580ns  i0123 18:50:11.952793 15708 master.cpp:2677] processing accept call for offers: [ 20150123185011167773433752615688o0 ] on slave 20150123185011167773433752615688s0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) for framework 201501231850111677734337526156880000 (default) at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526  i0123 18:50:11.952852 15708 master.cpp:2513] authorizing framework principal 'testprincipal' to launch task 1 as user 'jenkins'  w0123 18:50:11.954649 15708 master.cpp:2130] executor default for task 1 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0123 18:50:11.954988 15708 master.cpp:2142] executor default for task 1 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0123 18:50:11.955579 15708 master.hpp:782] adding task 1 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 20150123185011167773433752615688s0 (localhost.localdomain)  i0123 18:50:11.956035 15703 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0123 18:50:11.957592 15704 replica.cpp:511] replica received write request for position 4  i0123 18:50:11.958485 15708 master.cpp:2885] launching task 1 of framework 201501231850111677734337526156880000 (default) at scheduler2cecb105ca234048970712b1e4422e11@127.0.0.1:37526 with resources cpus(...",1,train
MESOS-2305,Refactor validators in Master.,"there are several motivation for this. we are in the process of adding dynamic reservations and persistent volumes support in master. to do that, master needs to validate relevant operations from the framework (see offer::operation in mesos.proto). the existing validator style in master is hard to extend, compose and reuse.    another motivation for this is for unit testing (mesos1064). right now, we write integration tests for those validators which is unfortunate.",3,train
MESOS-2306,MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.,"good run:      [ run      ] masterauthorizationtest.frameworkremovedbeforereregistration  using temporary directory '/tmp/masterauthorizationtestframeworkremovedbeforereregistrationzu7oad'  i0122 19:23:06.481690 17483 leveldb.cpp:176] opened db in 21.058723ms  i0122 19:23:06.488590 17483 leveldb.cpp:183] compacted db in 6.6715ms  i0122 19:23:06.488816 17483 leveldb.cpp:198] created db iterator in 30034ns  i0122 19:23:06.489053 17483 leveldb.cpp:204] seeked to beginning of db in 2908ns  i0122 19:23:06.489073 17483 leveldb.cpp:273] iterated through 0 keys in the db in 492ns  i0122 19:23:06.489148 17483 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0122 19:23:06.490272 17504 recover.cpp:449] starting replica recovery  i0122 19:23:06.490900 17504 recover.cpp:475] replica is in empty status  i0122 19:23:06.492422 17504 replica.cpp:641] replica in empty status received a broadcasted recover request  i0122 19:23:06.492694 17504 recover.cpp:195] received a recover response from a replica in empty status  i0122 19:23:06.493185 17504 recover.cpp:566] updating replica status to starting  i0122 19:23:06.514881 17504 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 21.459963ms  i0122 19:23:06.514920 17504 replica.cpp:323] persisted replica status to starting  i0122 19:23:06.515861 17501 master.cpp:262] master 20150122192306168428794628317483 (lucid) started on 127.0.1.1:46283  i0122 19:23:06.515910 17501 master.cpp:308] master only allowing authenticated frameworks to register  i0122 19:23:06.515923 17501 master.cpp:313] master only allowing authenticated slaves to register  i0122 19:23:06.515946 17501 credentials.hpp:36] loading credentials for authentication from '/tmp/masterauthorizationtestframeworkremovedbeforereregistrationzu7oad/credentials'  i0122 19:23:06.516150 17501 master.cpp:357] authorization enabled  i0122 19:23:06.517511 17501 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0122 19:23:06.517607 17501 whitelistwatcher.cpp:65] no whitelist given  i0122 19:23:06.518066 17498 master.cpp:1219] the newly elected leader is master@127.0.1.1:46283 with id 20150122192306168428794628317483  i0122 19:23:06.518095 17498 master.cpp:1232] elected as the leading master!  i0122 19:23:06.518121 17498 master.cpp:1050] recovering from registrar  i0122 19:23:06.518333 17498 registrar.cpp:313] recovering registrar  i0122 19:23:06.523987 17504 recover.cpp:475] replica is in starting status  i0122 19:23:06.525090 17504 replica.cpp:641] replica in starting status received a broadcasted recover request  i0122 19:23:06.525337 17504 recover.cpp:195] received a recover response from a replica in starting status  i0122 19:23:06.525693 17504 recover.cpp:566] updating replica status to voting  i0122 19:23:06.532680 17504 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 6.810884ms  i0122 19:23:06.532714 17504 replica.cpp:323] persisted replica status to voting  i0122 19:23:06.532835 17504 recover.cpp:580] successfully joined the paxos group  i0122 19:23:06.533004 17504 recover.cpp:464] recover process terminated  i0122 19:23:06.533833 17500 log.cpp:660] attempting to start the writer  i0122 19:23:06.535225 17500 replica.cpp:477] replica received implicit promise request with proposal 1  i0122 19:23:06.540340 17500 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 5.086139ms  i0122 19:23:06.540371 17500 replica.cpp:345] persisted promised to 1  i0122 19:23:06.541502 17504 coordinator.cpp:230] coordinator attemping to fill missing position  i0122 19:23:06.543021 17504 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0122 19:23:06.548140 17504 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 5.083443ms  i0122 19:23:06.548171 17504 replica.cpp:679] persisted action at 0  i0122 19:23:06.549746 17500 replica.cpp:511] replica received write request for position 0  i0122 19:23:06.549926 17500 leveldb.cpp:438] reading position from leveldb took 31962ns  i0122 19:23:06.555033 17500 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 5.065823ms  i0122 19:23:06.555064 17500 replica.cpp:679] persisted action at 0  i0122 19:23:06.556094 17504 replica.cpp:658] replica received learned notice for position 0  i0122 19:23:06.558815 17504 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 2.688382ms  i0122 19:23:06.558847 17504 replica.cpp:679] persisted action at 0  i0122 19:23:06.558868 17504 replica.cpp:664] replica learned nop action at position 0  i0122 19:23:06.559917 17500 log.cpp:676] writer started with ending position 0  i0122 19:23:06.560995 17500 leveldb.cpp:438] reading position from leveldb took 27742ns  i0122 19:23:06.563467 17500 registrar.cpp:346] successfully fetched the registry (0b) in 45.095936ms  i0122 19:23:06.563551 17500 registrar.cpp:445] applied 1 operations in 19686ns; attempting to update the 'registry'  i0122 19:23:06.566107 17500 log.cpp:684] attempting to append 118 bytes to the log  i0122 19:23:06.566267 17500 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0122 19:23:06.567126 17500 replica.cpp:511] replica received write request for position 1  i0122 19:23:06.582588 17500 leveldb.cpp:343] persisting action (135 bytes) to leveldb took 15.425511ms  i0122 19:23:06.582631 17500 replica.cpp:679] persisted action at 1  i0122 19:23:06.583425 17500 replica.cpp:658] replica received learned notice for position 1  i0122 19:23:06.589001 17500 leveldb.cpp:343] persisting action (137 bytes) to leveldb took 5.549486ms  i0122 19:23:06.589200 17500 replica.cpp:679] persisted action at 1  i0122 19:23:06.589416 17500 replica.cpp:664] replica learned append action at position 1  i0122 19:23:06.596420 17500 registrar.cpp:490] successfully updated the 'registry' in 32.815104ms  i0122 19:23:06.596551 17500 registrar.cpp:376] successfully recovered registrar  i0122 19:23:06.596923 17500 master.cpp:1077] recovered 0 slaves from the registry (82b) ; allowing 10mins for slaves to reregister  i0122 19:23:06.597007 17500 log.cpp:703] attempting to truncate the log to 1  i0122 19:23:06.597239 17500 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0122 19:23:06.598464 17501 replica.cpp:511] replica received write request for position 2  i0122 19:23:06.604038 17501 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 5.536264ms  i0122 19:23:06.604084 17501 replica.cpp:679] persisted action at 2  i0122 19:23:06.608747 17503 replica.cpp:658] replica received learned notice for position 2  i0122 19:23:06.614094 17503 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 5.315347ms  i0122 19:23:06.614171 17503 leveldb.cpp:401] deleting ~1 keys from leveldb took 33021ns  i0122 19:23:06.614188 17503 replica.cpp:679] persisted action at 2  i0122 19:23:06.614208 17503 replica.cpp:664] replica learned truncate action at position 2  i0122 19:23:06.628820 17483 sched.cpp:151] version: 0.22.0  i0122 19:23:06.629879 17505 sched.cpp:248] new master detected at master@127.0.1.1:46283  i0122 19:23:06.629973 17505 sched.cpp:304] authenticating with master master@127.0.1.1:46283  i0122 19:23:06.629995 17505 sched.cpp:311] using default crammd5 authenticatee  i0122 19:23:06.630314 17505 authenticatee.hpp:138] creating new client sasl connection  i0122 19:23:06.630722 17505 master.cpp:4129] authenticating scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.630750 17505 master.cpp:4140] using default crammd5 authenticator  i0122 19:23:06.631115 17505 authenticator.hpp:170] creating new server sasl connection  i0122 19:23:06.631423 17505 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0122 19:23:06.631459 17505 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0122 19:23:06.631563 17505 authenticator.hpp:276] received sasl authentication start  i0122 19:23:06.631605 17505 authenticator.hpp:398] authentication requires more steps  i0122 19:23:06.631671 17505 authenticatee.hpp:275] received sasl authentication step  i0122 19:23:06.631748 17505 authenticator.hpp:304] received sasl authentication step  i0122 19:23:06.631774 17505 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'lucid' server fqdn: 'lucid' saslauxpropoverride: false saslauxpropauthzid: false   i0122 19:23:06.631784 17505 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0122 19:23:06.631822 17505 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0122 19:23:06.631856 17505 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'lucid' server fqdn: 'lucid' saslauxpropoverride: false saslauxpropauthzid: true   i0122 19:23:06.631870 17505 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0122 19:23:06.631877 17505 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0122 19:23:06.631892 17505 authenticator.hpp:390] authentication success  i0122 19:23:06.631988 17505 authenticatee.hpp:315] authentication success  i0122 19:23:06.632066 17505 master.cpp:4187] successfully authenticated principal 'testprincipal' at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.632359 17505 sched.cpp:392] successfully authenticated with master master@127.0.1.1:46283  i0122 19:23:06.632382 17505 sched.cpp:515] sending registration request to master@127.0.1.1:46283  i0122 19:23:06.632432 17505 sched.cpp:548] will retry registration in 598.155756ms if necessary  i0122 19:23:06.632575 17505 master.cpp:1420] received registration request for framework 'default' at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.632639 17505 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0122 19:23:06.632912 17505 master.cpp:1484] registering framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.633421 17505 hierarchicalallocatorprocess.hpp:319] added framework 201501221923061684287946283174830000  i0122 19:23:06.633448 17505 hierarchicalallocatorprocess.hpp:839] no resources available to allocate!  i0122 19:23:06.633458 17505 hierarchicalallocatorprocess.hpp:746] performed allocation for 0 slaves in 17704ns  i0122 19:23:06.633919 17505 sched.cpp:442] framework registered with 201501221923061684287946283174830000  i0122 19:23:06.633980 17505 sched.cpp:456] scheduler::registered took 37063ns  i0122 19:23:06.636554 17500 sched.cpp:242] scheduler::disconnected took 14843ns  i0122 19:23:06.636579 17500 sched.cpp:248] new master detected at master@127.0.1.1:46283  i0122 19:23:06.636625 17500 sched.cpp:304] authenticating with master master@127.0.1.1:46283  i0122 19:23:06.636641 17500 sched.cpp:311] using default crammd5 authenticatee  i0122 19:23:06.636914 17500 authenticatee.hpp:138] creating new client sasl connection  i0122 19:23:06.637313 17500 master.cpp:4129] authenticating scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.637341 17500 master.cpp:4140] using default crammd5 authenticator  i0122 19:23:06.637675 17500 authenticator.hpp:170] creating new server sasl connection  i0122 19:23:06.638056 17501 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0122 19:23:06.638083 17501 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0122 19:23:06.638182 17501 authenticator.hpp:276] received sasl authentication start  i0122 19:23:06.638221 17501 authenticator.hpp:398] authentication requires more steps  i0122 19:23:06.638286 17501 authenticatee.hpp:275] received sasl authentication step  i0122 19:23:06.638360 17501 authenticator.hpp:304] received sasl authentication step  i0122 19:23:06.638383 17501 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'lucid' server fqdn: 'lucid' saslauxpropoverride: false saslauxpropauthzid: false   i0122 19:23:06.638393 17501 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0122 19:23:06.638422 17501 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0122 19:23:06.638447 17501 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'lucid' server fqdn: 'lucid' saslauxpropoverride: false saslauxpropauthzid: true   i0122 19:23:06.638458 17501 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0122 19:23:06.638464 17501 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0122 19:23:06.638478 17501 authenticator.hpp:390] authentication success  i0122 19:23:06.638566 17501 authenticatee.hpp:315] authentication success  i0122 19:23:06.638643 17501 master.cpp:4187] successfully authenticated principal 'testprincipal' at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.638919 17501 sched.cpp:392] successfully authenticated with master master@127.0.1.1:46283  i0122 19:23:06.638942 17501 sched.cpp:515] sending registration request to master@127.0.1.1:46283  i0122 19:23:06.638994 17501 sched.cpp:548] will retry registration in 489.304713ms if necessary  i0122 19:23:06.639169 17501 master.cpp:1557] received reregistration request from framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.639242 17501 master.cpp:1298] authorizing framework principal 'testprincipal' to receive offers for role ''  i0122 19:23:06.639839 17483 sched.cpp:1471] asked to stop the driver  i0122 19:23:06.640379 17499 sched.cpp:808] stopping framework '201501221923061684287946283174830000'  i0122 19:23:06.640697 17499 master.cpp:745] framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283 disconnected  i0122 19:23:06.640723 17499 master.cpp:1789] disconnecting framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.640744 17499 master.cpp:1805] deactivating framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.640806 17499 master.cpp:767] giving framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283 0ns to failover  i0122 19:23:06.640951 17499 hierarchicalallocatorprocess.hpp:398] deactivated framework 201501221923061684287946283174830000  i0122 19:23:06.646342 17498 master.cpp:1604] dropping reregistration request of framework 201501221923061684287946283174830000 (default)  at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated  i0122 19:23:06.648844 17498 master.cpp:3941] framework failover timeout, removing framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.648871 17498 master.cpp:4499] removing framework 201501221923061684287946283174830000 (default) at scheduler4156eae68d7f423a920a02b11b7bd1ba@127.0.1.1:46283  i0122 19:23:06.649624 17498 hierarchicalallocatorprocess.hpp:352] removed framework 201501221923061684287946283174830000  i0122 19:23:06.656532 17483 master.cpp:654] master terminating  [       ok ] masterauthorizationtest.frameworkremovedbeforereregistration (216 ms)      bad run:      [ run      ] masterauthorizationtest.frameworkremovedbeforereregistration  using temporary directory '/tmp/masterauthorizationtestframeworkremovedbeforereregistrationjdm2sm'  i0126 19:19:55.517570  2381 leveldb.cpp:176] opened db in 34.341401ms  i0126 19:19:55.529630  2381 leveldb.cpp:183] compacted db in 11.824435ms  i0126 19:19:55.529878  2381 leveldb.cpp:198] created db iterator in 26176ns  i0126 19:19:55.530200  2381 leveldb.cpp:204] seeked to beginning of db in 3457ns  i0126 19:19:55.530455  2381 leveldb.cpp:273] iterated through 0 keys in the db in 902ns  i0126 19:19:55.530658  2381 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0126 19:19:55.531492  2397 recover.cpp:449] starting replica recovery  i0126 19:19:55.531793  2397 recover.cpp:475] replica is in empty status  i0126 19:19:55.533327  2397 replica.cpp:641] replica in empty status received a broadcasted recover request  i0126 19:19:55.533608  2397 recover.cpp:195] received a recover response from a replica in empty status  i0126 19:19:55.534101  2397 recover.cpp:566] updating replica status to starting  i0126 19:19:55.550417  2397 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 16.106821ms  i0126 19:19:55.550472  2397 replica.cpp:323] persisted replica status to starting  i0126 19:19:55.551434  2397 recover.cpp:475] replica is in starting status  i0126 19:19:55.552846  2397 replica.cpp:641] replica in starting status received a broadcasted recover request  i0126 19:19:55.553099  2397 recover.cpp:195] received a recover response from a replica in starting status  i0126 19:19:55.553565  2397 recover.cpp:566] updating replica status to voting  i0126 19:19:55.564590  2397 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 10.719218ms  i0126 19:19:55.564919  2397 replica.cpp:323] persisted replica status to voting  i0126 19:19:55.565982  2397 recover.cpp:580] successfully joined the paxos group  i0126 19:19:55.566231  2397 recover.cpp:464] recover process terminated  i0126 19:19:55.567878  2401 master.cpp:262] master 2015012619195516842879518622381 (lucid) started on 127.0.1.1:51862  i0126 19:19:55.567927  2401 master.cpp:308] master only allowing authenticated frameworks to register  i0126 19:19:55.567950  2401 master.cpp:313] master only allowing authenticated slaves to register  i0126 19:19:55.567978  2401 credentials.hpp:36] loading credentials for authentication from '/tmp/masterauthorizationtestframeworkremovedbeforereregistrationjdm2sm/credentials'  i0126 19:19:55.568220  2401 master.cpp:357] authorization enabled  i0126 19:19:55.569890  2401 hierarchicalallocatorprocess.hpp:285] initialized hierarchical allocator process  i0126 19:19:55.569999  2401 whitelistwatcher.cpp:65] no whitelist given  i0126 19:19:55.570694  2401 master.cpp:1219] the newly elected leader is master@127.0.1.1:51862 with id 201501261919551684287951862 2381  i0126 19:19:55.570721  2401 master.cpp:1232] elected as the leading master!  i0126 19:19:55.570742  2401 master.cpp:1050] recovering from registrar  i0126 19:19:55.570977  2401 registrar.cpp:313] recovering registrar  i0126 19:19:55.571959  2401 log.cpp:660] attempting to start the writer  i0126 19:19:55.573441  2401 replica.cpp:477] replica received implicit promise request with proposal 1  i0126 19:19:55.590724  2401 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 17.243964ms  i0126 19:19:55.590785  2401 replica.cpp:345] persisted promised to 1  i0126 19:19:55.592140  2396 coordinator.cpp:230] coordinator attemping to fill missing position  i0126 19:19:55.593834  2396 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0126 19:19:55.603837  2396 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 9.955824ms  i0126 19:19:55.603902  2396 replica.cpp:679] persisted action at 0  i0126 19:19:55.606082  2401 replica.cpp:511] replica received write request for position 0  i0126 19:19:55.606331  2401 leveldb.cpp:438] reading position from leveldb took 44524ns  i0126 19:19:55.612546  2401 leveldb.cpp:343] persisting action (14 bytes) to level...",1,train
MESOS-2309,Mesos rejects ExecutorInfo as incompatible when there is no functional difference,"in aurora1076 it was discovered that if an executorinfo was changed such that a previously unset optional field with a default value was changed to have the field set with the default value, it would be rejected as not compatible.    for example if we have an executorinfo with a commandinfo with the shell attribute unset and then we change the commandinfo to set the shell attribute to true mesos will reject the task with:      i0130 21:50:05.373389 50869 master.cpp:3441] sending status update task_lost (uuid: 82ef615c0d59442795d580cf0e52b3fc) for task systemgcc89c0c05200c462e958aecd7b9a76831 of framework 2011032822470000000019 0000 'task has invalid executorinfo (existing executorinfo with same executorid is not compatible).      this is not intuitive because the default value of the shell attribute is true. there should be no difference between not setting an optional field with a default value and setting that field to the default value.",3,train
MESOS-2314,remove unnecessary constants,"in src/slave/paths.cpp a number of string constants are defined to describe the formats of various paths. however, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.    in the cases where one constant depends on another (see the executorinfopath, executorpath, frameworkpath, slavepath, rootpath chain, for example) the function calls can just be chained together.    this will have the added benefit of removing some statically constructed string constants, which are dangerous.",2,train
MESOS-2315,Deprecate / Remove CommandInfo::ContainerInfo,iiuc this has been deprecated and all current code (except examples/dockernoexecutor_framework.cpp) uses the top level containerinfo?,2,train
MESOS-2317,Remove deprecated checkpoint=false code,"cody's plan from mesos444 was:  1) make it so the flag can't be changed at the command line  2) remove the checkpoint variable entirely from slave/flags.hpp. this is a fairly involved change since a number of unit tests depend on manually setting the flag, as well as the default being noncheckpointing.  3) remove logic around checkpointing in the slave, remove logic inside the master.  4) drop the flag from the slaveinfo struct (will require a deprecation cycle).  ",3,train
MESOS-2319,Unable to set --work_dir to a non /tmp device,"when starting mesosslave with workdir set to a directory which is not the same device as /tmp results in mesosslave throwing a core dump:    mesos # glogv=1 sbin/mesosslave master=zk:/10.171.59.83:2181/mesos workdir=/var/lib/mesos/  warning: logging before initgooglelogging() is written to stderr  i0204 18:24:49.274619 22922 process.cpp:958] libprocess is initialized on 10.169.146.67:5051 for 8 cpus  i0204 18:24:49.274978 22922 logging.cpp:177] logging to stderr  i0204 18:24:49.275111 22922 main.cpp:152] build: 20150203 22:59:30 by   i0204 18:24:49.275233 22922 main.cpp:154] version: 0.22.0  i0204 18:24:49.275485 22922 containerizer.cpp:103] using isolation: posix/cpu,posix/mem  20150204 18:24:49,275:22922(0x7ffdd4d5c700):zooinfo@logenv@712: client environment:zookeeper.version=zookeeper c client 3.4.5  20150204 18:24:49,275:22922(0x7ffdd4d5c700):zooinfo@logenv@716: client environment:host.name=ip1016914667.ec2.internal  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@723: client environment:os.name=linux  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@724: client environment:os.arch=3.18.2  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@725: client environment:os.version=#2 smp tue jan 27 23:34:36 utc 2015  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@733: client environment:user.name=core  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@741: client environment:user.home=/root  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@logenv@753: client environment:user.dir=/opt/mesosphere/dcos/0.0.10.1.20150203225612/mesos  20150204 18:24:49,276:22922(0x7ffdd4d5c700):zooinfo@zookeeperinit@786: initiating client connection, host=10.171.59.83:2181 sessiontimeout=10000 watcher=0x7ffdd97bccf0 sessionid=0 sessionpasswd=/ context=0x7ffdc8000ba0 flags=0  i0204 18:24:49.276793 22922 main.cpp:180] starting mesos slave  20150204 18:24:49,307:22922(0x7ffdd151f700):zooinfo@checkevents@1703: initiated connection to server [10.171.59.83:2181]  i0204 18:24:49.307548 22922 slave.cpp:173] slave started on 1)@10.169.146.67:5051  i0204 18:24:49.307955 22922 slave.cpp:300] slave resources: cpus():1; mem():2728; disk():24736; ports():[3100032000]  i0204 18:24:49.308404 22922 slave.cpp:329] slave hostname: ip1016914667.ec2.internal  i0204 18:24:49.308459 22922 slave.cpp:330] slave checkpoint: true  i0204 18:24:49.310431 22924 state.cpp:33] recovering state from '/var/lib/mesos/meta'  i0204 18:24:49.310583 22924 state.cpp:668] failed to find resources file '/var/lib/mesos/meta/resources/resources.info'  i0204 18:24:49.310670 22924 state.cpp:74] failed to find the latest slave from '/var/lib/mesos/meta'  i0204 18:24:49.310803 22924 statusupdatemanager.cpp:197] recovering status update manager  i0204 18:24:49.310916 22924 containerizer.cpp:300] recovering containerizer  i0204 18:24:49.311110 22924 slave.cpp:3527] finished recovery  f0204 18:24:49.311312 22924 slave.cpp:3537] checksome(state::checkpoint(path, bootid.get())): failed to rename '/tmp/pshlqv' to '/var/lib/mesos/meta/bootid': invalid crossdevice link   20150204 18:24:49,310:22922(0x7ffdd151f700):zooinfo@checkevents@1750: session establishment complete on server [10.171.59.83:2181], sessionid=0x14b51bc8506039a, negotiated timeout=10000   check failure stack trace:       @     0x7ffdd9a6596d  google::logmessage::fail()  i0204 18:24:49.313356 22930 group.cpp:313] group process (group(1)@10.169.146.67:5051) connected to zookeeper      @     0x7ffdd9a677ad  google::logmessage::sendtolog()  i0204 18:24:49.313786 22930 group.cpp:790] syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)  i0204 18:24:49.314487 22930 group.cpp:385] trying to create path '/mesos' in zookeeper  i0204 18:24:49.323668 22930 group.cpp:717] found nonsequence node 'logreplicas' at '/mesos' in zookeeper  i0204 18:24:49.323806 22930 detector.cpp:138] detected a new leader: (id='1')  i0204 18:24:49.323958 22930 group.cpp:659] trying to get '/mesos/info0000000001' in zookeeper  i0204 18:24:49.324595 22930 detector.cpp:433] a new leading master (upid=master@10.171.59.83:5050) is detected      @     0x7ffdd9a6555c  google::logmessage::flush()      @     0x7ffdd9a680a9  google::logmessagefatal::logmessagefatal()      @     0x7ffdd94b7179  checkfatal::checkfatal()      @     0x7ffdd96718e2  mesos::internal::slave::slave::recover()      @     0x7ffdd9a1524a  process::processmanager::resume()      @     0x7ffdd9a1550c  process::schedule()      @     0x7ffdd83832ad  (unknown)      @     0x7ffdd80b834d  (unknown)  aborted (core dumped)      removing the  workdir option results in the slave starting successfully.",2,train
MESOS-2324,MasterAllocatorTest/0.OutOfOrderDispatch is flaky,"    [ run      ] masterallocatortest/0.outoforderdispatch  using temporary directory '/tmp/masterallocatortest0outoforderdispatchkjlb9b'  i0206 07:55:44.084333 15065 leveldb.cpp:175] opened db in 25.006293ms  i0206 07:55:44.089635 15065 leveldb.cpp:182] compacted db in 5.256332ms  i0206 07:55:44.089695 15065 leveldb.cpp:197] created db iterator in 23534ns  i0206 07:55:44.089710 15065 leveldb.cpp:203] seeked to beginning of db in 2175ns  i0206 07:55:44.089720 15065 leveldb.cpp:272] iterated through 0 keys in the db in 417ns  i0206 07:55:44.089781 15065 replica.cpp:743] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0206 07:55:44.093750 15086 recover.cpp:448] starting replica recovery  i0206 07:55:44.094044 15086 recover.cpp:474] replica is in empty status  i0206 07:55:44.095473 15086 replica.cpp:640] replica in empty status received a broadcasted recover request  i0206 07:55:44.095724 15086 recover.cpp:194] received a recover response from a replica in empty status  i0206 07:55:44.096097 15086 recover.cpp:565] updating replica status to starting  i0206 07:55:44.106575 15086 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 10.289939ms  i0206 07:55:44.106613 15086 replica.cpp:322] persisted replica status to starting  i0206 07:55:44.108144 15086 recover.cpp:474] replica is in starting status  i0206 07:55:44.109122 15086 replica.cpp:640] replica in starting status received a broadcasted recover request  i0206 07:55:44.110879 15091 recover.cpp:194] received a recover response from a replica in starting status  i0206 07:55:44.117267 15087 recover.cpp:565] updating replica status to voting  i0206 07:55:44.124771 15087 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 2.66794ms  i0206 07:55:44.124814 15087 replica.cpp:322] persisted replica status to voting  i0206 07:55:44.124948 15087 recover.cpp:579] successfully joined the paxos group  i0206 07:55:44.125095 15087 recover.cpp:463] recover process terminated  i0206 07:55:44.126204 15087 master.cpp:344] master 20150206075544168428793889515065 (utopic) started on 127.0.1.1:38895  i0206 07:55:44.126268 15087 master.cpp:390] master only allowing authenticated frameworks to register  i0206 07:55:44.126281 15087 master.cpp:395] master only allowing authenticated slaves to register  i0206 07:55:44.126307 15087 credentials.hpp:35] loading credentials for authentication from '/tmp/masterallocatortest0outoforderdispatchkjlb9b/credentials'  i0206 07:55:44.126683 15087 master.cpp:439] authorization enabled  i0206 07:55:44.129329 15086 master.cpp:1350] the newly elected leader is master@127.0.1.1:38895 with id 20150206075544168428793889515065  i0206 07:55:44.129361 15086 master.cpp:1363] elected as the leading master!  i0206 07:55:44.129389 15086 master.cpp:1181] recovering from registrar  i0206 07:55:44.129653 15088 registrar.cpp:312] recovering registrar  i0206 07:55:44.130859 15088 log.cpp:659] attempting to start the writer  i0206 07:55:44.132334 15088 replica.cpp:476] replica received implicit promise request with proposal 1  i0206 07:55:44.135187 15088 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 2.825465ms  i0206 07:55:44.135390 15088 replica.cpp:344] persisted promised to 1  i0206 07:55:44.138062 15091 coordinator.cpp:229] coordinator attemping to fill missing position  i0206 07:55:44.139576 15091 replica.cpp:377] replica received explicit promise request for position 0 with proposal 2  i0206 07:55:44.142156 15091 leveldb.cpp:342] persisting action (8 bytes) to leveldb took 2.545543ms  i0206 07:55:44.142189 15091 replica.cpp:678] persisted action at 0  i0206 07:55:44.143414 15091 replica.cpp:510] replica received write request for position 0  i0206 07:55:44.143468 15091 leveldb.cpp:437] reading position from leveldb took 28872ns  i0206 07:55:44.145982 15091 leveldb.cpp:342] persisting action (14 bytes) to leveldb took 2.480277ms  i0206 07:55:44.146015 15091 replica.cpp:678] persisted action at 0  i0206 07:55:44.147050 15089 replica.cpp:657] replica received learned notice for position 0  i0206 07:55:44.154364 15089 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 7.281644ms  i0206 07:55:44.154400 15089 replica.cpp:678] persisted action at 0  i0206 07:55:44.154422 15089 replica.cpp:663] replica learned nop action at position 0  i0206 07:55:44.155506 15091 log.cpp:675] writer started with ending position 0  i0206 07:55:44.156746 15091 leveldb.cpp:437] reading position from leveldb took 30248ns  i0206 07:55:44.173681 15091 registrar.cpp:345] successfully fetched the registry (0b) in 43.977984ms  i0206 07:55:44.173821 15091 registrar.cpp:444] applied 1 operations in 30768ns; attempting to update the 'registry'  i0206 07:55:44.176213 15086 log.cpp:683] attempting to append 119 bytes to the log  i0206 07:55:44.176426 15086 coordinator.cpp:339] coordinator attempting to write append action at position 1  i0206 07:55:44.177608 15088 replica.cpp:510] replica received write request for position 1  i0206 07:55:44.180059 15088 leveldb.cpp:342] persisting action (136 bytes) to leveldb took 2.415145ms  i0206 07:55:44.180094 15088 replica.cpp:678] persisted action at 1  i0206 07:55:44.181324 15084 replica.cpp:657] replica received learned notice for position 1  i0206 07:55:44.183831 15084 leveldb.cpp:342] persisting action (138 bytes) to leveldb took 2.473724ms  i0206 07:55:44.183866 15084 replica.cpp:678] persisted action at 1  i0206 07:55:44.183887 15084 replica.cpp:663] replica learned append action at position 1  i0206 07:55:44.185510 15084 registrar.cpp:489] successfully updated the 'registry' in 11.619072ms  i0206 07:55:44.185678 15086 log.cpp:702] attempting to truncate the log to 1  i0206 07:55:44.186111 15086 coordinator.cpp:339] coordinator attempting to write truncate action at position 2  i0206 07:55:44.186944 15086 replica.cpp:510] replica received write request for position 2  i0206 07:55:44.187492 15084 registrar.cpp:375] successfully recovered registrar  i0206 07:55:44.188016 15087 master.cpp:1208] recovered 0 slaves from the registry (83b) ; allowing 10mins for slaves to reregister  i0206 07:55:44.189678 15086 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 2.702559ms  i0206 07:55:44.189713 15086 replica.cpp:678] persisted action at 2  i0206 07:55:44.190620 15086 replica.cpp:657] replica received learned notice for position 2  i0206 07:55:44.193383 15086 leveldb.cpp:342] persisting action (18 bytes) to leveldb took 2.737088ms  i0206 07:55:44.193455 15086 leveldb.cpp:400] deleting 1 keys from leveldb took 37762ns  i0206 07:55:44.193475 15086 replica.cpp:678] persisted action at 2  i0206 07:55:44.193496 15086 replica.cpp:663] replica learned truncate action at position 2  i0206 07:55:44.200028 15065 containerizer.cpp:102] using isolation: posix/cpu,posix/mem  i0206 07:55:44.212924 15088 slave.cpp:172] slave started on 46)@127.0.1.1:38895  i0206 07:55:44.213762 15088 credentials.hpp:83] loading credential for authentication from '/tmp/masterallocatortest0outoforderdispatchrunyvq/credential'  i0206 07:55:44.214251 15088 slave.cpp:281] slave using credential for: testprincipal  i0206 07:55:44.214653 15088 slave.cpp:299] slave resources: cpus():2; mem():1024; disk():24988; ports():[3100032000]  i0206 07:55:44.214918 15088 slave.cpp:328] slave hostname: utopic  i0206 07:55:44.215116 15088 slave.cpp:329] slave checkpoint: false  w0206 07:55:44.215332 15088 slave.cpp:331] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0206 07:55:44.217061 15090 state.cpp:32] recovering state from '/tmp/masterallocatortest0outoforderdispatchrunyvq/meta'  i0206 07:55:44.235409 15088 statusupdatemanager.cpp:196] recovering status update manager  i0206 07:55:44.235601 15088 containerizer.cpp:299] recovering containerizer  i0206 07:55:44.236486 15088 slave.cpp:3526] finished recovery  i0206 07:55:44.237709 15087 statusupdatemanager.cpp:170] pausing sending status updates  i0206 07:55:44.237890 15088 slave.cpp:620] new master detected at master@127.0.1.1:38895  i0206 07:55:44.241575 15088 slave.cpp:683] authenticating with master master@127.0.1.1:38895  i0206 07:55:44.247459 15088 slave.cpp:688] using default crammd5 authenticatee  i0206 07:55:44.248617 15089 authenticatee.hpp:137] creating new client sasl connection  i0206 07:55:44.249099 15089 master.cpp:3788] authenticating slave(46)@127.0.1.1:38895  i0206 07:55:44.249137 15089 master.cpp:3799] using default crammd5 authenticator  i0206 07:55:44.249728 15089 authenticator.hpp:169] creating new server sasl connection  i0206 07:55:44.250285 15089 authenticatee.hpp:228] received sasl authentication mechanisms: crammd5  i0206 07:55:44.250496 15089 authenticatee.hpp:254] attempting to authenticate with mechanism 'crammd5'  i0206 07:55:44.250452 15088 slave.cpp:656] detecting new master  i0206 07:55:44.251063 15091 authenticator.hpp:275] received sasl authentication start  i0206 07:55:44.251124 15091 authenticator.hpp:397] authentication requires more steps  i0206 07:55:44.251256 15089 authenticatee.hpp:274] received sasl authentication step  i0206 07:55:44.251451 15090 authenticator.hpp:303] received sasl authentication step  i0206 07:55:44.251575 15090 authenticator.hpp:389] authentication success  i0206 07:55:44.251687 15090 master.cpp:3846] successfully authenticated principal 'testprincipal' at slave(46)@127.0.1.1:38895  i0206 07:55:44.253306 15089 authenticatee.hpp:314] authentication success  i0206 07:55:44.258015 15089 slave.cpp:754] successfully authenticated with master master@127.0.1.1:38895  i0206 07:55:44.258468 15089 master.cpp:2913] registering slave at slave(46)@127.0.1.1:38895 (utopic) with id 20150206075544168428793889515065s0  i0206 07:55:44.259028 15089 registrar.cpp:444] applied 1 operations in 88902ns; attempting to update the 'registry'  i0206 07:55:44.269492 15065 sched.cpp:149] version: 0.22.0  i0206 07:55:44.270539 15090 sched.cpp:246] new master detected at master@127.0.1.1:38895  i0206 07:55:44.270614 15090 sched.cpp:302] authenticating with master master@127.0.1.1:38895  i0206 07:55:44.270634 15090 sched.cpp:309] using default crammd5 authenticatee  i0206 07:55:44.270900 15090 authenticatee.hpp:137] creating new client sasl connection  i0206 07:55:44.272300 15089 log.cpp:683] attempting to append 285 bytes to the log  i0206 07:55:44.272552 15089 coordinator.cpp:339] coordinator attempting to write append action at position 3  i0206 07:55:44.273609 15086 master.cpp:3788] authenticating schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.273643 15086 master.cpp:3799] using default crammd5 authenticator  i0206 07:55:44.273955 15086 authenticator.hpp:169] creating new server sasl connection  i0206 07:55:44.274617 15090 authenticatee.hpp:228] received sasl authentication mechanisms: crammd5  i0206 07:55:44.274813 15090 authenticatee.hpp:254] attempting to authenticate with mechanism 'crammd5'  i0206 07:55:44.275171 15088 authenticator.hpp:275] received sasl authentication start  i0206 07:55:44.275215 15088 authenticator.hpp:397] authentication requires more steps  i0206 07:55:44.275408 15090 authenticatee.hpp:274] received sasl authentication step  i0206 07:55:44.275696 15084 authenticator.hpp:303] received sasl authentication step  i0206 07:55:44.275774 15084 authenticator.hpp:389] authentication success  i0206 07:55:44.275876 15084 master.cpp:3846] successfully authenticated principal 'testprincipal' at schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.277593 15090 authenticatee.hpp:314] authentication success  i0206 07:55:44.278201 15086 sched.cpp:390] successfully authenticated with master master@127.0.1.1:38895  i0206 07:55:44.278548 15086 master.cpp:1568] received registration request for framework 'framework1' at schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.278642 15086 master.cpp:1429] authorizing framework principal 'testprincipal' to receive offers for role ''  i0206 07:55:44.279157 15086 master.cpp:1632] registering framework 201502060755441684287938895150650000 (framework1) at schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.280081 15086 sched.cpp:440] framework registered with 201502060755441684287938895150650000  i0206 07:55:44.280320 15086 hierarchicalallocatorprocess.hpp:318] added framework 201502060755441684287938895150650000  i0206 07:55:44.281411 15089 replica.cpp:510] replica received write request for position 3  i0206 07:55:44.282289 15085 master.cpp:2901] ignoring register slave message from slave(46)@127.0.1.1:38895 (utopic) as admission is already in progress  i0206 07:55:44.284984 15089 leveldb.cpp:342] persisting action (304 bytes) to leveldb took 3.368213ms  i0206 07:55:44.285020 15089 replica.cpp:678] persisted action at 3  i0206 07:55:44.285893 15089 replica.cpp:657] replica received learned notice for position 3  i0206 07:55:44.288350 15089 leveldb.cpp:342] persisting action (306 bytes) to leveldb took 2.430449ms  i0206 07:55:44.288384 15089 replica.cpp:678] persisted action at 3  i0206 07:55:44.288405 15089 replica.cpp:663] replica learned append action at position 3  i0206 07:55:44.290154 15089 registrar.cpp:489] successfully updated the 'registry' in 31.046912ms  i0206 07:55:44.290307 15085 log.cpp:702] attempting to truncate the log to 3  i0206 07:55:44.290671 15085 coordinator.cpp:339] coordinator attempting to write truncate action at position 4  i0206 07:55:44.291482 15085 replica.cpp:510] replica received write request for position 4  i0206 07:55:44.292559 15087 master.cpp:2970] registered slave 20150206075544168428793889515065s0 at slave(46)@127.0.1.1:38895 (utopic) with cpus():2; mem():1024; disk():24988; ports():[3100032000]  i0206 07:55:44.292940 15087 slave.cpp:788] registered with master master@127.0.1.1:38895; given slave id 20150206075544168428793889515065s0  i0206 07:55:44.293298 15087 hierarchicalallocatorprocess.hpp:450] added slave 20150206075544168428793889515065s0 (utopic) with cpus():2; mem():1024; disk():24988; ports():[3100032000] (and cpus():2; mem():1024; disk():24988; ports():[3100032000] available)  i0206 07:55:44.293684 15087 statusupdatemanager.cpp:177] resuming sending status updates  i0206 07:55:44.294085 15087 master.cpp:3730] sending 1 offers to framework 201502060755441684287938895150650000 (framework1) at schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.299957 15085 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 8.442691ms  i0206 07:55:44.300165 15085 replica.cpp:678] persisted action at 4  i0206 07:55:44.300698 15065 sched.cpp:1468] asked to stop the driver  i0206 07:55:44.301127 15090 sched.cpp:806] stopping framework '201502060755441684287938895150650000'  i0206 07:55:44.301503 15090 master.cpp:1892] asked to unregister framework 201502060755441684287938895150650000  i0206 07:55:44.301535 15090 master.cpp:4158] removing framework 201502060755441684287938895150650000 (framework1) at schedulerd6cac0a1d4614a05b19d5cbdae239eb0@127.0.1.1:38895  i0206 07:55:44.302376 15090 slave.cpp:1592] asked to shut down framework 201502060755441684287938895150650000 by master@127.0.1.1:38895  w0206 07:55:44.302407 15090 slave.cpp:1607] cannot shut down unknown framework 201502060755441684287938895150650000  i0206 07:55:44.302814 15090 hierarchicalallocatorprocess.hpp:397] deactivated framework 201502060755441684287938895150650000  i0206 07:55:44.302947 15090 hierarchicalallocatorprocess.hpp:351] removed framework 201502060755441684287938895150650000  i0206 07:55:44.309281 15086 hierarchicalallocatorprocess.hpp:642] recovered cpus():2; mem():1024; disk():24988; ports():[3100032000] (total allocatable: cpus():2; mem():1024; disk():24988; ports():[3100032000]) on slave 20150206075544168428793889515065s0 from framework 201502060755441684287938895150650000  i0206 07:55:44.310158 15084 replica.cpp:657] replica received learned notice for position 4  i0206 07:55:44.313246 15084 leveldb.cpp:342] persisting action (18 bytes) to leveldb took 3.055049ms  i0206 07:55:44.313328 15084 leveldb.cpp:400] deleting 2 keys from leveldb took 45270ns  i0206 07:55:44.313349 15084 replica.cpp:678] persisted action at 4  i0206 07:55:44.313374 15084 replica.cpp:663] replica learned truncate action at position 4  i0206 07:55:44.329591 15065 sched.cpp:149] version: 0.22.0  i0206 07:55:44.330258 15088 sched.cpp:246] new master detected at master@127.0.1.1:38895  i0206 07:55:44.330346 15088 sched.cpp:302] authenticating with master master@127.0.1.1:38895  i0206 07:55:44.330368 15088 sched.cpp:309] using default crammd5 authenticatee  i0206 07:55:44.330652 15088 authenticatee.hpp:137] creating new client sasl connection  i0206 07:55:44.331403 15088 master.cpp:3788] authenticating scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  i0206 07:55:44.331717 15088 master.cpp:3799] using default crammd5 authenticator  i0206 07:55:44.332293 15088 authenticator.hpp:169] creating new server sasl connection  i0206 07:55:44.332655 15088 authenticatee.hpp:228] received sasl authentication mechanisms: crammd5  i0206 07:55:44.332684 15088 authenticatee.hpp:254] attempting to authenticate with mechanism 'crammd5'  i0206 07:55:44.332792 15088 authenticator.hpp:275] received sasl authentication start  i0206 07:55:44.332835 15088 authenticator.hpp:397] authentication requires more steps  i0206 07:55:44.332903 15088 authenticatee.hpp:274] received sasl authentication step  i0206 07:55:44.332983 15088 authenticator.hpp:303] received sasl authentication step  i0206 07:55:44.333056 15088 authenticator.hpp:389] authentication success  i0206 07:55:44.333153 15088 authenticatee.hpp:314] authentication success  i0206 07:55:44.333297 15091 master.cpp:3846] successfully authenticated principal 'testprincipal' at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  i0206 07:55:44.334326 15087 sched.cpp:390] successfully authenticated with master master@127.0.1.1:38895  i0206 07:55:44.334645 15087 master.cpp:1568] received registration request for framework 'framework2' at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  i0206 07:55:44.334722 15087 master.cpp:1429] authorizing framework principal 'testprincipal' to receive offers for role ''  i0206 07:55:44.335153 15087 master.cpp:1632] registering framework 201502060755441684287938895150650001 (framework2) at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  i0206 07:55:44.336019 15087 sched.cpp:440] framework registered with 201502060755441684287938895150650001  i0206 07:55:44.336156 15087 hierarchicalallocatorprocess.hpp:318] added framework 201502060755441684287938895150650001  i0206 07:55:44.336796 15087 master.cpp:3730] sending 1 offers to framework 201502060755441684287938895150650001 (framework2) at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  i0206 07:55:44.337725 15065 sched.cpp:1468] asked to stop the driver  i0206 07:55:44.338002 15086 sched.cpp:806] stopping framework '201502060755441684287938895150650001'  i0206 07:55:44.338297 15090 master.cpp:1892] asked to unregister framework 201502060755441684287938895150650001  i0206 07:55:44.338353 15090 master.cpp:4158] removing framework 201502060755441684287938895150650001 (framework2) at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec@127.0.1.1:38895  ../../src/tests/masterallocatortests.cpp:300: failure  mock function called more times than expected  taking default action specified at:  ../../src/tests/mesos.hpp:713:      function call: deactivateframework(@0x7fdb74008d70 201502060755441684287938895150650001)           expected: to be called once             actual: called twice   over...",1,train
MESOS-2332,Report per-container metrics for network bandwidth throttling,"export metrics from the network isolation to identify scope and duration of container throttling.      packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.      $ tc s d qdisc show dev mesos19223  qdisc pfifofast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1   sent 158213287452 bytes 1030876393 pkt (dropped 0, overlimits 0 requeues 0)   backlog 0b 0p requeues 0  qdisc ingress ffff: parent ffff:fff1    sent 119381747824 bytes 1144549901 pkt (dropped 2044879, overlimits 0 requeues 0)   backlog 0b 0p requeues 0      note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.      add to the portmapping isolator usage() and the container statistics protobuf. carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack.",5,train
MESOS-2335,Mesos Lifecycle Modules,"a new kind of module that receives callbacks at significant life cycle events of its host libprocess process. typically the latter is a mesos slave or master and the life time of the libprocess process coincides with the underlying os process.     motivation and use cases    we want to add customized and experimental capabilities that concern the life time of mesos components without protruding into mesos source code and without creating new build process dependencies for everybody.     example use cases:  1. a slave or master life cycle module that gathers failover incidents and reports summaries thereof to a remote data sink.  2. a slave module that observes host computer metrics and correlates these with task activity. this can be used to find resources leaks and to prevent, respectively guide, oversubscription.  3. upgrades and provisioning that require shutdown and restart.    specifics    the specific life cycle events that we want to get notified about and want to be able to act upon are:     process is spawning/initializing    process is terminating/finalizing    in all these cases, a reference to the process is passed as a parameter, giving the module access for inspection and reaction.     module classification    unlike other named modules, a life cycle module does not directly replace or provide essential mesos functionality (such as an isolator module does). unlike a decorator module it does not directly add or inject data into mesos core either.",1,train
MESOS-2337,__init__.py not getting installed in $PREFIX/lib/pythonX.Y/site-packages/mesos,"when doing a make install, the src/python/native/src/mesos/init.py file is not getting installed in $prefix/lib/pythonx.y/sitepackages/mesos/.      this makes it impossible to do the following import when pythonpath is set to the sitepackages directory.      import mesos.interface.mesospb2      the directories $prefix/lib/pythonx.y/sitepackages/mesos/interface, native do have their corresponding init_.py files.    reproducing the bug:    ../configure prefix=$home/testinstall && make install  ",2,train
MESOS-2340,Add ability to decode JSON serialized MasterInfo from ZK,"currently to discover the master a client needs the zk node location and access to the masterinfo protobuf so it can deserialize the binary blob in the node.    i think it would be nice to publish json (like twitter's serversets) so clients are not tied to protobuf to do service discovery.    this ticket is an intermediate (compatibility) step: we add in 0.23 the ability for the detector to ""understand"" json alongside protobuf serialized format; this makes it compatible with both earlier versions, as well a future one (most likely, 0.24) that will write the masterinfo information in json format.",5,train
MESOS-2347,Add ability for schedulers to explicitly acknowledge status updates on the driver.,"in order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. this enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).    without this, an implicit reconciliation can overload a scheduler (hence the motivation for mesos 2308).",8,train
MESOS-2349,Provide a way to execute an arbitrary process in a MesosContainerizer container context,"include a separate binary that when provided with a containerid, path to an executable, and optional arguments will find the container context, enter it, and exec the executable.    e.g.,    mesoscontainerexec containerid=abc123 [] /path/to/executable [arg1 ...]      this need only support (initially) containers created with the mesoscontainerizer and will support all isolators shipped with mesos, i.e., it should find and enter the cgroups and namespaces for the running executor of the specified container.",5,train
MESOS-2350,Add support for MesosContainerizerLaunch to chroot to a specified path,"in preparation for the mesoscontainerizer to support a filesystem isolator the mesoscontainerizerlauncher must support chrooting. optionally, it should also configure the chroot environment by (re )mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional.",5,train
MESOS-2353,Improve performance of the state.json endpoint for large clusters.,"the master's state.json endpoint consistently takes a long time to compute the json result, for large clusters:      $ time curl s o /dev/null localhost:5050/master/state.json  mon jan 26 22:38:50 utc 2015    real 0m13.174s  user 0m0.003s  sys 0m0.022s      this can cause the master to get backlogged if there are many state.json requests in flight.    looking at perf data, it seems most of the time is spent doing memory allocation / de allocation. this ticket will try to capture any low hanging fruit to speed this up. possibly we can leverage moves if they are not already being used by the compiler.",5,train
MESOS-2366,MasterSlaveReconciliationTest.ReconcileLostTask is flaky,"https:/builds.apache.org/job/mesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebui/2746/changes      [ run      ] masterslavereconciliationtest.reconcilelosttask  using temporary directory '/tmp/masterslavereconciliationtestreconcilelosttaskrgb8ff'  i0218 01:53:26.881561 13918 leveldb.cpp:175] opened db in 2.891605ms  i0218 01:53:26.882547 13918 leveldb.cpp:182] compacted db in 953447ns  i0218 01:53:26.882596 13918 leveldb.cpp:197] created db iterator in 20629ns  i0218 01:53:26.882616 13918 leveldb.cpp:203] seeked to beginning of db in 2370ns  i0218 01:53:26.882627 13918 leveldb.cpp:272] iterated through 0 keys in the db in 348ns  i0218 01:53:26.882664 13918 replica.cpp:743] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0218 01:53:26.883124 13947 recover.cpp:448] starting replica recovery  i0218 01:53:26.883625 13941 recover.cpp:474] replica is in 4 status  i0218 01:53:26.884744 13945 replica.cpp:640] replica in 4 status received a broadcasted recover request  i0218 01:53:26.885118 13939 recover.cpp:194] received a recover response from a replica in 4 status  i0218 01:53:26.885565 13933 recover.cpp:565] updating replica status to 3  i0218 01:53:26.886548 13932 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 733223ns  i0218 01:53:26.886574 13932 replica.cpp:322] persisted replica status to 3  i0218 01:53:26.886714 13943 master.cpp:347] master 2015021801532631426977955726813918 (pomona.apache.org) started on 67.195.81.187:57268  i0218 01:53:26.886760 13943 master.cpp:393] master only allowing authenticated frameworks to register  i0218 01:53:26.886772 13943 master.cpp:398] master only allowing authenticated slaves to register  i0218 01:53:26.886798 13943 credentials.hpp:36] loading credentials for authentication from '/tmp/masterslavereconciliationtestreconcilelosttaskrgb8ff/credentials'  i0218 01:53:26.886826 13934 recover.cpp:474] replica is in 3 status  i0218 01:53:26.887151 13943 master.cpp:440] authorization enabled  i0218 01:53:26.887866 13944 replica.cpp:640] replica in 3 status received a broadcasted recover request  i0218 01:53:26.887969 13942 whitelistwatcher.cpp:78] no whitelist given  i0218 01:53:26.888021 13940 hierarchical.hpp:286] initialized hierarchical allocator process  i0218 01:53:26.888178 13934 recover.cpp:194] received a recover response from a replica in 3 status  i0218 01:53:26.889114 13943 master.cpp:1354] the newly elected leader is master@67.195.81.187:57268 with id 2015021801532631426977955726813918  i0218 01:53:27.064930 13948 process.cpp:2117] dropped / lost event for pid: hierarchicalallocator(183)@67.195.81.187:57268  i0218 01:53:27.911870 13943 master.cpp:1367] elected as the leading master!  i0218 01:53:27.911911 13943 master.cpp:1185] recovering from registrar  i0218 01:53:27.912106 13948 process.cpp:2117] dropped / lost event for pid: scheduler93f780065b69498bb4e387cdf8062263@67.195.81.187:57268  i0218 01:53:27.912255 13932 registrar.cpp:312] recovering registrar  i0218 01:53:27.912307 13948 process.cpp:2117] dropped / lost event for pid: hierarchicalallocator(179)@67.195.81.187:57268  i0218 01:53:27.912626 13940 hierarchical.hpp:831] no resources available to allocate!  i0218 01:53:27.912658 13940 hierarchical.hpp:738] performed allocation for 0 slaves in 60316ns  i0218 01:53:27.912838 13947 recover.cpp:565] updating replica status to 1  i0218 01:53:27.913966 13947 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 921045ns  i0218 01:53:27.913998 13947 replica.cpp:322] persisted replica status to 1  i0218 01:53:27.914106 13932 recover.cpp:579] successfully joined the paxos group  i0218 01:53:27.914378 13932 recover.cpp:463] recover process terminated  i0218 01:53:27.914916 13939 log.cpp:659] attempting to start the writer  i0218 01:53:27.916374 13937 replica.cpp:476] replica received implicit promise request with proposal 1  i0218 01:53:27.916941 13937 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 534122ns  i0218 01:53:27.916967 13937 replica.cpp:344] persisted promised to 1  i0218 01:53:27.917795 13936 coordinator.cpp:229] coordinator attemping to fill missing position  i0218 01:53:27.919147 13941 replica.cpp:377] replica received explicit promise request for position 0 with proposal 2  i0218 01:53:27.919492 13941 leveldb.cpp:342] persisting action (8 bytes) to leveldb took 306270ns  i0218 01:53:27.919517 13941 replica.cpp:678] persisted action at 0  i0218 01:53:27.920755 13934 replica.cpp:510] replica received write request for position 0  i0218 01:53:27.920819 13934 leveldb.cpp:437] reading position from leveldb took 33747ns  i0218 01:53:27.921195 13934 leveldb.cpp:342] persisting action (14 bytes) to leveldb took 340479ns  i0218 01:53:27.921221 13934 replica.cpp:678] persisted action at 0  i0218 01:53:27.921916 13932 replica.cpp:657] replica received learned notice for position 0  i0218 01:53:27.922339 13932 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 392653ns  i0218 01:53:27.922365 13932 replica.cpp:678] persisted action at 0  i0218 01:53:27.922386 13932 replica.cpp:663] replica learned 1 action at position 0  i0218 01:53:27.923009 13945 log.cpp:675] writer started with ending position 0  i0218 01:53:27.924167 13937 leveldb.cpp:437] reading position from leveldb took 29219ns  i0218 01:53:27.927683 13932 registrar.cpp:345] successfully fetched the registry (0b) in 15.376128ms  i0218 01:53:27.927789 13932 registrar.cpp:444] applied 1 operations in 23004ns; attempting to update the 'registry'  i0218 01:53:27.929957 13947 log.cpp:683] attempting to append 139 bytes to the log  i0218 01:53:27.930058 13936 coordinator.cpp:339] coordinator attempting to write 2 action at position 1  i0218 01:53:27.930637 13934 replica.cpp:510] replica received write request for position 1  i0218 01:53:27.930954 13934 leveldb.cpp:342] persisting action (158 bytes) to leveldb took 286664ns  i0218 01:53:27.930975 13934 replica.cpp:678] persisted action at 1  i0218 01:53:27.931521 13942 replica.cpp:657] replica received learned notice for position 1  i0218 01:53:27.931813 13942 leveldb.cpp:342] persisting action (160 bytes) to leveldb took 267316ns  i0218 01:53:27.931833 13942 replica.cpp:678] persisted action at 1  i0218 01:53:27.931849 13942 replica.cpp:663] replica learned 2 action at position 1  i0218 01:53:27.932617 13935 registrar.cpp:489] successfully updated the 'registry' in 4.722944ms  i0218 01:53:27.932726 13935 registrar.cpp:375] successfully recovered registrar  i0218 01:53:27.932751 13940 log.cpp:702] attempting to truncate the log to 1  i0218 01:53:27.932865 13944 coordinator.cpp:339] coordinator attempting to write 3 action at position 2  i0218 01:53:27.932998 13939 master.cpp:1212] recovered 0 slaves from the registry (101b) ; allowing 10mins for slaves to reregister  i0218 01:53:27.933732 13936 replica.cpp:510] replica received write request for position 2  i0218 01:53:27.934146 13936 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 386584ns  i0218 01:53:27.934167 13936 replica.cpp:678] persisted action at 2  i0218 01:53:27.934708 13935 replica.cpp:657] replica received learned notice for position 2  i0218 01:53:27.935081 13935 leveldb.cpp:342] persisting action (18 bytes) to leveldb took 350891ns  i0218 01:53:27.935127 13935 leveldb.cpp:400] deleting 1 keys from leveldb took 24983ns  i0218 01:53:27.935140 13935 replica.cpp:678] persisted action at 2  i0218 01:53:27.935158 13935 replica.cpp:663] replica learned 3 action at position 2  i0218 01:53:27.947561 13918 containerizer.cpp:104] using isolation: posix/cpu,posix/mem  i0218 01:53:27.948971 13941 slave.cpp:173] slave started on 150)@67.195.81.187:57268  i0218 01:53:27.949003 13941 credentials.hpp:84] loading credential for authentication from '/tmp/masterslavereconciliationtestreconcilelosttask5no5rj/credential'  i0218 01:53:27.949167 13941 slave.cpp:280] slave using credential for: testprincipal  i0218 01:53:27.949465 13941 slave.cpp:298] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0218 01:53:27.949556 13941 slave.cpp:327] slave hostname: pomona.apache.org  i0218 01:53:27.949575 13941 slave.cpp:328] slave checkpoint: false  w0218 01:53:27.949587 13941 slave.cpp:330] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0218 01:53:27.950536 13932 state.cpp:34] recovering state from '/tmp/masterslavereconciliationtestreconcilelosttask5no5rj/meta'  i0218 01:53:27.950783 13940 statusupdatemanager.cpp:196] recovering status update manager  i0218 01:53:27.953531 13944 containerizer.cpp:301] recovering containerizer  i0218 01:53:27.953944 13918 sched.cpp:151] version: 0.22.0  i0218 01:53:27.954617 13932 slave.cpp:3611] finished recovery  i0218 01:53:27.954732 13935 sched.cpp:248] new master detected at master@67.195.81.187:57268  i0218 01:53:27.954833 13935 sched.cpp:304] authenticating with master master@67.195.81.187:57268  i0218 01:53:27.954856 13935 sched.cpp:311] using default crammd5 authenticatee  i0218 01:53:27.955037 13947 authenticatee.hpp:138] creating new client sasl connection  i0218 01:53:27.955198 13944 statusupdatemanager.cpp:170] pausing sending status updates  i0218 01:53:27.955195 13941 slave.cpp:623] new master detected at master@67.195.81.187:57268  i0218 01:53:27.955238 13934 master.cpp:3811] authenticating scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:27.955270 13934 master.cpp:3822] using default crammd5 authenticator  i0218 01:53:27.955317 13941 slave.cpp:686] authenticating with master master@67.195.81.187:57268  i0218 01:53:27.955348 13941 slave.cpp:691] using default crammd5 authenticatee  i0218 01:53:27.955518 13933 authenticator.hpp:169] creating new server sasl connection  i0218 01:53:27.955534 13939 authenticatee.hpp:138] creating new client sasl connection  i0218 01:53:27.955693 13935 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0218 01:53:27.955732 13935 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0218 01:53:27.955844 13932 authenticator.hpp:275] received sasl authentication start  i0218 01:53:27.955905 13932 authenticator.hpp:397] authentication requires more steps  i0218 01:53:27.955999 13935 authenticatee.hpp:275] received sasl authentication step  i0218 01:53:27.956120 13932 authenticator.hpp:303] received sasl authentication step  i0218 01:53:27.957321 13941 slave.cpp:659] detecting new master  i0218 01:53:27.957473 13934 master.cpp:3811] authenticating slave(150)@67.195.81.187:57268  i0218 01:53:28.009866 13948 process.cpp:2117] dropped / lost event for pid: slave(146)@67.195.81.187:57268  i0218 01:53:28.592335 13932 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0218 01:53:28.592350 13934 master.cpp:3822] using default crammd5 authenticator  i0218 01:53:28.592367 13932 auxprop.cpp:170] looking up auxiliary property 'userpassword'  i0218 01:53:28.592434 13932 auxprop.cpp:170] looking up auxiliary property 'cmusaslsecretcrammd5'  i0218 01:53:28.592483 13932 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0218 01:53:28.592501 13932 auxprop.cpp:120] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0218 01:53:28.592510 13932 auxprop.cpp:120] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0218 01:53:28.592530 13932 authenticator.hpp:389] authentication success  i0218 01:53:28.592646 13935 authenticatee.hpp:315] authentication success  i0218 01:53:28.592686 13948 process.cpp:2117] dropped / lost event for pid: scheduler4eee5e93d6bb4af497950aec0916dfa5@67.195.81.187:57268  i0218 01:53:28.592800 13939 authenticator.hpp:169] creating new server sasl connection  i0218 01:53:28.592836 13948 process.cpp:2117] dropped / lost event for pid: hierarchicalallocator(180)@67.195.81.187:57268  i0218 01:53:28.592864 13934 master.cpp:3869] successfully authenticated principal 'testprincipal' at scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:28.592990 13933 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0218 01:53:28.593029 13933 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0218 01:53:28.593245 13933 authenticator.hpp:275] received sasl authentication start  i0218 01:53:28.593364 13933 authenticator.hpp:397] authentication requires more steps  i0218 01:53:28.593490 13941 sched.cpp:392] successfully authenticated with master master@67.195.81.187:57268  i0218 01:53:28.593519 13941 sched.cpp:515] sending registration request to master@67.195.81.187:57268  i0218 01:53:28.593531 13945 authenticatee.hpp:275] received sasl authentication step  i0218 01:53:28.593606 13941 sched.cpp:548] will retry registration in 1.707160316secs if necessary  i0218 01:53:28.593720 13933 authenticator.hpp:303] received sasl authentication step  i0218 01:53:28.593731 13939 master.cpp:1572] received registration request for framework 'default' at scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:28.593757 13933 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0218 01:53:28.593780 13933 auxprop.cpp:170] looking up auxiliary property 'userpassword'  i0218 01:53:28.593818 13939 master.cpp:1433] authorizing framework principal 'testprincipal' to receive offers for role ''  i0218 01:53:28.593823 13933 auxprop.cpp:170] looking up auxiliary property 'cmusaslsecretcrammd5'  i0218 01:53:28.593891 13933 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0218 01:53:28.593909 13933 auxprop.cpp:120] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0218 01:53:28.593919 13933 auxprop.cpp:120] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0218 01:53:28.593947 13933 authenticator.hpp:389] authentication success  i0218 01:53:28.594048 13945 authenticatee.hpp:315] authentication success  i0218 01:53:28.594140 13946 master.cpp:3869] successfully authenticated principal 'testprincipal' at slave(150)@67.195.81.187:57268  i0218 01:53:28.594383 13947 slave.cpp:757] successfully authenticated with master master@67.195.81.187:57268  i0218 01:53:28.594571 13947 slave.cpp:1089] will retry registration in 17.484321ms if necessary  i0218 01:53:28.594606 13946 master.cpp:1636] registering framework 20150218015326314269779557268139180000 (default) at scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:28.594995 13944 hierarchical.hpp:320] added framework 20150218015326314269779557268139180000  i0218 01:53:28.595034 13944 hierarchical.hpp:831] no resources available to allocate!  i0218 01:53:28.595057 13944 hierarchical.hpp:738] performed allocation for 0 slaves in 35451ns  i0218 01:53:28.595185 13937 sched.cpp:442] framework registered with 20150218015326314269779557268139180000  i0218 01:53:28.595232 13937 sched.cpp:456] scheduler::registered took 22922ns  i0218 01:53:28.595273 13946 master.cpp:2936] registering slave at slave(150)@67.195.81.187:57268 (pomona.apache.org) with id 2015021801532631426977955726813918s0  i0218 01:53:28.595803 13934 registrar.cpp:444] applied 1 operations in 74798ns; attempting to update the 'registry'  i0218 01:53:28.598387 13939 log.cpp:683] attempting to append 316 bytes to the log  i0218 01:53:28.598578 13938 coordinator.cpp:339] coordinator attempting to write 2 action at position 3  i0218 01:53:28.599488 13932 replica.cpp:510] replica received write request for position 3  i0218 01:53:28.599758 13932 leveldb.cpp:342] persisting action (335 bytes) to leveldb took 234907ns  i0218 01:53:28.599786 13932 replica.cpp:678] persisted action at 3  i0218 01:53:28.600777 13939 replica.cpp:657] replica received learned notice for position 3  i0218 01:53:28.601304 13939 leveldb.cpp:342] persisting action (337 bytes) to leveldb took 503852ns  i0218 01:53:28.601326 13939 replica.cpp:678] persisted action at 3  i0218 01:53:28.601346 13939 replica.cpp:663] replica learned 2 action at position 3  i0218 01:53:28.602901 13934 log.cpp:702] attempting to truncate the log to 3  i0218 01:53:28.603011 13938 coordinator.cpp:339] coordinator attempting to write 3 action at position 4  i0218 01:53:28.603135 13932 registrar.cpp:489] successfully updated the 'registry' in 7.035904ms  i0218 01:53:28.603687 13932 replica.cpp:510] replica received write request for position 4  i0218 01:53:28.603844 13934 slave.cpp:2666] received ping from slaveobserver(147)@67.195.81.187:57268  i0218 01:53:28.603945 13941 master.cpp:2993] registered slave 2015021801532631426977955726813918s0 at slave(150)@67.195.81.187:57268 (pomona.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0218 01:53:28.604046 13933 hierarchical.hpp:452] added slave 2015021801532631426977955726813918s0 (pomona.apache.org) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports( ):[3100032000] available)  i0218 01:53:28.604112 13932 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 399822ns  i0218 01:53:28.604131 13932 replica.cpp:678] persisted action at 4  i0218 01:53:28.605741 13933 hierarchical.hpp:756] performed allocation for slave 2015021801532631426977955726813918s0 in 1.649293ms  i0218 01:53:28.605836 13934 slave.cpp:791] registered with master master@67.195.81.187:57268; given slave id 2015021801532631426977955726813918s0  i0218 01:53:28.606003 13933 replica.cpp:657] replica received learned notice for position 4  i0218 01:53:28.606037 13947 statusupdate_manager.cpp:177] resuming sending status updates  i0218 01:53:28.606075 13937 master.cpp:3753] sending 1 offers to framework 20150218015326314269779557268139180000 (default) at scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:28.606547 13933 leveldb.cpp:342] persisting action (18 bytes) to leveldb took 517378ns  i0218 01:53:29.008322 13933 leveldb.cpp:400] deleting 2 keys from leveldb took 86406ns  i0218 01:53:29.008350 13933 replica.cpp:678] persisted action at 4  i0218 01:53:29.008380 13933 replica.cpp:663] replica learned 3 action at position 4  i0218 01:53:28.912961 13946 hierarchical.hpp:831] no resources available to allocate!  i0218 01:53:29.008543 13946 hierarchical.hpp:738] performed allocation for 1 slaves in 95.683965ms  i0218 01:53:29.008621 13944 sched.cpp:605] scheduler::resourceoffers took 74896ns  i0218 01:53:29.009996 13932 master.cpp:2266] processing accept call for offers: [ 2015021801532631426977955726813918o0 ] on slave 2015021801532631426977955726813918s0 at slave(150)@67.195.81.187:57268 (pomona.apache.org) for framework 20150218015326314269779557268139180000 (default) at scheduler17aa8fa2195f43d685d787b949d4419b@67.195.81.187:57268  i0218 01:53:29.010035 13932 master.cpp:2110] authorizing framework principal 'test principal' to launch task 1 as user 'jenkins'  w0218 01:53:29.011081 13932 validation.cpp:326] executor default for task 1 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be...",1,train
MESOS-2367,Improve slave resiliency in the face of orphan containers ,"right now there's a case where a misbehaving executor can cause a slave process to flap:        1) user tries to kill an instance  2) slave sends killtaskmessage to executor  3) executor sends kill signals to task processes  4) executor sends task_killed to slave  5) slave updates container cpu limit to be 0.01 cpus  6) a userprocess is still processing the kill signal  7) the task process cannot exit since it has too little cpu share and is throttled  8) executor itself terminates  9) slave tries to destroy the container, but cannot because the userprocess is stuck in the exit path.  10) slave restarts, and is constantly flapping because it cannot kill orphan containers        the slave's orphan container handling should be improved to deal with this case despite ill behaved users (framework writers).",5,train
MESOS-2372,Test script for verifying compatibility between Mesos components,"while our current unit/integration test suite catches functional bugs, it doesn't catch compatibility bugs (e.g, mesos 2371). this is really crucial to provide operators the ability to do seamless upgrades on live clusters.    we should have a test suite / framework (ideally running on ci vetting each review on rb) that tests upgrade paths between master, slave, scheduler and executor.",2,train
MESOS-2373,DRFSorter needs to distinguish resources from different slaves.,"currently the drfsorter aggregates total and allocated resources across multiple slaves, which only works for scalar resources. we need to distinguish resources from different slaves.    suppose we have 2 slaves and 1 framework. the framework is allocated all resources from both slaves.      resources slaveresources =    resources::parse(""cpus:2;mem:512;ports:[3100032000]"").get();    drfsorter sorter;    sorter.add(slaveresources);  / add slave1 resources  sorter.add(slaveresources);  / add slave2 resources    / total resources in sorter at this point is  / cpus():4; mem():1024; ports():[3100032000].  / the scalar resources get aggregated correctly but ports do not.    sorter.add(""f"");    / the 2 calls to allocated only works because we simply do:  /   allocation[name] += resources;  / without checking that the 'resources' is available in the total.    sorter.allocated(""f"", slaveresources);  sorter.allocated(""f"", slaveresources);    / at this point, sorter.allocation(""f"") is:  / cpus():4; mem():1024; ports():[3100032000].      to provide some context, this issue came up while trying to reserve all unreserved resources from every offer.      for (const offer& offer : offers) , );   }       suppose the slave resources are the same as above:      slave1: cpus(\):2; mem(\):512; ports(\):\[3100032000\]  slave2: cpus(\):2; mem(\):512; ports(\):\[3100032000\]      initial (incorrect) total resources in the drfsorter is:      cpus(\):4; mem(\):1024; ports(\):\[3100032000\]      we receive 2 offers, 1 from each slave:      offer1: cpus(\):2; mem(\):512; ports(\):\[3100032000\]  offer2: cpus(\):2; mem(\):512; ports(\):\[3100032000\]      at this point, the resources allocated for the framework is:      cpus(\):4; mem(\):1024; ports(\):\[3100032000\]      after first reserve operation with offer1:    the allocated resources for the framework becomes:      cpus(\):2; mem(\):512; cpus(role):2; mem(role):512; ports(role):\[3100032000\]      during second reserve operation with offer2:        / ...      frameworksorter frameworksorter =      frameworksorters[frameworks\[frameworkid\].role];      resources allocation = frameworksorter>allocation(frameworkid.value());      / update the allocated resources.    try/ updatedallocation = allocation.apply(operations);    checksome(updatedallocation);      / ...      allocation in the above code is:      cpus(\):2; mem(\):512; cpus(role):2; mem(role):512; ports(role):\[3100032000\]      we try to apply a reserve operation and we fail to find ports(\):\[3100032000\] which leads to the check fail at check_some(updatedallocation);",2,train
MESOS-2382,"replace unsafe ""find | xargs"" with ""find -exec""","the problem exists in   1194:src/makefile.am   47:src/tests/balloonframeworktest.sh    the current ""find  xargs"" should be nul delimited with ""find print0 | xargs 0"" for safer execution or can just be replaced with the find buildin option ""find exec '{}' \+"" which behaves similar to xargs.    there was a second occurrence of this in a test script, though in that case it would only rmdir empty folders, so is less critical.    i submitted a pr here: https:/github.com/apache/mesos/pull/36  ",1,train
MESOS-2387,SlaveTest.TaskLaunchContainerizerUpdateFails is flaky,"observed on internal ci      [ run      ] slavetest.tasklaunchcontainerizerupdatefails  using temporary directory '/tmp/slavetesttasklaunchcontainerizerupdatefailstujtci'  i0222 04:59:56.568491 21813 process.cpp:2117] dropped / lost event for pid: slave(52)@192.168.122.68:39461  i0222 04:59:56.595433 21791 leveldb.cpp:175] opened db in 27.59732ms  i0222 04:59:56.603965 21791 leveldb.cpp:182] compacted db in 8.49192ms  i0222 04:59:56.604019 21791 leveldb.cpp:197] created db iterator in 19206ns  i0222 04:59:56.604037 21791 leveldb.cpp:203] seeked to beginning of db in 1802ns  i0222 04:59:56.604046 21791 leveldb.cpp:272] iterated through 0 keys in the db in 467ns  i0222 04:59:56.604081 21791 replica.cpp:743] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0222 04:59:56.607413 21809 recover.cpp:448] starting replica recovery  i0222 04:59:56.607687 21809 recover.cpp:474] replica is in 4 status  i0222 04:59:56.609011 21809 replica.cpp:640] replica in 4 status received a broadcasted recover request  i0222 04:59:56.609262 21809 recover.cpp:194] received a recover response from a replica in 4 status  i0222 04:59:56.609709 21809 recover.cpp:565] updating replica status to 3  i0222 04:59:56.610749 21811 master.cpp:347] master 2015022204595611488892803946121791 (centos7) started on 192.168.122.68:39461  i0222 04:59:56.610791 21811 master.cpp:393] master only allowing authenticated frameworks to register  i0222 04:59:56.610802 21811 master.cpp:398] master only allowing authenticated slaves to register  i0222 04:59:56.610821 21811 credentials.hpp:36] loading credentials for authentication from '/tmp/slavetesttasklaunchcontainerizerupdatefailstujtci/credentials'  i0222 04:59:56.611042 21811 master.cpp:440] authorization enabled  i0222 04:59:56.612329 21811 hierarchical.hpp:286] initialized hierarchical allocator process  i0222 04:59:56.612416 21811 whitelistwatcher.cpp:78] no whitelist given  i0222 04:59:56.613005 21811 master.cpp:1354] the newly elected leader is master@192.168.122.68:39461 with id 2015022204595611488892803946121791  i0222 04:59:56.613034 21811 master.cpp:1367] elected as the leading master!  i0222 04:59:56.613050 21811 master.cpp:1185] recovering from registrar  i0222 04:59:56.613229 21811 registrar.cpp:312] recovering registrar  i0222 04:59:56.622866 21809 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 12.988429ms  i0222 04:59:56.622913 21809 replica.cpp:322] persisted replica status to 3  i0222 04:59:56.623118 21809 recover.cpp:474] replica is in 3 status  i0222 04:59:56.624419 21809 replica.cpp:640] replica in 3 status received a broadcasted recover request  i0222 04:59:56.624685 21809 recover.cpp:194] received a recover response from a replica in 3 status  i0222 04:59:56.625200 21809 recover.cpp:565] updating replica status to 1  i0222 04:59:56.635154 21809 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 9.799671ms  i0222 04:59:56.635197 21809 replica.cpp:322] persisted replica status to 1  i0222 04:59:56.635296 21809 recover.cpp:579] successfully joined the paxos group  i0222 04:59:56.635426 21809 recover.cpp:463] recover process terminated  i0222 04:59:56.635812 21809 log.cpp:659] attempting to start the writer  i0222 04:59:56.637075 21809 replica.cpp:476] replica received implicit promise request with proposal 1  i0222 04:59:56.648674 21809 leveldb.cpp:305] persisting metadata (8 bytes) to leveldb took 11.566146ms  i0222 04:59:56.648717 21809 replica.cpp:344] persisted promised to 1  i0222 04:59:56.649456 21809 coordinator.cpp:229] coordinator attemping to fill missing position  i0222 04:59:56.650800 21809 replica.cpp:377] replica received explicit promise request for position 0 with proposal 2  i0222 04:59:56.659916 21809 leveldb.cpp:342] persisting action (8 bytes) to leveldb took 9.078258ms  i0222 04:59:56.659981 21809 replica.cpp:678] persisted action at 0  i0222 04:59:56.661075 21809 replica.cpp:510] replica received write request for position 0  i0222 04:59:56.661129 21809 leveldb.cpp:437] reading position from leveldb took 26387ns  i0222 04:59:56.671227 21809 leveldb.cpp:342] persisting action (14 bytes) to leveldb took 10.064302ms  i0222 04:59:56.671262 21809 replica.cpp:678] persisted action at 0  i0222 04:59:56.671821 21809 replica.cpp:657] replica received learned notice for position 0  i0222 04:59:56.684200 21809 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 12.346897ms  i0222 04:59:56.684242 21809 replica.cpp:678] persisted action at 0  i0222 04:59:56.684262 21809 replica.cpp:663] replica learned 1 action at position 0  i0222 04:59:56.684875 21809 log.cpp:675] writer started with ending position 0  i0222 04:59:56.685932 21809 leveldb.cpp:437] reading position from leveldb took 27308ns  i0222 04:59:56.688256 21809 registrar.cpp:345] successfully fetched the registry (0b) in 74.992128ms  i0222 04:59:56.688344 21809 registrar.cpp:444] applied 1 operations in 19566ns; attempting to update the 'registry'  i0222 04:59:56.690690 21809 log.cpp:683] attempting to append 129 bytes to the log  i0222 04:59:56.690848 21809 coordinator.cpp:339] coordinator attempting to write 2 action at position 1  i0222 04:59:56.691661 21809 replica.cpp:510] replica received write request for position 1  i0222 04:59:56.701247 21809 leveldb.cpp:342] persisting action (148 bytes) to leveldb took 9.550768ms  i0222 04:59:56.701292 21809 replica.cpp:678] persisted action at 1  i0222 04:59:56.702066 21809 replica.cpp:657] replica received learned notice for position 1  i0222 04:59:56.712136 21809 leveldb.cpp:342] persisting action (150 bytes) to leveldb took 10.041696ms  i0222 04:59:56.712175 21809 replica.cpp:678] persisted action at 1  i0222 04:59:56.712198 21809 replica.cpp:663] replica learned 2 action at position 1  i0222 04:59:56.713289 21809 registrar.cpp:489] successfully updated the 'registry' in 24.890112ms  i0222 04:59:56.713397 21809 registrar.cpp:375] successfully recovered registrar  i0222 04:59:56.713537 21809 log.cpp:702] attempting to truncate the log to 1  i0222 04:59:56.713795 21809 master.cpp:1212] recovered 0 slaves from the registry (93b) ; allowing 10mins for slaves to reregister  i0222 04:59:56.713871 21809 coordinator.cpp:339] coordinator attempting to write 3 action at position 2  i0222 04:59:56.714879 21809 replica.cpp:510] replica received write request for position 2  i0222 04:59:56.725225 21809 leveldb.cpp:342] persisting action (16 bytes) to leveldb took 10.311704ms  i0222 04:59:56.725270 21809 replica.cpp:678] persisted action at 2  i0222 04:59:56.726066 21809 replica.cpp:657] replica received learned notice for position 2  i0222 04:59:56.734110 21809 leveldb.cpp:342] persisting action (18 bytes) to leveldb took 8.012327ms  i0222 04:59:56.734180 21809 leveldb.cpp:400] deleting ~1 keys from leveldb took 36578ns  i0222 04:59:56.734201 21809 replica.cpp:678] persisted action at 2  i0222 04:59:56.734221 21809 replica.cpp:663] replica learned 3 action at position 2  i0222 04:59:56.747556 21809 slave.cpp:173] slave started on 53)@192.168.122.68:39461  i0222 04:59:56.747601 21809 credentials.hpp:84] loading credential for authentication from '/tmp/slavetesttasklaunchcontainerizerupdatefailsqkhajp/credential'  i0222 04:59:56.747774 21809 slave.cpp:280] slave using credential for: testprincipal  i0222 04:59:56.748021 21809 slave.cpp:298] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0222 04:59:56.748682 21809 slave.cpp:327] slave hostname: centos7  i0222 04:59:56.748705 21809 slave.cpp:328] slave checkpoint: false  w0222 04:59:56.748714 21809 slave.cpp:330] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0222 04:59:56.749826 21809 state.cpp:34] recovering state from '/tmp/slavetesttasklaunchcontainerizerupdatefailsqkhajp/meta'  i0222 04:59:56.750191 21809 statusupdatemanager.cpp:196] recovering status update manager  i0222 04:59:56.750465 21809 slave.cpp:3775] finished recovery  i0222 04:59:56.751260 21809 slave.cpp:623] new master detected at master@192.168.122.68:39461  i0222 04:59:56.751349 21809 slave.cpp:686] authenticating with master master@192.168.122.68:39461  i0222 04:59:56.751369 21809 slave.cpp:691] using default crammd5 authenticatee  i0222 04:59:56.751502 21809 slave.cpp:659] detecting new master  i0222 04:59:56.751596 21809 statusupdatemanager.cpp:170] pausing sending status updates  i0222 04:59:56.751668 21809 authenticatee.hpp:138] creating new client sasl connection  i0222 04:59:56.752781 21809 master.cpp:3811] authenticating slave(53)@192.168.122.68:39461  i0222 04:59:56.752820 21809 master.cpp:3822] using default crammd5 authenticator  i0222 04:59:56.753124 21809 authenticator.hpp:169] creating new server sasl connection  i0222 04:59:56.755609 21809 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0222 04:59:56.755641 21809 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0222 04:59:56.755708 21809 authenticator.hpp:275] received sasl authentication start  i0222 04:59:56.755751 21809 authenticator.hpp:397] authentication requires more steps  i0222 04:59:56.755813 21809 authenticatee.hpp:275] received sasl authentication step  i0222 04:59:56.755887 21809 authenticator.hpp:303] received sasl authentication step  i0222 04:59:56.755920 21809 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'centos7' server fqdn: 'centos7' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0222 04:59:56.755934 21809 auxprop.cpp:170] looking up auxiliary property 'userpassword'  i0222 04:59:56.756005 21809 auxprop.cpp:170] looking up auxiliary property 'cmusaslsecretcrammd5'  i0222 04:59:56.756036 21809 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'centos7' server fqdn: 'centos7' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0222 04:59:56.756047 21809 auxprop.cpp:120] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0222 04:59:56.756054 21809 auxprop.cpp:120] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0222 04:59:56.756068 21809 authenticator.hpp:389] authentication success  i0222 04:59:56.756155 21809 authenticatee.hpp:315] authentication success  i0222 04:59:56.756219 21809 master.cpp:3869] successfully authenticated principal 'testprincipal' at slave(53)@192.168.122.68:39461  i0222 04:59:56.756503 21809 slave.cpp:757] successfully authenticated with master master@192.168.122.68:39461  i0222 04:59:56.756611 21809 slave.cpp:1089] will retry registration in 11.221976ms if necessary  i0222 04:59:56.756876 21809 master.cpp:2936] registering slave at slave(53)@192.168.122.68:39461 (centos7) with id 2015022204595611488892803946121791s0  i0222 04:59:56.757323 21809 registrar.cpp:444] applied 1 operations in 70787ns; attempting to update the 'registry'  i0222 04:59:56.759790 21809 log.cpp:683] attempting to append 299 bytes to the log  i0222 04:59:56.760000 21809 coordinator.cpp:339] coordinator attempting to write 2 action at position 3  i0222 04:59:56.760920 21809 replica.cpp:510] replica received write request for position 3  i0222 04:59:56.762037 21791 sched.cpp:154] version: 0.22.0  i0222 04:59:56.762763 21806 sched.cpp:251] new master detected at master@192.168.122.68:39461  i0222 04:59:56.762835 21806 sched.cpp:307] authenticating with master master@192.168.122.68:39461  i0222 04:59:56.762856 21806 sched.cpp:314] using default crammd5 authenticatee  i0222 04:59:56.763082 21806 authenticatee.hpp:138] creating new client sasl connection  i0222 04:59:56.763753 21806 master.cpp:3811] authenticating schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.763784 21806 master.cpp:3822] using default crammd5 authenticator  i0222 04:59:56.764040 21806 authenticator.hpp:169] creating new server sasl connection  i0222 04:59:56.764624 21806 authenticatee.hpp:229] received sasl authentication mechanisms: crammd5  i0222 04:59:56.764653 21806 authenticatee.hpp:255] attempting to authenticate with mechanism 'crammd5'  i0222 04:59:56.764719 21806 authenticator.hpp:275] received sasl authentication start  i0222 04:59:56.764758 21806 authenticator.hpp:397] authentication requires more steps  i0222 04:59:56.764819 21806 authenticatee.hpp:275] received sasl authentication step  i0222 04:59:56.764889 21806 authenticator.hpp:303] received sasl authentication step  i0222 04:59:56.764911 21806 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'centos7' server fqdn: 'centos7' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0222 04:59:56.764922 21806 auxprop.cpp:170] looking up auxiliary property 'userpassword'  i0222 04:59:56.764974 21806 auxprop.cpp:170] looking up auxiliary property 'cmusaslsecretcrammd5'  i0222 04:59:56.765005 21806 auxprop.cpp:98] request to lookup properties for user: 'testprincipal' realm: 'centos7' server fqdn: 'centos7' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0222 04:59:56.765017 21806 auxprop.cpp:120] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0222 04:59:56.765023 21806 auxprop.cpp:120] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0222 04:59:56.765036 21806 authenticator.hpp:389] authentication success  i0222 04:59:56.765120 21806 authenticatee.hpp:315] authentication success  i0222 04:59:56.765182 21806 master.cpp:3869] successfully authenticated principal 'testprincipal' at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.765442 21806 sched.cpp:395] successfully authenticated with master master@192.168.122.68:39461  i0222 04:59:56.765465 21806 sched.cpp:518] sending registration request to master@192.168.122.68:39461  i0222 04:59:56.765522 21806 sched.cpp:551] will retry registration in 1.283564292secs if necessary  i0222 04:59:56.765637 21806 master.cpp:1572] received registration request for framework 'default' at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.765699 21806 master.cpp:1433] authorizing framework principal 'testprincipal' to receive offers for role ''  i0222 04:59:56.766120 21806 master.cpp:1636] registering framework 20150222045956114888928039461217910000 (default) at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.766572 21806 hierarchical.hpp:320] added framework 20150222045956114888928039461217910000  i0222 04:59:56.766598 21806 hierarchical.hpp:831] no resources available to allocate!  i0222 04:59:56.766609 21806 hierarchical.hpp:738] performed allocation for 0 slaves in 15902ns  i0222 04:59:56.766753 21806 sched.cpp:445] framework registered with 20150222045956114888928039461217910000  i0222 04:59:56.766790 21806 sched.cpp:459] scheduler::registered took 15076ns  i0222 04:59:56.773710 21806 slave.cpp:1089] will retry registration in 3.454005ms if necessary  i0222 04:59:56.773900 21806 master.cpp:2924] ignoring register slave message from slave(53)@192.168.122.68:39461 (centos7) as admission is already in progress  i0222 04:59:56.775297 21809 leveldb.cpp:342] persisting action (318 bytes) to leveldb took 14.319807ms  i0222 04:59:56.775344 21809 replica.cpp:678] persisted action at 3  i0222 04:59:56.776139 21809 replica.cpp:657] replica received learned notice for position 3  i0222 04:59:56.778630 21806 slave.cpp:1089] will retry registration in 32.764468ms if necessary  i0222 04:59:56.778779 21806 master.cpp:2924] ignoring register slave message from slave(53)@192.168.122.68:39461 (centos7) as admission is already in progress  i0222 04:59:56.783778 21809 leveldb.cpp:342] persisting action (320 bytes) to leveldb took 7.609533ms  i0222 04:59:56.783828 21809 replica.cpp:678] persisted action at 3  i0222 04:59:56.783849 21809 replica.cpp:663] replica learned 2 action at position 3  i0222 04:59:56.785058 21809 registrar.cpp:489] successfully updated the 'registry' in 27.669248ms  i0222 04:59:56.785274 21809 log.cpp:702] attempting to truncate the log to 3  i0222 04:59:56.785815 21809 master.cpp:2993] registered slave 2015022204595611488892803946121791s0 at slave(53)@192.168.122.68:39461 (centos7) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0222 04:59:56.785913 21809 coordinator.cpp:339] coordinator attempting to write 3 action at position 4  i0222 04:59:56.786267 21809 hierarchical.hpp:452] added slave 2015022204595611488892803946121791s0 (centos7) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0222 04:59:56.786600 21809 hierarchical.hpp:756] performed allocation for slave 2015022204595611488892803946121791s0 in 292298ns  i0222 04:59:56.786684 21809 slave.cpp:791] registered with master master@192.168.122.68:39461; given slave id 2015022204595611488892803946121791s0  i0222 04:59:56.786792 21809 slave.cpp:2830] received ping from slaveobserver(52)@192.168.122.68:39461  i0222 04:59:56.787230 21809 master.cpp:3753] sending 1 offers to framework 20150222045956114888928039461217910000 (default) at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.787334 21809 statusupdatemanager.cpp:177] resuming sending status updates  i0222 04:59:56.788156 21809 sched.cpp:608] scheduler::resourceoffers took 557128ns  i0222 04:59:56.788936 21809 master.cpp:2266] processing accept call for offers: [ 2015022204595611488892803946121791o0 ] on slave 2015022204595611488892803946121791s0 at slave(53)@192.168.122.68:39461 (centos7) for framework 20150222045956114888928039461217910000 (default) at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461  i0222 04:59:56.789000 21809 master.cpp:2110] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  w0222 04:59:56.790506 21809 validation.cpp:327] executor default for task 0 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0222 04:59:56.790546 21809 validation.cpp:339] executor default for task 0 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0222 04:59:56.790808 21809 master.hpp:821] adding task 0 with resources cpus():1; mem():128 on slave 2015022204595611488892803946121791s0 (centos7)  i0222 04:59:56.790885 21809 master.cpp:2543] launching task 0 of framework 20150222045956114888928039461217910000 (default) at schedulerd9c22c4e8dec42a6a350a98472642891@192.168.122.68:39461 with resources cpus():1; mem( ):128 on slave 2015022204595611488892803946121791s0 at slave(53)@192.168.122.68:39461 (centos7)  i0222 04:59:56.791201 21809 replica.cpp:510] replica received write request for position 4  i0222 04:59:56.791610 21806 slave.cpp:1120] got assigned task 0 for framework 20150222045956114888928039461217910000  i0222 04:59:56.792140 21806 slave.cpp:1230] launching task 0 for framework 20150222045956114888928039461217910000  i0222 04:59:56.794872 21806 slave.cpp:4177] launching executor default of framework 20150222045956114888928039461217910000 in work directory '/tmp/slavetesttasklaunchcontainerizerupdatefails_qkhajp/slaves/2015022204595611488892803946121791s0/frameworks/20150222045956114888928039461217910000/executors/default/runs/753232b543ff4fbfb29a 0f76161132ab'  i0222 04:59:56.796846 21806 exec.cpp:130] version: 0.22.0  i0222 04:59:56.797173 21806 slave.cpp:1377] queuing task '0' for execut...",1,train
MESOS-2388,GroupTest.LabelledGroup segfaults,"observed this on internal ci. not sure if it is due to ""grouptest.labelledgroup"" or an earlier test.      i0219 01:04:17.980598 27766 zookeepertestserver.cpp:117] shutting down zookeepertestserver on port 39597  [       ok ] grouptest.retryableerrors (30150 ms)  [ run      ] grouptest.labelledgroup  makefile:6656: recipe for target 'checklocal' failed  make[3]:   [checklocal] segmentation fault (core dumped)    ",2,train
MESOS-2391,Provide user doc for the new posix disk isolator in Mesos containerizer,we introduced a posix disk isolator for mesos containerizer in 0.22.0. this isolator allows us to get container disk usage as well as enforcing container disk quota. it's based on 'du'. we need to document this feature.,2,train
MESOS-2392,Rate limit slaves removals during master recovery.,"much like we rate limit slave removals in the common path (mesos1148), we need to rate limit slave removals that occur during master recovery. when a master recovers and is using a strict registry, slaves that do not reregister within a timeout will be removed.    currently there is a safeguard in place to abort when too many slaves have not re registered. however, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting.",3,train
MESOS-2394,Create styleguide for documentation,as of right now different pages in our documentation use quite different styles. consider for example the different emphasis for note:   > note: http:/mesos.apache.org/documentation/latest/slave recovery/    note: http:/mesos.apache.org/documentation/latest/upgrades/      would be great to establish a common style for the documentation!,2,train
MESOS-2400,Improve NsTest.ROOT_setns, use symbol name directly to launch the subprocess instead of the hardcoded string.     replaced the static string with char[].  ,1,train
MESOS-2401,MasterTest.ShutdownFrameworkWhileTaskRunning is flaky,"looks like the executorshutdowntimeout() was called immediately after executorshutdown() was called!      [ run      ] mastertest.shutdownframeworkwhiletaskrunning  using temporary directory '/tmp/mastertestshutdownframeworkwhiletaskrunningsbd6vk'  i0224 18:51:17.385068 30213 leveldb.cpp:176] opened db in 1.262442ms  i0224 18:51:17.386360 30213 leveldb.cpp:183] compacted db in 985102ns  i0224 18:51:17.387025 30213 leveldb.cpp:198] created db iterator in 78043ns  i0224 18:51:17.387420 30213 leveldb.cpp:204] seeked to beginning of db in 25814ns  i0224 18:51:17.387804 30213 leveldb.cpp:273] iterated through 0 keys in the db in 25025ns  i0224 18:51:17.388270 30213 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0224 18:51:17.389760 30227 recover.cpp:449] starting replica recovery  i0224 18:51:17.395699 30227 recover.cpp:475] replica is in 4 status  i0224 18:51:17.398294 30227 replica.cpp:641] replica in 4 status received a broadcasted recover request  i0224 18:51:17.398816 30227 recover.cpp:195] received a recover response from a replica in 4 status  i0224 18:51:17.402415 30230 recover.cpp:566] updating replica status to 3  i0224 18:51:17.403473 30229 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 273857ns  i0224 18:51:17.404093 30229 replica.cpp:323] persisted replica status to 3  i0224 18:51:17.404930 30229 recover.cpp:475] replica is in 3 status  i0224 18:51:17.407995 30233 replica.cpp:641] replica in 3 status received a broadcasted recover request  i0224 18:51:17.410697 30231 recover.cpp:195] received a recover response from a replica in 3 status  i0224 18:51:17.415710 30230 recover.cpp:566] updating replica status to 1  i0224 18:51:17.416987 30227 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 221966ns  i0224 18:51:17.417579 30227 replica.cpp:323] persisted replica status to 1  i0224 18:51:17.418803 30234 recover.cpp:580] successfully joined the paxos group  i0224 18:51:17.419699 30227 recover.cpp:464] recover process terminated  i0224 18:51:17.430594 30234 master.cpp:349] master 2015022418511722729627524495030213 (fedora19) started on 192.168.122.135:44950  i0224 18:51:17.431082 30234 master.cpp:395] master only allowing authenticated frameworks to register  i0224 18:51:17.431453 30234 master.cpp:400] master only allowing authenticated slaves to register  i0224 18:51:17.431828 30234 credentials.hpp:37] loading credentials for authentication from '/tmp/mastertestshutdownframeworkwhiletaskrunningsbd6vk/credentials'  i0224 18:51:17.432740 30234 master.cpp:442] authorization enabled  i0224 18:51:17.434224 30229 hierarchical.hpp:287] initialized hierarchical allocator process  i0224 18:51:17.434994 30233 whitelistwatcher.cpp:79] no whitelist given  i0224 18:51:17.440687 30234 master.cpp:1356] the newly elected leader is master@192.168.122.135:44950 with id 2015022418511722729627524495030213  i0224 18:51:17.441764 30234 master.cpp:1369] elected as the leading master!  i0224 18:51:17.442430 30234 master.cpp:1187] recovering from registrar  i0224 18:51:17.443053 30229 registrar.cpp:313] recovering registrar  i0224 18:51:17.445468 30228 log.cpp:660] attempting to start the writer  i0224 18:51:17.449970 30233 replica.cpp:477] replica received implicit promise request with proposal 1  i0224 18:51:17.451359 30233 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 339488ns  i0224 18:51:17.451949 30233 replica.cpp:345] persisted promised to 1  i0224 18:51:17.456845 30235 process.cpp:2117] dropped / lost event for pid: hierarchicalallocator(154)@192.168.122.135:44950  i0224 18:51:17.461741 30231 coordinator.cpp:230] coordinator attemping to fill missing position  i0224 18:51:17.464686 30228 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0224 18:51:17.465515 30228 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 170261ns  i0224 18:51:17.465991 30228 replica.cpp:679] persisted action at 0  i0224 18:51:17.470512 30229 replica.cpp:511] replica received write request for position 0  i0224 18:51:17.471437 30229 leveldb.cpp:438] reading position from leveldb took 139178ns  i0224 18:51:17.472129 30229 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 141560ns  i0224 18:51:17.472705 30229 replica.cpp:679] persisted action at 0  i0224 18:51:17.476305 30228 replica.cpp:658] replica received learned notice for position 0  i0224 18:51:17.477991 30228 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 208112ns  i0224 18:51:17.478574 30228 replica.cpp:679] persisted action at 0  i0224 18:51:17.479044 30228 replica.cpp:664] replica learned 1 action at position 0  i0224 18:51:17.484371 30233 log.cpp:676] writer started with ending position 0  i0224 18:51:17.487396 30233 leveldb.cpp:438] reading position from leveldb took 96498ns  i0224 18:51:17.498906 30233 registrar.cpp:346] successfully fetched the registry (0b) in 55.234048ms  i0224 18:51:17.499781 30233 registrar.cpp:445] applied 1 operations in 97308ns; attempting to update the 'registry'  i0224 18:51:17.503955 30231 log.cpp:684] attempting to append 131 bytes to the log  i0224 18:51:17.505009 30231 coordinator.cpp:340] coordinator attempting to write 2 action at position 1  i0224 18:51:17.507428 30228 replica.cpp:511] replica received write request for position 1  i0224 18:51:17.508517 30228 leveldb.cpp:343] persisting action (150 bytes) to leveldb took 316570ns  i0224 18:51:17.508985 30228 replica.cpp:679] persisted action at 1  i0224 18:51:17.512902 30229 replica.cpp:658] replica received learned notice for position 1  i0224 18:51:17.517261 30229 leveldb.cpp:343] persisting action (152 bytes) to leveldb took 427860ns  i0224 18:51:17.517470 30229 replica.cpp:679] persisted action at 1  i0224 18:51:17.517796 30229 replica.cpp:664] replica learned 2 action at position 1  i0224 18:51:17.532624 30232 registrar.cpp:490] successfully updated the 'registry' in 32.31104ms  i0224 18:51:17.533957 30228 log.cpp:703] attempting to truncate the log to 1  i0224 18:51:17.534366 30228 coordinator.cpp:340] coordinator attempting to write 3 action at position 2  i0224 18:51:17.536684 30227 replica.cpp:511] replica received write request for position 2  i0224 18:51:17.537406 30227 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 196455ns  i0224 18:51:17.537946 30227 replica.cpp:679] persisted action at 2  i0224 18:51:17.537695 30232 registrar.cpp:376] successfully recovered registrar  i0224 18:51:17.544136 30231 master.cpp:1214] recovered 0 slaves from the registry (95b) ; allowing 10mins for slaves to reregister  i0224 18:51:17.546041 30227 replica.cpp:658] replica received learned notice for position 2  i0224 18:51:17.546728 30227 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 192442ns  i0224 18:51:17.547058 30227 leveldb.cpp:401] deleting 1 keys from leveldb took 61064ns  i0224 18:51:17.547363 30227 replica.cpp:679] persisted action at 2  i0224 18:51:17.547669 30227 replica.cpp:664] replica learned 3 action at position 2  i0224 18:51:17.565460 30234 slave.cpp:174] slave started on 138)@192.168.122.135:44950  i0224 18:51:17.566038 30234 credentials.hpp:85] loading credential for authentication from '/tmp/mastertestshutdownframeworkwhiletaskrunninglrugms/credential'  i0224 18:51:17.566584 30234 slave.cpp:281] slave using credential for: testprincipal  i0224 18:51:17.567198 30234 slave.cpp:299] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0224 18:51:17.567930 30234 slave.cpp:328] slave hostname: fedora19  i0224 18:51:17.568172 30234 slave.cpp:329] slave checkpoint: false  w0224 18:51:17.568435 30234 slave.cpp:331] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0224 18:51:17.570539 30227 state.cpp:35] recovering state from '/tmp/mastertestshutdownframeworkwhiletaskrunninglrugms/meta'  i0224 18:51:17.573499 30232 statusupdatemanager.cpp:197] recovering status update manager  i0224 18:51:17.574209 30234 slave.cpp:3775] finished recovery  i0224 18:51:17.576277 30229 statusupdatemanager.cpp:171] pausing sending status updates  i0224 18:51:17.576680 30234 slave.cpp:624] new master detected at master@192.168.122.135:44950  i0224 18:51:17.577131 30234 slave.cpp:687] authenticating with master master@192.168.122.135:44950  i0224 18:51:17.577385 30234 slave.cpp:692] using default crammd5 authenticatee  i0224 18:51:17.577945 30228 authenticatee.hpp:139] creating new client sasl connection  i0224 18:51:17.578837 30234 slave.cpp:660] detecting new master  i0224 18:51:17.579270 30228 master.cpp:3813] authenticating slave(138)@192.168.122.135:44950  i0224 18:51:17.579900 30228 master.cpp:3824] using default crammd5 authenticator  i0224 18:51:17.580572 30228 authenticator.hpp:170] creating new server sasl connection  i0224 18:51:17.581501 30231 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0224 18:51:17.581805 30231 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0224 18:51:17.582222 30228 authenticator.hpp:276] received sasl authentication start  i0224 18:51:17.582531 30228 authenticator.hpp:398] authentication requires more steps  i0224 18:51:17.582945 30230 authenticatee.hpp:276] received sasl authentication step  i0224 18:51:17.583351 30228 authenticator.hpp:304] received sasl authentication step  i0224 18:51:17.583643 30228 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0224 18:51:17.583911 30228 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0224 18:51:17.584241 30228 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0224 18:51:17.584517 30228 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0224 18:51:17.584787 30228 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0224 18:51:17.585075 30228 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0224 18:51:17.585358 30228 authenticator.hpp:390] authentication success  i0224 18:51:17.585750 30233 authenticatee.hpp:316] authentication success  i0224 18:51:17.586354 30232 master.cpp:3871] successfully authenticated principal 'testprincipal' at slave(138)@192.168.122.135:44950  i0224 18:51:17.590953 30234 slave.cpp:758] successfully authenticated with master master@192.168.122.135:44950  i0224 18:51:17.591686 30233 master.cpp:2938] registering slave at slave(138)@192.168.122.135:44950 (fedora19) with id 2015022418511722729627524495030213s0  i0224 18:51:17.592718 30233 registrar.cpp:445] applied 1 operations in 100358ns; attempting to update the 'registry'  i0224 18:51:17.595989 30227 log.cpp:684] attempting to append 302 bytes to the log  i0224 18:51:17.596757 30227 coordinator.cpp:340] coordinator attempting to write 2 action at position 3  i0224 18:51:17.599280 30227 replica.cpp:511] replica received write request for position 3  i0224 18:51:17.599481 30234 slave.cpp:1090] will retry registration in 12.331173ms if necessary  i0224 18:51:17.601940 30227 leveldb.cpp:343] persisting action (321 bytes) to leveldb took 999045ns  i0224 18:51:17.602339 30227 replica.cpp:679] persisted action at 3  i0224 18:51:17.612349 30229 replica.cpp:658] replica received learned notice for position 3  i0224 18:51:17.612934 30229 leveldb.cpp:343] persisting action (323 bytes) to leveldb took 152139ns  i0224 18:51:17.613471 30229 replica.cpp:679] persisted action at 3  i0224 18:51:17.613796 30229 replica.cpp:664] replica learned 2 action at position 3  i0224 18:51:17.615980 30229 master.cpp:2926] ignoring register slave message from slave(138)@192.168.122.135:44950 (fedora19) as admission is already in progress  i0224 18:51:17.614302 30233 slave.cpp:1090] will retry registration in 11.014835ms if necessary  i0224 18:51:17.617490 30234 registrar.cpp:490] successfully updated the 'registry' in 24.179968ms  i0224 18:51:17.618989 30234 master.cpp:2995] registered slave 2015022418511722729627524495030213s0 at slave(138)@192.168.122.135:44950 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0224 18:51:17.619567 30233 hierarchical.hpp:455] added slave 2015022418511722729627524495030213s0 (fedora19) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (and cpus():2; mem():1024; disk():1024; ports():[3100032000] available)  i0224 18:51:17.621080 30233 hierarchical.hpp:834] no resources available to allocate!  i0224 18:51:17.621441 30233 hierarchical.hpp:759] performed allocation for slave 2015022418511722729627524495030213s0 in 544608ns  i0224 18:51:17.619704 30229 slave.cpp:792] registered with master master@192.168.122.135:44950; given slave id 2015022418511722729627524495030213s0  i0224 18:51:17.622195 30229 slave.cpp:2830] received ping from slaveobserver(125)@192.168.122.135:44950  i0224 18:51:17.622385 30227 statusupdatemanager.cpp:178] resuming sending status updates  i0224 18:51:17.620266 30232 log.cpp:703] attempting to truncate the log to 3  i0224 18:51:17.623522 30232 coordinator.cpp:340] coordinator attempting to write 3 action at position 4  i0224 18:51:17.624835 30229 replica.cpp:511] replica received write request for position 4  i0224 18:51:17.625727 30229 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 259831ns  i0224 18:51:17.626122 30229 replica.cpp:679] persisted action at 4  i0224 18:51:17.627686 30227 replica.cpp:658] replica received learned notice for position 4  i0224 18:51:17.628228 30227 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 93777ns  i0224 18:51:17.628785 30227 leveldb.cpp:401] deleting 2 keys from leveldb took 57660ns  i0224 18:51:17.629176 30227 replica.cpp:679] persisted action at 4  i0224 18:51:17.629443 30227 replica.cpp:664] replica learned 3 action at position 4  i0224 18:51:17.636715 30213 sched.cpp:157] version: 0.23.0  i0224 18:51:17.638003 30229 sched.cpp:254] new master detected at master@192.168.122.135:44950  i0224 18:51:17.638602 30229 sched.cpp:310] authenticating with master master@192.168.122.135:44950  i0224 18:51:17.639024 30229 sched.cpp:317] using default crammd5 authenticatee  i0224 18:51:17.639580 30228 authenticatee.hpp:139] creating new client sasl connection  i0224 18:51:17.640455 30235 process.cpp:2117] dropped / lost event for pid: scheduler11bb6bcbcd514927a28bdbca9d63772f@192.168.122.135:44950  i0224 18:51:17.641150 30228 master.cpp:3813] authenticating schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.641597 30228 master.cpp:3824] using default crammd5 authenticator  i0224 18:51:17.642643 30228 authenticator.hpp:170] creating new server sasl connection  i0224 18:51:17.643698 30234 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0224 18:51:17.644296 30234 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0224 18:51:17.644739 30228 authenticator.hpp:276] received sasl authentication start  i0224 18:51:17.645143 30228 authenticator.hpp:398] authentication requires more steps  i0224 18:51:17.645654 30230 authenticatee.hpp:276] received sasl authentication step  i0224 18:51:17.646122 30228 authenticator.hpp:304] received sasl authentication step  i0224 18:51:17.646421 30228 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0224 18:51:17.646746 30228 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0224 18:51:17.647203 30228 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0224 18:51:17.647644 30228 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'fedora19' server fqdn: 'fedora19' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0224 18:51:17.648454 30228 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0224 18:51:17.648788 30228 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxprop_authzid == true  i0224 18:51:17.649210 30228 authenticator.hpp:390] authentication success  i0224 18:51:17.649705 30231 authenticatee.hpp:316] authentication success  i0224 18:51:17.653314 30231 sched.cpp:398] successfully authenticated with master master@192.168.122.135:44950  i0224 18:51:17.653766 30232 master.cpp:3871] successfully authenticated principal 'testprincipal' at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.654683 30231 sched.cpp:521] sending registration request to master@192.168.122.135:44950  i0224 18:51:17.655138 30231 sched.cpp:554] will retry registration in 1.028970132secs if necessary  i0224 18:51:17.657112 30232 master.cpp:1574] received registration request for framework 'default' at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.658509 30232 master.cpp:1435] authorizing framework principal 'testprincipal' to receive offers for role ''  i0224 18:51:17.659765 30232 master.cpp:1638] registering framework 20150224185117227296275244950302130000 (default) at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.660727 30233 hierarchical.hpp:321] added framework 20150224185117227296275244950302130000  i0224 18:51:17.661730 30233 hierarchical.hpp:741] performed allocation for 1 slaves in 529369ns  i0224 18:51:17.662911 30229 sched.cpp:448] framework registered with 20150224185117227296275244950302130000  i0224 18:51:17.663374 30229 sched.cpp:462] scheduler::registered took 35637ns  i0224 18:51:17.664552 30232 master.cpp:3755] sending 1 offers to framework 20150224185117227296275244950302130000 (default) at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.668009 30234 sched.cpp:611] scheduler::resourceoffers took 2.574292ms  i0224 18:51:17.671038 30232 master.cpp:2268] processing accept call for offers: [ 2015022418511722729627524495030213o0 ] on slave 2015022418511722729627524495030213s0 at slave(138)@192.168.122.135:44950 (fedora19) for framework 20150224185117227296275244950302130000 (default) at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950  i0224 18:51:17.672071 30232 master.cpp:2112] authorizing framework principal 'testprincipal' to launch task 1 as user 'jenkins'  w0224 18:51:17.674675 30232 validation.cpp:326] executor default for task 1 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0224 18:51:17.675395 30232 validation.cpp:338] executor default for task 1 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0224 18:51:17.676460 30232 master.hpp:822] adding task 1 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 2015022418511722729627524495030213s0 (fedora19)  i0224 18:51:17.677078 30232 master.cpp:2545] launching task 1 of framework 20150224185117227296275244950302130000 (default) at schedulerfc72e828078341b69892ffc961e8567e@192.168.122.135:44950 with resources cpus():2; mem():1024; disk():1024; ports( ):[3100032000] on slave 2015022418511722729627524495030213s0 at slave(138)@192.168.122.135:44950 (fedora 19)  i0224 18:51:17.678084 30230 slave.cpp:1121] got ass...",1,train
MESOS-2402,MesosContainerizerDestroyTest.LauncherDestroyFailure is flaky,"""failed to os::execvpe in childmain"". never seen this one before.      [ run      ] mesoscontainerizerdestroytest.launcherdestroyfailure  using temporary directory '/tmp/mesoscontainerizerdestroytestlauncherdestroyfailureqpjqen'  i0224 18:55:49.326912 21391 containerizer.cpp:461] starting container 'testcontainer' for executor 'executor' of framework ''  i0224 18:55:49.332252 21391 launcher.cpp:130] forked child with pid '23496' for container 'testcontainer'  abort: (src/subprocess.cpp:165): failed to os::execvpe in childmain   aborted at 1424832949 (unix time) try ""date d @1424832949"" if you are using gnu date   pc: @     0x2b178c5db0d5 (unknown)  i0224 18:55:49.340955 21392 process.cpp:2117] dropped / lost event for pid: scheduler509d37ac296f4429b101af433c1800e9@127.0.1.1:39647  i0224 18:55:49.342300 21386 containerizer.cpp:911] destroying container 'testcontainer'   sigabrt (@0x3e800005bc8) received by pid 23496 (tid 0x2b178f9f0700) from pid 23496; stack trace:       @     0x2b178c397cb0 (unknown)      @     0x2b178c5db0d5 (unknown)      @     0x2b178c5de83b (unknown)      @           0x87a945 abort()      @     0x2b1789f610b9 process::childmain()  i0224 18:55:49.391793 21386 containerizer.cpp:1120] executor for container 'testcontainer' has exited  i0224 18:55:49.400478 21391 process.cpp:2770] handling http event for process 'metrics' with path: '/metrics/snapshot'  tests/containerizertests.cpp:485: failure  value of: metrics.values[""containerizer/mesos/containerdestroyerrors""]    actual: 16byte object /  expected: 1u  which is: 1  [  failed  ] mesoscontainerizerdestroytest.launcherdestroyfailure (89 ms)    ",2,train
MESOS-2403,MasterAllocatorTest/0.FrameworkReregistersFirst is flaky,"  [ run      ] masterallocatortest/0.frameworkreregistersfirst  using temporary directory '/tmp/masterallocatortest0frameworkreregistersfirstvy5nml'  i0224 23:22:31.681670 30589 leveldb.cpp:176] opened db in 2.943518ms  i0224 23:22:31.682152 30619 process.cpp:2117] dropped / lost event for pid: slave(65)@67.195.81.187:38391  i0224 23:22:31.682732 30589 leveldb.cpp:183] compacted db in 1.029469ms  i0224 23:22:31.682777 30589 leveldb.cpp:198] created db iterator in 15460ns  i0224 23:22:31.682792 30589 leveldb.cpp:204] seeked to beginning of db in 1832ns  i0224 23:22:31.682802 30589 leveldb.cpp:273] iterated through 0 keys in the db in 319ns  i0224 23:22:31.682833 30589 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0224 23:22:31.683228 30605 recover.cpp:449] starting replica recovery  i0224 23:22:31.683537 30605 recover.cpp:475] replica is in 4 status  i0224 23:22:31.684624 30615 replica.cpp:641] replica in 4 status received a broadcasted recover request  i0224 23:22:31.684978 30616 recover.cpp:195] received a recover response from a replica in 4 status  i0224 23:22:31.685405 30610 recover.cpp:566] updating replica status to 3  i0224 23:22:31.686249 30609 master.cpp:349] master 2015022423223131426977953839130589 (pomona.apache.org) started on 67.195.81.187:38391  i0224 23:22:31.686265 30617 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 717897ns  i0224 23:22:31.686319 30617 replica.cpp:323] persisted replica status to 3  i0224 23:22:31.686336 30609 master.cpp:395] master only allowing authenticated frameworks to register  i0224 23:22:31.686357 30609 master.cpp:400] master only allowing authenticated slaves to register  i0224 23:22:31.686390 30609 credentials.hpp:37] loading credentials for authentication from '/tmp/masterallocatortest0frameworkreregistersfirstvy5nml/credentials'  i0224 23:22:31.686511 30606 recover.cpp:475] replica is in 3 status  i0224 23:22:31.686563 30609 master.cpp:442] authorization enabled  i0224 23:22:31.686929 30607 whitelistwatcher.cpp:79] no whitelist given  i0224 23:22:31.686954 30603 hierarchical.hpp:287] initialized hierarchical allocator process  i0224 23:22:31.687134 30605 replica.cpp:641] replica in 3 status received a broadcasted recover request  i0224 23:22:31.687731 30609 master.cpp:1356] the newly elected leader is master@67.195.81.187:38391 with id 2015022423223131426977953839130589  i0224 23:22:31.839818 30609 master.cpp:1369] elected as the leading master!  i0224 23:22:31.839834 30609 master.cpp:1187] recovering from registrar  i0224 23:22:31.839926 30605 registrar.cpp:313] recovering registrar  i0224 23:22:31.840000 30613 recover.cpp:195] received a recover response from a replica in 3 status  i0224 23:22:31.840504 30606 recover.cpp:566] updating replica status to 1  i0224 23:22:31.841599 30611 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 990330ns  i0224 23:22:31.841627 30611 replica.cpp:323] persisted replica status to 1  i0224 23:22:31.841743 30611 recover.cpp:580] successfully joined the paxos group  i0224 23:22:31.841904 30611 recover.cpp:464] recover process terminated  i0224 23:22:31.842366 30608 log.cpp:660] attempting to start the writer  i0224 23:22:31.843557 30607 replica.cpp:477] replica received implicit promise request with proposal 1  i0224 23:22:31.844312 30607 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 722368ns  i0224 23:22:31.844337 30607 replica.cpp:345] persisted promised to 1  i0224 23:22:31.844889 30615 coordinator.cpp:230] coordinator attemping to fill missing position  i0224 23:22:31.846043 30614 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0224 23:22:31.846729 30614 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 660024ns  i0224 23:22:31.846746 30614 replica.cpp:679] persisted action at 0  i0224 23:22:31.847671 30611 replica.cpp:511] replica received write request for position 0  i0224 23:22:31.847723 30611 leveldb.cpp:438] reading position from leveldb took 27349ns  i0224 23:22:31.848429 30611 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 671461ns  i0224 23:22:31.848454 30611 replica.cpp:679] persisted action at 0  i0224 23:22:31.849041 30615 replica.cpp:658] replica received learned notice for position 0  i0224 23:22:31.849762 30615 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 690386ns  i0224 23:22:31.849787 30615 replica.cpp:679] persisted action at 0  i0224 23:22:31.849808 30615 replica.cpp:664] replica learned 1 action at position 0  i0224 23:22:31.850416 30612 log.cpp:676] writer started with ending position 0  i0224 23:22:31.851490 30615 leveldb.cpp:438] reading position from leveldb took 30659ns  i0224 23:22:31.854452 30610 registrar.cpp:346] successfully fetched the registry (0b) in 14.491136ms  i0224 23:22:31.854543 30610 registrar.cpp:445] applied 1 operations in 18024ns; attempting to update the 'registry'  i0224 23:22:31.857095 30604 log.cpp:684] attempting to append 139 bytes to the log  i0224 23:22:31.857208 30608 coordinator.cpp:340] coordinator attempting to write 2 action at position 1  i0224 23:22:31.858073 30609 replica.cpp:511] replica received write request for position 1  i0224 23:22:31.858808 30609 leveldb.cpp:343] persisting action (158 bytes) to leveldb took 701708ns  i0224 23:22:31.858835 30609 replica.cpp:679] persisted action at 1  i0224 23:22:31.859508 30618 replica.cpp:658] replica received learned notice for position 1  i0224 23:22:31.860267 30618 leveldb.cpp:343] persisting action (160 bytes) to leveldb took 731035ns  i0224 23:22:31.860309 30618 replica.cpp:679] persisted action at 1  i0224 23:22:31.860332 30618 replica.cpp:664] replica learned 2 action at position 1  i0224 23:22:31.860983 30609 registrar.cpp:490] successfully updated the 'registry' in 6.39616ms  i0224 23:22:31.861071 30609 registrar.cpp:376] successfully recovered registrar  i0224 23:22:31.861126 30608 log.cpp:703] attempting to truncate the log to 1  i0224 23:22:31.861249 30603 coordinator.cpp:340] coordinator attempting to write 3 action at position 2  i0224 23:22:31.861248 30617 master.cpp:1214] recovered 0 slaves from the registry (101b) ; allowing 10mins for slaves to reregister  i0224 23:22:31.861831 30613 replica.cpp:511] replica received write request for position 2  i0224 23:22:31.862504 30613 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 648125ns  i0224 23:22:31.862531 30613 replica.cpp:679] persisted action at 2  i0224 23:22:31.863067 30603 replica.cpp:658] replica received learned notice for position 2  i0224 23:22:31.863689 30603 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 602784ns  i0224 23:22:31.863737 30603 leveldb.cpp:401] deleting 1 keys from leveldb took 28697ns  i0224 23:22:31.863751 30603 replica.cpp:679] persisted action at 2  i0224 23:22:31.863767 30603 replica.cpp:664] replica learned 3 action at position 2  i0224 23:22:31.875962 30610 slave.cpp:174] slave started on 66)@67.195.81.187:38391  i0224 23:22:31.876008 30610 credentials.hpp:85] loading credential for authentication from '/tmp/masterallocatortest0frameworkreregistersfirstikvxqm/credential'  i0224 23:22:31.876144 30610 slave.cpp:281] slave using credential for: testprincipal  i0224 23:22:31.876404 30610 slave.cpp:299] slave resources: cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000]  i0224 23:22:31.876489 30610 slave.cpp:328] slave hostname: pomona.apache.org  i0224 23:22:31.876502 30610 slave.cpp:329] slave checkpoint: false  w0224 23:22:31.876507 30610 slave.cpp:331] disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release. please avoid using this flag  i0224 23:22:31.877014 30603 state.cpp:35] recovering state from '/tmp/masterallocatortest0frameworkreregistersfirstikvxqm/meta'  i0224 23:22:31.877230 30610 statusupdatemanager.cpp:197] recovering status update manager  i0224 23:22:31.877495 30609 slave.cpp:3776] finished recovery  i0224 23:22:31.877879 30607 statusupdatemanager.cpp:171] pausing sending status updates  i0224 23:22:31.877879 30604 slave.cpp:624] new master detected at master@67.195.81.187:38391  i0224 23:22:31.877959 30604 slave.cpp:687] authenticating with master master@67.195.81.187:38391  i0224 23:22:31.877975 30604 slave.cpp:692] using default crammd5 authenticatee  i0224 23:22:31.878069 30604 slave.cpp:660] detecting new master  i0224 23:22:31.878093 30608 authenticatee.hpp:139] creating new client sasl connection  i0224 23:22:31.878223 30604 master.cpp:3813] authenticating slave(66)@67.195.81.187:38391  i0224 23:22:31.878244 30604 master.cpp:3824] using default crammd5 authenticator  i0224 23:22:31.878412 30613 authenticator.hpp:170] creating new server sasl connection  i0224 23:22:31.878525 30603 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0224 23:22:31.878551 30603 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0224 23:22:31.878625 30617 authenticator.hpp:276] received sasl authentication start  i0224 23:22:31.878662 30617 authenticator.hpp:398] authentication requires more steps  i0224 23:22:31.878727 30603 authenticatee.hpp:276] received sasl authentication step  i0224 23:22:31.878815 30617 authenticator.hpp:304] received sasl authentication step  i0224 23:22:31.878839 30617 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0224 23:22:31.878847 30617 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0224 23:22:31.878875 30617 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0224 23:22:31.878891 30617 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0224 23:22:31.878900 30617 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0224 23:22:31.878906 30617 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0224 23:22:31.878916 30617 authenticator.hpp:390] authentication success  i0224 23:22:31.880717 30589 sched.cpp:157] version: 0.23.0  i0224 23:22:32.017823 30611 authenticatee.hpp:316] authentication success  i0224 23:22:32.017901 30618 master.cpp:3871] successfully authenticated principal 'testprincipal' at slave(66)@67.195.81.187:38391  i0224 23:22:32.018156 30615 sched.cpp:254] new master detected at master@67.195.81.187:38391  i0224 23:22:32.018240 30615 sched.cpp:310] authenticating with master master@67.195.81.187:38391  i0224 23:22:32.018263 30615 sched.cpp:317] using default crammd5 authenticatee  i0224 23:22:32.018496 30613 slave.cpp:758] successfully authenticated with master master@67.195.81.187:38391  i0224 23:22:32.018579 30611 authenticatee.hpp:139] creating new client sasl connection  i0224 23:22:32.018620 30613 slave.cpp:1090] will retry registration in 363167ns if necessary  i0224 23:22:32.018811 30615 master.cpp:2938] registering slave at slave(66)@67.195.81.187:38391 (pomona.apache.org) with id 2015022423223131426977953839130589s0  i0224 23:22:32.019122 30615 master.cpp:3813] authenticating scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.019156 30615 master.cpp:3824] using default crammd5 authenticator  i0224 23:22:32.019232 30612 registrar.cpp:445] applied 1 operations in 57599ns; attempting to update the 'registry'  i0224 23:22:32.019394 30603 authenticator.hpp:170] creating new server sasl connection  i0224 23:22:32.019541 30611 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0224 23:22:32.019568 30611 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0224 23:22:32.019666 30605 authenticator.hpp:276] received sasl authentication start  i0224 23:22:32.019717 30605 authenticator.hpp:398] authentication requires more steps  i0224 23:22:32.019805 30615 authenticatee.hpp:276] received sasl authentication step  i0224 23:22:32.019942 30605 authenticator.hpp:304] received sasl authentication step  i0224 23:22:32.019979 30605 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0224 23:22:32.019994 30605 auxprop.cpp:171] looking up auxiliary property 'userpassword'  i0224 23:22:32.020025 30605 auxprop.cpp:171] looking up auxiliary property 'cmusaslsecretcrammd5'  i0224 23:22:32.020036 30610 slave.cpp:1090] will retry registration in 10.850555ms if necessary  i0224 23:22:32.020053 30605 auxprop.cpp:99] request to lookup properties for user: 'testprincipal' realm: 'pomona.apache.org' server fqdn: 'pomona.apache.org' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0224 23:22:32.020102 30605 auxprop.cpp:121] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0224 23:22:32.020117 30605 auxprop.cpp:121] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0224 23:22:32.020133 30605 authenticator.hpp:390] authentication success  i0224 23:22:32.020151 30611 master.cpp:2926] ignoring register slave message from slave(66)@67.195.81.187:38391 (pomona.apache.org) as admission is already in progress  i0224 23:22:32.020226 30603 authenticatee.hpp:316] authentication success  i0224 23:22:32.020256 30611 master.cpp:3871] successfully authenticated principal 'testprincipal' at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.020534 30615 sched.cpp:398] successfully authenticated with master master@67.195.81.187:38391  i0224 23:22:32.020561 30615 sched.cpp:521] sending registration request to master@67.195.81.187:38391  i0224 23:22:32.020635 30615 sched.cpp:554] will retry registration in 490.035142ms if necessary  i0224 23:22:32.020720 30613 master.cpp:1574] received registration request for framework 'default' at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.020787 30613 master.cpp:1435] authorizing framework principal 'testprincipal' to receive offers for role ''  i0224 23:22:32.021122 30607 master.cpp:1638] registering framework 20150224232231314269779538391305890000 (default) at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.021502 30611 hierarchical.hpp:321] added framework 20150224232231314269779538391305890000  i0224 23:22:32.021531 30611 hierarchical.hpp:834] no resources available to allocate!  i0224 23:22:32.021543 30611 hierarchical.hpp:741] performed allocation for 0 slaves in 18915ns  i0224 23:22:32.021618 30609 sched.cpp:448] framework registered with 20150224232231314269779538391305890000  i0224 23:22:32.021673 30609 sched.cpp:462] scheduler::registered took 26310ns  i0224 23:22:32.022400 30613 log.cpp:684] attempting to append 316 bytes to the log  i0224 23:22:32.022523 30608 coordinator.cpp:340] coordinator attempting to write 2 action at position 3  i0224 23:22:32.023232 30607 replica.cpp:511] replica received write request for position 3  i0224 23:22:32.024055 30607 leveldb.cpp:343] persisting action (335 bytes) to leveldb took 798548ns  i0224 23:22:32.024073 30607 replica.cpp:679] persisted action at 3  i0224 23:22:32.024651 30610 replica.cpp:658] replica received learned notice for position 3  i0224 23:22:32.025252 30610 leveldb.cpp:343] persisting action (337 bytes) to leveldb took 580525ns  i0224 23:22:32.025271 30610 replica.cpp:679] persisted action at 3  i0224 23:22:32.025297 30610 replica.cpp:664] replica learned 2 action at position 3  i0224 23:22:32.025995 30618 registrar.cpp:490] successfully updated the 'registry' in 6.586112ms  i0224 23:22:32.026228 30604 log.cpp:703] attempting to truncate the log to 3  i0224 23:22:32.026360 30609 coordinator.cpp:340] coordinator attempting to write 3 action at position 4  i0224 23:22:32.026669 30609 slave.cpp:2831] received ping from slaveobserver(66)@67.195.81.187:38391  i0224 23:22:32.026772 30609 slave.cpp:792] registered with master master@67.195.81.187:38391; given slave id 2015022423223131426977953839130589s0  i0224 23:22:32.026737 30603 master.cpp:2995] registered slave 2015022423223131426977953839130589s0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000]  i0224 23:22:32.026867 30603 statusupdate_manager.cpp:178] resuming sending status updates  i0224 23:22:32.026868 30617 hierarchical.hpp:455] added slave 2015022423223131426977953839130589s0 (pomona.apache.org) with cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000] (and cpus():2; mem():1024; disk():3.70122e06; ports():[3100032000] available)  i0224 23:22:32.026921 30615 replica.cpp:511] replica received write request for position 4  i0224 23:22:32.027276 30617 hierarchical.hpp:759] performed allocation for slave 2015022423223131426977953839130589s0 in 351257ns  i0224 23:22:32.027580 30615 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 624249ns  i0224 23:22:32.027604 30615 replica.cpp:679] persisted action at 4  i0224 23:22:32.027642 30618 master.cpp:3755] sending 1 offers to framework 20150224232231314269779538391305890000 (default) at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.028223 30617 replica.cpp:658] replica received learned notice for position 4  i0224 23:22:32.028621 30607 sched.cpp:611] scheduler::resourceoffers took 648326ns  i0224 23:22:32.028916 30617 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 662416ns  i0224 23:22:32.028991 30617 leveldb.cpp:401] deleting 2 keys from leveldb took 47386ns  i0224 23:22:32.029021 30617 replica.cpp:679] persisted action at 4  i0224 23:22:32.029044 30617 replica.cpp:664] replica learned 3 action at position 4  i0224 23:22:32.029534 30613 master.cpp:2268] processing accept call for offers: [ 2015022423223131426977953839130589o0 ] on slave 2015022423223131426977953839130589s0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) for framework 20150224232231314269779538391305890000 (default) at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391  i0224 23:22:32.190521 30613 master.cpp:2112] authorizing framework principal 'testprincipal' to launch task 0 as user 'jenkins'  w0224 23:22:32.191864 30604 validation.cpp:328] executor default for task 0 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w0224 23:22:32.191905 30604 validation.cpp:340] executor default for task 0 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i0224 23:22:32.192206 30604 master.hpp:822] adding task 0 with resources cpus():1; mem():500 on slave 2015022423223131426977953839130589s0 (pomona.apache.org)  i0224 23:22:32.192318 30604 master.cpp:2545] launching task 0 of framework 20150224232231314269779538391305890000 (default) at scheduler9a3224ccaef049a7a2404b85b913ff44@67.195.81.187:38391 with resources cpus():1; mem():500 on slave 2015022423223131426977953839130589s0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)  i0224 23:22:32.192659 30611 slave.cpp:1121] got assigned task 0 for framework 20150224232231314269779538391305890000  i0224 23:22:32.192847 30609 hierarchical.hpp:648] recovered cpus():1; mem():524; disk():3.70122e+06; ports():[3100032000] (total allocatable: cpus():1; mem( ):524; ...",2,train
MESOS-2404,Add an example framework to test persistent volumes.,this serves two purposes:    1) testing the new persistence feature  2) served as an example for others to use the new feature,3,train
MESOS-2405,Add user doc for using persistent volumes.,nan,2,train
MESOS-2408,Slave should garbage collect released persistent volumes.,"this is tricky in the case when a persistence id is reused. when a persistent volume is destroyed explicitly by the framework, master deletes all information about this volume. that mean the master no longer has the ability to check if the persistence id is reused (and reject the later attempt). on the slave side, we'll use some gc policy to remove directories associated with deleted persistent volumes (similar to how we gc sandboxes). that means the persistent volume directory won't be deleted immediately when the volume is destroyed by the framework explicitly. when the same persistence id is reused, we'll see the persistent volume still exists and we need to cancel the gc of that directory (similar to what we cancel the gc for meta directories during runtask).",5,train
MESOS-2422,Use fq_codel qdisc for egress network traffic isolation,nan,8,train
MESOS-2427,Add Java binding for the acceptOffers API.,we introduced the new acceptoffers api in c driver. we need to provide java binding for this api as well.,2,train
MESOS-2428,Add Python bindings for the acceptOffers API.,we introduced the new acceptoffers api in c driver. we need to provide python binding for this api as well.,2,train
MESOS-2438,Improve support for streaming HTTP Responses in libprocess.,"currently libprocess' http::response supports a pipe construct for doing streaming responses:      struct response   type;      ...  };      this interface is too low level and difficult to program against:     connection closure is signaled with sigpipe, which is difficult for callers to deal with (must suppress sigpipe locally or globally in order to get epipe instead).   pipes are generally for interprocess communication, and the pipe has finite size. with a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. with a non blocking pipe, the caller must deal with retrying the write.    we'll want to consider a few use cases:  # sending an http::response with streaming data.  # making a request with http::get and http::post in which the data is returned in a streaming manner.  # making a request in which the request content is streaming.    this ticket will focus on 1 as it is required for the http api.",8,train
MESOS-2447,Mesos replicated log does not log the Action type name.,this is a regression introduced during the internal namespace refactor.    0.21.0 master:    i0224 02:43:29.806895 50982 replica.cpp:661] replica learned append action at position 1655      0.22.0 master:    i0303 21:45:39.406929  1302 replica.cpp:664] replica learned 2 action at position 2079  ,1,train
MESOS-2452,The recovered executor directory points to the meta directory.,the bug was introduced in this review:  https:/reviews.apache.org/r/29687     runstate.directory points to the metadata directory.    this would cause the posixdiskisolator to report incorrect disk usages after slave recovery.    we also need a test to test the slave recovery path for the posixdiskisolator.,2,train
MESOS-2454,Add support for /proc/self/mountinfo on Linux,"/proc/self/mountinfo provides mount information specific to the calling process. this includes information on optional fields describing mount propagation, e.g., shared/slave mounts.     initially, add this to linux/fs then perhaps move existing users of mounttable to use the mountinfo, deprecating and removing the mostly (but not entirely) redundant code.",3,train
MESOS-2455,Add operator endpoints to create/destroy persistent volumes.,"persistent volumes will not be released automatically.    so we probably need an endpoint for operators to forcefully release persistent volumes. we probably need to add principal to persistence struct and use acls to control who can release what.    additionally, it would be useful to have an endpoint for operators to create persistent volumes.",3,train
MESOS-2461,Slave should provide details on processes running in its cgroups,"the slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. see the slave flag: slave_subsystems    it currently refuses to start if there are any processes in its cgroups  this could be another slave or some subprocess started by a previous slave  and only logs the pids of those processes.    improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time.",1,train
MESOS-2462,Add option for Subprocess to set a death signal for the forked child,"currently, children forked by the slave, including those through subprocess, will continue running if the slave exits. for some processes, including helper processes like the fetcher, du, or perf, we'd like them to be terminated when the slave exits.    add support to subprocess to optionally set a deathsig for the child, e.g., setting sigterm would mean the child would get sigterm when the slave terminates.    this can be done (after forking) with prsetdeathsig. see ""man prctl"". it is preserved through an exec call.",3,train
MESOS-2464,Authentication failure may lead to slave crash,"when slave authentication fails, the following attempt to transmit a unregisterslavemessage may cause a crash within the slave.      e0309 01:08:34.819758 336699392 slave.cpp:740] master master@192.168.178.20:5050 refused authentication  i0309 01:08:34.819787 336699392 slave.cpp:538] master refused authentication; unregistering and shutting down    [libprotobuf fatal google/protobuf/messagelite.cc:273] check failed: isinitialized(): can't serialize message of type ""mesos.internal.unregisterslavemessage"" because it is missing required fields: slaveid.value  libprocess: slave(1)@192.168.178.20:5051 terminating due to check failed: isinitialized(): can't serialize message of type ""mesos.internal.unregisterslavemessage"" because it is missing required fields: slaveid.value      the problem here is the following code:            unregisterslavemessage message;        message.mutableslaveid() >mergefrom(info.id());      authentication happens before registration. info.id is an optional member (of slaveinfo) and not known yet. it is set later, while registering. so slaveid will remain unset.",1,train
MESOS-2466,Write documentation for all the LIBPROCESS_* environment variables.,"libprocess uses a set of environment variables to modify its behaviour; however, these variables are not documented anywhere, nor it is defined where the documentation should be.    what would be needed is a decision whether the environment variables should be documented (a new doc file or reusing an existing one), and then add the documentation there.    after searching in the code, these are the variables which need to be documented:    # libprocessip  # libprocessport  # libprocessadvertiseip  # libprocessadvertiseport",2,train
MESOS-2467,Allow --resources flag to take JSON.,"currently, we used a customized format for resources flag. as we introduce more and more stuffs (e.g., persistence, reservation) in resource object, we need a more generic way to specify resources.    for backward compatibility, we can scan the first character. if it is '[', then we invoke the json parser. otherwise, we use the existing parser.",3,train
MESOS-2469,Mesos master/slave should be able to bind to 127.0.0.1 if explicitly requested,"with the current refactoring to ip it looks like master and slave can no longer bind to 127.0.0.1 even if explicitly requested via ""ip"" flag.     among other things, this breaks the balloon framework test which uses this flag.",1,train
MESOS-2475,Add the Resource::ReservationInfo protobuf message,"the resource::reservationinfo protobuf message encapsulates information needed to keep track of reservations. it's named reservationinfo rather than reservation to keep consistency with resource::diskinfo.    here's what it will look like:      message reservationinfo     / if this is set, this resource was dynamically reserved by an  / operator or a framework. otherwise, this resource was  / statically configured by an operator via the resources flag.  optional reservationinfo reservation;  ",2,train
MESOS-2476,Enable Resources to handle Resource::ReservationInfo,"after https:/issues.apache.org/jira/browse/mesos2475, our c resources class needs to know how to handle resource protobuf messages that have the reservation field set.",2,train
MESOS-2477,Enable Resources::apply to handle reservation operations.,resources::apply currently only handles create and destroy operations which exist for persistent volumes. we need to handle the reserve and unreserve operations for dynamic reservations as well.,3,train
MESOS-2485,Add ability to distinguish slave removals metrics by reason.,"currently we only expose a single removal metric (""master/slave_removals"") which makes it difficult to distinguish between removal reasons in the alerting.    currently, a slave can be removed for the following reasons:    # health checks failed.  # slave unregistered.  # slave was replaced by a new slave (on the same endpoint).    in the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",3,train
MESOS-2489,Enable a framework to perform reservation operations.,"goal    this is the first step to supporting dynamic reservations. the goal of this task is to enable a framework to reply to a resource offer with reserve and unreserve offer operations as defined by offer::operation in mesos.proto.    overview    it's divided into a few subtasks so that it's clear what the small chunks to be addressed are. in summary, we need to introduce the resource::reservationinfo protobuf message to encapsulate the reservation information, enable the c resources class to handle it then enable the master to handle reservation operations.    expected outcome     the framework will be able to send back reservation operations to (un)reserve resources.   the reservations are kept only in the master since we don't send the checkpointresources message to checkpoint the reservations on the slave yet.    the reservations are considered to be reserved for the framework's role.",4,train
MESOS-2491,Persist the reservation state on the slave,goal    the goal for this task is to persist the reservation state stored on the master on the corresponding slave. the needcheckpointing predicate is used to capture the condition for which a resource needs to be checkpointed. currently the only condition is ispersistentvolume. we'll update this to include dynamically reserved resources.    expected outcome      the dynamically reserved resources will be persisted on the slave.,5,train
MESOS-2497,Create synchronous validations for Calls,/call endpoint will return a 202 accepted code but has to do some basic validations before. in case of invalidation it will return a 4xx code. we have to create a mechanism that will validate the 'request' and send back the appropriate code.,8,train
MESOS-2500,Doxygen setup for libprocess,"goals:    initial doxygen setup.    enable interested developers to generate already available doxygen content locally in their workspace and view it.   form the basis for future contributions of more doxygen content.    1. devise a way to use doxygen with mesos source code. (for example, solve this by adding optional brew/aptget installation to the ""getting started"" doc.)  2. create a make target for libprocess documentation that can be manually triggered.  3. create initial library top level documentation.  4. enhance one header file with doxygen. make sure the generated output has all necessary links to navigate from the lib to the file and back, etc.  ",2,train
MESOS-2501,Doxygen style for libprocess,"create a description of the doxygen style to use for libprocess documentation.     it is expected that this will later also become the doxygen style for stout and mesos, but we are working on libprocess only for now.    possible outcome: a file named docs/doxygen style.md    we hope for much input and expect a lot of discussion!  ",1,train
MESOS-2507,Performance issue in the master when a large number of slaves are registering.,"for large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. perf revealed the following:      events: 14k cycles   25.44%  libmesos0.22.0x.so  [.] mesos::internal::master::master::registerslave(process::upid const&, mesos::slaveinfo const&, std::vector/ > cons   11.18%  libmesos0.22.0x.so  [.] pipecb    5.88%  libc2.5.so             [.] mallocconsolidate    5.33%  libc2.5.so             [.] intfree    5.25%  libc2.5.so             [.] malloc    5.23%  libc2.5.so             [.] intmalloc    4.11%  libstdc.so.6.0.8      [.] std::string::assign(std::string const&)    3.22%  libmesos0.22.0x.so  [.] mesos::resource::shareddtor()    3.10%  [kernel]                [k] rawspinlock    1.97%  libmesos0.22.0x.so  [.] mesos::attribute::shareddtor()    1.28%  libc2.5.so             [.] memcmp    1.08%  libc2.5.so             [.] free      this is likely because we loop over all the slaves for each registration:      void master::registerslave(      const upid& from,      const slaveinfo& slaveinfo,      const vector/& checkpointedresources,      const string& version)      }    / ...  }  ",5,train
MESOS-2512,FetcherTest.ExtractNotExecutable is flaky,"observed in our internal ci.      [ run      ] fetchertest.extractnotexecutable  using temporary directory '/tmp/fetchertestextractnotexecutabler5r7cn'  tar: removing leading `/' from member names  i0316 18:55:48.509306 14678 fetcher.cpp:155] starting to fetch uris for container: de1e516582b4434b91498667cf652c64, directory: /tmp/fetchertestextractnotexecutabler5r7cn  i0316 18:55:48.509845 14678 fetcher.cpp:238] fetching uris using command '/var/jenkins/workspace/mesosfedora20gcc/src/mesosfetcher'  i0316 18:55:48.568611 15028 logging.cpp:177] logging to stderr  i0316 18:55:48.574928 15028 fetcher.cpp:214] fetching uri '/tmp/dijmjv.tar.gz'  i0316 18:55:48.575166 15028 fetcher.cpp:194] copying resource from '/tmp/dijmjv.tar.gz' to '/tmp/fetchertestextractnotexecutabler5r7cn'  tar: this does not look like a tar archive  tar: exiting with failure status due to previous errors  failed to extract /tmp/fetchertestextractnotexecutabler5r7cn/dijmjv.tar.gz:failed to extract: command tar c '/tmp/fetchertestextractnotexecutabler5r7cn' xf '/tmp/fetchertestextractnotexecutabler5r7cn/dijmjv.tar.gz' exited with status: 512  tests/fetcher_tests.cpp:686: failure  (fetch).failure(): failed to fetch uris for container 'de1e516582b4434b91498667cf652c64'with exit status: 256  [  failed  ] fetchertest.extractnotexecutable (208 ms)  ",2,train
MESOS-2514,Change the default leaf qdisc to fq_codel inside containers,"when we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifofast, which is known to have buffer bloat. change the default leaf qdisc to fqcodel too:    `tc qd add dev eth0 parent 1:1 fq_codel`    i can no longer see packet drops after this change.",1,train
MESOS-2519,Log IP addresses from HTTP requests,"querying /master/state.json is an expensive operation when a cluster is large, and it's possible to dos the master via frequent and repeated queries (which is a separate problem). querying the endpoint results in a log entry being written, but the entry lacks useful information, such as an ip address, response code and response size. these details are useful for tracking down who/what is querying the endpoint. consider adding these details to the log entry, or even writing a separate https:/httpd.apache.org/docs/trunk/logs.html#accesslog https:/httpd.apache.org/docs/trunk/logs.html#common. also consider writing log entries for all http requests (/metrics/snapshot produces no log entries).      i0319 18:06:18.824846 10521 http.cpp:478] http request for '/master/state.json'  ",3,train
MESOS-2528,Symlink the namespace handle with ContainerID for the port mapping isolator.,"this serves two purposes:  1) allows us to enter the network namespace using container id (instead of pid): ""ip netns exec / [commands] [args]"".  2) allows us to get container id for orphan containers during recovery. this will be helpful for solving mesos2367.    the challenge here is to solve it in a backward compatible way. i propose to create symlinks under /var/run/netns. for example:  /var/run/netns/containeridxxxx  > /var/run/netns/12345  (12345 is the pid)    the old code will only remove the bind mounts and leave the symlinks, which i think is fine since containerid is globally unique (uuid).",3,train
MESOS-2533,Support HTTP checks in Mesos health check program,"currently, only commands are supported but our health check protobuf enables users to encode http checks as well. we should wire up this in the health check program or remove the http field from the protobuf.",8,train
MESOS-2534,PerfTest.ROOT_SampleInit test fails.,"from mesos 2300 as well, it looks like this test is not reliable:      [ run      ] perftest.rootsampleinit  ../../src/tests/perftests.cpp:147: failure  expected: (0u) < (statistics.get().cycles()), actual: 0 vs 0  ../../src/tests/perftests.cpp:150: failure  expected: (0.0) < (statistics.get().taskclock()),      it looks like this test samples pid 1, which is either init or systemd. per a chat with  this should probably sample something that is guaranteed to be consuming cycles.",2,train
MESOS-2538,Remove unnecessary default flags from PortMappingMesosTest.,"as all the explicitly set flags are defaults, we can remove them and simplify the code.  mesos 2375 removed other occurrences of these default flags.",1,train
MESOS-2545,Developer guide for libprocess,create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in mesos with examples.     this could be similar to stout/readme.md.  ,2,train
MESOS-2547,Cleanup stale bind mounts for port mapping isolator during slave recovery.,"leaked bind mount under /var/run/netns for port mapping isolator is a known issue. there are many ways it can get leaked. for example, if the slave crashes after creating the bind mount but before creating the veth, the bind mount will be leaked. also, if the detached unmount does not finish in time and the subsequent os::rm fails, the bind mount will be leaked as well.    since leaked bind mount is inevitable, we need to clean them up during startup (slave recovery).",2,train
MESOS-2548,new `make distcheck` failures inside a docker container,"after the commits:    change #21    category  none  changed by  jie yu /  changed at  wed 25 mar 2015 00:12:14  repository  https:/gitwipus.apache.org/repos/asf/mesos.git  branch  master  revision  6c6473febac40be1e01c9ab005cca20ad2a48e18  comments    disallowed multiple cgroups base hierarchies in tests.  review: https:/reviews.apache.org/r/32452  changed files    src/tests/mesos.cpp  change #22    category  none  changed by  jie yu /  changed at  wed 25 mar 2015 00:15:37  repository  https:/gitwipus.apache.org/repos/asf/mesos.git  branch  master  revision  212b88c4d20a89dcd9f319b3be984f5646a47499  comments    allowed mesoscontainerizer to take empty isolation flag.  review: https:/reviews.apache.org/r/32467      numerous tests inside our internal ci started failing:    [ run      ] slaverecoverytest/0.recoverslavestate  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.recoverslavestate, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.recoverstatusupdatemanager  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.recoverstatusupdatemanager, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.reconnectexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.reconnectexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.recoverunregisteredexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.recoverunregisteredexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.recoverterminatedexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.recoverterminatedexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.recovercompletedexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.recovercompletedexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (23 ms)  [ run      ] slaverecoverytest/0.cleanupexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.cleanupexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.removenoncheckpointingframework  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.removenoncheckpointingframework, where typeparam = mesos::internal::slave::mesoscontainerizer (25 ms)  [ run      ] slaverecoverytest/0.noncheckpointingframework  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.noncheckpointingframework, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.killtask  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.killtask, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.reboot  20150325 00:32:56,830:40596(0x7f7cbf4f4700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:32810] zk retcode=4, errno=111(connection refused): server refused to accept the client  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.reboot, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.gcexecutor  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.gcexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.shutdownslave  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.shutdownslave, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.shutdownslavesigusr1  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.shutdownslavesigusr1, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.registerdisconnectedslave  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.registerdisconnectedslave, where typeparam = mesos::internal::slave::mesoscontainerizer (25 ms)  [ run      ] slaverecoverytest/0.reconcilekilltask  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.reconcilekilltask, where typeparam = mesos::internal::slave::mesoscontainerizer (24 ms)  [ run      ] slaverecoverytest/0.reconcileshutdownframework  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.reconcileshutdownframework, where typeparam = mesos::internal::slave::mesoscontainerizer (23 ms)  [ run      ] slaverecoverytest/0.reconciletasksmissingfromslave  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.reconciletasksmissingfromslave, where typeparam = mesos::internal::slave::mesoscontainerizer (25 ms)  [ run      ] slaverecoverytest/0.schedulerfailover  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.schedulerfailover, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.partitionedslave  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.partitionedslave, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.masterfailover  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.masterfailover, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.multipleframeworks  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.multipleframeworks, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.multipleslaves  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.multipleslaves, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [ run      ] slaverecoverytest/0.restartbeforecontainerizerlaunch  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] slaverecoverytest/0.restartbeforecontainerizerlaunch, where typeparam = mesos::internal::slave::mesoscontainerizer (26 ms)  [] 24 tests from slaverecoverytest/0 (596 ms total)    [] 4 tests from mesoscontainerizerslaverecoverytest  [ run      ] mesoscontainerizerslaverecoverytest.resourcestatistics  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] mesoscontainerizerslaverecoverytest.resourcestatistics (25 ms)  [ run      ] mesoscontainerizerslaverecoverytest.cgroupsrootperfrollforward  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootperfrollforward (24 ms)  [ run      ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespaceforward  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespaceforward (25 ms)  [ run      ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespacebackward  ../../src/tests/mesos.cpp:555: failure  value of: basehierarchy    actual: ""/sys/fs/cgroup/cpu,""  expected: basehierarchy  which is: ""/sys/fs/cgroup/""    multiple cgroups base hierarchies detected:    '/sys/fs/cgroup/'    '/sys/fs/cgroup/cpu,'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespacebackward (24 ms)  [ ] 4 tests from mesoscontainerizerslaverecoverytest (98 ms total)        [  failed  ] 28 tests, listed below:  [  failed  ] slaverecoverytest/0.recoverslavestate, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.recoverstatusupdatemanager, wh...",1,train
MESOS-2551,C++ Scheduler library should send Call messages to Master,"currently, the c library sends different messages to master instead of a single call message. to vet the new call api it should send call messages. master should be updated to handle all types of calls.",8,train
MESOS-2552,C++ Scheduler library should send HTTP Calls to master,"once the scheduler library sends call messages, we should update it to send calls as http requests to ""/call"" endpoint on master.",3,train
MESOS-2555,Document issue with slave recovery when using systemd.,as the problem encountered in mesos 2419 is a common problem with the default systemd configuration it would make sense to document this in the upgrade guide or somewhere else in the documentation.,1,train
MESOS-2559,Do not use RunTaskMessage.framework_id.,assume that frameworkinfo.id is always set and so need to read/set runtaskmessage.framework_id.    this should land after https:/issues.apache.org/jira/browse/mesos 2558 has been shipped.,1,train
MESOS-2562,0.24.0 release,the main feature of this release is going to be v1 (beta) release of the http scheduler api (part of mesos 2288 epic).    unresolved issues tracker: https:/issues.apache.org/jira/issues/?jql=project%20%3d%20mesos%20and%20status%20!%3d%20resolved%20and%20%22target%20version%2fs%22%20%3d%200.24.0%20order%20by%20status%20desc,5,train
MESOS-2571,Expose Memory Pressure in MemIsolator,nan,3,train
MESOS-2572,Add memory statistics tests.,nan,5,train
MESOS-2573,Use Memory Test Helper to improve some test code.,nan,2,train
MESOS-2574,Namespace handle symlinks in port_mapping isolator should not be under /var/run/netns,consider putting symlinks under /var/run/messo/netns. this is because 'ip' command assumes all files under /var/run/netns are valid namespaces without duplication and it has command like:    ip  all netns exec ip link    to list all links for each network namespace.,3,train
MESOS-2578,Add '{' on newline for function declarations in style checker,similar to mesos 2577; another common style mistake is to not move curly braces on a newline for function and class declarations:      class foo   };      vs      class foo    };      this should be easy to check with our style checker too.,1,train
MESOS-2579,0.22.1 release,nan,1,train
MESOS-2581,"Document tips, best practices, guidelines for doing code reviews.","we currently have a https:/github.com/apache/mesos/blob/0.22.0/docs/committersguide.md, however most of this information is relevant to all contributors looking to be participating in the code review process.    i'm proposing we extract much of this information into a more general ""code reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.    this would be a great prerequisite for onboarding more committers and adding http:/mailarchives.apache.org/mod_mbox/mesos dev/201502.mbox/%3cca+8rcoreugmvqoopsnb8wgybela5fhwpa=j=yhje22iwzvsbeq@mail.gmail.com%3e.    the committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers.",3,train
MESOS-2582,Create optional release step: update PyPi repositories,one of the build artifacts for a release is the python package `mesos.interface`. that needs to be uploaded to pypi along with a release to allow for users of python frameworks to use that version of mesos.,2,train
MESOS-2590,Let the slave control the duration of the perf sampler instead of relying on a sleep command.,"right now, we use a sleep command to control the duration of perf sampling:    sudo perf stat a x, logfd 1 pid 10940  sleep 10      this causes an additional process (i.e., the sleep process) to be forked and causes troubles for us to terminate the perf sampler once the slave exits (see mesos2462).    seems that the additional sleep process is not necessary. the slave can just monitor the duration and send a sigint to the perf process when duration elapsed. this will cause the perf process to output the stats and terminate.",3,train
MESOS-2591,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,refactor launchhelper and statisticshelper in portmappingtests to allow reuse,2,train
MESOS-2595,Create docker executor,"currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:     we need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker   in general, mesos slave is assuming that the executor is the one starting the actual task. but the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. this can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed.     overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management.",8,train
MESOS-2596,Update allocator docs,"once allocator interface changes, so does the way of writing new allocators. this should be reflected in mesos docs. the modules doc should mention how to write and use allocator modules. configuration doc should mention the new allocator flag.",2,train
MESOS-2598,Slave state.json frameworks.executors.queued_tasks wrong format?,"queuedtasks.executorid is expected to be a string and not a complete json object. it should have the very same format as the tasks array on the same level.    example, directly taken from slave               ....  ""queuedtasks"": [          ],        ""value"": ""cd stormmesos  && python bin/storm supervisor storm.mesos.mesossupervisor""      },      ""data"": """",      ""executorid"": ""stageingestionstatsslave1111428421145"",      ""frameworkid"": ""20150401160104251662508505021970002"",      ""name"": """",      ""resources"":     },    ""id"": ""srv4.hw.ca1.foo.com31708"",    ""name"": ""worker srv4.hw.ca1.foo.com:31708"",    ""resources"": ,    ""slave_id"": ""2015032702555321810807650504122 s0""  },  ...  ]      ",3,train
MESOS-2600,Add /reserve and /unreserve endpoints on the master for dynamic reservation,enable operators to manage dynamic reservations by introducing the /reserve and /unreserve http endpoints on the master.,5,train
MESOS-2607,Notify dev / user mailing list of the upcoming mem stat renames in 0.23.0 ,nan,2,train
MESOS-2613,Change docker rm command,"right now it seems mesos is using docker rm f id to delete containers so bind mounts are not deleted. this means thousands of dirs in /var/lib/docker/vfs/dir   i would like to have the option to change it to docker rm f v id this deletes bind mounts but not persistant volumes.    best,    mike",2,train
MESOS-2615,Pipe 'updateFramework' path from master to Allocator to support framework re-registration,"pipe the 'updateframework' call from the master through the allocator, as described in the design doc in the epic: mesos 703",1,train
MESOS-2622,Document the semantic change in decorator return values,"in order to enable decorator modules to remove metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.    the result/ return values means:    before  error is propagated to the call site  the result of the decorator is not applied  the result of the decorator is appended",1,train
MESOS-2627,ExamplesTest.PersistentVolumeFramework is flaky,"this just failed for the first time on our os x bot (far less frequent flaky than the other examplestest, but still flaky) while compiling master at commit f6620f851f635b3346c6ebf878152f38b3932ad9. there weren't any commits which touched / changed anything in the test in the set.      [ run      ] examplestest.persistentvolumeframework ../../src/tests/script.cpp:83: failure failed persistentvolumeframework_test.sh terminated with signal abort trap: 6   [  failed  ] examplestest.persistentvolumeframework (7865 ms)  ",1,train
MESOS-2629,Update style guide to disallow capture by reference of temporaries,"we modify the style guide to disallow constant references to temporaries as a whole. this means disallowing both (1) and (2) below.    background  1. constant references to simple expression temporaries do extend the lifetime of the temporary till end of function scope:   temporary returned by function:        / see full example below.    t f(const char s)              temporary constructed as simple expression:        / see full example below.            2. constant references to expressions that result in a reference to a temporary do not extend the lifetime of the temporary:     temporary returned by function:        / see full example below.    t f(const char s)                temporary constructed as simple expression:        / see full example below.            mesos case     in mesos we use future/ a lot. many of our functions return futures by value:        class socket          sometimes we capture these futures:                 sometimes we chain these futures:                 sometimes we do both:                reasoning   although (1) is ok, and considered a http:/herbsutter.com/2008/01/01/gotw88acandidateforthemostimportantconst/, (2) is extremely dangerous and leads to hard to track bugs.   if we explicitly allow (1), but disallow (2), then my worry is that someone coming along to maintain the code later on may accidentally turn (1) into (2), without recognizing the severity of this mistake. for example:    / original code:  const t& val = t();  std::cout /    class t     ~t()     const t& member() const      private:    const char str;  };    t f(const char  s)     int main()     output:     t(ok)   t(ok function)   t(bad!)   t(bad!)  + t(bad function!)   t(bad function!)  end of function scope...   t(ok function)    t(ok)  ",1,train
MESOS-2630,Remove capture by reference of temporaries in Stout,nan,1,train
MESOS-2631,Remove capture by reference of temporaries in libprocess,nan,1,train
MESOS-2633,Move implementations of Framework struct functions out of master.hpp,"to help reduce compile time and keep the header easy to read, let's move the implementations of the framework struct functions out of master.hpp",1,train
MESOS-2636,"Segfault in inline Try<IP> getIP(const std::string& hostname, int family)","we saw a segfault in production. attaching the coredump, we see:    core was generated by `/usr/local/sbin/mesosslave port=5051  resources=cpus:23;mem:70298;ports:[31'.  program terminated with signal 11, segmentation fault.  #0  0x00007f639867c77e in free () from /lib64/libc.so.6  (gdb) bt  #0  0x00007f639867c77e in free () from /lib64/libc.so.6  #1  0x00007f63986c25d0 in freeaddrinfo () from /lib64/libc.so.6  #2  0x00007f6399deeafa in net::getip (hostname=""/"", family=2) at ./3rdparty/stout/include/stout/net.hpp:201  #3  0x00007f6399e1f273 in process::initialize (delegate=unhandled dwarf expression opcode 0xf3  ) at src/process.cpp:837  #4  0x000000000042342f in main ()",1,train
MESOS-2637,"Consolidate 'foo', 'bar', ... string constants in test and example code","we are using 'foo', 'bar', ... string constants and pairs in src/tests/mastertests.cpp, src/tests/slavetests.cpp, src/tests/hooktests.cpp and src/examples/testhook_module.cpp for label and hooks tests. these values should be stored in local variables to avoid the possibility of assignment getting out of sync with checking for that same value.",2,train
MESOS-2645,Design doc for resource oversubscription,nan,13,train
MESOS-2646,Update Master to send revocable resources in separate offers,"master will send separate offers for revocable and non revocable/regular resources. this allows master to rescind revocable offers (e.g, when a new oversubscribed resources estimate comes from the slave) without impacting regular offers.",3,train
MESOS-2647,Slave should validate tasks using oversubscribed resources,the latest oversubscribed resource estimate might render a revocable task launch invalid. slave should check this and send tasklost with appropriate reason.    we need to add a new reason for this (reasonresource_oversubscribed?).,5,train
MESOS-2648,Update Resource Monitor to return resource usage,add usage() api call to return usage of all containers,3,train
MESOS-2649,Implement Resource Estimator,resource estimator is the component in the slave that estimates the amount of oversubscribable resources.    this needs to be integrated with the slave and resource monitor.,5,train
MESOS-2650,Modularize the Resource Estimator,modularizing the resource estimator opens up the door for org specific implementations.    test the estimator module.,3,train
MESOS-2651,Implement QoS controller,"this is a component of the slave that informs the slave about the possible ""corrections"" that need to be performed (e.g., shutdown container using recoverable resources).    this needs to be integrated with the resource monitor.    need to figure out the metrics used for sending corrections (e.g., scheduling latency, usage, informed by executor/scheduler)    we also need to figure out the feedback loop between the qos controller and the resource estimator.      class qoscontroller ;      ",3,train
MESOS-2652,Update Mesos containerizer to understand revocable cpu resources,"the cpu isolator needs to properly set limits for revocable and nonrevocable containers.    the proposed strategy is to use a twoway split of the cpu cgroup hierarchy  normal (nonrevocable) and low priority (revocable) subtrees   and to use a biased split of cfs cpu.shares across the subtrees, e.g., a 20:1 split (tbd). containers would be present in only one of the subtrees. cfs quotas will not be set on subtree roots, only cpu.shares. each container would set cfs quota and shares as done currently.  ",5,train
MESOS-2653,Slave should act on correction events from QoS controller,"slave might want to kill revocable tasks based on correction events from the qos controller.    the qos controller communicates corrections through a stream (or process::queue) to the slave which corrections it needs to carry out, in order to mitigate interference with production tasks.    the correction is communicated through a message:  [code]  message qoscorrection   optional string reason = x;  optional executorid executorid = x;  / optional taskid taskid = x;  }  [/code]    and the slave will setup a handler to process these events. initially, only executor termination is supported and cause the slave to issue 'containerizer >destroy()'.",8,train
MESOS-2654,Update FrameworkInfo to opt in to revocable resources,add a new field to frameworkinfo that lets the frameworks explicitly choose revocable offers  (for backwards compatibility).,1,train
MESOS-2655,Implement a stand alone test framework that uses revocable cpu resources,"ideally this would be an example framework (or stand alone binary like load generator framework) that helps us evaluate oversubscription in a real cluster.    we need to come up with metrics that need to be exposed by this framework for evaluation (e.g., how many revocable offers, rescinds, preemptions etc).",5,train
MESOS-2660,ROOT_CGROUPS_Listen and ROOT_IncreaseRSS tests are flaky,"[==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from cgroupsanyhierarchywithcpumemorytest  [ run      ] cgroupsanyhierarchywithcpumemorytest.rootcgroupslisten  failed to allocate rss memory: failed to lock memory, mlock: resource temporarily unavailable../../../mesos/src/tests/cgroupstests.cpp:571: failure  failed to wait 15secs for future  [  failed  ] cgroupsanyhierarchywithcpumemorytest.rootcgroupslisten (15121 ms)  [] 1 test from cgroupsanyhierarchywithcpumemorytest (15121 ms total)    [] global test environment teardown  [==========] 1 test from 1 test case ran. (15174 ms total)  [  passed  ] 0 tests.  [  failed  ] 1 test, listed below:  [  failed  ] cgroupsanyhierarchywithcpumemorytest.rootcgroups_listen",3,train
MESOS-2665,Fix queuing discipline wrapper in linux/routing/queueing ,"qdisc search function is dependent on matching a single hard coded handle and does not correctly test for interface, making the implementation fragile.  additionally, the current setup scripts (using dynamically created shell commands) do not match the hard coded handles.  ",5,train
MESOS-2671,Port mapping isolator causes SIGABRT during slave recovery.,"there is a bug in the code. if there are namespaces created by other party (say ip netns), the slave recovery will abort.",1,train
MESOS-2672,ContainerizerTest.ROOT_CGROUPS_BalloonFramework flaky,"  i0429 00:58:35.267629  2086 slave.cpp:3210] executor 'default' of framework 2015042900583016777343543220230000 terminated with signal aborted  i0429 00:58:35.270761  2086 slave.cpp:2512] handling status update tasklost (uuid: f969e3506f914fa9980e1852554bd704) for task 1 of framework 201  5042900583016777343543220230000 from @0.0.0.0:0  i0429 00:58:35.270983  2086 slave.cpp:4604] terminating task 1  w0429 00:58:35.271574  2080 containerizer.cpp:903] ignoring update for unknown container: 1298549aa3d246ffaad09dbc777affcc  i0429 00:58:35.272541  2074 statusupdatemanager.cpp:317] received status update tasklost (uuid: f969e3506f914fa9980e1852554bd704) for task 1 o  f framework 2015042900583016777343543220230000  i0429 00:58:35.272624  2074 statusupdatemanager.cpp:494] creating statusupdate stream for task 1 of framework 20150429005830167773435432202300  00  i0429 00:58:35.273217  2053 master.cpp:3493] executor default of framework 2015042900583016777343543220230000 on slave 2015042900583016777343  54322023s0 at slave(1)@10.35.12.124:5051 (smfdaki27sr1.devel.twitter.com): terminated with signal aborted        which is from       60    / we use mlock and memset here to make sure that the memory                                                                                     61    / actually gets paged in and thus accounted for.                                                                                                62    if (mlock(buffer, chunk) != 0)                                                                                                                                                 66                                                                                                                                                     67    if (memset(buffer, 1, chunk) != buffer)         this is the same as mesos2660: i've confirmed that swapping them fixed it.    ",1,train
MESOS-2673,Follow Google Style Guide for header file include order completely.,the header include order for mesos actually follows the google styleguide but omits step 1 without mentioning this exception in the mesos styleguide. this proposal suggests to adapt to the include order explained in the google styleguide i.e. include the direct headers first in the .cpp files implementing them.    a gist of the proposal can be found here:   https:/gist.github.com/joerg84/65cb9611d24b2e35b69b    the corresponding review board review can be found here:  https:/reviews.apache.org/r/33646/ ,5,train
MESOS-2680,Update modules doc with hook usage example,"modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",1,train
MESOS-2687,Add a slave flag to enable oversubscription,slave sends oversubscribable resources to master only when the flag is enabled.,2,train
MESOS-2688,Slave should kill revocable tasks if oversubscription is disabled,"if oversubscription is disabled on a restarted slave (that had it previously enabled), it should kill revocable tasks.    slave knows this information from the resources of a container that it checkpoints and recovers.    add a new reason oversubscription_disabled.",3,train
MESOS-2689,Slave should forward oversubscribable resources to the master,slave simply forwards resource estimates from resourceestimator to the master.    use a new message and handler on the master.     a slave flag for the interval between the messages.  ,5,train
MESOS-2691,Update Resource message to include revocable resources,"need to update resource message with a new subtype that indicates that the resource is revocable. it might also need to specify ""why"" it is revocable (e.g., oversubscribed).    also need to make sure all the operations on resource(s) takes this new message into account.",3,train
MESOS-2693,"Printing a resource should show information about reservation, disk etc","while new fields like diskinfo and reservationinfo have been added to resource protobuf, the output stream operator hasn't been updated to show these. this is valuable information to have in the logs during debugging.",1,train
MESOS-2695,Add master flag to enable/disable oversubscription,this flag lets an operator control cluster level oversubscription.     the master should send revocable offers to framework if this flag is enabled and the framework opts in to receive them.    master should ignore revocable resources from slaves if the flag is disabled.    need tests for all these scenarios.,5,train
MESOS-2696,Explore exposing stats from kernel,exploratory work.  additional tickets to follow.,5,train
MESOS-2697,Add a /teardown endpoint on master to teardown a framework,"we plan to rename ""/shutdown"" endpoint to ""/teardown"" to be compatible with the new api. ""/shutdown"" will be deprecated in 0.23.0 or later.",2,train
MESOS-2700,Determine CFS behavior with biased cpu.shares subtrees,"see this https:/issues.apache.org/jira/browse/mesos2652 for context.     understand the relationship between cpu.shares and cfs quota.   determine range of possible bias splits   determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50   rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (nonrevokable), and cpu intensive loads for the low biased tasks (revokable).    discover any performance edge cases?",13,train
MESOS-2701,Implement bi-level cpu.shares subtrees in cgroups/cpu isolator.,see this https:/issues.apache.org/jira/browse/mesos2652 for context.    # configurable bias  # change cgroup layout   implement rollforward migration path in isolator recover   document roll back migration path,8,train
MESOS-2702,Compare split/flattened cgroup hierarchy for CPU oversubscription,investigate if a flat hierarchy is sufficient for oversubscription of cpu or if a two way split is necessary/preferred.,3,train
MESOS-2703,Modularize the QoS Controller,modularize the qos controller to enable custom correction policies,3,train
MESOS-2704,Add tests for QoS controller corrections,nan,5,train
MESOS-2705,Add correct format template declarations to the styleguide,"the general rule to format templates is to declare them as:      template / / notice the space between template and <  class foo ;      however, the style is not documented anywhere nor it is inherited from the google style guide.",1,train
MESOS-2707,Incorrect zh:// URI scheme causes Slave to SegFault,"i have 4 slave nodes with the same hardware, operating system and mesos configuration.     few minutes ago, all 4 nodes were functioning well. i tried to change the config of master from 10.172.230.69:5050 to zh:/10.172.230.69:2181/mesos and restarted them in turn. the other three had started normally but the last one got a segmentation fault as you can see below.      [root@iz25to7d407z ]# mesosslave master=zh:/10.172.230.69:2181/mesos hostname=123.57.42.237 containerizers=docker,mesos quiet &  [1] 1216  [root@iz25to7d407z ]#  aborted at 1431085131 (unix time) try ""date d @1431085131"" if you are using gnu date   pc: @       0x3aede7b53c (unknown)   sigsegv (@0x0) received by pid 1216 (tid 0x7f12f984b820) from pid 0; stack trace:       @       0x3aee20f710 (unknown)      @       0x3aede7b53c (unknown)      @       0x3aedecf630 (unknown)      @     0x7f12fce1593f net::getip()      @     0x7f12fce507ae process::operator>>()      @     0x7f12fce50107 process::upid::upid()      @     0x7f12fc52af71 mesos::internal::masterdetector::create()      @           0x4b1290 main      @       0x3aede1ed5d (unknown)      @           0x4b00b9 (unknown)    [1]+  segmentation fault      mesosslave master=zh:/10.172.230.69:2181/mesos hostname=123.57.42.237 containerizers=docker,mesos  quiet  ",2,train
MESOS-2708,Design doc for the Executor HTTP API,this tracks the design of the executor http api.  ,2,train
MESOS-2709,Design Master discovery functionality for HTTP-only clients,"when building clients that do not bind to libmesos and only use the http api (via ""pure"" language bindings  eg, javaonly) there is no simple way to discover the master's ip address to connect to.    rather than relying on 'outofband' configuration mechanisms, we would like to enable the ability of interrogating the zookeeper ensemble to discover the master's ip address (and, possibly, other information) to which the http api requests can be addressed to.",3,train
MESOS-2719,Deprecating '.json' extension in master endpoints urls,add an endpoint for each master endpoint with a '.json' extension such as `/master/stats.json` so it becomes `/master/stats` after a deprecation cycle.,1,train
MESOS-2720,Implement protobufs for master operator endpoints,we should define protobufs for master operator endpoints so as to provide a structure we can refer to for each possible return from an endpoint.  ,2,train
MESOS-2721,"Architecture document for per-container IP assignment, enforcement and isolation","there are many ways in which we can go around wiring up percontainer ips in mesos.    as there are multiple underlying mechanisms and systems for keeping track of ip pools, we probably need to aim for a very flexible architecture, similar to the oversubscription project.    there are a couple of folks, companies and vendors interested in getting this capability into mesos asap to provide a stronger networking story (https:/ so let's start discussing and architecting this.",13,train
MESOS-2722,"Create access to the Mesos ""state abstraction"" that does not require linking with libmesos","see ""src/state/state.hpp"" and ""src/java/src/org/apache/mesos/state/ .java"" for what the ""state abstraction"" is.    with the new http api (see mesos2288, mesos2289), there will be no need to link to libmesos to a framework for it to communicate with a mesos master. however, if a framework uses the mesos ""state abstraction"", either directly in c or through other language bindings (e.g., java), it still needs to link with libmesos. so, in order to achieve libmesosfree frameworks that can leverage all apis mesos has to offer, we need a different way to access the ""state abstraction"".         one approach is to provide an http api for state queries that get routed through the mesos master, which relays them by making calls into libmesos. details tbd, including how separate this will be from the general http api.  ",13,train
MESOS-2726,Add support for enabling network namespace without enabling the network isolator,"following the discussion kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies).  following the pattern of enabling pid namespaces (isolation=""namespaces/pid""). one possible solution could be to add another one for network i.e. ""namespaces/network"".    ",13,train
MESOS-2729,Update DRF sorter to update total resources,"drf sorter currently keeps track of allocated resources and total resources, but there is no way to update the total resources. for oversubscription, we need the ability to update total resources because total oversubscribed resources change overtime.",2,train
MESOS-2730,Add a new API call to the allocator to update oversubscribed resources,this tracks just the work of adding the api call to the allocator interface.    master makes this call on the allocator whenever it gets a new oversubscribed resources estimate from the slave.,2,train
MESOS-2733,Update master to handle oversubscribed resource estimate from the slave,"whenever the master gets a new oversubscribed resources estimate from the slave, it should rescind any outstanding revocable offers (with oversubscribed resources) from that slave. it should then call the allocator to update the oversubscribed resources.",3,train
MESOS-2734,Update allocator to allocate revocable resources,the simplest way to add support for oversubscribed resources to the allocator is to simply add them to the already existing 'slave.total' and 'slave.available' variables. it is easy to distinguish the revocable resources by doing a .revocable() filter.       ,5,train
MESOS-2735,Change the interaction between the slave and the resource estimator from polling to pushing ,"this will make the semantics more clear. the resource estimator can control the speed of sending resources estimation to the slave.    to avoid cyclic dependency, slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when there's a new estimation ready. the callback will be a defer to the slave's main event queue.",3,train
MESOS-2736,Upgrade the design of MasterInfo,"currently, the masterinfo pb only supports an ip field as an int32.    beyond making it harder (and opaque; open to subtle bugs) for languages other than c/c to decode into an ipv4 octets, this does not allow mesos to support ipv6 master nodes.    we should consider ways to upgrade it in ways that permit us to support both ipv4 / ipv6 nodes, and, possibly, in a way that makes it easy for languages such as java/python that already have pb support, so could easily deserialize this information.    see also mesos 2709 for more info.",3,train
MESOS-2737,Add documentation for maintainers.,"in order to scale the number of committers in the project, we proposed the concept of maintainers here:    http:/markmail.org/thread/cjmdn3d7qfzbxhpm    to follow up on that proposal, we'll need some documentation to capture the concept of maintainers. both how contributors can benefit from maintainer feedback and the expectations of ""maintainer ship"".    in order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",3,train
MESOS-2738,Reported used resources for tasks in frameworks do not match slave tally, recently observed that according to the master's and the slave's state.json summing up the resources allocated to tasks from different frameworks on a slave does not always match the total that is reported for the slave. the latter number is sometimes higher.    it would be desirable for tools that display allocation statistics to find balanced tallies.  ,3,train
MESOS-2741,Exposing Resources along with ResourceStatistics from resource monitor,"right now, the resource monitor returns a usage which contains containerid, executorinfo and resourcestatistics. in order for resource estimator/qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the resources that are currently assigned to the container.    this requires us the change the containerizer interface to get the resources as well while calling 'usage()'.",5,train
MESOS-2742,Architecture doc on global resources,nan,3,train
MESOS-2743,Include ExecutorInfos for custom executors in master/state.json,"the slave/state.json already reports executorinfos:  https:/github.com/apache/mesos/blob/0.22.1/src/slave/http.cpp#l215 219    would be great to see this in the master/state.json as well, so external tools don't have to query each slave to find out executor resources, sandbox directories, etc.",3,train
MESOS-2746,As a Framework User I want to be able to discover my Task's IP,"the information exposed by the framework via the webuiurl does not always resolves to a routable endpoint (eg, when the hostname is not publicly resolvable, or resolvable at all).    in order to facilitate service discovery (via, eg, marathon ui) we want to add the information in frameworkspid via the /state summary endpoint.",3,train
MESOS-2748,/help generated links point to wrong URLs,"as reported by michael lune / (see also mesos329 and mesos913 for background):      in mesos/3rdparty/libprocess/src/help.cpp a markdown file is created, which is then converted to html through a javascript library     all endpoints point to /help/..., they need to work dynamically for reverse proxy to do its thing. /mesos/help works, and displays the endpoints, but they each need to go to their respective /help/... endpoint.     note that this needs to work both for master, and for slaves. i think the route to slaves help is something like this: /mesos/slaves/20150518210216169502762850501366 s0/help, but please double check this.      the fix appears to be not too complex (as it would require to simply manipulate the generated url) but a quick skim of the code would suggest that something more substantial may be desirable too.",2,train
MESOS-2750,Extend queueing discipline wrappers to expose network isolator statistics,export traffic control statistics in queueing library to enable reporting out impact of network bandwidth statistics.,3,train
MESOS-2752,Add HTB queueing discipline wrapper class,network isolator uses a hierarchical token bucket (htb) traffic control discipline on the egress filter inside each container as the root for adding traffic filters.  a htb wrapper is needed to access the network statistics for this interface.,3,train
MESOS-2753,Master should validate tasks using oversubscribed resources,current implementation out for https:/reviews.apache.org/r/34310 only supports setting the priority of containers with revocable cpu if it's specified in the initial executor info resources. this should be enforced at the master.    also master should make sure that oversubscribed resources used by the task are valid.,3,train
MESOS-2754,Reduce multiple use of string literals,"we have several instances of string literals (e.g. ""mesos containerizer"", ""nettcp""rttmicroseconds_p50"") being used in multiple locations where mismatches would result in correctness issues.  we should replace these with a single definition to reduce the risk.",1,train
MESOS-2756,Update style guide: Avoid object slicing,"in order to improve the safety of our code base, let's augment the style guide to:  ""disallow public construction of base classes""  so that we can avoid the object slicing problem. this is a good pattern to follow in general as it prevents subtle semantic bugs like the following:    #include /  #include /    class base     virtual int get() const     protected:    int v;  };    class derived : public base     virtual int get() const   };    int main()   }  ",1,train
MESOS-2757,"Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.",let's add operator overloads to option/ to allow access to the underlying t using the ` >` operator.  ,3,train
MESOS-2758,Reflect in documentation that isolator flags are only relevant for Mesos Containerizer,the isolator flags are only relevant when using the mesos containerizer. we should reflect this in the flag description to avoid confusion.,1,train
MESOS-2760,Add correction message to inform slave about QoS Controller actions,"the qos controller informs the slave about correcting actions (kill, resize, throttle besteffort containers, tasks, and so forth) through a protobuf message, called a qoscorrection. this ticket tracks designing and creating this message.    for example:    message qoscorrection     /kill action which will be performed on an executor    message kill       required type action = 1;    optional string reason = 2;    optional double timestamp = 3;    optional kill kill = 4;  }  ",1,train
MESOS-2761,Delegating constructors are not allowed by styleguide,"as of right now the styleguide does not allow delegating constructors (being a c 11 feature).  they are already used in the code base (e.g. stout/option.hpp), are supported by all relevant compiler (gcc 4.7 and clang 3.0), and enhance readability.   therefore we should officially whitelist them in the styleguide.",1,train
MESOS-2762,Explicitly-defaulted functions are not allowed by styleguide,"as of right now the styleguide does not allow explicitly defaulted functions (being a c 11 feature).  they enhance readability, are supported by all relevant compiler (gcc 4.4 and clang 3.0), and are introduced by some patches (e.g. https:/reviews.apache.org/r/34277/).   therefore we should officially whitelist them in the styleguide.",1,train
MESOS-2763,Consolidate functionality in stout/net and process/http,"stout/net.hpp and process/http.hpp offer overlapping functionality that could be consolidated in one place, presumably the latter, since it is more elaborate to begin with. this would also remove the dependency of the former on libcurl.    while we are at it, we could then turn net::contentlength() into a generalized, asynchronous process::http::head() call.    (prerequisite: mesos 2247, with the suggestion to enhance process::http, not stout, see a comment in that jira.)  ",8,train
MESOS-2764,Allow Resource Estimator to get Resource Usage information.,"this includes two things:  1) we need to expose resourcemonitor::usage so that module writers can access it. we could define a protobuf message for that.  2) we need to allow resourceestimator to call 'resourcemonitor::usages()'. we could either expose the resourcemonitor, or pass in a lambda to the resources estimator.",5,train
MESOS-2766,Add validation behavior to FlagsBase,"in every ""launcher"" file (ie, those containing some variation on main()) there is a minor variation on:      if (flags.help)       as this is default behavior, and we've added support for the help flag in the flagsbase class, we should add this too there and remove it from everywhere else.    additionally, a recurring behavior is checking for the presence of a required flag:    if (flags.master.isnone())       or some variation thereof: we should add automatic validation for required flags during parsing.    this follows the dry principle.",1,train
MESOS-2769,Metric for cpu scheduling latency from all components,"the metric will provide statistics on the scheduling latency for processes/threads in a container, i.e., statistics on the delay before application code can run. this will be the aggregate effect of the normal scheduling period, contention from other threads/processes, both in the container and on the system, and any effects from the cfs bandwidth control (if enabled) or other cpu isolation strategies.",8,train
MESOS-2770,Slave should forward total amount of oversubscribed resources to the master,"in addition to the unallocated oversubscribed resources, the slave should also send the oversubscribed resources that are already allocated.    this is needed by the master/allocator to accurately calculate the available oversubscribed resources to offer.",3,train
MESOS-2771,SIGSEGV received during ResourceMonitorProcess::usage(),"observed in production.      i0523 17:03:59.830229 56587 portmapping.cpp:2616] freed ephemeral ports [33792,34816) for container with pid 47791  i0523 17:03:59.849773 56587 portmapping.cpp:2764] successfully performed cleanup for pid 47791   aborted at 1432400641 (unix time) try ""date d @1432400641"" if you are using gnu date   pc: @     0x7f100fcbfd85 znst17functionhandlerifvrksseznk7process6futurein5mesos8internal5slave15resourcemonitor5usageee8onfailedizns722resourcemonitorprocess5usageens511containerideeuls1eveerksaotnsa6prefereeuls1ee9minvokeerkst9anydatas1  i0523 17:03:59.898959 56587 slave.cpp:3246] executor 'thermos1432400210944mesostestexhaustdiskspace54744d0fbe0a14e40bb2256bd5cbd9524' of framework 20110328224700000000190000 terminated with signal killed  i0523 17:04:03.419869 56587 slave.cpp:2547] handling status update taskfailed (uuid: 3be19404f7374a70a330d1d924a85dbb) for task 1432400210944mesostestexhaustdiskspace54744d0fbe0a14e40bb2256bd5cbd9524 of framework 20110328224700000000190000 from @0.0.0.0:0  i0523 17:04:03.773061 56587 slave.cpp:4077] received a new estimation of the oversubscribable resources   i0523 17:04:03.773907 56587 slave.cpp:4077] received a new estimation of the oversubscribable resources   i0523 17:04:03.774683 56587 slave.cpp:4077] received a new estimation of the oversubscribable resources   i0523 17:04:03.776345 56587 slave.cpp:4077] received a new estimation of the oversubscribable resources    sigsegv (@0x0) received by pid 56573 (tid 0x7f100a190940) from pid 0; stack trace:       @     0x7f100f181ca0 (unknown)      @     0x7f100fcbfd85 znst17functionhandlerifvrksseznk7process6futurein5mesos8internal5slave15resourcemonitor5usageee8onfailedizns722resourcemonitorprocess5usageens511containerideeuls1eveerksaotnsa6prefereeuls1ee9minvokeerkst9anydatas1      @     0x7f100fb01506 process::internal::run/()      @     0x7f100fcc701b process::future/::fail()      @     0x7f100fccfbde process::internal::thenf/()      @     0x7f100fd64bee zn7process8internal3runist8functionifvrkns6futurein5mesos18resourcestatisticseeeeejrs6eeevrkst6vectoritsaisdeedpot0      @     0x7f100fd656dd process::future/::fail()      @     0x7f100fd6c332 process::promise/::associate()      @     0x7f100fe2777e znst17functionhandlerifvpn7process11processbaseeezns08dispatchin5mesos18resourcestatisticsens58internal5slave25mesoscontainerizerprocesserkns511containeridesaeens06futureiteerkns03pidit0eemshfsft1et2euls2ee9minvokeerkst9anydatas2      @     0x7f101015561a process::processmanager::resume()      @     0x7f10101558dc process::schedule()      @     0x7f100f17983d startthread      @     0x7f100e96bfcd clone  /usr/local/bin/mesosslave.sh: line 102: 56573 segmentation fault      (core dumped) $debug /usr/local/sbin/mesosslave ""$""  slave exit status: 139        thread 20 (thread 0x7f100a190940 (lwp 56574)):  #0  mdata (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/bits/basicstring.h:293  #1  mrep (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/bits/basicstring.h:301  #2  size (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/bits/basicstring.h:716  #3  operator/, std::allocator/ > (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/bits/basicstring.h:2758  #4  operator/, std::allocator/ >&), process::future/::onfailed(f&&, process::future/::prefer) const [with f = mesos::internal::slave::resourcemonitorprocess::usage(mesos::containerid)::lambda180; / = void; t = mesos::internal::slave::resourcemonitor::usage]::lambda2>::minvoke(const std::anydata &, const std::basicstring/, std::allocator/ > &) (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/functional:2071  #8  0x00007f100fb01506 in process::internal::run/&)>, std::basicstring/, std::allocator/ >&>(const std::vector/, std::allocator/ >&)>, std::allocator/, std::allocator/ >&)> > > &) (callbacks=std::vector of length 1, capacity 1 = )      at ../3rdparty/libprocess/include/process/future.hpp:420  #9  0x00007f100fcc701b in process::future/::fail (this=0x7f0ffc185ca8, message=""unknown container: c0ab6cd3fe4f49bd8dd632b388fcfab2"")      at ../3rdparty/libprocess/include/process/future.hpp:1406  #10 0x00007f100fccfbde in fail (f=unhandled dwarf expression opcode 0xf3  ) at ../3rdparty/libprocess/include/process/future.hpp:649  #11 process::internal::thenf/(const std::function/(const mesos::resourcestatistics&)> &, const std::sharedptr/ > &, const process::future/ &) (f=unhandled dwarf expression opcode 0xf3  ) at ../3rdparty/libprocess/include/process/future.hpp:1193  #12 0x00007f100fd64bee in operator() (callbacks=std::vector of length 1, capacity 1 = ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/functional:2464  #13 process::internal::run/&)>, process::future/&>(const std::vector/&)>, std::allocator/&)> > > &) (callbacks=std::vector of length 1, capacity 1 = ) at ../3rdparty/libprocess/include/process/future.hpp:420  #14 0x00007f100fd656dd in process::future/::fail (this=0x7f0ff8046230, message=""unknown container: c0ab6cd3fe4f49bd8dd632b388fcfab2"") at ../3rdparty/libprocess/include/process/future.hpp:1407  #15 0x00007f100fd6c332 in onfailed (this=unhandled dwarf expression opcode 0xf3  ) at ../3rdparty/libprocess/include/process/future.hpp:1121  #16 onfailed/::)(const std::basicstring/&)>(process::future/, std::placeholder/)>, bool> (this=unhandled dwarf expression opcode 0xf3  )      at ../3rdparty/libprocess/include/process/future.hpp:221  #17 onfailed/::)(const std::basicstring/&)>(process::future/, std::placeholder/)> > (this=unhandled dwarf expression opcode 0xf3  )      at ../3rdparty/libprocess/include/process/future.hpp:270  #18 process::promise/::associate (this=unhandled dwarf expression opcode 0xf3  ) at ../3rdparty/libprocess/include/process/future.hpp:635  #19 0x00007f100fe2777e in operator() (functor=unhandled dwarf expression opcode 0xf3  ) at ../3rdparty/libprocess/include/process/dispatch.hpp:239  #20 std::functionhandler/&, process::future/ (t::)(p0), a0) [with r = mesos::resourcestatistics; t = mesos::internal::slave::mesoscontainerizerprocess; p0 = const mesos::containerid&; a0 = mesos::containerid]::lambda21>::minvoke(const std::anydata &, process::processbase  ) (functor=unhandled dwarf expression opcode 0xf3  ) at /opt/rh/devtoolset2/root/usr/include/c/4.8.2/functional:2071  #21 0x00007f101015561a in process::processmanager::resume (this=0xc24d20, process=0x7f0ffc0169b0) at src/process.cpp:2172  #22 0x00007f10101558dc in process::schedule (arg=unhandled dwarf expression opcode 0xf3  ) at src/process.cpp:602  #23 0x00007f100f17983d in startthread () from /lib64/libpthread.so.0  #24 0x00007f100e96bfcd in clone () from /lib64/libc.so.6  ",1,train
MESOS-2772,Define protobuf for ResourceMonitor::Usage.,we need to expose resourcemonitor::usage so that module writers can access it. we will define a protobuf message for that.,1,train
MESOS-2775,Slave should expose metrics about oversubscribed resources,metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). ,2,train
MESOS-2776,Master should expose metrics about oversubscribed resources,metrics/snapshot should expose metrics on oversubscribed resources (allocated and available).  ,5,train
MESOS-2778,Non-POD static variables used in fq_codel and ingress.,we declare const non pod static variables for the following:    fq_codel::handle  ingress::root  ingress::handle    we can eliminate the risk of indeterminate initialization by converting to c11 constexpr,1,train
MESOS-2781,getQdisc function in routing::queueing::internal.cpp returns incorrect qdisc,the getqdisc function ignores the passed link parameter and returns the first qdisc of the required type from any available interface.,1,train
MESOS-2783,document the fetcher,"for framework developers specifically, mesos provides a fetcher to move binaries. this needs mvp documentation.     what is it   how does it help   what protocols or schemas are supported   can it be extended    this is important to get framework developers over the hump of learning to code against mesos and grow the ecosystem.",5,train
MESOS-2784,Added constexpr to C++11 whitelist.,constexpr is currently used to eliminate initialization dependency issues for non pod objects.  we should add it to the whitelist of acceptable c11 features in the style guide.,1,train
MESOS-2791,Create a FixedResourceEstimator to return fixed amount of oversubscribable resources.,"this will be useful for testing oversubscription in a real environment. also, it will be useful for people who has a prior knowledge about the amount of resources that can be safely oversubscribed on each slave.",5,train
MESOS-2792,Remove duplicate literals in ingress & fq_codel queueing disciplines,"fqcodel and ingress queueing disciplines include multiple uses of the string literals ""ingress"" and ""fqcodel"".  any mismatch in these would cause runtime errors which can be prevented at compile time.",1,train
MESOS-2793,Add support for container rootfs to Mesos isolators,mesos containers can have a different rootfs to the host. update isolator interface to pass rootfs during isolator::prepare(). update isolators where  necessary.,1,train
MESOS-2794,Implement filesystem isolators,"move persistent volume support from mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.    use symlinks for posix systems without container rootfs. use bind mounts for linux with/without container rootfs.",13,train
MESOS-2795,Introduce filesystem provisioner abstraction,"optional filesystem provisioner component for the mesos containerizer that can provision per container filesystems.    this is different to a filesystem isolators because it just provisions a root filesystem for a container and doesn't actually do any isolation (e.g., through a mount namespace + pivot or chroot).",5,train
MESOS-2796,Implement AppC image provisioner.,implement a filesystem provisioner that can provision container images compliant with the application container image (aci) https:/github.com/appc/spec.,5,train
MESOS-2798,"Export statistics on ""unevictable"" memory",nan,1,train
MESOS-2800,Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function,"as suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies.   if we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.    as of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",3,train
MESOS-2801,Remove dynamic allocation from Future<T>,remove the dynamic allocation of `t ` inside `future::data`,3,train
MESOS-2804,Log framework capabilities in the master.,"now that capabilities has been added to frameworkinfo, we should log these in the master when a framework (re )registers (i.e. which capabilities are enabled and disabled). this would make debugging easier for framework developers.    ideally, folding in the old checkpoint capability and logging that as well. in the past, the fact that checkpoint defaults to false has tripped up a lot of developers.",1,train
MESOS-2805,Make synchronized as primary form of synchronization.,"re organize synchronized to allow synchronized(m) to work on:    1. std::mutex    2. std::recursivemutex    3. std::atomicflag    move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.    remove references to internal.hpp and replace them with std::atomic_flag synchronization.",8,train
MESOS-2806,Jira workflow appears inconsistent,"see attached screenshot   the story is in the accepted state, so it should now have a start progress button, but it has a stop progress one instead.    also, when in the in progress it has an accept button (i think) or something similar; also other states appear inconsistent.    this story is about first looking at the workflow; ensuring the stories and their status(es) are consistent; that button in the ui are consistently applied and then correct any issues that may have been identified.    the assumption here is that the workflow is:    open >> accepted >> progress >> reviewable >> resolved >> closed     accept       start      ready         resolve      close    and, at each stage, it can be moved ""back by one"" (unaccept, stop progress, unresolve) and that, at any stage, it can be moved to closed (for whatever reason).",2,train
MESOS-2807,As a developer I need an easy way to convert MasterInfo protobuf to/from JSON,"as a preliminary to mesos 2340, this requires the implementation of a simple (de)serialization mechanism to json from/to masterinfo protobuf.",3,train
MESOS-2808,Slave should call into resource estimator whenever it wants to forward oversubscribed resources,"currently, the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources.    now that the slave only sends updates when there is a change from the previous estimate, it can just poll the resource estimator whenever it wants to send an estimate. one advantage with this is that if the estimator is slow to respond, the slave doesn't keep forwarding estimates with the stale 'oversubscribable' value causing more revocable tasks to be unintentionally launched.",3,train
MESOS-2814,os::read should have one implementation,"in master there are currently three implementations of the function:   https:/github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#l42  https:/github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#l82  https:/github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#l42    all of them have fairly radically different implementations (one uses c read(), one uses c ifstream, one uses c fopen)    the read() based one does an excess / unnecessary copy / buffer allocation (it is going to read into one temporary buffer, then copy into the result string. would be more efficient to do a .reserve() on the result string, and then fill the result buffer).    the ifstream/ifstreambuf_iterator ignores that you can have an error partially through reading a file / doesn't find the error or propagate it up.    the fopen() variant reads one newline separated line at a time. this could produce interesting / unexpected reading in the context of a binary file. it also causes glibc to insert null bytes at the end of the buffer it reads (excess computation). result isn't pre allocated to be the right length, meaning that most of the continually read lines will result in realloc() and a lot of memory copies which will be inefficient on large files.",3,train
MESOS-2815,Flaky test: FetcherCacheHttpTest.HttpCachedSerialized,"fetchercachehttptest.httpcachedserialized has been observed to fail (once  so far), but normally works fine. here is the failure output:    [ run      ] fetchercachehttptest.httpcachedserialized    gmock warning:  uninteresting mock function call  returning directly.      function call: resourceoffers(0x3cca8e0, @0x2b1053422b20 )  stack trace:  f0604 13:08:16.377907  6813 fetchercachetests.cpp:354] checkready(offers): is pending failed to wait for resource offers   check failure stack trace:       @     0x2b10488ff6c0  google::logmessage::fail()      @     0x2b10488ff60c  google::logmessage::sendtolog()      @     0x2b10488ff00e  google::logmessage::flush()      @     0x2b1048901f22  google::logmessagefatal::logmessagefatal()      @           0x9721e4  checkfatal::checkfatal()      @           0xb4da86  mesos::internal::tests::fetchercachetest::launchtask()      @           0xb53f8d  mesos::internal::tests::fetchercachehttptesthttpcachedserializedtest::testbody()      @          0x116ac21  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x1165e1e  testing::internal::handleexceptionsinmethodifsupported/()      @          0x114e1df  testing::test::run()      @          0x114e902  testing::testinfo::run()      @          0x114ee8a  testing::testcase::run()      @          0x1153b54  testing::internal::unittestimpl::runalltests()      @          0x116ba93  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x1166b0f  testing::internal::handleexceptionsinmethodifsupported/()      @          0x1152a60  testing::unittest::run()      @           0xcbc50f  main      @     0x2b104af78ec5  (unknown)      @           0x867559  (unknown)  make[4]:  [checklocal] aborted  make[4]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesosreviewbot/mesos0.23.0/build/src'  make[3]:  [checkam] error 2  make[3]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesosreviewbot/mesos0.23.0/build/src'  make[2]:  [check] error 2  make[2]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesosreviewbot/mesos0.23.0/build/src'  make[1]:  [checkrecursive] error 1  make[1]: leaving directory `/home/jenkins/jenkinsslave/workspace/mesosreviewbot/mesos 0.23.0/_build'  make:   [distcheck] error 1  ",2,train
MESOS-2817,Support revocable/non-revocable CPU updates in Mesos containerizer,"mesos 2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. improve this to support updates to/from revocable cpu. note, any revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. higher level logic is responsible for adding/removing based on some policy.",3,train
MESOS-2818,Pass 'allocated' resources for each executor to the resource estimator.,"resource estimator obviously need this information to calculate, say the usage slack. now the question is how. there are two approaches:    1) pass in the allocated resources for each executor through the 'oversubscribable()' interface.    2) let containerizer return total resources allocated for each container when 'usages()' are invoked.    i would suggest to take route (1) for several reasons:    1) eventually, we'll need to pass in slave's total resources to the resource estimator (so that re can calculate allocation slack). there is no way that we can get that from containerizer. the slave's total resources keep changing due to dynamic reservation. so we cannot pass in the slave total resources during initialization.    2) the current implementation of usages() might skip some containers if it fails to get statistics for that container (not an error). this will cause incomplete information to the re.    3) we may want to calculate 'unallocated = total  allocated' so that we can send allocation slack as well. getting 'total' and 'allocated' from two different components might result in inconsistent value. remember that 'total' keeps changing due to dynamic reservation.",3,train
MESOS-2821,Document and consolidate qdisc handles,the structure of traffic control qdiscs and filters in non trivial with the knowledge of which handles are the parents of which filters or qdiscs are in the create and recovery functions and will be needed to collect statistics on the links.  lets pull out the constants and document them.,1,train
MESOS-2822,Add `EXPECT_NO_FUTURE_DISPATCHES` macro for tests.,"we already have expectnofuturemessages, expectnofuturedispatches should be done the same way.    we already have a use case for it: https:/github.com/apache/mesos/blob/master/src/tests/mastercontenderdetector_tests.cpp#l251",1,train
MESOS-2823,Pass callback to the QoS Controller to retrieve ResourceUsage from Resource Monitor on demand.,we need to allow qos controller to call 'resourcemonitor::usages()'. we will pass it in a lambda.  ,2,train
MESOS-2824,Support pre-fetching images,"default container images can be specified with the defaultcontainerinfo flag to the slave. this may be a large image that will take a long time to initially fetch/hash/extract when the first container is provisioned. add optional support to start fetching the image when the slave starts and consider not registering until the fetch is complete.    to extend that, we should support an operator endpoint so that operators can specify images to pre fetch.",5,train
MESOS-2830,Add an endpoint to slaves to allow launching system administration tasks,"as a system administrator often times i need to run a organizationmandated task on every machine in the cluster. ideally i could do this within the framework of mesos resources if it is a ""cleanup"" or auditing task, but sometimes i just have to run something, and run it now, regardless if a machine has unaccounted resources  (ex: adding/removing a user).    currently to do this i have to completely bypass mesos and ssh to the box. ideally i could tell a mesos slave (with proper authentication) to run a container with the limited special permissions needed to get the task done.",8,train
MESOS-2832,Enable configuring Mesos with environment variables without having them leak to tasks launched,"currently if mesos is configured with environment variables (mesos_modules), those show up in every task which is launched unless the executor explicitly cleans them up.     if the task being launched happens to be something libprocess / mesos based, this can often prevent the task from starting up (a scheduler has issues loading a module intended for the slave).    there are also cases where it would be nice to be able to change what the path is that tasks launch with (the host may have more in the path than tasks are supposed to / allowed to depend upon).",8,train
MESOS-2834,Support different perf output formats,the output format of perf changes in 3.14 (inserting an additional field) and in again in 4.1 (appending additional) fields. see kernel commits:  410136f5dd96b6013fe6d1011b523b1c247e1ccb  d73515c03c6a2706e088094ff6095a3abefd398b    update the perf::parse() function to understand all these formats.,3,train
MESOS-2836,Report per-container metrics for network bandwidth throttling to the slave,report percontainer metrics for network bandwidth throttling to the slave in the output of mesosnetwork helper.,1,train
MESOS-2837,Decode network statistics from mesos-network-helper,decode network statistics from mesosnetworkhelper and output to slave statistics.json,1,train
MESOS-2838,In Resources JSON model() resources of the same name overwrite each other.,"as shown here: https:/github.com/apache/mesos/blob/8559d7b7356ec91795e564767588c6f4519653a5/src/common/http.cpp#l50    so if there are two ""cpus"" of different roles, whichever comes later will overwrite the previous.    we should instead aggregate different resources of the same name.    however, in the presence of revocable resources, in order to maintain backwards compatibility we should exclude revocable resources.",2,train
MESOS-2841,"FrameworkInfo should include a Labels field to support arbitrary, lightweight metadata","a framework instance may offer specific capabilities to the cluster: storage, smartlybalanced request handling across deployed tasks, access to 3rd party services outside of the cluster, etc. these capabilities may or may not be utilized by all, or even most mesos clusters. however, it should be possible for processes running in the cluster to discover capabilities or features of frameworks in order to achieve a higher level of functionality and a more seamless integration experience across the cluster.    a rich discovery api attached to the frameworkinfo could result in some form of early lockin: there are probably many ways to realize crossframework integration and external services integration that we haven't considered yet. rather than overspecify a discovery info message type at the framework level i think frameworkinfo should expose a very generic way to supply metadata for interested consumers (other processes, tasks, etc).    adding a labels field to frameworkinfo reuses an existing message type and seems to fit well with the overall intent: attaching generic metadata to a framework instance. these labels should be visible when querying a mesos master's state.json endpoint.",8,train
MESOS-2844,Add and document new labels field to framework info,add and document new labels field to framework info:      message frameworkinfo   ,1,train
MESOS-2848,Local filesystem docker image discovery,"given a docker image name and the local directory where images can be found, creates a uri with a path to the corresponding image.    done when system successfully checks for the image, untars the image if necessary, and returns the proper uri to the image.",2,train
MESOS-2849,Implement Docker local image store,"given a local docker image name and path to the image or image tarball, fetches the image's dependent layers, untarring if necessary. it will also parse the image layers' configuration json and place the layers and image into persistent store.    done when a docker image can be successfully stored and retrieved using 'put' and 'get' methods. ",5,train
MESOS-2850,Implement Docker image provisioner,"provisions a docker image (provisions all its dependent layers), fetch an image from persistent store, and also destroy an image.     done when tested for local discovery and copy backend. ",3,train
MESOS-2851,Add Docker Image Type to protobuf API,nan,1,train
MESOS-2853,Report per-container metrics from host egress filter,export in statistics.json the fq_codel flow statistics for each container.,1,train
MESOS-2854,Resources::parse(...) allows different resources of the same name to have different types.,"so code like this doesn't raise error.    resources::parse(""foo(role1):1;foo(role2):[0 1]"")      doesn't look like allowing this adds value and this complicates resource maths/validation/reporting.    we should disallow this.",2,train
MESOS-2857,FetcherCacheTest.LocalCachedExtract is flaky.,"from jenkins:      [ run      ] fetchercachetest.localcachedextract  using temporary directory '/tmp/fetchercachetestlocalcachedextractcwdcdj'  i0610 20:04:48.591573 24561 leveldb.cpp:176] opened db in 3.512525ms  i0610 20:04:48.592456 24561 leveldb.cpp:183] compacted db in 828630ns  i0610 20:04:48.592512 24561 leveldb.cpp:198] created db iterator in 32992ns  i0610 20:04:48.592531 24561 leveldb.cpp:204] seeked to beginning of db in 8967ns  i0610 20:04:48.592545 24561 leveldb.cpp:273] iterated through 0 keys in the db in 7762ns  i0610 20:04:48.592604 24561 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0610 20:04:48.593438 24587 recover.cpp:449] starting replica recovery  i0610 20:04:48.593698 24587 recover.cpp:475] replica is in empty status  i0610 20:04:48.595641 24580 replica.cpp:641] replica in empty status received a broadcasted recover request  i0610 20:04:48.596086 24590 recover.cpp:195] received a recover response from a replica in empty status  i0610 20:04:48.596607 24590 recover.cpp:566] updating replica status to starting  i0610 20:04:48.597507 24590 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 717888ns  i0610 20:04:48.597535 24590 replica.cpp:323] persisted replica status to starting  i0610 20:04:48.597697 24590 recover.cpp:475] replica is in starting status  i0610 20:04:48.599165 24584 replica.cpp:641] replica in starting status received a broadcasted recover request  i0610 20:04:48.599434 24584 recover.cpp:195] received a recover response from a replica in starting status  i0610 20:04:48.599915 24590 recover.cpp:566] updating replica status to voting  i0610 20:04:48.600545 24590 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 432335ns  i0610 20:04:48.600574 24590 replica.cpp:323] persisted replica status to voting  i0610 20:04:48.600659 24590 recover.cpp:580] successfully joined the paxos group  i0610 20:04:48.600797 24590 recover.cpp:464] recover process terminated  i0610 20:04:48.602905 24594 master.cpp:363] master 2015061020044838755414203290724561 (dbade881e927) started on 172.17.0.231:32907  i0610 20:04:48.602957 24594 master.cpp:365] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" credentials=""/tmp/fetchercachetestlocalcachedextractcwdcdj/credentials"" frameworksorter=""drf"" help=""false"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.23.0/inst/share/mesos/webui"" workdir=""/tmp/fetchercachetestlocalcachedextractcwdcdj/master"" zksessiontimeout=""10secs""  i0610 20:04:48.603374 24594 master.cpp:410] master only allowing authenticated frameworks to register  i0610 20:04:48.603392 24594 master.cpp:415] master only allowing authenticated slaves to register  i0610 20:04:48.603404 24594 credentials.hpp:37] loading credentials for authentication from '/tmp/fetchercachetestlocalcachedextractcwdcdj/credentials'  i0610 20:04:48.603751 24594 master.cpp:454] using default 'crammd5' authenticator  i0610 20:04:48.604928 24594 master.cpp:491] authorization enabled  i0610 20:04:48.606034 24593 hierarchical.hpp:309] initialized hierarchical allocator process  i0610 20:04:48.606106 24593 whitelistwatcher.cpp:79] no whitelist given  i0610 20:04:48.607430 24594 master.cpp:1476] the newly elected leader is master@172.17.0.231:32907 with id 2015061020044838755414203290724561  i0610 20:04:48.607466 24594 master.cpp:1489] elected as the leading master!  i0610 20:04:48.607481 24594 master.cpp:1259] recovering from registrar  i0610 20:04:48.607712 24594 registrar.cpp:313] recovering registrar  i0610 20:04:48.608543 24588 log.cpp:661] attempting to start the writer  i0610 20:04:48.610231 24588 replica.cpp:477] replica received implicit promise request with proposal 1  i0610 20:04:48.611335 24588 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 1.086439ms  i0610 20:04:48.611382 24588 replica.cpp:345] persisted promised to 1  i0610 20:04:48.612303 24588 coordinator.cpp:230] coordinator attemping to fill missing position  i0610 20:04:48.613883 24593 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0610 20:04:48.619205 24593 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 5.228235ms  i0610 20:04:48.619257 24593 replica.cpp:679] persisted action at 0  i0610 20:04:48.621919 24593 replica.cpp:511] replica received write request for position 0  i0610 20:04:48.621987 24593 leveldb.cpp:438] reading position from leveldb took 49394ns  i0610 20:04:48.622689 24593 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 668412ns  i0610 20:04:48.622716 24593 replica.cpp:679] persisted action at 0  i0610 20:04:48.623507 24584 replica.cpp:658] replica received learned notice for position 0  i0610 20:04:48.624155 24584 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 612283ns  i0610 20:04:48.624186 24584 replica.cpp:679] persisted action at 0  i0610 20:04:48.624215 24584 replica.cpp:664] replica learned nop action at position 0  i0610 20:04:48.625144 24593 log.cpp:677] writer started with ending position 0  i0610 20:04:48.626724 24589 leveldb.cpp:438] reading position from leveldb took 72013ns  i0610 20:04:48.629276 24591 registrar.cpp:346] successfully fetched the registry (0b) in 21.520128ms  i0610 20:04:48.629663 24591 registrar.cpp:445] applied 1 operations in 129587ns; attempting to update the 'registry'  i0610 20:04:48.632237 24579 log.cpp:685] attempting to append 131 bytes to the log  i0610 20:04:48.632624 24579 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0610 20:04:48.633739 24579 replica.cpp:511] replica received write request for position 1  i0610 20:04:48.634351 24579 leveldb.cpp:343] persisting action (150 bytes) to leveldb took 583937ns  i0610 20:04:48.634382 24579 replica.cpp:679] persisted action at 1  i0610 20:04:48.635073 24583 replica.cpp:658] replica received learned notice for position 1  i0610 20:04:48.635442 24583 leveldb.cpp:343] persisting action (152 bytes) to leveldb took 357122ns  i0610 20:04:48.635469 24583 replica.cpp:679] persisted action at 1  i0610 20:04:48.635494 24583 replica.cpp:664] replica learned append action at position 1  i0610 20:04:48.636337 24583 registrar.cpp:490] successfully updated the 'registry' in 6.534144ms  i0610 20:04:48.636725 24594 log.cpp:704] attempting to truncate the log to 1  i0610 20:04:48.636858 24583 registrar.cpp:376] successfully recovered registrar  i0610 20:04:48.637073 24594 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0610 20:04:48.637789 24594 master.cpp:1286] recovered 0 slaves from the registry (95b) ; allowing 10mins for slaves to reregister  i0610 20:04:48.638630 24583 replica.cpp:511] replica received write request for position 2  i0610 20:04:48.639127 24583 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 396272ns  i0610 20:04:48.639153 24583 replica.cpp:679] persisted action at 2  i0610 20:04:48.639804 24583 replica.cpp:658] replica received learned notice for position 2  i0610 20:04:48.640965 24583 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 1.147322ms  i0610 20:04:48.641054 24583 leveldb.cpp:401] deleting 1 keys from leveldb took 72395ns  i0610 20:04:48.641197 24583 replica.cpp:679] persisted action at 2  i0610 20:04:48.641345 24583 replica.cpp:664] replica learned truncate action at position 2  i0610 20:04:48.652274 24561 containerizer.cpp:111] using isolation: posix/cpu,posix/mem  i0610 20:04:48.658994 24590 slave.cpp:188] slave started on 42)@172.17.0.231:32907  i0610 20:04:48.659049 24590 slave.cpp:189] flags at startup: authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/fetchercachetestlocalcachedextractlchuum/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerremovedelay=""6hrs"" dockersandboxdirectory=""/mnt/mesos/sandbox"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/fetchercachetestlocalcachedextractlchuum/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.23.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resourcemonitoringinterval=""1secs"" resources=""cpus():1000; mem():1000"" revocablecpulowpriority=""true"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/fetchercachetestlocalcachedextractlchuum""  i0610 20:04:48.659570 24590 credentials.hpp:85] loading credential for authentication from '/tmp/fetchercachetestlocalcachedextractlchuum/credential'  i0610 20:04:48.659803 24590 slave.cpp:319] slave using credential for: testprincipal  i0610 20:04:48.660441 24590 slave.cpp:352] slave resources: cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000]  i0610 20:04:48.660555 24590 slave.cpp:382] slave hostname: dbade881e927  i0610 20:04:48.660578 24590 slave.cpp:387] slave checkpoint: true  i0610 20:04:48.661550 24588 state.cpp:35] recovering state from '/tmp/fetchercachetestlocalcachedextractlchuum/meta'  i0610 20:04:48.661913 24590 statusupdatemanager.cpp:201] recovering status update manager  i0610 20:04:48.662253 24590 containerizer.cpp:312] recovering containerizer  i0610 20:04:48.663207 24581 slave.cpp:3950] finished recovery  i0610 20:04:48.663761 24581 slave.cpp:4104] querying resource estimator for oversubscribable resources  i0610 20:04:48.664077 24581 slave.cpp:678] new master detected at master@172.17.0.231:32907  i0610 20:04:48.664088 24586 statusupdatemanager.cpp:175] pausing sending status updates  i0610 20:04:48.664245 24581 slave.cpp:741] authenticating with master master@172.17.0.231:32907  i0610 20:04:48.664388 24581 slave.cpp:746] using default crammd5 authenticatee  i0610 20:04:48.664611 24581 slave.cpp:714] detecting new master  i0610 20:04:48.664647 24594 authenticatee.hpp:139] creating new client sasl connection  i0610 20:04:48.664813 24581 slave.cpp:4125] received oversubscribable resources  from the resource estimator  i0610 20:04:48.665060 24581 slave.cpp:4129] no master detected. requerying resource estimator after 15secs  i0610 20:04:48.665096 24594 master.cpp:4181] authenticating slave(42)@172.17.0.231:32907  i0610 20:04:48.665247 24581 authenticator.cpp:406] starting authentication session for crammd5authenticatee(130)@172.17.0.231:32907  i0610 20:04:48.665657 24581 authenticator.cpp:92] creating new server sasl connection  i0610 20:04:48.666013 24581 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0610 20:04:48.666159 24581 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0610 20:04:48.666443 24592 authenticator.cpp:197] received sasl authentication start  i0610 20:04:48.666591 24592 authenticator.cpp:319] authentication requires more steps  i0610 20:04:48.666779 24592 authenticatee.hpp:276] received sasl authentication step  i0610 20:04:48.667007 24585 authenticator.cpp:225] received sasl authentication step  i0610 20:04:48.667043 24585 auxprop.cpp:101] request to lookup properties for user: 'testprincipal' realm: 'dbade881e927' server fqdn: 'dbade881e927' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0610 20:04:48.667058 24585 auxprop.cpp:173] looking up auxiliary property 'userpassword'  i0610 20:04:48.667110 24585 auxprop.cpp:173] looking up auxiliary property 'cmusaslsecretcrammd5'  i0610 20:04:48.667142 24585 auxprop.cpp:101] request to lookup properties for user: 'testprincipal' realm: 'dbade881e927' server fqdn: 'dbade881e927' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0610 20:04:48.667155 24585 auxprop.cpp:123] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0610 20:04:48.667163 24585 auxprop.cpp:123] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0610 20:04:48.667181 24585 authenticator.cpp:311] authentication success  i0610 20:04:48.667331 24585 authenticatee.hpp:316] authentication success  i0610 20:04:48.667414 24585 master.cpp:4211] successfully authenticated principal 'testprincipal' at slave(42)@172.17.0.231:32907  i0610 20:04:48.667505 24585 authenticator.cpp:424] authentication session cleanup for crammd5authenticatee(130)@172.17.0.231:32907  i0610 20:04:48.667809 24585 slave.cpp:812] successfully authenticated with master master@172.17.0.231:32907  i0610 20:04:48.667982 24585 slave.cpp:1146] will retry registration in 7.257154ms if necessary  i0610 20:04:48.668226 24585 master.cpp:3157] registering slave at slave(42)@172.17.0.231:32907 (dbade881e927) with id 2015061020044838755414203290724561s0  i0610 20:04:48.668737 24585 registrar.cpp:445] applied 1 operations in 90255ns; attempting to update the 'registry'  i0610 20:04:48.672297 24585 log.cpp:685] attempting to append 305 bytes to the log  i0610 20:04:48.672541 24585 coordinator.cpp:340] coordinator attempting to write append action at position 3  i0610 20:04:48.673528 24593 replica.cpp:511] replica received write request for position 3  i0610 20:04:48.674321 24593 leveldb.cpp:343] persisting action (324 bytes) to leveldb took 766804ns  i0610 20:04:48.674355 24593 replica.cpp:679] persisted action at 3  i0610 20:04:48.675138 24587 replica.cpp:658] replica received learned notice for position 3  i0610 20:04:48.675866 24587 leveldb.cpp:343] persisting action (326 bytes) to leveldb took 714643ns  i0610 20:04:48.675897 24587 replica.cpp:679] persisted action at 3  i0610 20:04:48.675922 24587 replica.cpp:664] replica learned append action at position 3  i0610 20:04:48.677471 24587 registrar.cpp:490] successfully updated the 'registry' in 8.656128ms  i0610 20:04:48.677759 24587 log.cpp:704] attempting to truncate the log to 3  i0610 20:04:48.678423 24593 coordinator.cpp:340] coordinator attempting to write truncate action at position 4  i0610 20:04:48.678621 24587 master.cpp:3214] registered slave 2015061020044838755414203290724561s0 at slave(42)@172.17.0.231:32907 (dbade881e927) with cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000]  i0610 20:04:48.678959 24593 hierarchical.hpp:496] added slave 2015061020044838755414203290724561s0 (dbade881e927) with cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000] (and cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000] available)  i0610 20:04:48.679157 24593 hierarchical.hpp:933] no resources available to allocate!  i0610 20:04:48.679183 24593 hierarchical.hpp:852] performed allocation for slave 2015061020044838755414203290724561s0 in 175519ns  i0610 20:04:48.679805 24593 replica.cpp:511] replica received write request for position 4  i0610 20:04:48.684160 24587 slave.cpp:846] registered with master master@172.17.0.231:32907; given slave id 2015061020044838755414203290724561s0  i0610 20:04:48.684229 24587 fetcher.cpp:77] clearing fetcher cache  i0610 20:04:48.684666 24587 slave.cpp:869] checkpointing slaveinfo to '/tmp/fetchercachetestlocalcachedextractlchuum/meta/slaves/2015061020044838755414203290724561s0/slave.info'  i0610 20:04:48.687366 24587 slave.cpp:2895] received ping from slaveobserver(42)@172.17.0.231:32907  i0610 20:04:48.687453 24584 statusupdatemanager.cpp:182] resuming sending status updates  i0610 20:04:48.690901 24593 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 3.385583ms  i0610 20:04:48.690975 24593 replica.cpp:679] persisted action at 4  i0610 20:04:48.692137 24593 replica.cpp:658] replica received learned notice for position 4  i0610 20:04:48.692603 24593 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 449838ns  i0610 20:04:48.692674 24593 leveldb.cpp:401] deleting 2 keys from leveldb took 52471ns  i0610 20:04:48.692699 24593 replica.cpp:679] persisted action at 4  i0610 20:04:48.692726 24593 replica.cpp:664] replica learned truncate action at position 4  i0610 20:04:48.693544 24561 sched.cpp:157] version: 0.23.0  i0610 20:04:48.695550 24590 sched.cpp:254] new master detected at master@172.17.0.231:32907  i0610 20:04:48.697090 24590 sched.cpp:310] authenticating with master master@172.17.0.231:32907  i0610 20:04:48.697136 24590 sched.cpp:317] using default crammd5 authenticatee  i0610 20:04:48.697511 24586 authenticatee.hpp:139] creating new client sasl connection  i0610 20:04:48.697937 24586 master.cpp:4181] authenticating scheduler51f5c1b5bb504118bde84dcdfd69205d@172.17.0.231:32907  i0610 20:04:48.698185 24584 authenticator.cpp:406] starting authentication session for crammd5authenticatee(131)@172.17.0.231:32907  i0610 20:04:48.698575 24584 authenticator.cpp:92] creating new server sasl connection  i0610 20:04:48.698807 24584 authenticatee.hpp:230] received sasl authentication mechanisms: crammd5  i0610 20:04:48.699898 24584 authenticatee.hpp:256] attempting to authenticate with mechanism 'crammd5'  i0610 20:04:48.700040 24584 authenticator.cpp:197] received sasl authentication start  i0610 20:04:48.700119 24584 authenticator.cpp:319] authentication requires more steps  i0610 20:04:48.700193 24584 authenticatee.hpp:276] received sasl authentication step  i0610 20:04:48.700287 24584 authenticator.cpp:225] received sasl authentication step  i0610 20:04:48.700320 24584 auxprop.cpp:101] request to lookup properties for user: 'testprincipal' realm: 'dbade881e927' server fqdn: 'dbade881e927' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0610 20:04:48.700333 24584 auxprop.cpp:173] looking up auxiliary property 'userpassword'  i0610 20:04:48.700392 24584 auxprop.cpp:173] looking up auxiliary property 'cmusaslsecretcrammd5'  i0610 20:04:48.700425 24584 auxprop.cpp:101] request to lookup properties for user: 'testprincipal' realm: 'dbade881e927' server fqdn: 'dbade881e927' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0610 20:04:48.700439 24584 auxprop.cpp:123] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0610 20:04:48.700448 24584 auxprop.cpp:123] skipping auxiliary property ' cmusaslsecretcram md5' since saslauxpropauthzid == true  i0610 20:04:48.700467 24584 authenticator.cpp:311] authentication success  i0610 20:04:48.700640 24584 authenticatee.hpp:316] authentication success  i0610 20:04:48.700742 24584 authenticator.cpp:424] authentication session cleanup for crammd5authenticatee(131)@172.17.0.231:32907  i0610 20:04:48.701282 24590 sched.cpp:398] successfully authenticated with master master@172.17.0.231:32907  i0610 20:04:48.701315 24590 sched.cpp:521] sending registration request to master@172.17.0.231:32907  i0610 20:04:48.701386 24590 sched.cpp:554] will retry registration in 1.128089605secs if necessary ...",1,train
MESOS-2860,Create the basic infrastructure to handle /scheduler endpoint,"this is the first basic step in ensuring the basic /call functionality: processing a     post /call    and returning:     202 if all goes well;   401 if not authorized; and   403 if the request is malformed.    we'll get more sophisticated as the work progressed (eg, supporting 415 if the contenttype is not of the right kind).",3,train
MESOS-2862,"mesos-fetcher won't fetch uris which begin with a "" ""","discovered while running mesos with marathon on top. if i launch a marathon task with a uri which is "" http:/apache.osuosl.org/mesos/0.22.1/mesos0.22.1.tar.gz"" mesos will log to stderr:      i0611 22:39:22.815636 35673 logging.cpp:177] logging to stderr  i0611 22:39:25.643889 35673 fetcher.cpp:214] fetching uri ' http:/apache.osuosl.org/mesos/0.22.1/mesos0.22.1.tar.gz'  i0611 22:39:25.648111 35673 fetcher.cpp:94] hadoop client not available, skipping fetch with hadoop client  failed to fetch:  http:/apache.osuosl.org/mesos/0.22.1/mesos 0.22.1.tar.gz  failed to synchronize with slave (it's probably exited)      it would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. ",2,train
MESOS-2866,Slave should send oversubscribed resource information after master failover.,"after a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the updateslave message to the new master. the slave needs to send the information to the new master regardless of this.",3,train
MESOS-2869,OversubscriptionTest.FixedResourceEstimator is flaky,"came up in https:/reviews.apache.org/r/35395/      [ run      ] oversubscriptiontest.fixedresourceestimator  i0613 13:41:02.604904 19367 exec.cpp:132] version: 0.23.0  i0613 13:41:02.610995 19398 exec.cpp:206] executor registered on slave 2015061313410231426977954829513678s0  registered executor on pomona.apache.org  starting task 7d78a3ef2de946c9811cb2c0e2d50578  forked command at 19410  sh c 'sleep 1000'  ../../src/tests/oversubscription_tests.cpp:579: failure  mock function called more times than expected  returning directly.      function call: statusupdate(0x7ffffbc0c4e0, @0x2ade2bffa910 96byte object /)           expected: to be called once             actual: called twice  oversaturated and active  [  failed  ] oversubscriptiontest.fixedresourceestimator (714 ms)  ",1,train
MESOS-2873,style hook prevent's valid markdown files from getting committed,"according to the original http:/daringfireball.net/projects/markdown/syntax#p and to the most http:/spec.commonmark.org/0.20/#hardlinebreaks effort, two spaces at the end of a line create a hard line break (it breaks the line without starting a new paragraph), similar to the html code /.     however, there's a hook in mesos which prevent files with trailing whitespace to be committed.",1,train
MESOS-2874,Convert PortMappingStatistics to use automatic JSON encoding/decoding,simplify portmappingstatistics by using json::protocol and protobuf::parse to convert resourcestatistics to/from line format.    this change will simplify the implementation of mesos 2332.,2,train
MESOS-2879,Random recursive_mutex errors in when running make check,"while running make check on os x, from time to time recursivemutex errors appear after running all the test successfully. just one of the experience messages actually stops make check reporting an error.    the following error messages have been experienced:      libcabi.dylib: libcabi.dylib: libcabi.dylib: libcabi.dylib: libcabi.dylib: libcabi.dylib: terminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argument       aborted at 1434553937 (unix time) try ""date d @1434553937"" if you are using gnu date         libcabi.dylib: terminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argument   aborted at 1434557001 (unix time) try ""date d @1434557001"" if you are using gnu date   libcabi.dylib: pc: @     0x7fff93855286 pthreadkill  libcabi.dylib:  sigabrt (@0x7fff93855286) received by pid 88060 (tid 0x10fc40000) stack trace:       @     0x7fff8e1d6f1a sigtramp  libcabi.dylib:     @        0x10fc3f1a8 (unknown)  libcabi.dylib:     @     0x7fff979deb53 abort  libcabi.dylib: libcabi.dylib: libcabi.dylib: terminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentterminating with uncaught exception of type std::1::systemerror: recursivemutex lock failed: invalid argumentmaking check in include        assertion failed: (e == 0), function recursivemutex, file /sourcecache/libcxx/libcxx120/src/mutex.cpp, line 82.   aborted at 1434555685 (unix time) try ""date d @1434555685"" if you are using gnu date   pc: @     0x7fff93855286 pthreadkill   sigabrt (@0x7fff93855286) received by pid 60235 (tid 0x7fff7ebdc300) stack trace:       @     0x7fff8e1d6f1a sigtramp      @        0x10b512350 google::checknotnull/()      @     0x7fff979deb53 abort      @     0x7fff979a6c39 assertrtn      @     0x7fff9bffdcc9 std::1::recursivemutex::recursive_mutex()      @        0x10b881928 process::processmanager::processmanager()      @        0x10b874445 process::processmanager::processmanager()      @        0x10b874418 process::finalize()      @        0x10b2f7aec main      @     0x7fff98edc5c9 start  make[5]:  [checklocal] abort trap: 6  make[4]:  [checkam] error 2  make[3]:  [checkrecursive] error 1  make[2]:  [checkrecursive] error 1  make[1]:  [check] error 2  make:  [check recursive] error 1  ",1,train
MESOS-2883,Do not call hook manager if no hooks installed,"hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. this is an unnecessary overhead if there are no hooks installed.    the proper way would be to call decorators via the hook manager only if there are some hooks installed. this would prevent unnecessary copying overhead if no hooks are available.",2,train
MESOS-2884,Allow isolators to specify required namespaces,"currently, the linuxlauncher looks into slaveflags to compute the namespaces that should be enabled when launching the executor. this means that a custom isolator module doesn't have any way to specify dependency on a set of namespaces.    the proposed solution is to extend the isolator interface to also export the namespaces dependency. this way the mesoscontainerizer can directly query all loaded isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. this set of namespaces is then passed on to the linuxlauncher.  ",5,train
MESOS-2886,Capture some testing patterns we use in a doc,in mesos tests we use some tricks and patterns to express certain expectations. these are not always obvious and not documented. the intent of the ticket is to kick start the document with the description of those tricks for posterity.,1,train
MESOS-2888,Add SSL socket tests,commit beac384c77d4a9c235a813e9286716f4509bdd55  author: joris van remoortere /  date:   fri jun 26 18:30:12 2015  0700        add ssl tests.            review: https:/reviews.apache.org/r/35889,5,train
MESOS-2889,Add SSL switch to python configuration,the python egg requires explicit dependencies for ssl. add these to the python configuration if ssl is enabled.,3,train
MESOS-2890,Sandbox URL doesn't work in web-ui when using SSL,the links to the sandbox in the web ui don't work when ssl is enabled.   this can happen if the certificate for the master and the slave do not match. this is a consequence of the redirection that happens when serving files.  the resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.,3,train
MESOS-2891,Performance regression in hierarchical allocator.,"for large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. after the following slave was reregistered, it took the allocator a long time to work through the backlog of slaves to add:      i0618 18:55:40.738399 10172 master.cpp:3419] reregistered slave 20150422211121214834689050503253s4695  i0618 19:40:14.960636 10164 hierarchical.hpp:496] added slave 20150422211121214834689050503253s4695      empirically, https:/github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#l462 and https:/github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#l533 have become expensive.    some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to addslave and updateslave, when there are tens of thousands of slaves this amounts to the large delay seen above.    we also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.    a synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size.",3,train
MESOS-2892,Add benchmark for hierarchical allocator.,"in light of the performance regression in mesos 2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements.",3,train
MESOS-2893,Add queue size metrics for the allocator.,"in light of the performance regression in mesos2891, we'd like to have visibility into the queue size of the allocator. this will enable alerting on performance problems.    we currently have no metrics in the allocator.    i will also look into mesos1286 now that we have gcc 4.8, current queue size gauges require a trip through the process' queue.",1,train
MESOS-2898,Write tests for new JSON (ZooKeeper) functionality,"follow up from mesos 2340, need to ensure this does not break the zookeeper discovery functionality.",2,train
MESOS-2902,"Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME","currently mesos tries to guess the ip, hostname by doing a reverse dns lookup. this doesn't work on a lot of clouds as we want things like public ips (which aren't the default dns), there aren't fqdn names (azure), or the correct way to figure it out is to call some cloudspecific endpoint.    if mesos / libprocess could load a mesosmodule (or run a script) which is provided percloud, we can figure out perfectly the ip / hostname for the given environment. it also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the dns scheme + hostnames that libprocess/mesos expects. currently we have to generate hostspecific config files which mesos uses to guess.    the host specific files break / fall apart if machines change ip / hostname without being reinstalled.",5,train
MESOS-2903,Network isolator should not fail when target state already exists,"network isolator has multiple instances of the following pattern:        try/ something = ....::create();                                      if (something.iserror())  else if (!icmpvethtoeth0.get())                                                                                        these failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    we should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.",3,train
MESOS-2904,Add slave metric to count container launch failures,"we have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",1,train
MESOS-2906,Slave : Synchronous Validation for Calls,"/call endpoint on the slave will return a 202 accepted code but has to do some basic validations before. in case of invalidation it will return a badrequest back to the client.      we need to create the required infrastructure to validate the request and then process it similar to src/master/validation.cpp in the namespace scheduler i.e. check if the protobuf is properly initialized, has the required attributes set pertaining to the call message etc.",3,train
MESOS-2907,Agent : Create Basic Functionality to handle /call endpoint,"this is the first basic step in ensuring the basic /call functionality:      set up the route on the agent for ""api/v1/executor"" endpoint.   the endpoint should perform basic header/protobuf validation and return 501 notimplemented for now.    introduce initial tests in executorapitests.cpp that just verify the status code.  ",5,train
MESOS-2909,Add version field to RegisterFrameworkMessage and ReregisterFrameworkMessage,"in the same way we added 'version' field to registerslavemessage and reregisterslavemessage, we should do it framework (re )registration messages. this would help master determine which version of scheduler driver it is talking to.    we want this so that master can start sending event messages to the scheduler driver (and scheduler library). in the long term, master will send a streaming response to the libraries, but in the meantime we can test the event protobufs by sending event messages.",3,train
MESOS-2910,Add an Event message handler to scheduler driver,adding this handler lets master send event messages to the driver.    see mesos 2909 for additional context.,8,train
MESOS-2911,Add an Event message handler to scheduler library,adding this handler lets master send event messages to the library.    see mesos 2909 for additional context.    this ticket only tracks the installation of the handler and maybe handling of a single event for testing. additional events handling will be captured in a different ticket(s).,3,train
MESOS-2912,Provide a Python library for master detection,"when schedulers start interacting with mesos master via http endpoints, they need a way to detect masters.     mesos should provide a master detection python library to make this easy for frameworks.",5,train
MESOS-2913,Scheduler driver should send Call messages to the master,"to vet the new call protobufs, it is prudent to have the scheduler driver (sched.cpp) send call messages to the master (similar to what we are doing with the scheduler library).",8,train
MESOS-2914,Port mapping isolator should cleanup unknown orphan containers after all known orphan containers are recovered during recovery.,"otherwise, the icmp/arp filter on host eth0 might be removed as a result of cleanup if 'infos' is empty, causing subsequent 'cleanup' to fail on both known/unknown orphan containers.      i0612 17:46:51.518501 16308 containerizer.cpp:314] recovering containerizer  i0612 17:46:51.520612 16308 portmapping.cpp:1567] discovered network namespace handle symlink ddcb8397355244f9bc99b5b69aa72944 > 31607  i0612 17:46:51.521183 16308 portmapping.cpp:1567] discovered network namespace handle symlink d8c48a4afdfb47ddb8d807188c21600d > 41020  i0612 17:46:51.521883 16308 portmapping.cpp:1567] discovered network namespace handle symlink 8953fc7f9fca4931b0cb2f4959ddee74 > 3302  i0612 17:46:51.522542 16308 portmapping.cpp:1567] discovered network namespace handle symlink 50f9986febbc440d86a79fa1a7c55a75 > 19805  i0612 17:46:51.523643 16308 portmapping.cpp:2597] removing ip packet filters with ports [33792,34815] for container with pid 52304  i0612 17:46:51.525063 16308 portmapping.cpp:2616] freed ephemeral ports [33792,34816) for container with pid 52304  i0612 17:46:51.547696 16308 portmapping.cpp:2762] successfully performed cleanup for pid 52304  i0612 17:46:51.550027 16308 portmapping.cpp:1698] network isolator recovery complete  i0612 17:46:51.550946 16329 containerizer.cpp:449] removing orphan container 111ea69c61844da1a0e9c34e8c6deb30  i0612 17:46:51.552686 16329 containerizer.cpp:449] removing orphan container ddcb8397355244f9bc99b5b69aa72944  i0612 17:46:51.552734 16309 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/111ea69c61844da1a0e9c34e8c6deb30  i0612 17:46:51.554932 16329 containerizer.cpp:449] removing orphan container 8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:46:51.555032 16309 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397355244f9bc99b5b69aa72944  i0612 17:46:51.555629 16308 cgroups.cpp:1420] successfully froze cgroup /sys/fs/cgroup/freezer/mesos/111ea69c61844da1a0e9c34e8c6deb30 after 1.730304ms  i0612 17:46:51.557507 16329 containerizer.cpp:449] removing orphan container 50f9986febbc440d86a79fa1a7c55a75  i0612 17:46:51.557611 16309 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:46:51.557896 16313 cgroups.cpp:1420] successfully froze cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397355244f9bc99b5b69aa72944 after 1.685248ms  i0612 17:46:51.559412 16310 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/111ea69c61844da1a0e9c34e8c6deb30  i0612 17:46:51.561564 16329 containerizer.cpp:449] removing orphan container d8c48a4afdfb47ddb8d807188c21600d  i0612 17:46:51.562489 16315 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/50f9986febbc440d86a79fa1a7c55a75  i0612 17:46:51.562988 16313 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397355244f9bc99b5b69aa72944  i0612 17:46:51.563303 16310 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/111ea69c61844da1a0e9c34e8c6deb30 after 2.076928ms  i0612 17:46:51.566052 16308 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4afdfb47ddb8d807188c21600d  i0612 17:46:51.566102 16313 slave.cpp:3911] finished recovery  w0612 17:46:51.566432 16323 disk.cpp:299] ignoring cleanup for unknown container 111ea69c61844da1a0e9c34e8c6deb30  i0612 17:46:51.566651 16317 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397355244f9bc99b5b69aa72944 after 2.12096ms  i0612 17:46:51.566987 16313 slave.cpp:3944] garbage collecting old slave 201503192131332080910346505057551s3314  i0612 17:46:51.567777 16318 cgroups.cpp:1420] successfully froze cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4afdfb47ddb8d807188c21600d after 1.323008ms  w0612 17:46:51.568042 16323 portmapping.cpp:2544] ignoring cleanup for unknown container 111ea69c61844da1a0e9c34e8c6deb30  i0612 17:46:51.569522 16311 gc.cpp:56] scheduling '/var/lib/mesos/slaves/201503192131332080910346505057551s3314' for gc 6.99999341503407days in the future  w0612 17:46:51.569725 16329 disk.cpp:299] ignoring cleanup for unknown container ddcb8397355244f9bc99b5b69aa72944  i0612 17:46:51.570911 16325 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4afdfb47ddb8d807188c21600d  i0612 17:46:51.573581 16316 portmapping.cpp:2597] removing ip packet filters with ports [35840,36863] for container with pid 31607  i0612 17:46:51.575127 16316 portmapping.cpp:2616] freed ephemeral ports [35840,36864) for container with pid 31607  i0612 17:46:51.588284 16330 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4afdfb47ddb8d807188c21600d after 14.503936ms  e0612 17:46:51.622140 16310 containerizer.cpp:480] failed to clean up an isolator when destroying orphan container ddcb8397355244f9bc99b5b69aa72944: the icmp packet filter on host eth0 does not exist, the arp packet filter on host eth0 does not exist  w0612 17:46:51.773123 16313 disk.cpp:299] ignoring cleanup for unknown container d8c48a4afdfb47ddb8d807188c21600d  i0612 17:46:51.774153 16325 portmapping.cpp:2597] removing ip packet filters with ports [32768,33791] for container with pid 41020  i0612 17:46:51.775167 16325 portmapping.cpp:2616] freed ephemeral ports [32768,33792) for container with pid 41020  e0612 17:46:51.817221 16323 containerizer.cpp:480] failed to clean up an isolator when destroying orphan container d8c48a4afdfb47ddb8d807188c21600d: the icmp packet filter on host eth0 does not exist, the arp packet filter on host eth0 does not exist  i0612 17:46:51.872231 16314 cgroups.cpp:1420] successfully froze cgroup /sys/fs/cgroup/freezer/mesos/50f9986febbc440d86a79fa1a7c55a75 after 308.33792ms  i0612 17:46:51.874572 16314 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/50f9986febbc440d86a79fa1a7c55a75  i0612 17:46:51.876566 16314 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/50f9986febbc440d86a79fa1a7c55a75 after 1.593344ms  20150612 17:46:54,833:16307(0x7f172eb07940):zooinfo@authcompletionfunc@1286: authentication scheme digest succeeded  i0612 17:46:54.835737 16321 group.cpp:385] trying to create path '/home/mesos/prod/master' in zookeeper  i0612 17:46:54.839110 16321 detector.cpp:138] detected a new leader: (id='1')  i0612 17:46:54.840276 16330 group.cpp:659] trying to get '/home/mesos/prod/master/info0000000001' in zookeeper  i0612 17:46:54.842350 16330 detector.cpp:452] a new leading master (upid=master@10.44.14.132:5050) is detected  i0612 17:46:54.843297 16330 slave.cpp:653] new master detected at master@10.44.14.132:5050  i0612 17:46:54.843298 16312 statusupdatemanager.cpp:171] pausing sending status updates  i0612 17:46:54.844091 16330 slave.cpp:678] no credentials provided. attempting to register without authentication  i0612 17:46:54.845087 16330 slave.cpp:689] detecting new master  i0612 17:47:01.561920 16309 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:01.564687 16309 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74 after 1.924096ms  i0612 17:47:01.565467 16309 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  w0612 17:47:09.978946 16326 disk.cpp:299] ignoring cleanup for unknown container 50f9986febbc440d86a79fa1a7c55a75  i0612 17:47:09.979818 16327 portmapping.cpp:2597] removing ip packet filters with ports [34816,35839] for container with pid 19805  i0612 17:47:09.981474 16327 portmapping.cpp:2616] freed ephemeral ports [34816,35840) for container with pid 19805  e0612 17:47:10.278715 16325 containerizer.cpp:480] failed to clean up an isolator when destroying orphan container 50f9986febbc440d86a79fa1a7c55a75: the icmp packet filter on host eth0 does not exist, the arp packet filter on host eth0 does not exist  i0612 17:47:11.568151 16326 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:11.570915 16326 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74 after 1.987072ms  i0612 17:47:11.571728 16326 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:14.549536 16316 slave.cpp:821] registered with master master@10.44.14.132:5050; given slave id 201506021901002215521290505039399s23257  i0612 17:47:14.550220 16318 statusupdate_manager.cpp:178] resuming sending status updates  i0612 17:47:21.574513 16319 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:21.576817 16319 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74 after 1.587968ms  i0612 17:47:21.577466 16319 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:31.580281 16310 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:31.582365 16310 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74 after 1.410048ms  i0612 17:47:31.582895 16310 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:41.585619 16322 cgroups.cpp:2394] thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:41.587703 16322 cgroups.cpp:1449] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74 after 1.418752ms  i0612 17:47:41.588436 16322 cgroups.cpp:2377] freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f9fca4931b0cb2f4959ddee74  i0612 17:47:51.515689 16330 slave.cpp:3733] current disk usage 11.43%. max allowed age: 5.499938588861215days  e0612 17:47:51.557831 16330 containerizer.cpp:468] failed to destroy orphan container 8953fc7f9fca4931b0cb 2f4959ddee74: timed out after 1mins  ",3,train
MESOS-2917,Specify correct libnl version for configure check,"currently configure.ac lists 3.2.24 as the required libnl version. however, https:/reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. the configure check thus fails to error out during execution and the dependency is captured only during the build step.",1,train
MESOS-2919,Framework can overcommit oversubscribable resources during master failover.,"this is due to a bug in the hierarchical allocator. here is the sequence of events:    1) slave uses a fixed resource estimator which advertise 4 revocable cpus  2) a framework a launches a task that uses all the 4 revocable cpus  3) master fails over  4) slave reregisters with the new master, and sends updateslavemessage with 4 revocable cpus as oversubscribed resources  5) framework a hasn't registered yet, therefore, the slave's available resources will be 4 revocable cpus  6) framework a registered and will receive an additional 4 revocable cpus. so it can launch another task with 4 revocable cpus (that means 8 total!)    the problem is due to the way we calculate 'allocated' resource in allocator when 'updateslave'. if the framework is not registered, the 'allocation' below is not accurate (check that if block in 'addslave').      template /  void  hierarchicalallocatorprocess/::updateslave(      const slaveid& slaveid,      const resources& oversubscribed)        / update the available resources.      / first remove the old oversubscribed resources from available.    slaves[slaveid].available = slaves[slaveid].available.revocable();      / now add the new estimate of available oversubscribed resources.    slaves[slaveid].available += oversubscribed  allocation;      log(info) /  void  hierarchicalallocatorprocess/::addslave(      const slaveid& slaveid,      const slaveinfo& slaveinfo,      const resources& total,      const hashmap/& used)      }    ...  }  ",3,train
MESOS-2920,Add move constructors / assignment to Try.,"now that we have c11, let's add move constructors and move assignment operators for try, similarly to what was done for option.",3,train
MESOS-2921,Add move constructors / assignment to Result.,"now that we have c11, let's add move constructors and move assignment operators for result, similarly to what was done for option.",3,train
MESOS-2922,Add move constructors / assignment to Future.,"now that we have c11, let's add move constructors and move assignment operators for future, similarly to what was done for option. there is currently one move constructor for future/, but not for t, u, and no assignment operator.",3,train
MESOS-2923,fetcher.cpp - problem with certificates..?,"mesos 0.22.0/0.22.1 built and installed from sources accordingly to the instructions given http:/mesos.apache.org/gettingstarted/ has some problem with certificates.  every time i try to deploy something that requires downloading any resource via https (with uri specified via marathon), such deployment fails and i get this message in failed app's sandbox:    e0617 09:58:44.339409 12380 fetcher.cpp:138] error downloading resource: problem with the ssl ca cert (path? access rights?)    trying to download the same resource on the same slave with curl or wget works without problems.  moreover, when i install exactly the same version of mesos from mesosphere's debs on identical machines (i.e., set up by the same ansible scripts), everything works fine as well.  i guess it must be something related to the way how mesos is built   maybe some missing switch for configure or make..?    any ideas..?",2,train
MESOS-2925,Invalid usage of ATOMIC_FLAG_INIT in member initialization,"the c specification states:    the macro atomicflaginit shall be defined in such a way that it can be used to initialize an object of type atomicflag to the clear state. the macro can be used in the form: ""atomicflag guard = atomicflaginit; ""it is unspecified whether the macro can be used in other initialization contexts.""     clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",1,train
MESOS-2926,Extend mesos-style.py/cpplint.py to check #include files,"cpplint.py provides the capability to enforce the style guide requirements for #including everything you use and ordering files based on type but it does not work for mesos because we do use #include / for project files where it expects #include ""..."".      we should update the style checker to support our include usage and then turn it on by default in the commit hook.",1,train
MESOS-2928,Update stout #include headers,update stout to #include headers for symbols we rely on and reorder to comply with the style guide.,2,train
MESOS-2936,Create a design document for Quota support in Master,create a design document for the quota feature support in mesos master (excluding allocator) to be shared with the mesos community.    design doc:  https:/docs.google.com/document/d/16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq 9i/edit?usp=sharing,8,train
MESOS-2937,Initial design document for Quota support in Allocator.,create a design document for the quota feature support in the built in hierarchical drf allocator to be shared with the mesos community.,5,train
MESOS-2938,Linux docker inspect crashes,"on linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:     aborted at 1435254156 (unix time) try ""date  d @1435254156"" if you are using gnu date   pc: @     0x7ffff2b1364d (unknown)   sigsegv (@0xfffffffffffffff8) received by pid 88424 (tid 0x7fffe88fb700) from pid 18446744073709551608; stack trace:       @     0x7ffff25a4340 (unknown)      @     0x7ffff2b1364d (unknown)      @     0x7ffff2b724df (unknown)      @           0x4a6466 docker::container::container()      @     0x7ffff5bfa49a option/::option()      @     0x7ffff5c15989 option/::operator=()      @     0x7ffff5c09e9f try/::operator=()      @     0x7ffff5c09ee3 result/::operator=()      @     0x7ffff5c0a938 process::future/::set()      @     0x7ffff5bff412 process::promise/::set()      @     0x7ffff5be53e3 docker::inspect()      @     0x7ffff5be3cf8 zzn6docker9inspecterkssrkn7process5ownedins27promiseins9containereeeeerk6optioni8durationens26futureisseerkns210subprocesseenkulrksge1clesl      @     0x7ffff5be91e9 zznk7process6futureisse5onanyizn6docker9inspecterkssrkns5ownedins7promiseins39containereeeeerk6optioni8durationes1rkns10subprocesseeulrks1e1veesmotns16prefereenulsmeclesm      @     0x7ffff5be9d9d znst17functionhandlerifvrkn7process6futureisseeeznks25onanyizn6docker9inspecterkssrkns05ownedins07promiseins79containereeeeerk6optioni8durationes2rkns010subprocesseeuls4e1vees4otns26prefereeuls4ee9minvokeerkst9anydatas4      @     0x7ffff5c1eadd std::function/::operator()()      @     0x7ffff5c15e07 process::future/::onany()      @     0x7ffff5be93a1 znk7process6futureisse5onanyizn6docker9inspecterkssrkns5ownedins7promiseins39containereeeeerk6optioni8durationes1rkns10subprocesseeulrks1e1veesmotns16prefere      @     0x7ffff5be87f6 znk7process6futureisse5onanyizn6docker9inspecterkssrkns5ownedins7promiseins39containereeeeerk6optioni8durationes1rkns10subprocesseeulrks1e1eesmot      @     0x7ffff5be459c docker::inspect()      @     0x7ffff5be337c zzn6docker8inspecterkssrkn7process5ownedins27promiseins9containereeeeerk6optioni8durationeenkulveclev      @     0x7ffff5be8c5a zznk7process6futurei6optioniiee5onanyizn6docker8inspecterkssrkns5ownedins7promiseins59containereeeeerks1i8durationeeulveveerks3otns310lessprefereenulsleclesl      @     0x7ffff5be9b36 znst17functionhandlerifvrkn7process6futurei6optioniieeeeznks45onanyizn6docker8inspecterkssrkns05ownedins07promiseins99containereeeeerks2i8durationeeulvevees6otns410lessprefereeuls6ee9minvokeerkst9anydatas6      @     0x7ffff5c1e9b3 std::function/::operator()()      @     0x7ffff6184a1a zn7process8internal3runist8functionifvrkns6futurei6optioniieeeeejrs6eeevrkst6vectoritsaisdeedpot0      @     0x7ffff617e64d process::future/::set()      @     0x7ffff6752e46 process::promise/::set()      @     0x7ffff675faec process::internal::cleanup()      @     0x7ffff6765293 znst5bindifpfvrkn7process6futurei6optioniieeepns07promiseis3eerkns010subprocesseest12placeholderili1ees9saee6callivis6eilm0elm1elm2eeeetost5tupleiidpt0eest12indextupleiixspt1eee      @     0x7ffff6764bcd znst5bindifpfvrkn7process6futurei6optioniieeepns07promiseis3eerkns010subprocesseest12placeholderili1ees9saeeclijs6eveet0dpot      @     0x7ffff67642a5 zznk7process6futurei6optioniiee5onanyist5bindifpfvrks3pns7promiseis2eerkns10subprocesseest12placeholderili1eesasbeevees7otns36prefereenuls7ecles7      @     0x7ffff676531d znst17functionhandlerifvrkn7process6futurei6optioniieeeeznks45onanyist5bindifpfvs6pns07promiseis3eerkns010subprocesseest12placeholderili1eescsdeevees6otns46prefereeuls6ee9minvokeerkst9anydatas6_      @     0x7ffff5c1e9b3 std::function/::operator()()  (end)   ",1,train
MESOS-2939,Testing the new workflow,"this is a simple test story to try out the new workflow.    unfortunately, testing and getting it to work seems to be something that actually does take up time, so i'm tracking this here.",3,train
MESOS-2940,Reconciliation is expensive for large numbers of tasks.,"we've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:      i0625 20:55:23.716320 21937 master.cpp:3863] performing explicit task state reconciliation for n tasks of framework f (name) at s@ip:port  i0625 20:56:34.812464 21937 master.cpp:5041] removing task t with resources r of framework f on slave s at slave(1)@ip:port (host)        i0625 20:25:22.310601 21936 master.cpp:3802] performing implicit task state reconciliation for framework f (name) at s@ip:port  i0625 20:26:23.874528 21921 master.cpp:218] scheduling shutdown of slave s due to health check timeout      let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",3,train
MESOS-2941,Add a benchmark for task reconciliation.,"per mesos 2940, it would be great to have a benchmark for task reconciliation, given large numbers of tasks.    this can guide attempts at improving performance.",1,train
MESOS-2942,Create documentation for using SSL,nan,5,train
MESOS-2943,mesos fails to compile under mac when libssl and libevent are enabled,"../configure enabledebug enablelibevent enablessl && make    produces the following error:    poll.cpp'  echo '../../../3rdparty/libprocess/'`src/libeventpoll.cpp  libtool: compile:  g dpackagename=\""libprocess\"" dpackagetarname=\""libprocess\"" dpackageversion=\""0.0.1\"" ""dpackagestring=\""libprocess 0.0.1\"""" dpackagebugreport=\""\"" dpackageurl=\""\"" dpackage=\""libprocess\"" dversion=\""0.0.1\"" dstdcheaders=1 dhavesystypesh=1 dhavesysstath=1 dhavestdlibh=1 dhavestringh=1 dhavememoryh=1 dhavestringsh=1 dhaveinttypesh=1 dhavestdinth=1 dhaveunistdh=1 dhavedlfcnh=1 dltobjdir=\"".libs/\"" dhaveaprpoolsh=1 dhavelibapr1=1 dhavesvnversionh=1 dhavelibsvnsubr1=1 dhavesvndeltah=1 dhavelibsvndelta1=1 dhavelibcurl=1 dhaveevent2eventh=1 dhavelibevent=1 dhaveevent2threadh=1 dhavelibeventpthreads=1 dhaveopensslsslh=1 dhavelibssl=1 dhavelibcrypto=1 dhaveevent2buffereventsslh=1 dhavelibeventopenssl=1 dusesslsocket=1 dhavepthreadprioinherit=1 dhavepthread=1 dhavelibz=1 dhavelibdl=1 i. i../../../3rdparty/libprocess i../../../3rdparty/libprocess/include i../../../3rdparty/libprocess/3rdparty/stout/include i3rdparty/boost1.53.0 i3rdparty/libev4.15 i3rdparty/picojson4f93734 i3rdparty/glog0.3.3/src i3rdparty/ryhttpparser1c3624a i/usr/local/opt/openssl/include i/usr/local/opt/libevent/include i/usr/local/opt/subversion/include/subversion1 i/usr/include/apr1 i/usr/include/apr1.0 g1 o0 std=c11 stdlib=libc dgtestuseowntr1tuple=1 mt libprocesslalibeventpoll.lo md mp mf .deps/libprocesslalibeventpoll.tpo c ../../../3rdparty/libprocess/src/libeventpoll.cpp  fnocommon dpic o libprocesslalibeventpoll.o  mv f .deps/libprocesslasocket.tpo .deps/libprocesslasocket.plo  mv f .deps/libprocesslasubprocess.tpo .deps/libprocesslasubprocess.plo  mv f .deps/libprocesslalibevent.tpo .deps/libprocesslalibevent.plo  mv f .deps/libprocesslametrics.tpo .deps/libprocesslametrics.plo  in file included from ../../../3rdparty/libprocess/src/libeventsslsocket.cpp:11:  in file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9:  ../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::future/ >' to 'const process::network::socket'   set(u);         ../../../3rdparty/libprocess/src/libeventsslsocket.cpp:769:10: note: in instantiation of function template specialization 'process::future/::future/ > >' requested here   return acceptqueue.get()            ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::future/ >' to       'process::network::socket &&' for 1st argument  class socket         ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::future/ >' to       'const process::network::socket &' for 1st argument  class socket         ../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter 't' here   bool set(const t& t);                     ^  1 error generated.  make[4]:  [libprocesslalibeventsslsocket.lo] error 1  make[4]:  waiting for unfinished jobs....  mv f .deps/libprocesslalibeventpoll.tpo .deps/libprocesslalibeventpoll.plo  mv f .deps/libprocesslaopenssl.tpo .deps/libprocesslaopenssl.plo  mv f .deps/libprocesslaprocess.tpo .deps/libprocesslaprocess.plo  make[3]:  [allrecursive] error 1  make[2]:  [allrecursive] error 1  make[1]:  [all] error 2  make:  [allrecursive] error 1",2,train
MESOS-2944,Use of EXPECT in test and relying on the checked condition afterwards.,"in dockercontainerizertest we have the following pattern.          expectne(0u, offers.get().size());        const offer& offer = offers.get()[0];      as we rely on the value afterwards we should use assertne instead. in that case the test will fail immediately. ",1,train
MESOS-2946,Authorizer Module: Interface design,h4.motivation  design an interface covering authorizer modules while staying minimally invasive in regards to changes to the existing localauthorizer implementation.  ,2,train
MESOS-2947,"Authorizer Module: Implementation, Integration & Tests",h4.motivation  provide an example authorizer module based on the localauthorizer implementation. make sure that such authorizer module can be fully unit and integration tested within the mesos test suite.  ,8,train
MESOS-2949,Draft design for generalized Authorizer interface,as mentioned in mesos 2948 the current mesos::authorizer interface is rather inflexible if new actions or objects need to be added.    a new api needs to be designed in a way that allows for arbitrary actions and objects to be added to the authorization mechanism without having to recompile mesos.,3,train
MESOS-2950,Implement current mesos Authorizer in terms of generalized Authorizer interface,"in order to maintain compatibility with existent versions of mesos, as well as to prove the flexibility of the generalized mesos::authorizer design, the current authorization mechanism through acl definitions needs to run under the updated interface without any changes being noticeable by the current authorization users.",8,train
MESOS-2951,Inefficient container usage collection,"docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). there is scope for making this efficient, say by querying cgroups file system.    ",3,train
MESOS-2956,Stack trace in isolator tests on Linux VM,"perfeventisolatortest fails with stack trace when run in linux vm    [] 1 test from perfeventisolatortest  [ run      ] perfeventisolatortest.rootcgroupssample  f0629 11:38:17.088412 14114 isolatortests.cpp:837] checksome(isolator): failed to create perfevent isolator, invalid events:     check failure stack trace:       @     0x2ab5e5aeeb1a  google::logmessage::fail()      @     0x2ab5e5aeea66  google::logmessage::sendtolog()      @     0x2ab5e5aee468  google::logmessage::flush()      @     0x2ab5e5af137c  google::logmessagefatal::logmessagefatal()      @           0x864b0c  checkfatal::checkfatal()      @           0xc458ed  mesos::internal::tests::perfeventisolatortestrootcgroupssampletest::testbody()      @          0x119fb17  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x119ac9e  testing::internal::handleexceptionsinmethodifsupported/()      @          0x118305f  testing::test::run()      @          0x1183782  testing::testinfo::run()      @          0x1183d0a  testing::testcase::run()      @          0x11889d4  testing::internal::unittestimpl::runalltests()      @          0x11a09ae  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x119b9c3  testing::internal::handleexceptionsinmethodifsupported/()      @          0x11878e0  testing::unittest::run()      @           0xcdc8c7  main      @     0x2ab5e7fdbec5  (unknown)      @           0x861a89  (unknown)  make[3]:  [checklocal] aborted (core dumped)    [ run      ] usercgroupisolatortest/2.rootcgroupsusercgroup  f0629 11:49:38.763434 18836 isolatortests.cpp:1200] checksome(isolator): failed to create perfevent isolator, invalid events:     check failure stack trace:       @     0x2ba40eb2db1a  google::logmessage::fail()      @     0x2ba40eb2da66  google::logmessage::sendtolog()      @     0x2ba40eb2d468  google::logmessage::flush()      @     0x2ba40eb3037c  google::logmessagefatal::logmessagefatal()      @           0x864b0c  checkfatal::checkfatal()      @           0xc5ddb1  mesos::internal::tests::usercgroupisolatortestrootcgroupsusercgrouptest/::testbody()      @          0x119fc43  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x119adca  testing::internal::handleexceptionsinmethodifsupported/()      @          0x118318b  testing::test::run()      @          0x11838ae  testing::testinfo::run()      @          0x1183e36  testing::testcase::run()      @          0x1188b00  testing::internal::unittestimpl::runalltests()      @          0x11a0ada  testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x119baef  testing::internal::handleexceptionsinmethodifsupported/()      @          0x1187a0c  testing::unittest::run()      @           0xcdc9f3  main      @     0x2ba41101aec5  (unknown)      @           0x861a89  (unknown)  make[3]:  [checklocal] aborted (core dumped)    ",1,train
MESOS-2957,Add version to MasterInfo,this will help schedulers figure out the version of the master that they are interacting with. see mesos 2736 for additional context.,1,train
MESOS-2958,Update Call protobuf to move top level FrameworkInfo inside Subscribe,it is better for frameworkinfo to be only included in 'subscribe' message (that needs to be added) instead of for every call. instead the top level call should contain a frameworkid to identify the framework making the call.,3,train
MESOS-2961,Add cpuacct subsystem utils to cgroups,"current cgroups implementation does not have a cpuacct subsystem implementation. this subsystem reports important metrics like user and system cpu ticks spent by a process. ""cgroups"" namespace has subsystem specific utilities for ""cpu"", ""memory"" etc. it could use other subsystems specific utils (eg. cpuacct).    in the future, we could also view cgroups as a mesossubsystem with  features like event notifications.    although refactoring cgroups would be a different epic, listing the possible tasks:      have hierarchies, subsystems abstracted to represent the domain      create  ""cgroups service""      ""cgroups service"" listen to update events from the os on files like stats. this would be an interrupt based system(maybe use linux fsnotify)      ""cgroups service"" services events to mesos (containers for example).    ",2,train
MESOS-2962,Slave fails with Abort stacktrace when DNS cannot resolve hostname,"if the dns cannot resolve the hostnametoip for a slave node, we correctly return an error object, but we then fail with a segfault.    this code adds a more userfriendly message and exits normally (with an exit_failure code).    for example, forcing net::getip() to always return an error, now causes the slave to exit like this:      $ ./bin/mesosslave.sh master=10.10.1.121:5405  warning: logging before initgooglelogging() is written to stderr  e0630 11:31:45.777465 1944417024 process.cpp:899] could not obtain the ip address for stratos.local; the dns service may not be able to resolve it: >>> marco was here    $ echo $?  1  ",1,train
MESOS-2963,Configure Jenkins to build ssl,nan,5,train
MESOS-2964,libprocess io does not support peek(),"finally, i so wish we could just do:      io::peek(request >socket, 6)    .then([request](const string& data)  else if (...)  else if (...)             if (ssl)  else     });      from:  https:/reviews.apache.org/r/31207/",3,train
MESOS-2965,Add implicit cast to string operator to Path.,for example:    inline try/ rm(const std::string& path) does not have an overload for inline try/ rm(const path& path)    the implementation should be something like:     inline try/ rm(const path& path)    ,2,train
MESOS-2966,socket::peer() and socket::address() might fail with SSL enabled,libevent ssl currently uses a secondary fd so we need to virtualize the get() function on socket interface. ,5,train
MESOS-2967,Missing doxygen documentation for libprocess socket interface ,convert existing comments to doxygen format.  ,5,train
MESOS-2968,Implement shared copy based provisioner backend,"currently appc and docker both implemented its own copy backend, but most of the logic is the same where the input is just a image name with its dependencies.  we can refactor both so that we just have one implementation that is shared between both provisioners, so appc and docker can reuse the shared copy backend.",3,train
MESOS-2971,Implement OverlayFS based provisioner backend,"part of the image provisioning process is to call a backend to create a root filesystem based on the image on disk layout.  the problem with the copy backend is that it's both waste of io and space, and bind only can deal with one layer.  overlayfs backend allows us to utilize the filesystem to merge multiple filesystems into one efficiently.",5,train
MESOS-2972,Serialize Docker image spec as protobuf,"the docker image specification defines a schema for the metadata json that it puts into each image. currently the docker image provisioner needs to be able to parse and understand this metadata json, and we should create a protobuf equivelent schema so we can utilize the json to protobuf conversion to read and validate the metadata.",3,train
MESOS-2973,SSL tests don't work with --gtest_repeat,commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f  author: joris van remoortere /  date:   wed jul 1 16:16:52 2015 0700        mesos2973: allow ssl tests to run using gtest_repeat.            the ssl ctx object carried some settings between reinitialize()      calls. re construct the object to avoid this state transition.            review: https:/reviews.apache.org/r/36074,3,train
MESOS-2974,stout flags can't have their defaults reset,"stout flags don't remember their default values, and so can't have their defaults reset. this makes it hard to reset flags to their defaults between tests.",5,train
MESOS-2975,SSL tests don't work with --gtest_shuffle,nan,3,train
MESOS-2980,Allow runtime configuration to be returned from provisioner,"image specs also includes execution configuration (e.g: env, user, ports, etc).  we should support passing those information from the image provisioner back to the containerizer.",5,train
MESOS-2983,Deprecating '.json' extension in slave endpoints url,remove the '.json' extension on endpoints such as `/slave/state.json` so it become `/slave/state`,1,train
MESOS-2984,Deprecating '.json' extension in files endpoints url,remove the '.json' extension on endpoints such as `/files/browse.json` so it become `/files/browse`,1,train
MESOS-2986,Docker version output is not compatible with Mesos,"we currently use docker version to get docker version, in docker master branch and soon in docker 1.8 [1] the output for this command changes. the solution for now will be to use the unchanged docker version output, in the long term we should consider stop using the cli and use the api instead.     [1] https:/github.com/docker/docker/pull/14047",1,train
MESOS-2991,Compilation Error on Mac OS 10.10.4 with clang 3.5.0,"compiling 0.23.0 (rc1) produces compilation errors on mac os 10.10.4 with g based on llvm 3.5. it looks like the issue was introduced in a5640ad813e6256b548fca068f04fd9fa3a03eda, https:/reviews.apache.org/r/32838. in contrast to the commit message, compiling the rc with gcc4.4 on centos worked fine for me.     according to 0.23 release notes and mesos2604, we should support clang 3.5.       ../../../../../3rdparty/libprocess/3rdparty/stout/tests/ostests.cpp:543:25: error: conversion from 'void ()' to 'const option/' is ambiguous                     fork(dosetsid,          / greatgreatgranchild.                          ~  ../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:40:3: note: candidate constructor    option(const t& t) : state(some), t(t) {}      ../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:42:3: note: candidate constructor    option(t&& t) : state(some), t(std::move(t)) {}      ../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:45:3: note: candidate constructor [with u = void ()]    option(const u& u) : state(some), t(u) {}          compiler version:    $ g version  configured with: prefix=/applications/xcode.app/contents/developer/usr withgxxincludedir=/usr/include/c/4.2.1  apple llvm version 6.0 (clang600.0.54) (based on llvm 3.5svn)  target: x8664apple darwin14.4.0  thread model: posix    ",1,train
MESOS-2993,Document  per container unique egress flow and network queueing statistics,document new network isolation capabilities in 0.23,3,train
MESOS-2994,Design doc for creating user namespaces inside containers,nan,5,train
MESOS-2995,Standardize use of Path ,"as per the discussion in mesos 2965, the use of the path object should be standardized:   functions which effectively use paths (as strings) should instead take paths.   functions which modify and return paths (as strings) should instead return paths.    extraneous uses of path.value should be removed.",3,train
MESOS-2997,SSL connection failure causes failed CHECK.,  [ run      ] ssltest.basicsameprocess  f0706 18:32:28.465451 238583808 libeventsslsocket.cpp:507] check failed: 'self >bev' must be non null  ,3,train
MESOS-3001,"Create a ""demo"" HTTP API client","we want to create a simple ""demo"" http api client (in java, python or go) that can serve as an ""example framework"" for people who will want to use the new api for their frameworks.    the scope should be fairly limited (eg, launching a simple container task?) but sufficient to exercise most of the new api endpoint messages/capabilities.    scope: tbd    nongoals:      create a ""bestofbreed"" framework to deliver any specific functionality;    create an integration test for the http api.",8,train
MESOS-3002,Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator,"change to option from get() to getorelse() breaks network isolator.  building with '../configure withnetworkisolator' generates the following error:    ../../src/slave/containerizer/isolators/network/portmapping.cpp: in static member function 'static try/ mesos::internal::slave::portmappingisolatorprocess::create(const mesos::internal::slave::flags&)':  ../../src/slave/containerizer/isolators/network/portmapping.cpp:1103:29: error: no matching function for call to 'option/ >::get(const char [1]) const'         flags.resources.get(""""),                                 ../../src/slave/containerizer/isolators/network/portmapping.cpp:1103:29: note: candidates are:  in file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,                   from ../../3rdparty/libprocess/include/process/check.hpp:19,                   from ../../3rdparty/libprocess/include/process/collect.hpp:7,                   from ../../src/slave/containerizer/isolators/network/portmapping.cpp:30:  ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const t& option/::get() const [with t = std::basicstring/]     const t& get() const                 ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided  ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: t& option/::get() [with t = std::basicstring/]     t& get()         ^  ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided  make[2]:  [slave/containerizer/isolators/network/libmesosno3rdpartylaport_mapping.lo] error 1  make[2]: leaving directory `/home/pbrett/sandbox/mesos.master/build/src'  make[1]:  [check] error 2  make[1]: leaving directory `/home/pbrett/sandbox/mesos.master/build/src'  make:  [checkrecursive] error 1  ",1,train
MESOS-3004,Design support running the command executor with provisioned image for running a task in a container,"mesos containerizer uses the command executor to actually launch the user defined command, and the command executor then can communicate with the slave about the process lifecycle.  when we provision a new container with the user specified image, we also need to be able to run the command executor in the container to support the same semantics.  one approach is to dynamically mount in a static binary of the command executor with all its dependencies in a special directory so it doesn't interfere with the provisioned root filesystem and configure the mesos containerizer to run the command executor in that directory.",5,train
MESOS-3005,SSL tests can fail depending on hostname configuration,"depending on how /etc/hosts is configured, the ssl tests can fail with a bad hostname match for the certificate.  we can avoid this by explicitly matching the hostname for the certificate to the ip that will be used during the test.",3,train
MESOS-3006,Add cgroups memory stats API,"cgroups api current does expose ""stats"" from the memory namespace. having this api would enable isolators to use its various fields(eg. rss, rss_huge, writeback etc) in use cases like usage metrics.",2,train
MESOS-3008,Libevent SSL doesn't use EPOLL,we currently disable to epoll in libevent to allow ssl to work.  it would be more scalable if we didn't have to do that.,8,train
MESOS-3009,Reproduce systemd cgroup behavior ,"it has been noticed before that systemd reorganizes cgroup hierarchy created by mesos slave. because of this mesos is no longer able to find the cgroup, and there is also a chance of undoing the isolation that mesos slave puts in place. ",5,train
MESOS-3012,Support existing message passing optimization with Event/Call.,"see the thread here:  http:/markmail.org/thread/wvapc7vkbv7z6gbx    the scheduler driver currently sends framework messages directly to the slave, when possible:                        (through master)      scheduler  > master  >  slave >  executor       driver    >                driver                     (skip master)      the slave always sends messages directly to the scheduler driver:        scheduler         master          slave <  executor       driver    <                driver                     (skip master)      in order for the scheduler driver to receive events from the master, it needs enough information to continue directly sending messages to slaves. this was previously accomplished by sending the slave's pid inside the https:/github.com/apache/mesos/blob/0.23.0rc1/src/messages/messages.proto#l168:      message resourceoffersmessage       we could add an 'address' to the offer protobuf to provide the scheduler driver with the same information:      message address     message offer       the path prefix is required for testing purposes, where we can have multiple slaves within a process (e.g. localhost:5051/slave(1)/state.json vs. localhost:5051/slave(2)/state.json).    this provides enough information to allow the scheduler driver to continue to directly send messages to the slaves, which unblocks mesos2910.",1,train
MESOS-3013,"Extend ContainerInfo to include ""NetworkInfo"" message","as per the https:/docs.google.com/document/d/17mxtamdaxcnbwpjfrxmzcqrs7eo6ancsbejrqjlq0g, we need to enable frameworks to specify network requirements. the proposed message could be along the lines of:      /    collection of network request.    todo(kapil): add a highlevel explanation/motivation.    /  message networkinfo       / todo: document how to use this field to request an    / 1) ipv4 address    / 2) ipv6 address    / 3) any of the above    optional protocol protocol = 1;      / statically assigned ips provided by the framework.    optional string ipaddress = 2;      / a group is the name given to a set of logicallyrelated ips that are    / allowed to communicate within themselves. for example, one might want     / to create separate groups for dev, testing, qa and prod deployment     / environments.      repeated string groups = 3;      / to tag certain metadata to be used by isolator/ipam. e.g., rack, pop, etc.    optional labels labels = 4;  };    message containerinfo ;    message containerstatus     message taskstatus ;  ",2,train
MESOS-3015,Add hooks for Slave exits,"the hook will be triggered on slave exits. a master hook module can use this to do slave specific cleanups.    in our particular use case, the hook would trigger cleanup of ips assigned to the given slave (see the  https:/docs.google.com/document/d/17mxtamdaxcnbwp_jfrxmzcqrs7eo6ancsbejrqjlq0g/edit#).",2,train
MESOS-3016,Add task status update hooks for Master/Slave,the task termination hooks are needed for doing task specific cleanup in master/slave.,3,train
MESOS-3017,Make container-IP available via Master endpoint,nan,5,train
MESOS-3018,A mechanism for messages between Master modules and Slave modules,a slave module should be able to send a message to a master module and viceversa to allow outof band communication between master/slave modules.,8,train
MESOS-3020,"Expose major, minor and patch components from stout Version  ","stout version class does not expose version components, preventing computations manipulation of version information.  solution is to make major, minor and patch public.",1,train
MESOS-3021,Implement Docker Image Provisioner Reference Store,"create a comprehensive store to look up an image and tag's associated image layer id. implement add, remove, save, and update images and their associated tags.",3,train
MESOS-3023,Factoring out the pattern for URL generation ,"fetchertest.cpp uses the following code for generating urls:    string url = ""http:/""  net::gethostname(process.self().address.ip).get()  "":""  stringify(process.self().address.port)  ""/""  process.self().id    it would be good to isolate that code in a function, and replace the code above with something like:    string url = ""http:/""  endpointurl(process, ""uri_test"");  ",1,train
MESOS-3024,HTTP endpoint authN is enabled merely by specifying --credentials,"if i set `credentials` on the master, framework and slave authentication are allowed, but not required. on the other hand, http authentication is now required for authenticated endpoints (currently only `/shutdown`). that means that i cannot enable framework or slave authentication without also enabling http endpoint authentication. this is undesirable.    framework and slave authentication have separate flags (`\authenticate` and `\authenticate_slaves`) to require authentication for each. it would be great if there was also such a flag for http authentication. or maybe we get rid of these flags altogether and rely on acls to determine which unauthenticated principals are even allowed to authenticate for each endpoint/action.",8,train
MESOS-3025,0.22.x scheduler driver drops 0.23.x reconciliation status updates due to missing StatusUpdate.uuid.,"in the process of fixing mesos2940, we accidentally introduced a nonbackwards compatible change:    > statusupdate.uuid was required in 0.22.x and was always set.  > statusupdate.uuid is optional in 0.23.x and the master is not setting it for master generated updates.    in 0.22.x, the scheduler driver ignores the 'uuid' for master/driver generated updates already. i'd suggest the following fix:    # in 0.23.x, rather than not setting statusupdate.uuid, set it to an empty string.  # in 0.23.x, ensure the scheduler driver also ignores empty statusupdate.uuids.  # in 0.24.x, stop setting statusupdate.uuid.",3,train
MESOS-3026,ProcessTest.Cache fails and hangs,"  [ run      ] processtest.cache  ../../../3rdparty/libprocess/src/tests/process_tests.cpp:1726: failure  value of: response.get().status    actual: ""200 ok""  expected: ""304 not modified""  [  failed  ] processtest.cache (1 ms)    the tests then finish running, but the gtest framework fails to terminate and uses 100% cpu.",5,train
MESOS-3032,Document containerizer launch ,"we currently dont have enough documentation for the containerizer component. this task adds documentation for containerizer launch sequence.  the mail goals are:   have diagrams (state, sequence, class etc) depicting the containerizer launch process.   make the documentation newbie friendly.    usable for future design discussions.",3,train
MESOS-3035,As a Developer I would like a standard way to run a Subprocess in libprocess,"as part of mesos2830 and mesos2902 i have been researching the ability to run a subprocess and capture the stdout / stderr along with the exit status code.    process::subprocess() offers much of the functionality, but in a way that still requires a lot of handiwork on the developer's part; we would like to further abstract away the ability to just pass a string, an optional set of command line arguments and then collect the output of the command (bonus: without blocking).",3,train
MESOS-3037,Add a SUPPRESS call to the scheduler,"suppress call is the complement to the current revive call i.e., it will inform mesos to stop sending offers to the framework.     for the scheduler driver to send only call messages (mesos 2913), deactivateframeworkmessage needs to be converted to call(s). we can implement this by having the driver send a suppress call followed by a decline call for outstanding offers.",3,train
MESOS-3038,"Resource offers do not contain Unavailability, given a maintenance schedule","given a schedule, defined elsewhere, any resource offers to affected slaves must include an unavailability field.    the maintenance schedule for a single slave should be held in mesos2075 and locally by the master.  i.e. in src/master/master.hpp:    struct slave     the new field should be populated via an api call (see [mesos2067]).    the unavailability field can be added to master::offer (src/master/master.cpp).    offer>mutable_unavailability()>mergefrom(slave >pendingdowntime);      possible test(s):   pendingunavailibilitytest   start master, slave.   check unavailability of offer == none.   set unavailability to the future.    check offer has unavailability.  ",8,train
MESOS-3039,Allow executors binding IP to be different than Slave binding IP,"currently, the slave will bind either to the loopback ip (127.0.0.1) or to the ip passed via the 'ip' flag. when it launches a containerized executor (e.g, via mesos containerizer), the executor inherits the binding ip of the slave. this is due to the fact that the 'ip' flags sets the environment variable `libprocessip` to the passed ip. the executor then inherits this environment variable and is forced to bind to the slave ip.    if an executor is running in its own containerized environment, with a separate ip than that of the slave, currently there is no way of forcing it to bind to its own ip. a potential solution is to use the executor environment decorator hooks to update libprocessip environment variable for the executor.",2,train
MESOS-3041,"Decline call does not include an optional ""reason"", in the Event/Call API","in the event/call api, the decline call is currently used by frameworks to reject resource offers.    in the case of inverseoffers, the framework could give additional information to the operators and/or allocator, as to why the inverseoffer is declined. i.e. suppose a cluster running some consensus algorithm is given an inverseoffer on one of its nodes.  it may decline saying ""too few nodes"" (or, more verbosely, ""specified inverseoffer would lower the number of active nodes below quorum"").    this change requires the following changes:   include/mesos/scheduler/scheduler.proto:    message call       ...  }     src/master/master.cpp  change master::decline to either store the reason, or log it.   add a declineoffer overload in the (mesos)schedulerdriver with an optional ""reason"".   extend the interface in include/mesos/scheduler.hpp    add/change the declineoffer method in src/sched/sched.cpp",3,train
MESOS-3042,Master/Allocator does not send InverseOffers to resources to be maintained,"offers are currently sent from master/allocator to framework via resourceoffersmessage's.  inverseoffers, which are roughly equivalent to negative offers, can be sent in the same package.  in src/messages/messages.proto    message resourceoffersmessage       sent inverseoffers can be tracked in the master's local state:  i.e. in src/master/master.hpp:    struct slave       one actor (master or allocator) should populate the new inverseoffers field.   in master (src/master/master.cpp)   master::offer is where the resourceoffersmessage and offer object is constructed.   the same method could also check for maintenance and send inverseoffers.   in the allocator (src/master/allocator/mesos/hierarchical.hpp)   hierarchicalallocatorprocess::allocate is where slave resources are aggregated an sent off to the frameworks.   inverseoffers (i.e. negative resources) allocation could be calculated in this method.   a change to master::offer (i.e. the ""offercallback"") may be necessary to account for the negative resources.    possible test(s):   inverseoffertest   start master, slave, framework.   accept resource offer, start task.   set maintenance schedule to the future.   check that inverseoffer(s) are sent to the framework.   decline inverseoffer.   check that more inverseoffer(s) are sent.   accept inverseoffer.   check that more inverseoffer(s) are sent.",8,train
MESOS-3043,Master does not handle InverseOffers in the Accept call (Event/Call API),"inverseoffers are similar to offers in that they are accepted or declined based on their offerid.      some additional logic may be neccesary in master::accept (src/master/master.cpp) to gracefully handle the acceptance of inverseoffers.   the inverseoffer needs to be removed from the set of pending inverseoffers.   the inverseoffer should not result any errors/warnings.      note: accepted inverseoffers do not preclude further inverseoffers from being sent to the framework.  instead, an accepted inverseoffer merely signifies that the framework is currently fine with the expected downtime.",3,train
MESOS-3044,Slaves are not deactivated upon reaching a maintenance window,"after a maintenance window is reached, the slave should be deactivated to prevent further tasks from utilizing it.       for slaves that have completely drained, simply deactivate the slave.  see master::deactivate(slave).   for tasks which have not explicitly declined the inverseoffers (i.e. they've accepted them or do not understand inverseoffers), send kill signals.  see master::killtask   if a slave has tasks that have declined the inverseoffers, do not deactivate the slave.    possible test(s):   slavedrainedtest   start master, slave.   set maintenance to now.   check that slave gets deactivated   inverseofferagnostictest   start master, slave, framework.   have a task run on the slave (ignores inverseoffers).   set maintenance to now.   check that task gets killed.   check that slave gets deactivated.   inverseofferacceptancetest   start master, slave, framework.   run a task on the slave.   set maintenance to future.   have task accept inverseoffer.   check task gets killed, slave gets deactivated.   inverseofferdeclinedtest   start master, slave, framework.   run task on slave.   set maintenance to future.   have task decline maintenance with reason.   check task lives, slave still active.",8,train
MESOS-3045,Maintenance information is not populated in case of failover,"when a master starts up, or after a master has failed, it must re populate maintenance information (i.e. from the registry to the local state).    particularly, master::recover in src/master/master.cpp should be changed to process maintenance information.",3,train
MESOS-3046,Stout's UUID re-seeds a new random generator during each call to UUID::random.,"per [stephanerb] and [kevints]'s observations on mesos2940, stout's uuid abstraction is reseeding the random generator during each call to uuid::random(), which is really expensive.    this is confirmed in the perf graph from mesos 2940.",3,train
MESOS-3050,Failing ROOT_ tests on CentOS 7.1,"running `sudo make check` on centos 7.1 for mesos 0.23.0rc3 causes several several failures/errors:      [ run      ] dockertest.rootdockercheckportresource  ../../src/tests/dockertests.cpp:303: failure  (run).failure(): container exited on error: exited with status 1  [  failed  ] dockertest.rootdockercheckportresource (709 ms)    ...    [ run      ] perfeventisolatortest.rootcgroupssample  ../../src/tests/isolatortests.cpp:837: failure  isolator: failed to create perfevent isolator, invalid events:   [  failed  ] perfeventisolatortest.rootcgroupssample (9 ms)  [] 1 test from perfeventisolatortest (9 ms total)       [] 2 tests from sharedfilesystemisolatortest  [ run      ] sharedfilesystemisolatortest.rootrelativevolume   mount n bind /tmp/sharedfilesystemisolatortestrootrelativevolume4yteac/var/tmp /var/tmp   touch /var/tmp/492407e15dec4b348f2f130430f41aac  ../../src/tests/isolatortests.cpp:1001: failure  value of: os::exists(file)    actual: true  expected: false  [  failed  ] sharedfilesystemisolatortest.rootrelativevolume (92 ms)  [ run      ] sharedfilesystemisolatortest.rootabsolutevolume   mount n bind /tmp/sharedfilesystemisolatortestrootabsolutevolumeowyrxk /var/tmp   touch /var/tmp/7de712aa52eb4976b0f932b6a006418d  ../../src/tests/isolatortests.cpp:1086: failure  value of: os::exists(path::join(containerpath, filename))    actual: true  expected: false  [  failed  ] sharedfilesystemisolatortest.rootabsolutevolume (100 ms)    ...    [] 1 test from usercgroupisolatortest/0, where typeparam = mesos::internal::slave::cgroupsmemisolatorprocess  userdel: user 'mesos.test.unprivileged.user' does not exist  [ run      ] usercgroupisolatortest/0.rootcgroupsusercgroup  bash: /sys/fs/cgroup/blkio/user.slice/cgroup.procs: permission denied  mkdir: cannot create directory /sys/fs/cgroup/blkio/user.slice/user: permission denied  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/blkio/user.slice/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/memory/mesos/bbf8c8f03d6740dfa269b3dc6a9597aa/cgroup.procs: permission denied  bash: /sys/fs/cgroup/cpuacct,cpu/user.slice/cgroup.procs: no such file or directory  mkdir: cannot create directory /sys/fs/cgroup/cpuacct,cpu/user.slice/user: no such file or directory  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpuacct,cpu/user.slice/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/cgroup.procs: no such file or directory  mkdir: cannot create directory /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/user: no such file or directory  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  [  failed  ] usercgroupisolatortest/0.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupsmemisolatorprocess (1034 ms)  [] 1 test from usercgroupisolatortest/0 (1034 ms total)  [] 1 test from usercgroupisolatortest/1, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  userdel: user 'mesos.test.unprivileged.user' does not exist  [ run      ] usercgroupisolatortest/1.rootcgroupsusercgroup  bash: /sys/fs/cgroup/blkio/user.slice/cgroup.procs: permission denied  mkdir: cannot create directory /sys/fs/cgroup/blkio/user.slice/user: permission denied  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/blkio/user.slice/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f07c5c4185869d635d51dcc6e1/cgroup.procs: no such file or directory  mkdir: cannot create directory /sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f07c5c4185869d635d51dcc6e1/user: no such file or directory  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f07c5c4185869d635d51dcc6e1/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/cgroup.procs: no such file or directory  mkdir: cannot create directory /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/user: no such file or directory  ../../src/tests/isolatortests.cpp:1274: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/name=systemd/user.slice/user2004.slice/session3865.scope/user/cgroup.procs: no such file or directory  ../../src/tests/isolatortests.cpp:1283: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess (763 ms)  [] 1 test from usercgroupisolatortest/1 (763 ms total)  [] 1 test from usercgroupisolatortest/2, where typeparam = mesos::internal::slave::cgroupsperfeventisolatorprocess  userdel: user 'mesos.test.unprivileged.user' does not exist  [ run      ] usercgroupisolatortest/2.rootcgroupsusercgroup  ../../src/tests/isolatortests.cpp:1200: failure  isolator: failed to create perfevent isolator, invalid events:   [  failed  ] usercgroupisolatortest/2.rootcgroups_usercgroup, where typeparam = mesos::internal::slave::cgroupsperfeventisolatorprocess (6 ms)  [] 1 test from usercgroupisolatortest/2 (6 ms total)  ",5,train
MESOS-3051,performance issues with port ranges comparison,"testing in an environment with lots of frameworks (>200), where the frameworks permanently decline resources they don't need. the allocator ends up spending a lot of time figuring out whether offers are refused (the code path through hierarchicalallocatorprocess::isfiltered().    in profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of resources::contains() run time is in operator = (resource). 35% of resources::contains() run time is in resources::contains().    the heaviest call chain through resources::contains is:      running time          self (ms)         symbol name  7237.0ms   35.5%          4.0            mesos::resources::contains(mesos::resource const&) const  7200.0ms   35.3%          1.0             mesos::contains(mesos::resource const&, mesos::resource const&)  7133.0ms   35.0%        121.0              mesos::operator/::delete(mesos::valuerange)  3209.0ms   15.7% 0.0     void google::protobuf::internal::repeatedptrfieldbase::destroy/::typehandler>()  3209.0ms   15.7% 0.0      google::protobuf::repeatedptrfield/::repeatedptrfield()  3209.0ms   15.7% 0.0       google::protobuf::repeatedptrfield/::repeatedptrfield()  3209.0ms   15.7% 0.0        mesos::valueranges::valueranges()  3209.0ms   15.7% 0.0         mesos::valueranges::valueranges()  2441.0ms   11.9% 0.0          mesos::coalesce(mesos::valueranges, mesos::valuerange const&)   452.0ms    2.2% 0.0          mesos::remove(mesos::valueranges, mesos::valuerange const&)   169.0ms    0.8% 0.0          mesos::operator/::new()  2541.0ms   12.4% 0.0    google::protobuf::repeatedptrfield/::typehandler::type google::protobuf::internal::repeatedptrfieldbase::add/::typehandler>()  2305.0ms   11.3% 0.0     google::protobuf::repeatedptrfield/::add()  2305.0ms   11.3% 0.0      mesos::valueranges::addrange()  1962.0ms    9.6% 0.0       mesos::coalesce(mesos::valueranges, mesos::valuerange const&)   343.0ms    1.6% 0.0       mesos::ranges::add(mesos::valueranges, long long, long long)    236.0ms    1.1% 0.0     void google::protobuf::internal::repeatedptrfieldbase::mergefrom/::typehandler>(google::protobuf::internal::repeatedptrfieldbase const&)  1471.0ms    7.2% 1471.0   google::protobuf::internal::repeatedptrfieldbase::reserve(int)  1333.0ms    6.5% 0.0    google::protobuf::repeatedptrfield/::typehandler::type google::protobuf::internal::repeatedptrfieldbase::add/::typehandler>()  1333.0ms    6.5% 0.0     google::protobuf::repeatedptrfield/::add()  1333.0ms    6.5% 0.0      mesos::valueranges::addrange()  1086.0ms    5.3% 0.0       mesos::coalesce(mesos::valueranges, mesos::valuerange const&)   247.0ms    1.2% 0.0       mesos::ranges::add(mesos::valueranges, long long, long long)    107.0ms    0.5% 0.0    void google::protobuf::internal::repeatedptrfieldbase::mergefrom/::typehandler>(google::protobuf::internal::repeatedptrfieldbase const&)  107.0ms    0.5% 0.0     google::protobuf::repeatedptrfield/::mergefrom(google::protobuf::repeatedptrfield/ const&)  107.0ms    0.5% 0.0      mesos::valueranges::mergefrom(mesos::valueranges const&)  105.0ms    0.5% 0.0       mesos::valueranges::copyfrom(mesos::valueranges const&)  105.0ms    0.5% 0.0        mesos::valueranges::operator=(mesos::valueranges const&)  104.0ms    0.5% 0.0         mesos::coalesce(mesos::valueranges, mesos::valuerange const&)    1.0ms    0.0% 0.0         mesos::remove(mesos::valueranges, mesos::valuerange const&)    2.0ms    0.0% 0.0       mesos::resource::mergefrom(mesos::resource const&)    2.0ms    0.0% 0.0        google::protobuf::internal::generictypehandler/::merge(mesos::resource const&, mesos::resource)    2.0ms    0.0% 0.0         void google::protobuf::internal::repeatedptrfieldbase::mergefrom/::typehandler>(google::protobuf::internal::repeatedptrfieldbase const&)   29.0ms    0.1% 0.0    void google::protobuf::internal::repeatedptrfieldbase::mergefrom/::typehandler>(google::protobuf::internal::repeatedptrfieldbase const&)    898.0ms    4.4% 898.0   google::protobuf::repeatedptrfield/::typehandler::type google::protobuf::internal::repeatedptrfieldbase::add/::typehandler>()  517.0ms    2.5% 0.0    google::protobuf::repeatedptrfield/::add()  517.0ms    2.5% 0.0     mesos::valueranges::addrange()  429.0ms    2.1% 0.0      mesos::coalesce(mesos::valueranges, mesos::valuerange const&)   88.0ms    0.4% 0.0      mesos::ranges::add(mesos::valueranges , long long, long long)  379.0ms    1.8% 0.0    void google::protobuf::internal::repeatedptrfieldbase::mergefrom/::typehandler>(google::protobuf::internal::repeatedptrfieldbase const&)    ",8,train
MESOS-3055,Master doesn't properly handle SUBSCRIBE call,"master::subscribe() incorrectly handles reregistration. it handles it as a registration request (not ""reregistration"") because of a bug in the if loop (should have been !frameworkinfo.hasid()).      void master::subscribe(      const upid& from,      const scheduler::call::subscribe& subscribe)   else   }    ",2,train
MESOS-3060,FTP response code for success not recognized by fetcher.,"the response code for successful http requests is 200, the response code for successful ftp file transfers is 226. the fetcher currently only checks for a response code of 200 even for ftp uris. this results in failed fetching even though the resource gets downloaded successfully. this has been found by a dedicated external test using an ftp server.  ",1,train
MESOS-3061,Expose docker container IP in Master's state.json,"we want to expose docker container ip to mesos dns. one potential solution is to make it available via master's state.json. we can set a label ""docker.networksettings.ipaddress"" in taskstatus message (when it is sent the first time with task_running status).",2,train
MESOS-3062,Add authorization for dynamic reservation,"dynamic reservations should be authorized with the principal of the reserving entity (framework or master). the idea is to introduce reserve and unreserve into the acl.        message reserve       message unreserve       when a framework/operator reserves resources, ""reserve"" acls are checked to see if the framework (frameworkinfo.principal) or the operator (credential.user) is authorized to reserve the specified resources. if not authorized, the reserve operation is rejected.    when a framework/operator unreserves resources, ""unreserve"" acls are checked to see if the framework (frameworkinfo.principal) or the operator (credential.user) is authorized to unreserve the resources reserved by a framework or operator (resource.reservationinfo.principal). if not authorized, the unreserve operation is rejected.",2,train
MESOS-3064,Add 'principal' field to 'Resource.DiskInfo.Persistence',"in order to support authorization for persistent volumes, we should add the principal to resource.diskinfo, analogous to resource.reservationinfo.principal.",1,train
MESOS-3065,Add framework authorization for persistent volume,"this is the third in a series of tickets that adds authorization support to persistent volumes.    when a framework creates a persistent volume, ""create"" acls are checked to see if the framework (frameworkinfo.principal) or the operator (credential.user) is authorized to create persistent volumes. if not authorized, the create operation is rejected.    when a framework destroys a persistent volume, ""destroy"" acls are checked to see if the framework (frameworkinfo.principal) or the operator (credential.user) is authorized to destroy the persistent volume created by a framework or operator (resource.diskinfo.principal). if not authorized, the destroy operation is rejected.    a separate ticket will use the structures created here to enable authorization of the ""/create"" and ""/destroy"" http endpoints: https:/issues.apache.org/jira/browse/mesos 3903",5,train
MESOS-3066,Replicated registry needs a representation of maintenance schedules,"in order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry.    this means adding an additional message in the registry protobuf in src/master/registry.proto.  the status of each individual slave's maintenance will also be persisted in this way.    message maintenance       message schedule       message schedules       optional schedules schedules = 1;  }      note: there can be multiple slaveid's attached to a single hostname.",3,train
MESOS-3067,Implement a streaming response decoder for events stream,"we need a streaming response decoder to deserialize chunks sent from the master on the events stream.    from the http api design doc:  master encodes each event in recordio format, i.e. a string representation of length of the event in bytes followed by json or binary protobuf  (possibly compressed) encoded event.    as of now for getting the basic features right , this is being done in the testcases:        auto reader = response.get().reader;    assertsome(reader);      future/ eventfuture = reader.get().read();    awaitready(eventfuture);      event event;    event.parsefromstring(eventfuture.get());      two things need to happen:   we need master to emit events in recordio format i.e. event size followed by the serialized event instead of just the serialized events as is the case now.   the decoder class should then abstract away the logic of reading the response and de serializing events from the stream.    ideally, the decoder should work with both ""json"" and ""protobuf"" responses.  ",3,train
MESOS-3068,Registry operations are hardcoded for a single key (Registry object),"this is primarily a refactoring.    the prototype for modifying the registry is currently:    try/ operator () (      registry registry,      hashset/ slaveids,      bool strict);      in order to support maintenance schedules (possibly quotas as well), there should be an alternate prototype for maintenance.  something like:    try/ operation () (      maintenance  maintenance,      bool strict);      the existing registrarprocess::update (src/master/registrar.cpp) should be refactored to allow for more than one key.  if necessary, refactor existing operations defined in src/master/master.hpp (adminslave, readminslave,  removeslave).",5,train
MESOS-3069,Registry operations do not exist for manipulating maintanence schedules,"in order to modify the maintenance schedule in the replicated registry, we will need operations (src/master/registrar.hpp).    the operations will likely correspond to the http api:   updatemaintenanceschedule: given a blob representing a maintenance schedule, perform some verification on the blob.  write the blob to the registry.     startmaintenance:  given a set of machines, verify then transition machines from draining to deactivated.    stopmaintenance:  given a set of machines, verify then transition machines from deactivated to normal.  remove affected machines from the schedule(s).",8,train
MESOS-3072,Unify initialization of modularized components,"h1.introduction    as it stands right now, default implementations of modularized components are required to have a non parametrized create() static method. this allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way.    for example, with the interface foo:      class foo       virtual future/ hello() = 0;    protected:    foo() {}  };      with a default implementation:      class localfoo       virtual future/ hello()   };      this allows to create typed tests which look as following:      typedef ::testing::types/>    footesttypes;    typedtestcase(footest, footesttypes);    typedtest(footest, atest)        the test will be applied to each of types in the template parameters of footesttypes. this allows to test different implementation of an interface. in our code, it tests default implementations and a module which uses the same default implementation.    the class tests::module/ needs a little explanation, it is a wrapper around modulemanager which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method. the wrapper around create, the real important method looks as follows:      template/  static try/ test::module/::create()      return mesos::modules::modulemanager::create/(modulename.get());  }      h1.the problem    consider the following implementation of foo:      class parameterfoo       parameterfoo(int i) : i(i) {}      virtual future/ hello()     private:    int i;  };      as it can be seen, this implementation cannot be used as a default implementation since its create api does not match the one of test::module/: create() has a different signature for both types. it is still a common situation to require initialization parameters for objects, however this constraint (keeping both interfaces alike) forces default implementations of modularized components to have default constructors, therefore the tests are forcing the design of the interfaces.    implementations which are supposed to be used as modules only, i.e. non default implementations are allowed to have constructor parameters, since the actual signature of their factory method is, this factory method's function is to decode the parameters and call the appropriate constructor:      template/  t module/::create(const parameters& params);      where parameters is just an array of keyvalue string pairs whose interpretation is left to the specific module. sadly, this call is wrapped by   modulemanager which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules.    h1.the ugly workaround    with the requirement of a default constructor and parameters devoid create() factory function, a common pattern (see https:/github.com/apache/mesos/blob/9d4ac11ed757aa5869da440dfe5343a61b07199a/include/mesos/authentication/authenticator.hpp) has been introduced to feed construction parameters into default implementation, this leads to adding an initialize() call to the public interface, which will have foo become:      class foo       virtual try/ initialize(option/ i) = 0;      virtual future/ hello() = 0;    protected:    foo() {}  };      parameterfoo will thus look as follows:      class parameterfoo       parameterfoo() : i(none()) {}      virtual try/ initialize(option/ i)         i = i;        return nothing;    }      virtual future/ hello()         return i.get();    }    private:    option/ i;  };      look that this initialize() method now has to be implemented by all descendants of foo, even if there's a databasefoo which takes is  return value for hello() from a db, it will need to support int as an initialization parameter.    the problem is more severe the more specific the parameter to initialize() is. for example, if there is a very complex structure implementing acls, all implementations of an authorizer will need to import this structure even if they can completely ignore it.    in the foo example if parameterfoo were to become the default implementation of foo, the tests would look as follows:      typedef ::testing::types/>    footesttypes;    typedtestcase(footest, footesttypes);    typedtest(footest, atest)    ",3,train
MESOS-3073,Introduce HTTP endpoints for Quota,we need to implement the http endpoints for quota as outlined in the design doc: (https:/docs.google.com/document/d/16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq 9i).  ,3,train
MESOS-3074,Add capacity heuristic for quota requests in Master,"we need to to validate quota requests in the mesos master as outlined in the design doc: https:/docs.google.com/document/d/16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq 9i    this ticket aims to validate satisfiability (in terms of available resources) of a quota request using a heuristic algorithm in the mesos master, rather than validating the syntax of the request.",3,train
MESOS-3076,Add Labels to TaskStatus and expose them via state.json,this would allow the executors and slave modules to expose some metadata to frameworks and mesosdns via state.json.    a typical use case is to allow the containers to expose their ip to framework/mesos dns.,2,train
MESOS-3077,Registry recovery does not recover the maintenance object.,"persisted info is fetched from the registry when a master is elected or after failover.  currently, this process involves 3 steps:   fetch the ""registry"".   start an operation to add the new master to the fetched registry.   check the success of the operation and finish recovering.  these methods can be found in src/master/registrar.cpp registrarprocess::recover, ::recover, ::recover    since the maintenance schedule is stored in a separate key, the recover process must also fetch a new ""maintenance"" object.  this object needs to be passed along to the master along with the existing ""registry"" object.    possible test(s):   src/tests/registrartests.cpp   change the ""recovery"" test to include checks for the new object.",5,train
MESOS-3078,Recovered resources are not re-allocated until the next allocation delay.,"currently, when resources are recovered, we do not perform an allocation for that slave. rather, we wait until the next allocation interval.    for small task, high throughput frameworks, this can have a significant impact on overall throughput, see the following thread:  http:/markmail.org/thread/y6mzfwzlurv6nik3    we should consider immediately performing a re allocation for the slave upon resource recovery.",5,train
MESOS-3079,`sudo make distcheck` fails on Ubuntu 14.04 (and possibly other OSes too),"running tests as root causes a large number of failures.    $ lsbrelease a  lsb version:    core2.0amd64:core2.0noarch:core3.0amd64:core3.0noarch:core3.1amd64:core3.1noarch:core3.2amd64:core3.2noarch:core4.0amd64:core4.0noarch:core4.1amd64:core4.1noarch:cxx3.0amd64:cxx3.0noarch:cxx3.1amd64:cxx3.1noarch:cxx3.2amd64:cxx3.2noarch:cxx4.0amd64:cxx4.0noarch:cxx4.1amd64:cxx4.1noarch:desktop3.1amd64:desktop3.1noarch:desktop3.2amd64:desktop3.2noarch:desktop4.0amd64:desktop4.0noarch:desktop4.1amd64:desktop4.1noarch:graphics2.0amd64:graphics2.0noarch:graphics3.0amd64:graphics3.0noarch:graphics3.1amd64:graphics3.1noarch:graphics3.2amd64:graphics3.2noarch:graphics4.0amd64:graphics4.0noarch:graphics4.1amd64:graphics4.1noarch:languages3.2amd64:languages3.2noarch:languages4.0amd64:languages4.0noarch:languages4.1amd64:languages4.1noarch:multimedia3.2amd64:multimedia3.2noarch:multimedia4.0amd64:multimedia4.0noarch:multimedia4.1amd64:multimedia4.1noarch:printing3.2amd64:printing3.2noarch:printing4.0amd64:printing4.0noarch:printing4.1amd64:printing4.1noarch:qt43.1amd64:qt43.1noarch:security4.0amd64:security4.0noarch:security4.1amd64:security4.1noarch  distributor id: ubuntu  description:    ubuntu 14.04.2 lts  release:        14.04  codename:       trusty    $ sudo make j12 v=0 check    [==========] 712 tests from 116 test cases ran. (318672 ms total)  [  passed  ] 676 tests.  [  failed  ] 36 tests, listed below:  [  failed  ] perfeventisolatortest.rootcgroupssample  [  failed  ] usercgroupisolatortest/2.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupsperfeventisolatorprocess  [  failed  ] slaverecoverytest/0.recoverslavestate, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.recoverstatusupdatemanager, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.reconnectexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.recoverunregisteredexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.recoverterminatedexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.recovercompletedexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.cleanupexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.removenoncheckpointingframework, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.noncheckpointingframework, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.killtask, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.reboot, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.gcexecutor, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.shutdownslave, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.shutdownslavesigusr1, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.registerdisconnectedslave, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.reconcilekilltask, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.reconcileshutdownframework, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.reconciletasksmissingfromslave, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.schedulerfailover, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.partitionedslave, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.masterfailover, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.multipleframeworks, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.multipleslaves, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] slaverecoverytest/0.restartbeforecontainerizerlaunch, where typeparam = mesos::internal::slave::mesoscontainerizer  [  failed  ] mesoscontainerizerslaverecoverytest.resourcestatistics  [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootperfrollforward  [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespaceforward  [  failed  ] mesoscontainerizerslaverecoverytest.cgroupsrootpidnamespacebackward  [  failed  ] cgroupsanyhierarchywithperfeventtest.rootcgroupsperf  [  failed  ] memorypressuremesostest.cgroupsrootstatistics  [  failed  ] memorypressuremesostest.cgroupsrootslaverecovery  [  failed  ] nstest.rootsetns  [  failed  ] perftest.rootevents  [  failed  ] perftest.rootsamplepid    36 failed tests      full log attached.",2,train
MESOS-3082,Perf related tests rely on 'cycles' which might not always be present.,"when running the tests on ubuntu 14.04 the 'cycles' value collected by perf is always 0, meaning certain tests always fail. these lines in the test have been commented out for now and a todo has been attached which links to this jira issue, since the solution is unclear. in particular, 'cycles' might not properly be counted because it is a hardware counter and this particular machine was a virtual machine. either way, we should determine the best events to collect from perf in either vm or physical settings.",5,train
MESOS-3083,Doing 'clone' on Linux with the CLONE_NEWUSER namespace type can drop root privileges.,"the namespace tests attempt to clone a process with all namespaces that are available from the kernel which includes the 'user' namespace in ubuntu 14.04 which causes the child process to be user 'nobody' instead of user 'root' after invoking 'clone' which is bad because the test requires that the child process is 'root' and so things fail (because of insufficient permissions). for now, we explicitly ignore the 'user' namespace in the tests, but this issue is to track exactly how we might want to manage this going forward.",5,train
MESOS-3086,Create cgroups TasksKiller for non freeze subsystems.,"we have a number of test issues when we cannot remove cgroups (in case there are still related tasks running) in cases where the freezer subsystem is not available.   in the current code (https:/github.com/apache/mesos/blob/0.22.1/src/linux/cgroups.cpp#l1728)  we will fallback to a very simple mechnism of recursivly trying to remove the cgroups which fails if there are still tasks running.   therefore we need an additional  (nonfreeze)taskskiller which doesn't  rely on the freezer subsystem.    this problem caused issues when running 'sudo make check' during 0.23 release testing, where benh provided already a better error message with b1a23d6a52c31b8c5c840ab01902dbe00cb1feef / https:/reviews.apache.org/r/36604.  ",4,train
MESOS-3087,Typos in oversubscription doc," in docs/oversubscription.md: there are three cases where ""revocable"" is written as ""recovable"", including the name of a json field.      $ grep  nir recovable .  ./docs/oversubscription.md:51:with revocable resources.  further more, recovable resources cannot be  ./docs/oversubscription.md:95:launching tasks using recovable resources is done through the existing  ./docs/oversubscription.md:96:`launchtasks` api. revocable resources will have the `recovable` field set. see       also in `docs/oversubscription.md`: the last sentence doesn't make sense      to select custom a resource estimator and qos controller, please refer to the  [modules documentation](modules.md).      maybe should say ""to select a custom..."" or ""to install a custom...""",1,train
MESOS-3088,Update scheduler driver to send SUBSCRIBE call,see mesos 2913 for context.,2,train
MESOS-3089,Update scheduler library to send REQUEST call,"see mesos 2913 for context.    from the dev list it looks like users depend on this call for their custom allocator, so we need to support it going forward.",2,train
MESOS-3092,Configure Jenkins to run Docker tests,add a jenkin job to run the docker tests,2,train
MESOS-3093,Support HTTPS requests in libprocess,"in order to pull images from docker registries, https calls are needed to securely communicate with the registry hosts. currently, only http requests are supported through libprocess. now that ssl sockets are available through libprocess, support for https can be added.",3,train
MESOS-3095,PoC running command executor with image provisioner,this is to implement a poc of the alternative design choices with mesos 3004,3,train
MESOS-3096,Authentication for Communicating with Docker Registry,"in order to pull docker images from docker hub and private docker registries, the provisioner must support two primary authentication frameworks to authenticate with the registries, basic authentication and the oauth2.0 authorization framework, as per the docker registry spec. a docker registry can also operate in standalone mode and may not require authentication.",5,train
MESOS-3097,OS-specific code touched by the containerizer tests is not Windows compatible,"in the process of adding the cmake build system,  noted and stubbed out all osspecific code.  that sweep (mostly of libprocess and stout) is here:  https:/github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52    instead of having inline #if defined(...), the osspecific code will be separated into directories.  the windows code will be stubbed out.",13,train
MESOS-3098,Implement WindowsContainerizer and WindowsDockerContainerizer,"the mvp for windows support is a containerizer that (1) runs on windows, and (2) runs and passes all the tests that are relevant to the windows platform (e.g., not the tests that involve cgroups). to do this we require at least a `windowscontainerizer` (to be implemented alongside the `mesoscontainerizer`), which provides no meaningful (e.g.) process namespacing (much like the default unix containerizer). in the long term (hopefully before mesoscon) we want to support also the windows container api. this will require implementing a separate containerizer, maybe called `windowsdockercontainerizer`.    since the windows container api is actually officially supported through the docker interface (i.e., msft actually ported the docker engine to windows, and that is the official api), the interfaces (like the fetcher) shouldn't change much. the tests probably will have to change, as we don't have access to any isolation primitives like cgroups for those tests.    outstanding todo(): flesh out this description when more details are available, regarding:   the container api for windows (when we know them)   the nuances of windows vs linux (when we know them)    etc.",13,train
MESOS-3099,Validation of Docker Image Manifests from Docker Registry,docker image manifests pulled from remote docker registries should be verified against their signature digest before they are used. ,3,train
MESOS-3100,Validation of Docker Layers Pulled From Docker Registry,"docker layers should be verified against their checksum digests before they are stored to ensure the integrity of the docker layer content. this includes supporting sha256, sha384, sha512 hash algorithms.",3,train
MESOS-3101,Standardize separation of Windows/Linux-specific OS code,"there are 50+ files that must be touched to separate os specific code.    first, we will standardize the changes by using stout/abort.hpp as an example.  the review/discussion can be found here:  https:/reviews.apache.org/r/36625/",3,train
MESOS-3102,Separate OS-specific code in the stout library,this issue tracks changes for all files under 3rdparty/libprocess/3rdparty/stout/    the changes will be based on this commit:  https:/github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff a6d038bad64b154996452bec020cfa7c,5,train
MESOS-3103,Separate OS-specific code in the libprocess library,this issue tracks changes for all files under 3rdparty/libprocess/include/ and 3rdparty/libprocess/src.    the changes will be based on this commit:  https:/github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff a6d038bad64b154996452bec020cfa7c,5,train
MESOS-3105,Port flag generation logic from the autotools solution to CMake,"one major barrier to widespread adoption of the cmakebased build system (other than the fact that we haven't implemented it yet!) is that most of our institutional knowledge of the quirks of how to build mesos across many platforms is tied up in files like `configure.ac`.    therefore, a ""good"" cmakebased build system will require us to go through these files systematically and manually port this logic to cmake (as well as testing it).",3,train
MESOS-3106,Extend CMake build system to support building against third-party libraries from either the system or the local Mesos rebundling,"currently mesos has thirdparty dependencies of two types: (1) those that are expected to be on the system (such as apr, libsvn, etc.), and (2) those that have been historically bundled as tarballs inside the mesos repository, and are not expected to be on the system when mesos is installed (these are located in the `3rdparty/` directory, and includes things like boost and glog).    for type (2), the mvp of the cmakebased build system will always pull down a fresh tarball from an external source, instead of using the bundled tarballs in the `3rdparty/` folder.    however, many ci systems do not have internet access, so in the long term, we need to provide many options for getting these dependencies.",5,train
MESOS-3107,Define CMake style guide,"the short story is that it is important to be principled about how the cmake build system is maintained, because there cmake language makes it difficult to statically verify that a configuration is correct. it is not unique in this regard, but (make is arguably even worse) but it is something that's important to make sure we get right.    the longer story is, cmake's language is dynamically scoped and often has somewhat odd defaults for variable values (e.g., iirc, target names passed to externalprojectadd default to ""prefix"" instead of erroring out). this means that it is rare to get a configuration time error (i.e., cmake usually doesn't say something like ""hey this variable isn't defined""), and in large projects, this can make it very difficult to know where definitions come from, or whether it's important that one config routine runs before another. dynamic scoping also makes it particularly easy to write spaghetti code, which is clearly undesirable for something as important as a build system.    thus, it is particularly important that we lay down our expectations for how the cmake system is to be structured. this might include:     function naming (e.g., making it easy to tell whether a function was defined by us, and where it was defined; so we might say that we want our functions to have an underscore to start, and start with the package the come from, like libprocess, so that we know where to look for the definition.)   what assertions we want to check variable values against, so that we can replace subtle errors (e.g., a library is accidentally named something silly like ""prefix.0.0.1"") with an obvious ones (e.g., ""you have failed to define your target name, so cmake has defaulted to 'prefix'; please check your configuration routines"")   decisions of what goes where. (e.g._, the most complex parts of the cmake mvps is in the configuration routines, like `mesosconfigure.cmake`; to curb this, we should have strict rules about what goes in that file vs other files, and how we know what is to be run before what. part of this should probably be prominent comments explaining the structure of the project, so that people aren't confused!)   and so on.",3,train
MESOS-3108,Add autotools-style Mesos distributions to the CMake build system,"in the autoconf based build system, we there is a notion of building a ""distribution"" of mesos. essentially, it is a version of mesos that is configured for a specific platform (ubuntu, say); so, if a consumer knows their platform, and there is a mesos distribution, they need only run `make all` and mesos builds. this allows the consumer to skip the configure step.    in cmake, it should be possible to do this (should be!), and we should explore making it work after we complete the mvp.",3,train
MESOS-3109,Expand CMake build system to support building the containerizer and associated components,"in other tasks in epic mesos898, we implement a cmakebased build system that allows us to build process library, the process tests, and the stout tests.    for the cmake build system mvp, it's important that we expand this to build the containerizer, associated modules, and all related tests.",3,train
MESOS-3110,Harden the CMake system-dependency-locating routines,"currently the mesos project has two flavors of dependency: (1) the dependencies we expect are already on the system (e.g., apr, libsvn), and (2) the dependencies that are historically bundled with mesos (e.g., glog).    dependency type (1) requires solid modules that will locate them on any system: linux, bsd, or windows. this would come for free if we were using cmake 3.0, but we're using cmake 2.8 so that ubuntu users can install it out of the box, instead of upgrading cmake first.    this is additionally useful for dependency type (2), where we will expect to have to use these routines when we support both the rebundled dependencies in the `3rdparty/` folder, and system installations of those dependencies.",3,train
MESOS-3112,Fetcher should perform cache eviction based on cache file usage patterns.,"currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. this means that potentially a file that has just been used gets evicted the next moment. this performance loss can be avoided by even the simplest enhancement of the selection procedure.    proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `fetcherprocess::cache::selectvictims(const bytes& requiredspace)`. suggestion: approximate mruretention somehow.    unittest what actually happens!",8,train
MESOS-3113,Add resource usage section to containerizer documentation,"currently, the containerizer documentation doesn't touch upon the usage() api and how to interpret the collected statistics.",3,train
MESOS-3114,"Simplify JSON::* by providing ""jsonify"" along the lines of ""stringify""","we want to be able to do things like:      json::value number1 = 25;  json::number number2 = 26;    expectne(number1, number2);  expecteq(jsonify(12), number1);  expect_eq(jsonify(12), number2);  ",3,train
MESOS-3115,"Convert mesos::slave::{Limitation,ExecutorRunState} into protobufs.",published rr: https:/reviews.apache.org/r/36718/,1,train
MESOS-3116,Pass ExecutorInfo argument into Isolator::isolate().,"some isolators need to lookup the executor environment variables to customize their isolation needs. currently, one has to use the ""prepare()"" call to cache the executor info to use it later during isolate() call.",2,train
MESOS-3117,Pass ContainerId into `slaveExecutorEnvironmentDecorator` hook,nan,1,train
MESOS-3118,Remove pthread specific code from Stout,nan,3,train
MESOS-3119,Remove pthread specific code from Libprocess,nan,3,train
MESOS-3120,Remove pthread specific code from Mesos,nan,3,train
MESOS-3121,Always disable SSLV2,"the ssl protocol mismatch tests are failing on centos7 when matching sslv2 with sslv2. since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",2,train
MESOS-3122,Add configurable UNIMPLEMENTED macro to stout,"during the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un implemented.  to support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",2,train
MESOS-3124,Updating persistent volumes after slave restart is problematic.,"just realize that while reviewing https:/reviews.apache.org/r/34135    since we don't checkpoint 'resources' in mesos containerizer, when slave restarts and recovers, the 'resources' in container struct will be empty, but there are symlinks exists in the sandbox.    we'll end up with trying to create already exist symlinks (and fail). i think we should ignore the creation if it already exists.",3,train
MESOS-3125,DOCKER_HOST env variable stopped working for executors,"with https:/reviews.apache.org/r/36282/ no environment variables are available anymore in the docker executors. hence, setting docker_host outside of mesos stopped working.    setups which use a remote docker daemon or tools like powerstrip stopped working.",2,train
MESOS-3127,Improve task reconciliation documentation.,include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.,1,train
MESOS-3129,Move all MesosContainerizer related files under src/slave/containerizer/mesos,"currently, some mesoscontainerizer specific files are not in the correct location. for example:     src/slave/containerizer/isolators/   src/slave/containerizer/provisioner.hpp|cpp      they should be put under src/slave/containerizer/mesos/",2,train
MESOS-3130,Custom isolators should implement Isolator instead of IsolatorProcess.,"similar to mesos2213, we should not restrict custom isolators to use libprocess process. we should do a similar refactor as we did for mesos2213.",3,train
MESOS-3131,Master should send heartbeats on the subscription connection,"in order to deal with network partitions and ensuring network intermediately do not close the persistent subscription connection, master must periodically send heartbeats.     the expectation with schedulers is that they resubscribe when they do not receive heartbeats for some time.",3,train
MESOS-3132,Allow slave to forward messages through the master for HTTP schedulers.,"the master currently has no install handler for executortoframework messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.    we need to preserve this behavior for the driver, but http schedulers will not have a libprocess 'pid'. we'll have to ensure that the runtaskmessage and updateframeworkmessage have an optional pid. for now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available.",5,train
MESOS-3133,Isolator::prepare() should return Executor environment vars as well,"sometimes the isolators need to pass on some environment variables for the executor that is being launched. for example, to successfully launch an executor inside a network namespace, one needs to set libprocessip to point to the container ip, otherwise the executor tries to bind to the slave ip which may be invalid inside the namespace. another example is where the file system isolator should be able to specify the workdir depending on if a new rootfs is used.",2,train
MESOS-3134,Port bootstrap to CMake,"bootstrap does a lot of significant things, like setting up the git commit hooks. we will want something like bootstrap to run also on systems that don't have bash  ideally this should just run in cmake itself.",5,train
MESOS-3135,Publish MasterInfo to ZK using JSON,"following from mesos 2340, which now allows master to correctly decode json information (masterinfo) published to zookeeper, we can now enable the master leader contender to serialize it too in json.",2,train
MESOS-3138,PersistentVolumeTest.SlaveRecovery test fails on OSX,"with a clean build (make clean) running this tests fails:      gtestfilter=""persistentvolumetest. "" make check      this is the log:    [==========] running 7 tests from 1 test case.  [] global test environment setup.  [] 7 tests from persistentvolumetest  [ run      ] persistentvolumetest.sendingcheckpointresourcesmessage  [       ok ] persistentvolumetest.sendingcheckpointresourcesmessage (189 ms)  [ run      ] persistentvolumetest.resourcescheckpointing  [       ok ] persistentvolumetest.resourcescheckpointing (86 ms)  [ run      ] persistentvolumetest.preparepersistentvolume  [       ok ] persistentvolumetest.preparepersistentvolume (82 ms)  [ run      ] persistentvolumetest.masterfailover  [       ok ] persistentvolumetest.masterfailover (130 ms)  [ run      ] persistentvolumetest.incompatiblecheckpointedresources  [       ok ] persistentvolumetest.incompatiblecheckpointedresources (74 ms)  [ run      ] persistentvolumetest.accesspersistentvolume  i0723 11:21:40.265787 1955922688 exec.cpp:132] version: 0.24.0  i0723 11:21:40.268676 174858240 exec.cpp:206] executor registered on slave 2015072311214016777343618582866s0  e0723 11:21:40.268697 178077696 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  e0723 11:21:40.273510 178077696 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  registered executor on localhost  starting task 39e32f2b475e47549e3d39fd56fb787b  forked command at 2911  sh c 'echo abc > path1/file'  e0723 11:21:40.281900 178077696 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  command exited with status 0 (pid: 2911)  e0723 11:21:40.389068 178077696 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  [       ok ] persistentvolumetest.accesspersistentvolume (421 ms)  [ run      ] persistentvolumetest.slaverecovery  i0723 11:21:40.639749 1955922688 exec.cpp:132] version: 0.24.0  i0723 11:21:40.641904 187400192 exec.cpp:206] executor registered on slave 2015072311214016777343618582866s0  e0723 11:21:40.641943 191156224 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  e0723 11:21:40.646507 191156224 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  registered executor on localhost  starting task 809fa50fbee04c9ba770434183a9650b  sh c 'while true; do test d path1; done'  forked command at 2941  e0723 11:21:40.655097 191156224 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  i0723 11:21:40.671840 186863616 exec.cpp:252] received reconnect request from slave 2015072311214016777343618582866s0  e0723 11:21:40.671953 191156224 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  i0723 11:21:40.672744 187400192 exec.cpp:229] executor reregistered on slave 2015072311214016777343618582866s0  e0723 11:21:40.672839 191156224 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  reregistered executor on localhost  ../../src/tests/persistentvolumetests.cpp:709: failure  value of: status2.get().state()    actual: taskfailed  expected: task_killed  [  failed  ] persistentvolumetest.slaverecovery (286 ms)  [] 7 tests from persistentvolumetest (1268 ms total)    [] global test environment tear down  [==========] 7 tests from 1 test case ran. (1289 ms total)  [  passed  ] 6 tests.  [  failed  ] 1 test, listed below:  [  failed  ] persistentvolumetest.slaverecovery     1 failed test    you have 8 disabled tests  ",2,train
MESOS-3139,Incorporate CMake into standard documentation,"right now it's anyone's guess how to build with cmake. if we want people to use it, we should put up documentation. the central challenge is that the cmake instructions will be slightly different for different platforms.    for example, on linux, the gist of the build is basically the same as autotools; you pull down the system dependencies (like apr, etc.), and then:    ```  ./bootstrap  mkdir buildcmake && cd buildcmake  cmake ..  make  ```    but, on windows, it will be somewhat more complicated. there is no bootstrap step, for example, because windows doesn't have bash natively. and even when we put that in, you'll still have to build the glog stuff outofband because cmake has no way of booting up visual studio and calling ""build.""    so practically, we need to figure out:     what our build story is for different platforms   write specific instructions for our ""core"" target platforms.",13,train
MESOS-3140,Implement Docker remote puller,"given a docker image name and registry host url, fetches the image. if necessary, it will download the manifest and layers from the registry host. it will place the layers and image manifest into persistent store.  done when a docker image can be successfully stored and retrieved using 'put' and 'get' methods.",5,train
MESOS-3141,Compiler warning when mocking function type has an enum return type.,"the purpose of this ticket is to document a very cryptic error message (actually a warning that gets propagated by werror) that gets generated by clang3.5 from gmock source code when trying to mock a perfectly innocentlooking function.    problem    the following code is attempting to mock a mesosexecutordriver:      class mockmesosexecutordriver : public mesosexecutordriver       mockmethod1(sendstatusupdate, status(const taskstatus&));  };      the above code generates the following error message:      in file included from ../3rdparty/libprocess/3rdparty/gmock1.6.0/include/gmock/gmock.h:58:  in file included from ../3rdparty/libprocess/3rdparty/gmock1.6.0/include/gmock/gmockactions.h:46:  ../3rdparty/libprocess/3rdparty/gmock1.6.0/include/gmock/internal/gmockinternalutils.h:355:10: error: indirection of nonvolatile null pointer will be deleted, not trap https:/goo.gl/t1fepz    note that if the type is not an enum, the warning is not generated. this is why existing mocked functions that return nonenum types such as future/ does not encounter this issue.    solutions    the simplest solution is to add wnonulldeference to mesostestscppflags in src/makefile.am.      mesostestscppflags = $(mesoscppflags) wnonull dereference      another solution is to upgrade gmock from 1.6 to 1.7 because this problem is solved in the newer versions.    in gmock 1.7    template /  inline t invalid()       add volatile could avoid this warning. https:/goo.gl/opcilc    ",3,train
MESOS-3142,As a Developer I want a better way to run shell commands,"when reviewing the code in https:/reviews.apache.org/r/36425/  noticed that there is a better abstraction that is possible to introduce for os::shell() that will simplify the caller's life.    instead of having to handle all possible outcomes, we propose to refactor os::shell() as follows:      /    returns the output from running the specified command with the shell.   /  try/ shell(const string& command)        where the returned string is stdout and, should the program be signaled, or exit with a nonzero exit code, we will simply return a failure with an error message that will encapsulate both the returned/signaled state, and, possibly stderr.    and some test driven development:    expecterror(os::shell(""false""));  expectsome(os::shell(""true""));    expectsomeeq(""hello world"", os::shell(""echo hello world""));      alternatively, the caller can ask to have stderr conflated with stdout:    try/ outanderr = os::shell(""mycmd  foo 2>&1"");      however, stderr will be ignored by default:    / we don't read standard error by default.  expectsomeeq("""", os::shell(""echo hello world 1>&2""));    / we don't even read stderr if something fails (to return in try::error).  try/ output = os::shell(""echo hello world 1>&2 && false"");  expecterror(output);  expectfalse(strings::contains(output.error(), ""hello world""));      an analysis of existing usage shows that in almost all cases, the caller only cares if not error; in fact, the actual exit code is read only once, and even then, in a test case.    we believe this will simplify the api to the caller, and will significantly reduce the length and complexity at the calling sites (<6 loc against the current 20+).",2,train
MESOS-3143,Disable endpoints rule fails to recognize HTTP path delegates,"in mesos, one can use the flag firewallrules to disable endpoints. disabled endpoints will return a 403 forbidden_ response whenever someone tries to access endpoints.    libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. for example, the default id of the master libprocess process is master which is also set as the delegate for the master system process, so a request to the endpoint http:/masteraddress:5050/state.json will effectively be resolved by http:/masteraddress:5050/master/state.json. but if one disables  /state.json because of how delegates work, it can still access /master/state.json.    the only workaround is to disabled both enpoints.",2,train
MESOS-3144,Update Homebrew formula for Mesos (Mac OSX),"we have pushed a https:/github.com/homebrew/homebrew/pull/42099 to homebrew for the new 0.23 formula.    once accepted, we must verify that this works on a mac osx device.  this would also be a great time to ensure our documentation is uptodate.    currently, the homebrew check fails, as they have deprecated sha1 checksums:    error message    failed: brew audit mesos  stacktrace            error: 7 problems in 1 formula  mesos:    stable resource ""protobuf"": sha1 checksums are deprecated, please use sha256    stable resource ""pythongflags"": sha1 checksums are deprecated, please use sha256    stable resource ""six"": sha1 checksums are deprecated, please use sha256    stable resource ""googleapputils"": sha1 checksums are deprecated, please use sha256    stable resource ""pythondateutil"": sha1 checksums are deprecated, please use sha256    stable resource ""boto"": sha1 checksums are deprecated, please use sha256     stable resource ""pytz"": sha1 checksums are deprecated, please use sha256      don't know enough about homebrew to really figure out what is going on here; nor how to fix this.  the mesos sha256 has been correctly entered and computed via the https:/md5file.com/calculator.    i guess, we should go download the packages and compute their sha256 and/or research from the respective download sites whether they publish the sha.",1,train
MESOS-3145,Using a unresolvable hostname crashes the framework on registration,"the following commands trigger the crash:      $ sudo hostname foo  # an unresolvable hostname  $ sudo ./bin/mesosmaster.sh ip=127.0.0.1 workdir=/var/lib/mesos  $ libprocessip=127.0.0.1 ./src/mesosexecute master=127.0.0.1:5050 name=bar command=""while true; do sleep 100; done""      the crash output:      warning: logging before initgooglelogging() is written to stderr  w0724 14:20:39.960733 1925993216 sched.cpp:1487]    scheduler driver bound to loopback interface! cannot communicate with remote master(s). you might want to set 'libprocessip' environment variable to use a routable ip address.    abort: (../../3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:85): try::get() but state == error: nodename nor servname provided, or not known[1]    24560 abort      libprocessip=127.0.0.1 ./src/mesosexecute  master=127.0.0.1:5050  ",1,train
MESOS-3146,Add a new API call to the allocator to update available resources,this ticket is to track the updateavailable api call being added to the allocator which updates the available resources in the allocator. it's used for master endpoints for dynamic reservation and persistent volumes. updateavailable is similar to updateslave except that updateavailable never leaves the allocator in an over allocated state.,8,train
MESOS-3148,Resolve issue with hanging tests with Zookeeper,"see mesos 2736 for the original issue;  the submitted https:/reviews.apache.org/r/36663 currently has no tests, the one posted in the subsequent https:/reviews.apache.org/r/36807 currently hangs when ran after the other test_f(masterzookeepertest, lostzookeepercluster).    the issue is around the await() in startmaster() (cluster.hpp #430) that waits indefinitely for the master recovery.  ",1,train
MESOS-3152,Need for HTTP delete requests,as we decided to create a more restful api for managing quota request.  therefore we also want to use the http delete request and hence need to enable the libprocess/http to send delete request besides get and post requests.,1,train
MESOS-3153,Add tests for HTTPS SSL socket communication,"unit tests are lacking for the following cases:    1. https post with ""none"" payload.   2. verification of https payload on the ssl socket(maybe decode to a request object)  3. http > ssl socket  4. https > raw socket.",3,train
MESOS-3154,"Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME","following from mesos2902 we want to enable the same functionality in the mesos agents too.    this is probably best done once we implement the new os::shell semantics, as described in mesos3142.",1,train
MESOS-3158,Libprocess Process: Join runqueue workers during finalization,the lack of synchronization between processmanager destruction and the thread pool threads running the queued processes means that the shared state that is part of the processmanager gets destroyed prematurely.  synchronizing the processmanager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.,3,train
MESOS-3161,Document using the gold linker for faster development on linux.,"the https:/en.wikipedia.org/wiki/gold(linker) seems to provide a decent speedup (about ~20%) on a parallel build. from a quick test:      gold:    real 7m18.526s  user 81m21.213s  sys 5m17.224s    default ld:    real 9m7.908s  user 85m13.466s  sys 5m52.199s      on centos 5 w/ devtoolset2:      sudo /usr/sbin/alternatives altdir /opt/rh/devtoolset2/root/etc/alternatives admindir /opt/rh/devtoolset2/root/var/lib/alternatives set ld /opt/rh/devtoolset2/root/usr/bin/ld.gold      on ubuntu:    sudo updatealternatives install /usr/bin/ld ld /usr/bin/gold 1      ideally we could this out on the website, with instructions for each os.",3,train
MESOS-3162,Provide a means to check http connection equality for streaming connections.,"if one uses an http::pipe::writer to stream a response, one cannot compare the writer with another to see if the connection has changed.    this is useful for example, in the master's http api when there is asynchronous disconnection logic. when we handle the disconnection, it's possible for the scheduler to have re subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",3,train
MESOS-3163,Proper handling of 'query' and/or 'fragment' out of 'path' in http handler.,the libprocess http.cpp post/get handlers currently do not consider query and fragments parts of the path correctly.   e.g.    if (path.issome())   ,1,train
MESOS-3164,Introduce QuotaInfo message,a quotainfo protobuf message is internal representation for quota related information (e.g. for persisting quota). the protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. it may also be used to pass quota information to allocators.,3,train
MESOS-3165,Persist and recover quota to/from Registry,"to persist quotas across failovers, the master should save them in the registry. to support this, we shall:   introduce a quota state variable in registry.proto;   extend the operation interface so that it supports a quota accumulator (see src/master/registrar.hpp);   introduce addquota / removequota operations;   recover quotas from the registry on failover to the masters internal::master::role struct;    extend registrartest with quotaspecific tests.    note: registry variable can be rather big for production clusters (see mesos2075). while it should be fine for mvp to add quota information to registry, we should consider storing quota separately, as this does not need to be in sync with slaves update. however, currently adding more variable is not supported by the registrar.    while the agents are reregistering (note they may fail to do so), the information about what part of the quota is allocated is only partially available to the master. in other words, the state of the quota allocation is reconstructed as agents reregister. during this period, some roles may be under quota from the perspective of the newly elected master.    the same problem exists on the allocator side: it may think the cluster is under quota and may eagerly try to satisfy quotas before enough agents reregister, which may result in resources being allocated to frameworks beyond their quota. to address this issue and also to avoid panicking and generating under quota alerts, the master should give a certain amount of time for the majority (e.g. 80%) of the agents to reregister before reporting any quota status and notifying the allocator about granted quotas.",5,train
MESOS-3166,Design doc for docker image registry client,create design document for the docker registry authenticator component so that we have a baseline for the implementation. ,3,train
MESOS-3167,Design doc for versioning the HTTP API,"in concert with the release of the http api, we would also like to come up with a versioning strategy. this enables to do a meaningful 1.0 release.",3,train
MESOS-3168,MesosZooKeeperTest fixture can have side effects across tests,"mesoszookeepertest fixture doesn't restart the zookeeper server for each test. this means if a test shuts down the zookeeper server, the next test (using the same fixture) might fail.     for an example see https:/reviews.apache.org/r/36807/",2,train
MESOS-3169,FrameworkInfo should only be updated if the re-registration is valid,"see ben mahler's comment in https:/reviews.apache.org/r/32961/  frameworkinfo should not be updated if the reregistration is invalid. this can happen in a few cases under the branching logic, so this requires some refactoring.  notice that a frameworkerrormessage can be generated  both inside else if (from != framework>pid) as well as from inside failoverframework(framework, from);",2,train
MESOS-3171,Fetcher Tests use EXPECT while subsequent logic relies on the outcome.,the fetcher tests use expect validation for critical measures (e.g. non empty results) and the subsequent logic releis on this (i.e. by accessing the first element). in such cases we should use assert/check.,1,train
MESOS-3173,"Mark Path::basename, Path::dirname as const functions.","the functions path::basename and path::dirname in stout/path.hpp are not marked const, although they could. marking them const would remove some ambiguities in the usage of these functions.",1,train
MESOS-3174,Fetcher logs erroneous message when successfully extracting an archive,"when fetching an asset while not using the cache, the fetcher may erroneously report this: ""copying instead of extracting resource from uri with 'extract' flag, because it does not seem to be an archive: "".    this message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. it should be absent after successful extraction.  ",1,train
MESOS-3178,Perform a self bind mount of rootfs itself in fs::chroot::enter.,"syscall 'pivotroot' requires that the old and the new root are not in the same filesystem. otherwise, the user will receive a ""device or resource busy"" error.    currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivotroot can succeed. the drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics.    for instance, in the test, we create a test rootfs by copying the host files. we need to do a self bind mount so that we can pivotroot on it. that pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount:  https:/github.com/apache/mesos/blob/master/src/tests/containerizer/launchtests.cpp#l96 l102    what i propose is that we always perform a recursive self bind mount of rootfs itself in fs::chroot::enter (after enter the new mount namespace). seems that this is also done in libcontainer:  https:/github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go#l402",2,train
MESOS-3179,Create a test abstraction for preparing test rootfs.,"several tests need this abstraction, so it's better to unify them. for example, src/tests/containerizer/launch_tests.cpp needs to create a test rootfs. we also need that to test filesystem isolators.    the test rootfs can be created by copying files/directories from host file system.",3,train
MESOS-3182,Make Master::registerFramework() and Master::reregisterFramework() call into Master::subscribe(),currently master::subscribe() calls into master::registerframework() and master::reregisterframework(). we should do it the other way around to be consistent with how we did all the other calls.,3,train
MESOS-3183,Documentation images do not load,any images which are referenced from the generated docs (docs/.md) do not show up on the website.  for example:   http:/mesos.apache.org/documentation/latest/architecture/   http:/mesos.apache.org/documentation/latest/externalcontainerizer/   http:/mesos.apache.org/documentation/latest/fetchercache internals/   http:/mesos.apache.org/documentation/latest/maintenance/     http:/mesos.apache.org/documentation/latest/oversubscription/  ,3,train
MESOS-3185,Refactor Subprocess logic in linux/perf.cpp to use common subroutine,"mesos2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  in order to achieve this, it requires to execute the ""perf  version"" command.     we should decompose the existing subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",3,train
MESOS-3189,TimeTest.Now fails with --enable-libevent,[ run      ] timetest.now  ../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: failure  expected: (microseconds(10)) / vs 0ns  [  failed  ] timetest.now (0 ms),2,train
MESOS-3191,Implement a utility for computing hash,it is useful for both appc and docker to compute and verify image hash.,2,train
MESOS-3192,ContainerInfo::Image::AppC::id should be optional,"as i commented here: https:/reviews.apache.org/r/34136/    currently containerinfo::image::appc is defined as the following          message appc       in which the id is a required field. when users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use ubuntu or ubuntu:latest and seldom a sha512 id) and we should change it to be optional.    the motivating scenario is that: if the frameworks in the mesos use something like image=ubuntu:14.04"" to run a task and image=ubuntu defaults to image=ubuntu:latest, the operator can swap the latest version for all new tasks requesting image=ubuntu. if they allow users to specify image=ubuntu:live, they can swap the live version under the covers as well. this allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",1,train
MESOS-3193,Implement AppC image discovery.,"appc spec specifies two image discovery mechanisms: simple and meta discovery. we need to have an abstraction for image discovery in appcstore. for mvp, we can implement the simple discovery first.    https:/reviews.apache.org/r/34139/",2,train
MESOS-3194,Implement a 'read-only' AppC Image Store,"it's going to be derived from this: https:/reviews.apache.org/r/34140/ (and other related patches) but in the initial 'readonly' version the store's content is prepared by outof band mechanisms so the store component in mesos only needs to provide access to images already in it and recover images upon slave restart.    this greatly simplifies the initial version's responsibility and test cases. features that fetch the images into the store will be added later and they will take into consideration its impact on task start latency and slave restart responsiveness, etc.",5,train
MESOS-3195,Fix master metrics for scheduler calls,"currently the master increments metrics for old style messages from the driver but not when it receives calls. since the driver is now sending calls, master should update metrics correctly.",3,train
MESOS-3196,Always set TaskStatus.executor_id when sending a status update message from Executor,"currently, the executor doesn't always set taskstatus.executorid. this prevents the slave taskstatus label decorator hook from knowing the executor id.    an appropriate place to automatically fill in the executorid is executorprocess::sendstatusupdate() since we are already filling in some other information here.",1,train
MESOS-3197,"MemIsolatorTest/{0,1}.MemUsage fails on OS X","looks like this is due to mlockall being unimplemented on os x.      [] 1 test from memisolatortest/0, where typeparam = n5mesos8internal5slave23posixmemisolatorprocesse  [ run      ] memisolatortest/0.memusage  failed to allocate rss memory: failed to make pages to be mapped unevictable: function not implemented../../src/tests/containerizer/isolatortests.cpp:812: failure  helper.increaserss(allocation): failed to sync with the subprocess  ../../src/tests/containerizer/isolatortests.cpp:815: failure  (usage).failure(): failed to get usage: no process found at 40558  [  failed  ] memisolatortest/0.memusage, where typeparam = n5mesos8internal5slave23posixmemisolatorprocesse (56 ms)  [] 1 test from memisolatortest/0 (57 ms total)    [] 1 test from memisolatortest/1, where typeparam = n5mesos8internal5tests6moduleins5slave8isolatorelns18moduleide0eee  [ run      ] memisolatortest/1.memusage  failed to allocate rss memory: failed to make pages to be mapped unevictable: function not implemented../../src/tests/containerizer/isolatortests.cpp:812: failure  helper.increaserss(allocation): failed to sync with the subprocess  ../../src/tests/containerizer/isolatortests.cpp:815: failure  (usage).failure(): failed to get usage: no process found at 40572  [  failed  ] memisolatortest/1.memusage, where typeparam = n5mesos8internal5tests6moduleins5slave8isolatorelns18moduleide0eee (50 ms)  [] 1 test from memisolatortest/1 (50 ms total)  ",2,train
MESOS-3199,Validate Quota Requests.,we need to validate quota requests in terms of syntactical and semantical correctness.,3,train
MESOS-3200,Remove unused 'fatal' and 'fatalerror' macros,"there exist fatal and fatalerror macros in both libprocess and stout. none of them are currently used as we favor glog's log(fatal), and therefore should be removed.",1,train
MESOS-3201,Libev handle_async can deadlock with run_in_event_loop,"due to the arbitrary nature of the functions that are executed in handleasync, invoking them under the (a) watchersmutex can lead to deadlocks if (b) is acquired before calling runineventloop and (b) is also acquired within the arbitrary function.    ==82679== thread #10: lock order ""0x60774f8 before 0x60768c0"" violated  ==82679==   ==82679== observed (incorrect) order is: acquisition of lock at 0x60768c0  ==82679==    at 0x4c32145: pthreadmutexlock (in /usr/lib/valgrind/vgpreloadhelgrindamd64linux.so)  ==82679==    by 0x692c9b: gthreadmutexlock(pthreadmutext) (gthrdefault.h:748)  ==82679==    by 0x6950bf: std::mutex::lock() (mutex:134)  ==82679==    by 0x696219: synchronized/ synchronize/(std::mutex)::::operator()(std::mutex) const (synchronized.hpp:58)  ==82679==    by 0x696238: synchronized/ synchronize/(std::mutex)::::fun(std::mutex) (synchronized.hpp:58)  ==82679==    by 0x6984cf: synchronized/::synchronized(std::mutex, void ()(std::mutex), void ()(std::mutex)) (synchronized.hpp:35)  ==82679==    by 0x6962de: synchronized/ synchronize/(std::mutex) (synchronized.hpp:60)  ==82679==    by 0x728fe1: process::handleasync(evloop, evasync, int) (libev.cpp:48)  ==82679==    by 0x761384: evinvokepending (ev.c:2994)  ==82679==    by 0x7643c4: evrun (ev.c:3394)  ==82679==    by 0x728e37: evloop (ev.h:826)  ==82679==    by 0x729469: process::eventloop::run() (libev.cpp:135)  ==82679==   ==82679==  followed by a later acquisition of lock at 0x60774f8  ==82679==    at 0x4c32145: pthreadmutexlock (in /usr/lib/valgrind/vgpreloadhelgrindamd64linux.so)  ==82679==    by 0x4c6f9d: gthreadmutexlock(pthreadmutext) (gthrdefault.h:748)  ==82679==    by 0x4c6fed: gthreadrecursivemutexlock(pthreadmutext) (gthr default.h:810)  ==82679==    by 0x4f5d3d: std::recursivemutex::lock() (mutex:175)  ==82679==    by 0x516513: synchronized/ synchronize/(std::recursivemutex)::::operator()(std::recursivemutex) const (synchronized.hpp:58)  ==82679==    by 0x516532: synchronized/ synchronize/(std::recursivemutex)::::fun(std::recursivemutex) (synchronized.hpp:58)  ==82679==    by 0x52e619: synchronized/::synchronized(std::recursivemutex, void ()(std::recursivemutex), void ()(std::recursivemutex)) (synchronized.hpp:35)  ==82679==    by 0x5165d4: synchronized/ synchronize/(std::recursive_mutex) (synchronized.hpp:60)  ==82679==    by 0x6bf4e1: process::processmanager::use(process::upid const&) (process.cpp:2127)  ==82679==    by 0x6c2b8c: process::processmanager::terminate(process::upid const&, bool, process::processbase) (process.cpp:2604)  ==82679==    by 0x6c6c3c: process::terminate(process::upid const&, bool) (process.cpp:3107)  ==82679==    by 0x692b65: process::latch::trigger() (latch.cpp:53)      this was introduced in https:/github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2",3,train
MESOS-3203,MasterAuthorizationTest.DuplicateRegistration test is flaky,"[ run      ] masterauthorizationtest.duplicateregistration  using temporary directory '/tmp/masterauthorizationtestduplicateregistrationnkt3f7'  i0804 22:16:01.578500 26185 leveldb.cpp:176] opened db in 2.188338ms  i0804 22:16:01.579172 26185 leveldb.cpp:183] compacted db in 645075ns  i0804 22:16:01.579211 26185 leveldb.cpp:198] created db iterator in 15766ns  i0804 22:16:01.579227 26185 leveldb.cpp:204] seeked to beginning of db in 1658ns  i0804 22:16:01.579238 26185 leveldb.cpp:273] iterated through 0 keys in the db in 313ns  i0804 22:16:01.579282 26185 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0804 22:16:01.579787 26212 recover.cpp:449] starting replica recovery  i0804 22:16:01.580075 26212 recover.cpp:475] replica is in empty status  i0804 22:16:01.581014 26205 replica.cpp:641] replica in empty status received a broadcasted recover request  i0804 22:16:01.581357 26211 recover.cpp:195] received a recover response from a replica in empty status  i0804 22:16:01.581761 26207 recover.cpp:566] updating replica status to starting  i0804 22:16:01.582334 26218 master.cpp:377] master 2015080422160125501413565930226185 (d6d349cd895b) started on 172.17.0.152:59302  i0804 22:16:01.582355 26218 master.cpp:379] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" credentials=""/tmp/masterauthorizationtestduplicateregistrationnkt3f7/credentials"" frameworksorter=""drf"" help=""false"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.24.0/inst/share/mesos/webui"" workdir=""/tmp/masterauthorizationtestduplicateregistrationnkt3f7/master"" zksessiontimeout=""10secs""  i0804 22:16:01.582711 26218 master.cpp:424] master only allowing authenticated frameworks to register  i0804 22:16:01.582722 26218 master.cpp:429] master only allowing authenticated slaves to register  i0804 22:16:01.582728 26218 credentials.hpp:37] loading credentials for authentication from '/tmp/masterauthorizationtestduplicateregistrationnkt3f7/credentials'  i0804 22:16:01.582929 26204 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 421543ns  i0804 22:16:01.582950 26204 replica.cpp:323] persisted replica status to starting  i0804 22:16:01.583032 26218 master.cpp:468] using default 'crammd5' authenticator  i0804 22:16:01.583132 26211 recover.cpp:475] replica is in starting status  i0804 22:16:01.583154 26218 master.cpp:505] authorization enabled  i0804 22:16:01.583356 26214 whitelistwatcher.cpp:79] no whitelist given  i0804 22:16:01.583411 26217 hierarchical.hpp:346] initialized hierarchical allocator process  i0804 22:16:01.583976 26213 replica.cpp:641] replica in starting status received a broadcasted recover request  i0804 22:16:01.584187 26209 recover.cpp:195] received a recover response from a replica in starting status  i0804 22:16:01.584581 26213 master.cpp:1495] the newly elected leader is master@172.17.0.152:59302 with id 2015080422160125501413565930226185  i0804 22:16:01.584609 26213 master.cpp:1508] elected as the leading master!  i0804 22:16:01.584627 26213 master.cpp:1278] recovering from registrar  i0804 22:16:01.584656 26204 recover.cpp:566] updating replica status to voting  i0804 22:16:01.584770 26212 registrar.cpp:313] recovering registrar  i0804 22:16:01.585261 26218 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 370526ns  i0804 22:16:01.585285 26218 replica.cpp:323] persisted replica status to voting  i0804 22:16:01.585412 26216 recover.cpp:580] successfully joined the paxos group  i0804 22:16:01.585667 26216 recover.cpp:464] recover process terminated  i0804 22:16:01.586047 26213 log.cpp:661] attempting to start the writer  i0804 22:16:01.587164 26211 replica.cpp:477] replica received implicit promise request with proposal 1  i0804 22:16:01.587549 26211 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 358261ns  i0804 22:16:01.587568 26211 replica.cpp:345] persisted promised to 1  i0804 22:16:01.588173 26209 coordinator.cpp:230] coordinator attemping to fill missing position  i0804 22:16:01.589316 26208 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0804 22:16:01.589700 26208 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 351778ns  i0804 22:16:01.589721 26208 replica.cpp:679] persisted action at 0  i0804 22:16:01.590698 26213 replica.cpp:511] replica received write request for position 0  i0804 22:16:01.590754 26213 leveldb.cpp:438] reading position from leveldb took 31557ns  i0804 22:16:01.591147 26213 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 321842ns  i0804 22:16:01.591167 26213 replica.cpp:679] persisted action at 0  i0804 22:16:01.591790 26217 replica.cpp:658] replica received learned notice for position 0  i0804 22:16:01.592133 26217 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 315281ns  i0804 22:16:01.592155 26217 replica.cpp:679] persisted action at 0  i0804 22:16:01.592180 26217 replica.cpp:664] replica learned nop action at position 0  i0804 22:16:01.592686 26211 log.cpp:677] writer started with ending position 0  i0804 22:16:01.593729 26205 leveldb.cpp:438] reading position from leveldb took 26394ns  i0804 22:16:01.596165 26209 registrar.cpp:346] successfully fetched the registry (0b) in 11.343104ms  i0804 22:16:01.596281 26209 registrar.cpp:445] applied 1 operations in 26242ns; attempting to update the 'registry'  i0804 22:16:01.598415 26212 log.cpp:685] attempting to append 178 bytes to the log  i0804 22:16:01.598563 26215 coordinator.cpp:340] coordinator attempting to write append action at position 1  i0804 22:16:01.599324 26215 replica.cpp:511] replica received write request for position 1  i0804 22:16:01.599778 26215 leveldb.cpp:343] persisting action (197 bytes) to leveldb took 420523ns  i0804 22:16:01.599800 26215 replica.cpp:679] persisted action at 1  i0804 22:16:01.600349 26204 replica.cpp:658] replica received learned notice for position 1  i0804 22:16:01.600684 26204 leveldb.cpp:343] persisting action (199 bytes) to leveldb took 310315ns  i0804 22:16:01.600706 26204 replica.cpp:679] persisted action at 1  i0804 22:16:01.600723 26204 replica.cpp:664] replica learned append action at position 1  i0804 22:16:01.601632 26213 registrar.cpp:490] successfully updated the 'registry' in 5.287936ms  i0804 22:16:01.601747 26213 registrar.cpp:376] successfully recovered registrar  i0804 22:16:01.601826 26215 log.cpp:704] attempting to truncate the log to 1  i0804 22:16:01.601948 26210 coordinator.cpp:340] coordinator attempting to write truncate action at position 2  i0804 22:16:01.602145 26208 master.cpp:1305] recovered 0 slaves from the registry (139b) ; allowing 10mins for slaves to reregister  i0804 22:16:01.602859 26219 replica.cpp:511] replica received write request for position 2  i0804 22:16:01.603181 26219 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 284713ns  i0804 22:16:01.603209 26219 replica.cpp:679] persisted action at 2  i0804 22:16:01.603984 26211 replica.cpp:658] replica received learned notice for position 2  i0804 22:16:01.604313 26211 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 302445ns  i0804 22:16:01.604365 26211 leveldb.cpp:401] deleting ~1 keys from leveldb took 29354ns  i0804 22:16:01.604387 26211 replica.cpp:679] persisted action at 2  i0804 22:16:01.604408 26211 replica.cpp:664] replica learned truncate action at position 2  i0804 22:16:01.616402 26185 sched.cpp:164] version: 0.24.0  i0804 22:16:01.616902 26209 sched.cpp:262] new master detected at master@172.17.0.152:59302  i0804 22:16:01.617000 26209 sched.cpp:318] authenticating with master master@172.17.0.152:59302  i0804 22:16:01.617019 26209 sched.cpp:325] using default crammd5 authenticatee  i0804 22:16:01.617324 26212 authenticatee.cpp:115] creating new client sasl connection  i0804 22:16:01.617550 26209 master.cpp:4405] authenticating schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.617641 26212 authenticator.cpp:406] starting authentication session for crammd5authenticatee(259)@172.17.0.152:59302  i0804 22:16:01.617858 26208 authenticator.cpp:92] creating new server sasl connection  i0804 22:16:01.618140 26216 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0804 22:16:01.618191 26216 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0804 22:16:01.618324 26213 authenticator.cpp:197] received sasl authentication start  i0804 22:16:01.618413 26213 authenticator.cpp:319] authentication requires more steps  i0804 22:16:01.618557 26216 authenticatee.cpp:252] received sasl authentication step  i0804 22:16:01.618664 26216 authenticator.cpp:225] received sasl authentication step  i0804 22:16:01.618703 26216 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'd6d349cd895b' server fqdn: 'd6d349cd895b' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0804 22:16:01.618719 26216 auxprop.cpp:174] looking up auxiliary property 'userpassword'  i0804 22:16:01.618778 26216 auxprop.cpp:174] looking up auxiliary property 'cmusaslsecretcrammd5'  i0804 22:16:01.618820 26216 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'd6d349cd895b' server fqdn: 'd6d349cd895b' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0804 22:16:01.618834 26216 auxprop.cpp:124] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0804 22:16:01.618839 26216 auxprop.cpp:124] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0804 22:16:01.618857 26216 authenticator.cpp:311] authentication success  i0804 22:16:01.618954 26219 authenticatee.cpp:292] authentication success  i0804 22:16:01.619035 26204 master.cpp:4435] successfully authenticated principal 'testprincipal' at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.619083 26219 authenticator.cpp:424] authentication session cleanup for crammd5authenticatee(259)@172.17.0.152:59302  i0804 22:16:01.619309 26208 sched.cpp:407] successfully authenticated with master master@172.17.0.152:59302  i0804 22:16:01.619335 26208 sched.cpp:713] sending subscribe call to master@172.17.0.152:59302  i0804 22:16:01.619494 26208 sched.cpp:746] will retry registration in 439203ns if necessary  i0804 22:16:01.619627 26217 master.cpp:1812] received subscribe call for framework 'default' at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.619695 26217 master.cpp:1534] authorizing framework principal 'testprincipal' to receive offers for role ''  i0804 22:16:01.620848 26217 sched.cpp:713] sending subscribe call to master@172.17.0.152:59302  i0804 22:16:01.620929 26217 sched.cpp:746] will retry registration in 2.099193326secs if necessary  i0804 22:16:01.621036 26210 master.cpp:1812] received subscribe call for framework 'default' at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.621083 26210 master.cpp:1534] authorizing framework principal 'testprincipal' to receive offers for role ''  i0804 22:16:01.621727 26217 master.cpp:1876] subscribing framework default with checkpointing disabled and capabilities [  ]  i0804 22:16:01.621981 26208 sched.cpp:262] new master detected at master@172.17.0.152:59302  i0804 22:16:01.622131 26208 sched.cpp:318] authenticating with master master@172.17.0.152:59302  i0804 22:16:01.622153 26208 sched.cpp:325] using default crammd5 authenticatee  i0804 22:16:01.622323 26212 authenticatee.cpp:115] creating new client sasl connection  i0804 22:16:01.622324 26210 hierarchical.hpp:391] added framework 20150804221601255014135659302261850000  i0804 22:16:01.622369 26210 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:01.622386 26210 hierarchical.hpp:908] performed allocation for 0 slaves in 28592ns  i0804 22:16:01.622511 26210 sched.cpp:640] framework registered with 20150804221601255014135659302261850000  i0804 22:16:01.622586 26210 sched.cpp:654] scheduler::registered took 48005ns  i0804 22:16:01.622592 26208 master.cpp:4405] authenticating schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.622673 26212 authenticator.cpp:406] starting authentication session for crammd5authenticatee(260)@172.17.0.152:59302  i0804 22:16:01.622923 26205 authenticator.cpp:92] creating new server sasl connection  i0804 22:16:01.623112 26204 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0804 22:16:01.623133 26216 master.cpp:1870] dropping subscribe call for framework 'default' at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302: reauthentication in progress  i0804 22:16:01.623144 26204 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0804 22:16:01.623258 26215 authenticator.cpp:197] received sasl authentication start  i0804 22:16:01.623313 26215 authenticator.cpp:319] authentication requires more steps  i0804 22:16:01.623394 26215 authenticatee.cpp:252] received sasl authentication step  i0804 22:16:01.623512 26212 authenticator.cpp:225] received sasl authentication step  i0804 22:16:01.623546 26212 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'd6d349cd895b' server fqdn: 'd6d349cd895b' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0804 22:16:01.623564 26212 auxprop.cpp:174] looking up auxiliary property 'userpassword'  i0804 22:16:01.623603 26212 auxprop.cpp:174] looking up auxiliary property 'cmusaslsecretcrammd5'  i0804 22:16:01.623622 26212 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'd6d349cd895b' server fqdn: 'd6d349cd895b' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0804 22:16:01.623631 26212 auxprop.cpp:124] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0804 22:16:01.623636 26212 auxprop.cpp:124] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0804 22:16:01.623649 26212 authenticator.cpp:311] authentication success  i0804 22:16:01.623777 26212 authenticatee.cpp:292] authentication success  i0804 22:16:01.623846 26212 master.cpp:4435] successfully authenticated principal 'testprincipal' at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:01.623913 26212 authenticator.cpp:424] authentication session cleanup for crammd5authenticatee(260)@172.17.0.152:59302  i0804 22:16:01.624130 26212 sched.cpp:407] successfully authenticated with master master@172.17.0.152:59302  i0804 22:16:02.583772 26218 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:02.583818 26218 hierarchical.hpp:908] performed allocation for 0 slaves in 80538ns  i0804 22:16:03.585110 26211 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:03.585156 26211 hierarchical.hpp:908] performed allocation for 0 slaves in 69272ns  i0804 22:16:04.586539 26214 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:04.586586 26214 hierarchical.hpp:908] performed allocation for 0 slaves in 79232ns  i0804 22:16:05.587239 26209 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:05.587293 26209 hierarchical.hpp:908] performed allocation for 0 slaves in 85128ns  i0804 22:16:06.587935 26212 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:06.587985 26212 hierarchical.hpp:908] performed allocation for 0 slaves in 78141ns  i0804 22:16:07.588817 26214 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:07.588865 26214 hierarchical.hpp:908] performed allocation for 0 slaves in 81433ns  i0804 22:16:08.589857 26214 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:08.589906 26214 hierarchical.hpp:908] performed allocation for 0 slaves in 71929ns  i0804 22:16:09.591085 26207 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:09.591133 26207 hierarchical.hpp:908] performed allocation for 0 slaves in 78223ns  i0804 22:16:10.591737 26207 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:10.591785 26207 hierarchical.hpp:908] performed allocation for 0 slaves in 71894ns  i0804 22:16:11.593166 26210 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:11.593221 26210 hierarchical.hpp:908] performed allocation for 0 slaves in 89782ns  i0804 22:16:12.593647 26212 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:12.593689 26212 hierarchical.hpp:908] performed allocation for 0 slaves in 69426ns  i0804 22:16:13.594154 26210 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:13.594202 26210 hierarchical.hpp:908] performed allocation for 0 slaves in 70581ns  i0804 22:16:14.594712 26207 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:14.594758 26207 hierarchical.hpp:908] performed allocation for 0 slaves in 71201ns  i0804 22:16:15.595412 26219 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:15.595464 26219 hierarchical.hpp:908] performed allocation for 0 slaves in 85183ns  i0804 22:16:16.596201 26217 hierarchical.hpp:1008] no resources available to allocate!  i0804 22:16:16.596247 26217 hierarchical.hpp:908] performed allocation for 0 slaves in 95132ns  ../../src/tests/masterauthorizationtests.cpp:794: failure  failed to wait 15secs for frameworkregisteredmessage  i0804 22:16:16.624354 26212 master.cpp:966] framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302 disconnected  i0804 22:16:16.624398 26212 master.cpp:2092] disconnecting framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:16.624445 26212 master.cpp:2116] deactivating framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:16.624686 26212 master.cpp:988] giving framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302 0ns to failover  i0804 22:16:16.625641 26219 hierarchical.hpp:474] deactivated framework 20150804221601255014135659302261850000  i0804 22:16:16.626688 26218 master.cpp:4180] framework failover timeout, removing framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:16.626734 26218 master.cpp:4759] removing framework 20150804221601255014135659302261850000 (default) at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c@172.17.0.152:59302  i0804 22:16:16.627074 26218 master.cpp:858] master terminating  i0804 22:16:16.627218 26215 hierarchical.hpp:428] removed framework 20150804221601255014135659302261850000  ../../3rdparty/libprocess/include/process/gmock.hpp:365: failure  actual function call count doesn't match expect_call(filter>mock, filter(testing::a/()))...      expected args: message matcher (8byte object /, 1byte object /, 1 byte object /)           expected: to be call...",1,train
MESOS-3205,No need to checkpoint container root filesystem path.,"given the design discussed in https:/issues.apache.org/jira/browse/mesos3004, one container might have multiple provisioned root filesystems. only checkpointing the root filesystem for containerinfo::image does not make sense.    also, we realized that checkpointing container root filesystem path is not necessary because each provisioner should be able to destroy root filesystems for a given container based on a canonical directory layout (e.g., /xxx).",3,train
MESOS-3207,C++ style guide is not rendered correctly (code section syntax disregarded),some paragraphs at the bottom of docs/mesoscstyle guide.md containing code sections are not rendered correctly by the web site generator. it looks fine in a github gist and apparently the syntax used is correct.     ,1,train
MESOS-3208,Fetch checksum files to inform fetcher cache use,"this is the first part of phase 1 as described in the comments for mesos 2073. we add a field to commandinfo::uri that contains the uri of a checksum file. when this file has new content, then the contents of the associated value uri needs to be refreshed in the fetcher cache.     in this implementation step, we just add the above basic functionality (download, checksum comparison). in later steps, we will add more control flow to cover corner cases and thus make this feature more useful.  ",3,train
MESOS-3211,As a Python developer I want a simple way to obtain information about Master from ZooKeeper,"with the new json masterinfo published to zk, we want to provide a simple library class for python developers to retrieve info about the masters and the leader.",2,train
MESOS-3212,As a Java developer I want a simple way to obtain information about Master from ZooKeeper,"with the new json masterinfo published to zk, we want to provide a simple library class for java framework developers to retrieve info about the masters and the leader.",2,train
MESOS-3213,Design doc for docker registry token manager,create design document for describing the component and interaction between docker registry client and remote docker registry for token based authorization.,2,train
MESOS-3215,CgroupsAnyHierarchyWithPerfEventTest failing on Ubuntu 14.04,"[ run      ] cgroupsanyhierarchywithperfeventtest.rootcgroupsperf  ../../src/tests/containerizer/cgroupstests.cpp:172: failure  (cgroups::destroy(hierarchy, cgroup)).failure(): failed to remove cgroup '/sys/fs/cgroup/perfevent/mesostest': device or resource busy  ../../src/tests/containerizer/cgroupstests.cpp:190: failure  (cgroups::destroy(hierarchy, cgroup)).failure(): failed to remove cgroup '/sys/fs/cgroup/perfevent/mesostest': device or resource busy  [  failed  ] cgroupsanyhierarchywithperfeventtest.rootcgroupsperf (9 ms)  [] 1 test from cgroupsanyhierarchywithperfeventtest (9 ms total)  ",3,train
MESOS-3222,Implement docker registry client,implement the following functionality:     fetch manifest from remote registry based on authorization method dictated by the registry.   fetch image layers from remote registry  based on authorization method dictated by the registry..  ,5,train
MESOS-3223,Implement token manager for docker registry,"implement the following:   a component that fetches json web authorization token from a given registry.   caches the token keyed on registry, service and scope   validates the cache for expiry date    nice to have:   cache gets pruned as tokens are aged beyond expiration time. ",4,train
MESOS-3225,some variables in version.hpp use `Type &var` instead of `Type& var`,some variables in   3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp violate mesos code style of biding '&' and ' ' to the type name  (as opposed to binding to the variable name).,1,train
MESOS-3226,Introduce an Either type.,"we currently don't have an abstraction in stout to capture the notion of having a container with many types and a single value. for example, in our abstractions like try, rather than being able to say either/ t we must encode two options (option/, option/) with the implicit invariant that exactly one will be set.    this also comes in handy in many other places in the code. note that we have the ability to (1) use c11 unions now, as well as (2) use boost's variant directly instead of introducing either. however, creating a named union every time this is needed is verbose, and unions require that we externally track which member is set. for variant, we already use this (e.g. json.hpp), but we can benefit from the better naming as either.    many languages expose either as having only two values, left and right. i'd propose making this two or more, as is the case with variant.",5,train
MESOS-3227,Implement image chroot support into command executor,nan,3,train
MESOS-3235,FetcherCacheHttpTest.HttpCachedSerialized and FetcherCacheHttpTest.HttpCachedConcurrent are flaky,"on osx, make clean && make j8 v=0 check:    [] 3 tests from fetchercachehttptest  [ run      ] fetchercachehttptest.httpcachedserialized  http/1.1 200 ok  date: fri, 07 aug 2015 17:23:05 gmt  contentlength: 30    i0807 10:23:05.673596 2085372672 exec.cpp:133] version: 0.24.0  e0807 10:23:05.675884 184373248 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  i0807 10:23:05.675897 182226944 exec.cpp:207] executor registered on slave 201508071023051393950825233852313s0  e0807 10:23:05.683980 184373248 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  registered executor on 10.0.79.8  starting task 0  forked command at 54363  sh c './mesosfetchertestcmd 0'  e0807 10:23:05.694953 184373248 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  command exited with status 0 (pid: 54363)  e0807 10:23:05.793927 184373248 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  i0807 10:23:06.590008 2085372672 exec.cpp:133] version: 0.24.0  e0807 10:23:06.592244 355938304 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  i0807 10:23:06.592243 353255424 exec.cpp:207] executor registered on slave 201508071023051393950825233852313s0  e0807 10:23:06.597995 355938304 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  registered executor on 10.0.79.8  starting task 1  forked command at 54411  sh c './mesosfetchertestcmd 1'  e0807 10:23:06.608708 355938304 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  command exited with status 0 (pid: 54411)  e0807 10:23:06.707649 355938304 socket.hpp:173] shutdown failed on fd=18: socket is not connected [57]  ../../src/tests/fetchercachetests.cpp:860: failure  failed to wait 15secs for awaitfinished(task.get())   aborted at 1438968214 (unix time) try ""date d @1438968214"" if you are using gnu date   [  failed  ] fetchercachehttptest.httpcachedserialized (28685 ms)  [ run      ] fetchercachehttptest.httpcachedconcurrent  pc: @        0x113723618 process::owned/::get()   sigsegv (@0x0) received by pid 52313 (tid 0x118d59000) stack trace:       @     0x7fff8fcacf1a sigtramp      @     0x7f9bc3109710 (unknown)      @        0x1136f07e2 mesos::internal::slave::fetcher::fetch()      @        0x113862f9d mesos::internal::slave::mesoscontainerizerprocess::fetch()      @        0x1138f1b5d zzn7process8dispatchi7nothingn5mesos8internal5slave25mesoscontainerizerprocesserkns211containeriderkns211commandinfoerknst3112basicstringicnsc11chartraitsiceensc9allocatoriceeeerk6optionisierkns27slaveides6s9sismspeens6futureiteerkns3pidit0eemswfsut1t2t3t4t5et6t7t8t9t10enkulpns11processbaseeecles1d      @        0x1138f18cf znst3110function6funcizn7process8dispatchi7nothingn5mesos8internal5slave25mesoscontainerizerprocesserkns511containeriderkns511commandinfoerkns12basicstringicns11chartraitsiceens9allocatoriceeeerk6optioniskerkns57slaveides9scsksosreens26futureiteerkns23pidit0eemsyfswt1t2t3t4t5et6t7t8t9t10eulpns211processbaseeensiis1geefvs1feecleos1f      @        0x1143768cf std::1::function/::operator()()      @        0x11435ca7f process::processbase::visit()      @        0x1143ed6fe process::dispatchevent::visit()      @        0x1127aaaa1 process::processbase::serve()      @        0x114343b4e process::processmanager::resume()      @        0x1143431ca process::internal::schedule()      @        0x1143da646 znst3114threadproxyins5tupleijpfvveeeeeepvs5      @     0x7fff95090268 pthreadbody      @     0x7fff950901e5 pthreadstart      @     0x7fff9508e41d threadstart  failed to synchronize with slave (it's probably exited)  make[3]:  [checklocal] segmentation fault: 11  make[2]:  [checkam] error 2  make[1]:  [check] error 2  make:  [checkrecursive] error 1      this was encountered just once out of 3+ make checks.",2,train
MESOS-3236,Updated slave task label decorator hook to pass in ExecutorInfo.,"if that task being launched has a command executor, there is no way for  the hook to determine the executorid for that task. the executorid is sometimes required by the label decorators for accounting purposes and for preparing ground for executorenvironmentdecorator (which is not passed the taskinfo).",1,train
MESOS-3237,HTTP requests with nested path are not properly handled by libprocess,"for example, if master adds a route ""/api/v1/scheduler"",  a handler named ""api/v1/scheduler"" is added to 'master' libprocess.    but when a request is posted to the above path, process::visit() looks for a http handler named ""api"" instead of ""api/v1/scheduler"".    ideally libprocess should look for handlers in the following preference order:    ""api/v1/scheduler""  > ""api/v1"" > ""api""    ",2,train
MESOS-3251,"http::get API evaluates ""host"" wrongly","currently libprocess http api sets the ""host"" header field from the peer socket address (ip:port). the problem is that socket address might not be right http server and might be just a proxy. ",1,train
MESOS-3252,Ignore no statistics condition for containers with no qdisc,"in portmappingstatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.      failed to get the network statistics for the htb qdisc on eth0  failed to get the network statistics for the fq_codel qdisc on eth0      this can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).      we should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  we do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.",2,train
MESOS-3254,Cgroup CHECK fails test harness,"check in clean up of containerizertest causes test harness to abort rather than fail or skip only perf related tests.    [ run      ] slaverecoverytest/0.restartbeforecontainerizerlaunch  [       ok ] slaverecoverytest/0.restartbeforecontainerizerlaunch (628 ms)  [] 24 tests from slaverecoverytest/0 (38986 ms total)    [] 4 tests from mesoscontainerizerslaverecoverytest  [ run      ] mesoscontainerizerslaverecoverytest.resourcestatistics  ../../src/tests/mesos.cpp:720: failure  cgroups::mount(hierarchy, subsystem): 'perfevent' is already attached to another hierarchy    we cannot run any cgroups tests that require  a hierarchy with subsystem 'perfevent'  because we failed to find an existing hierarchy  or create a new one (tried '/tmp/mesostestcgroup/perfevent').  you can either remove all existing  hierarchies, or disable this test case  (i.e., gtestfilter=mesoscontainerizerslaverecoverytest.).     f0811 17:23:43.874696 12955 mesos.cpp:774] checksome(cgroups): '/tmp/mesostestcgroup/perfevent' is not a valid hierarchy    check failure stack trace:        @     0x7fb2fb4835fd  google::logmessage::fail()      @     0x7fb2fb48543d  google::logmessage::sendtolog()      @     0x7fb2fb4831ec  google::logmessage::flush()      @     0x7fb2fb485d39  google::logmessagefatal::logmessagefatal()      @           0x4e3f98  checkfatal::checkfatal()      @           0x82f25a  mesos::internal::tests::containerizertest/::teardown()      @           0xc030e3  testing::internal::handleexceptionsinmethodifsupported/()      @           0xbf9050  testing::test::run()      @           0xbf912e  testing::testinfo::run()      @           0xbf9235  testing::testcase::run()      @           0xbf94e8  testing::internal::unittestimpl::runalltests()      @           0xbf97a4  testing::unittest::run()      @           0x4a9df3  main      @     0x7fb2f9371ec5  (unknown)      @           0x4b63ee  (unknown)  build step 'execute shell' marked build as failure",2,train
MESOS-3262,HTTPTest.NestedGet is flaky,"[ run      ] httptest.nestedget  ../../../3rdparty/libprocess/src/tests/httptests.cpp:459: failure  value of: response.get().status    actual: ""202 accepted""  expected: http::statuses[200]  which is: ""200 ok""   aborted at 1439569965 (unix time) try ""date  d @1439569965"" if you are using gnu date   pc: @           0x63abe8 testing::unittest::addtestpartresult()   sigsegv (@0x0) received by pid 25766 (tid 0x7f499415c780) from pid 0; stack trace:       @     0x7f499224dca0 (unknown)      @           0x63abe8 testing::unittest::addtestpartresult()      @           0x62f6af testing::internal::asserthelper::operator=()      @           0x43cd78 httptestnestedgettest::testbody()      @           0x65935e testing::internal::handlesehexceptionsinmethodifsupported/()      @           0x653c5e testing::internal::handleexceptionsinmethodifsupported/()      @           0x6349a3 testing::test::run()      @           0x635128 testing::testinfo::run()      @           0x635778 testing::testcase::run()      @           0x63c0e2 testing::internal::unittestimpl::runalltests()      @           0x65a11d testing::internal::handlesehexceptionsinmethodifsupported/()      @           0x654958 testing::internal::handleexceptionsinmethodifsupported/()      @           0x63ae08 testing::unittest::run()      @           0x4877f9 runalltests()      @           0x487613 main      @     0x7f49915739f4 libcstart_main  ",2,train
MESOS-3265,Starting maintenance needs to deactivate agents and kill tasks.,"after using the /maintenance/start endpoint to begin maintenance on a machine, agents running on said machine should:   be deactivated such that no offers are sent from that agent.  (investigate if master::deactivate(slave) can be used or modified for this purpose.)   kill all tasks still running on the agent (see mesos 1475).   prevent other agents on that machine from registering or sending out offers.  this will likely involve some modifications to master::register and master::reregister.    ",8,train
MESOS-3266,Stopping/Completing maintenance needs to reactivate agents.,"after using the /maintenance/stop endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master.",5,train
MESOS-3267,JSON serialization/deserialization of bytes is incorrect,"currently, we use our own serialization of bytes in json.hpp but we use picojson for deserialization.    we've observed that for some bytes the serialization results in a string that is incorrectly decoded by picojson.    example:    string = """"\""/\b\f\n\r\t\x00\x19 !#[]\x7f\xff""    result of our own encoding:  ""\""\""/bfnrtu0000u0019 !#[]u007f\xff\""""    picojson's encoding: ""\""\""/bfnrtu0000u0019 !#[]u007fu00ff\""""    fix:  we just use picojson to serialize bytes for consistency.",2,train
MESOS-3273,EventCall Test Framework is flaky,"observed this on asf ci. h/t [haosdent@gmail.com]    looks like the http scheduler never sent a subscribe request to the master.      [ run      ] examplestest.eventcallframework  using temporary directory '/tmp/examplestesteventcallframeworkk4vxkx'  i0813 19:55:15.643579 26085 exec.cpp:443] ignoring exited event because the driver is aborted!  shutting down  sending sigterm to process tree at pid 26061  killing the following process trees:  [     ]  shutting down  sending sigterm to process tree at pid 26062  shutting down  killing the following process trees:  [     ]  sending sigterm to process tree at pid 26063  killing the following process trees:  [     ]  shutting down  sending sigterm to process tree at pid 26098  killing the following process trees:  [     ]  shutting down  sending sigterm to process tree at pid 26099  killing the following process trees:  [     ]  warning: logging before initgooglelogging() is written to stderr  i0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus  i0813 19:55:17.161888 26100 logging.cpp:177] logging to stderr  i0813 19:55:17.163625 26100 scheduler.cpp:157] version: 0.24.0  i0813 19:55:17.175302 26100 leveldb.cpp:176] opened db in 3.167446ms  i0813 19:55:17.176393 26100 leveldb.cpp:183] compacted db in 1.047996ms  i0813 19:55:17.176496 26100 leveldb.cpp:198] created db iterator in 77155ns  i0813 19:55:17.176518 26100 leveldb.cpp:204] seeked to beginning of db in 8429ns  i0813 19:55:17.176527 26100 leveldb.cpp:273] iterated through 0 keys in the db in 4219ns  i0813 19:55:17.176708 26100 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0813 19:55:17.178951 26136 recover.cpp:449] starting replica recovery  i0813 19:55:17.179934 26136 recover.cpp:475] replica is in empty status  i0813 19:55:17.181970 26126 master.cpp:378] master 201508131955171679077566024926100 (297daca2d01a) started on 172.17.2.10:60249  i0813 19:55:17.182317 26126 master.cpp:380] flags at startup: acls=""permissive: false  registerframeworks     roles   }  runtasks     users   }  "" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticateslaves=""false"" authenticators=""crammd5"" credentials=""/tmp/examplestesteventcallframeworkk4vxkx/credentials"" frameworksorter=""drf"" help=""false"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""5secs"" registrystrict=""false"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.24.0/src/webui"" workdir=""/tmp/mesosii8gua"" zksessiontimeout=""10secs""  i0813 19:55:17.183475 26126 master.cpp:427] master allowing unauthenticated frameworks to register  i0813 19:55:17.183536 26126 master.cpp:432] master allowing unauthenticated slaves to register  i0813 19:55:17.183615 26126 credentials.hpp:37] loading credentials for authentication from '/tmp/examplestesteventcallframeworkk4vxkx/credentials'  w0813 19:55:17.183859 26126 credentials.hpp:52] permissions on credentials file '/tmp/examplestesteventcallframeworkk4vxkx/credentials' are too open. it is recommended that your credentials file is not accessible by others.  i0813 19:55:17.183969 26123 replica.cpp:641] replica in empty status received a broadcasted recover request  i0813 19:55:17.184306 26126 master.cpp:469] using default 'crammd5' authenticator  i0813 19:55:17.184661 26126 authenticator.cpp:512] initializing server sasl  i0813 19:55:17.185104 26138 recover.cpp:195] received a recover response from a replica in empty status  i0813 19:55:17.185972 26100 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i0813 19:55:17.186058 26135 recover.cpp:566] updating replica status to starting  i0813 19:55:17.187001 26138 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 654586ns  i0813 19:55:17.187037 26138 replica.cpp:323] persisted replica status to starting  i0813 19:55:17.187499 26134 recover.cpp:475] replica is in starting status  i0813 19:55:17.187605 26126 auxprop.cpp:66] initialized inmemory auxiliary property plugin  i0813 19:55:17.187710 26126 master.cpp:506] authorization enabled  i0813 19:55:17.188657 26138 replica.cpp:641] replica in starting status received a broadcasted recover request  i0813 19:55:17.188853 26131 hierarchical.hpp:346] initialized hierarchical allocator process  i0813 19:55:17.189252 26132 whitelistwatcher.cpp:79] no whitelist given  i0813 19:55:17.189321 26134 recover.cpp:195] received a recover response from a replica in starting status  i0813 19:55:17.190001 26125 recover.cpp:566] updating replica status to voting  i0813 19:55:17.190696 26124 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 357331ns  i0813 19:55:17.190775 26124 replica.cpp:323] persisted replica status to voting  i0813 19:55:17.190970 26133 recover.cpp:580] successfully joined the paxos group  i0813 19:55:17.192183 26129 recover.cpp:464] recover process terminated  i0813 19:55:17.192699 26123 slave.cpp:190] slave started on 1)@172.17.2.10:60249  i0813 19:55:17.192741 26123 slave.cpp:191] flags at startup: authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.24.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resourcemonitoringinterval=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/mesosii8gua/0""  i0813 19:55:17.194514 26100 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i0813 19:55:17.194658 26123 slave.cpp:354] slave resources: cpus():2; mem():10240; disk():3.70122e06; ports():[3100032000]  i0813 19:55:17.194854 26123 slave.cpp:384] slave hostname: 297daca2d01a  i0813 19:55:17.194877 26123 slave.cpp:389] slave checkpoint: true  i0813 19:55:17.196751 26132 master.cpp:1524] the newly elected leader is master@172.17.2.10:60249 with id 201508131955171679077566024926100  i0813 19:55:17.196797 26132 master.cpp:1537] elected as the leading master!  i0813 19:55:17.196815 26132 master.cpp:1307] recovering from registrar  i0813 19:55:17.197032 26138 registrar.cpp:311] recovering registrar  i0813 19:55:17.197845 26132 slave.cpp:190] slave started on 2)@172.17.2.10:60249  i0813 19:55:17.198420 26125 log.cpp:661] attempting to start the writer  i0813 19:55:17.197948 26132 slave.cpp:191] flags at startup: authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.24.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resourcemonitoringinterval=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/mesosii8gua/1""  i0813 19:55:17.199121 26132 slave.cpp:354] slave resources: cpus():2; mem():10240; disk():3.70122e06; ports():[3100032000]  i0813 19:55:17.199235 26138 state.cpp:54] recovering state from '/tmp/mesosii8gua/0/meta'  i0813 19:55:17.199322 26132 slave.cpp:384] slave hostname: 297daca2d01a  i0813 19:55:17.199345 26132 slave.cpp:389] slave checkpoint: true  i0813 19:55:17.199676 26100 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i0813 19:55:17.200085 26135 state.cpp:54] recovering state from '/tmp/mesosii8gua/1/meta'  i0813 19:55:17.200317 26132 statusupdatemanager.cpp:202] recovering status update manager  i0813 19:55:17.200371 26129 statusupdatemanager.cpp:202] recovering status update manager  i0813 19:55:17.202003 26129 replica.cpp:477] replica received implicit promise request with proposal 1  i0813 19:55:17.202585 26131 slave.cpp:190] slave started on 3)@172.17.2.10:60249  i0813 19:55:17.202596 26129 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 523191ns  i0813 19:55:17.202756 26129 replica.cpp:345] persisted promised to 1  i0813 19:55:17.202770 26132 containerizer.cpp:379] recovering containerizer  i0813 19:55:17.203061 26135 containerizer.cpp:379] recovering containerizer  i0813 19:55:17.202663 26131 slave.cpp:191] flags at startup: authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.24.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resourcemonitoringinterval=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/mesosii8gua/2""  i0813 19:55:17.203819 26131 slave.cpp:354] slave resources: cpus():2; mem():10240; disk():3.70122e06; ports():[3100032000]  i0813 19:55:17.203930 26131 slave.cpp:384] slave hostname: 297daca2d01a  i0813 19:55:17.203948 26131 slave.cpp:389] slave checkpoint: true  i0813 19:55:17.204674 26137 state.cpp:54] recovering state from '/tmp/mesosii8gua/2/meta'  i0813 19:55:17.205178 26135 statusupdatemanager.cpp:202] recovering status update manager  i0813 19:55:17.205323 26135 containerizer.cpp:379] recovering containerizer  i0813 19:55:17.205521 26136 slave.cpp:4069] finished recovery  i0813 19:55:17.206074 26136 slave.cpp:4226] querying resource estimator for oversubscribable resources  i0813 19:55:17.206424 26128 slave.cpp:4069] finished recovery  i0813 19:55:17.206722 26137 statusupdatemanager.cpp:176] pausing sending status updates  i0813 19:55:17.206858 26136 slave.cpp:684] new master detected at master@172.17.2.10:60249  i0813 19:55:17.206902 26138 slave.cpp:4069] finished recovery  i0813 19:55:17.206962 26128 slave.cpp:4226] querying resource estimator for oversubscribable resources  i0813 19:55:17.208312 26134 scheduler.cpp:272] new master detected at master@172.17.2.10:60249  i0813 19:55:17.208364 26136 slave.cpp:709] no credentials provided. attempting to register without authentication  i0813 19:55:17.208608 26136 slave.cpp:720] detecting new master  i0813 19:55:17.208839 26138 slave.cpp:4226] querying resource estimator for oversubscribable resources  i0813 19:55:17.209216 26123 coordinator.cpp:231] coordinator attemping to fill missing position  i0813 19:55:17.209247 26127 statusupdatemanager.cpp:176] pausing sending status updates  i0813 19:55:17.209259 26128 slave.cpp:684] new master detected at master@172.17.2.10:60249  i0813 19:55:17.209322 26127 statusupdatemanager.cpp:176] pausing sending status updates  i0813 19:55:17.209364 26128 slave.cpp:709] no credentials provided. attempting to register without authentication  i0813 19:55:17.209344 26138 slave.cpp:684] new master detected at master@172.17.2.10:60249  i0813 19:55:17.209455 26128 slave.cpp:720] detecting new master  i0813 19:55:17.209492 26138 slave.cpp:709] no credentials provided. attempting to register without authentication  i0813 19:55:17.209573 26128 slave.cpp:4240] received oversubscribable resources  from the resource estimator  i0813 19:55:17.209601 26138 slave.cpp:720] detecting new master  i0813 19:55:17.209730 26138 slave.cpp:4240] received oversubscribable resources  from the resource estimator  i0813 19:55:17.209883 26136 slave.cpp:4240] received oversubscribable resources  from the resource estimator  i0813 19:55:17.211266 26136 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0813 19:55:17.211771 26136 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 462128ns  i0813 19:55:17.211797 26136 replica.cpp:679] persisted action at 0  i0813 19:55:17.212980 26130 replica.cpp:511] replica received write request for position 0  i0813 19:55:17.213124 26130 leveldb.cpp:438] reading position from leveldb took 67075ns  i0813 19:55:17.213580 26130 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 301649ns  i0813 19:55:17.213603 26130 replica.cpp:679] persisted action at 0  i0813 19:55:17.214284 26123 replica.cpp:658] replica received learned notice for position 0  i0813 19:55:17.214622 26123 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 284547ns  i0813 19:55:17.214648 26123 replica.cpp:679] persisted action at 0  i0813 19:55:17.214675 26123 replica.cpp:664] replica learned nop action at position 0  i0813 19:55:17.215420 26136 log.cpp:677] writer started with ending position 0  i0813 19:55:17.217463 26133 leveldb.cpp:438] reading position from leveldb took 47943ns  i0813 19:55:17.220762 26125 registrar.cpp:344] successfully fetched the registry (0b) in 23.649024ms  i0813 19:55:17.221081 26125 registrar.cpp:443] applied 1 operations in 136902ns; attempting to update the 'registry'  i0813 19:55:17.223667 26133 log.cpp:685] attempting to append 174 bytes to the log  i0813 19:55:17.223778 26125 coordinator.cpp:341] coordinator attempting to write append action at position 1  i0813 19:55:17.224516 26127 replica.cpp:511] replica received write request for position 1  i0813 19:55:17.225009 26127 leveldb.cpp:343] persisting action (193 bytes) to leveldb took 466230ns  i0813 19:55:17.225042 26127 replica.cpp:679] persisted action at 1  i0813 19:55:17.225653 26126 replica.cpp:658] replica received learned notice for position 1  i0813 19:55:17.225953 26126 leveldb.cpp:343] persisting action (195 bytes) to leveldb took 286966ns  i0813 19:55:17.225975 26126 replica.cpp:679] persisted action at 1  i0813 19:55:17.226013 26126 replica.cpp:664] replica learned append action at position 1  i0813 19:55:17.227545 26137 registrar.cpp:488] successfully updated the 'registry' in 6.328064ms  i0813 19:55:17.227722 26137 registrar.cpp:374] successfully recovered registrar  i0813 19:55:17.227918 26124 log.cpp:704] attempting to truncate the log to 1  i0813 19:55:17.228024 26133 coordinator.cpp:341] coordinator attempting to write truncate action at position 2  i0813 19:55:17.228193 26131 master.cpp:1334] recovered 0 slaves from the registry (135b) ; allowing 10mins for slaves to reregister  i0813 19:55:17.228659 26127 replica.cpp:511] replica received write request for position 2  i0813 19:55:17.228972 26127 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 297903ns  i0813 19:55:17.229004 26127 replica.cpp:679] persisted action at 2  i0813 19:55:17.229565 26127 replica.cpp:658] replica received learned notice for position 2  i0813 19:55:17.229837 26127 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 260326ns  i0813 19:55:17.229899 26127 leveldb.cpp:401] deleting 1 keys from leveldb took 48697ns  i0813 19:55:17.229923 26127 replica.cpp:679] persisted action at 2  i0813 19:55:17.229956 26127 replica.cpp:664] replica learned truncate action at position 2  i0813 19:55:17.325634 26138 slave.cpp:1209] will retry registration in 445.955946ms if necessary  i0813 19:55:17.326088 26124 master.cpp:3635] registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 201508131955171679077566024926100s0  i0813 19:55:17.327446 26124 registrar.cpp:443] applied 1 operations in 231072ns; attempting to update the 'registry'  i0813 19:55:17.330252 26136 log.cpp:685] attempting to append 344 bytes to the log  i0813 19:55:17.330407 26132 coordinator.cpp:341] coordinator attempting to write append action at position 3  i0813 19:55:17.331418 26128 replica.cpp:511] replica received write request for position 3  i0813 19:55:17.331753 26128 leveldb.cpp:343] persisting action (363 bytes) to leveldb took 264140ns  i0813 19:55:17.331778 26128 replica.cpp:679] persisted action at 3  i0813 19:55:17.332324 26133 replica.cpp:658] replica received learned notice for position 3  i0813 19:55:17.332809 26133 leveldb.cpp:343] persisting action (365 bytes) to leveldb took 313064ns  i0813 19:55:17.332834 26133 replica.cpp:679] persisted action at 3  i0813 19:55:17.332865 26133 replica.cpp:664] replica learned append action at position 3  i0813 19:55:17.334211 26132 registrar.cpp:488] successfully updated the 'registry' in 6.668032ms  i0813 19:55:17.334430 26127 log.cpp:704] attempting to truncate the log to 3  i0813 19:55:17.334566 26132 coordinator.cpp:341] coordinator attempting to write truncate action at position 4  i0813 19:55:17.335283 26129 replica.cpp:511] replica received write request for position 4  i0813 19:55:17.335615 26127 slave.cpp:3058] received ping from slaveobserver(1)@172.17.2.10:60249  i0813 19:55:17.335816 26129 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 458268ns  i0813 19:55:17.335908 26137 master.cpp:3698] registered slave 201508131955171679077566024926100s0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus():2; mem():10240; disk( ):3.70122e0...",5,train
MESOS-3280,Master fails to access replicated log after network partition,"in a 5 node cluster with 3 masters and 2 slaves, and zk on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. the leading master halts. unknown reasons, but presumably related to replicated log access. the others fail to recover from the replicated log. unknown reasons. this could have to do with zk setup, but it might also be a mesos bug.     this was observed in a chronos test drive scenario described in detail here:  https:/github.com/mesos/chronos/issues/511    with setup instructions here:  https:/github.com/mesos/chronos/issues/508    ",8,train
MESOS-3281,Create a user doc for Scheduler HTTP API,we need to convert the design doc into user doc that we can add to our docs folder.,3,train
MESOS-3284,JSON representation of Protobuf should use base64 encoding for 'bytes' fields.,"currently we encode 'bytes' fields as utf8 strings, which is lossy for binary data due to invalid byte sequences! in order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.    note that this is also how proto3 does its encoding (see https:/developers.google.com/protocolbuffers/docs/proto3?hl=en#json), so this would make migration easier as well.",3,train
MESOS-3287,downloadWithHadoop tries to access Error() for a valid Try<bool>,"this was reported while trying to install hadoop / mesos integration:    i0818 05:36:35.058688 24428 fetcher.cpp:409] fetcher info: }],""sandboxdirectory"":""\/var\/lib\/mesos\/slaves\/201507060752181611773194505028439s473\/frameworks\/2015070607521816117731945050284394532\/executors\/executortasktracker4129\/runs\/c26f52d4405546fab99911d73f2096dd"",""user"":""hadoop""}  i0818 05:36:35.059806 24428 fetcher.cpp:364] fetching uri 'hdfs:/hdfs.prod:54310/user/ashwanth/hadoopwithmesos2.6.0cdh5.4.4.tar.gz'  i0818 05:36:35.059821 24428 fetcher.cpp:238] fetching directly into the sandbox directory  i0818 05:36:35.059835 24428 fetcher.cpp:176] fetching uri 'hdfs:/hdfs.prod:54310/user/ashwanth/hadoopwithmesos2.6.0cdh5.4.4.tar.gz'  mesosfetcher: /tmp/mesosbuild/mesosrepo/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:90: const string& try/::error() const [with t = bool; std::string = std::basic_string/]: assertion `data.isnone()' failed.      this is, however, a genuine bug in src/launcher/fetcher.cpp#l99:       try/ available = hdfs.available();     if (available.iserror()  !available.get())     the root cause is that (probably) the hdfs client is not available on the slave; however, we do not error() but rather return a false result.    the bug is exposed in the return line, where we try to retrieve available.error() (which is not there   it's just `false`).    this was a 'latent' bug that has been exposed by (my) recent refactoring of os::shell which is used by hdfs.available() under the covers.",1,train
MESOS-3288,Implement docker registry client,implement the docker registry client as per design document:    https:/docs.google.com/document/d/1kehxpql4lqgampiiad4ytdrn4heqc4fne93whr4x4/edit,5,train
MESOS-3289,Add DockerRegistry unit tests,add unit tests suite for docker registry implementation.  this could include:     creating mock docker registry server   using openssl library for digest functions.,5,train
MESOS-3290,Master should drop HTTP calls when it's recovering,"much like what we do with pid based frameworks, master should drop http calls if it's not the leader and/or still recovering.",3,train
MESOS-3293,Failing ROOT_ tests on CentOS 7.1 - LimitedCpuIsolatorTest,"limitedcpuisolatortest.rootcgroupspidsandtids    this is one of several root failing tests: we want to track them individually and for each of them decide whether to:     fix;   remove; or    redesign.    (full verbose logs attached)    steps to reproduce    completely cleaned the build, removed directory, clean pull from master (sha: fb93d93)   same results, 9 failed tests:      [==========] 751 tests from 114 test cases ran. (231218 ms total)  [  passed  ] 742 tests.  [  failed  ] 9 tests, listed below:  [  failed  ] limitedcpuisolatortest.rootcgroupspidsandtids  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  [  failed  ] containerizertest.rootcgroupsballoonframework  [  failed  ] linuxfilesystemisolatortest.rootchangerootfilesystem  [  failed  ] linuxfilesystemisolatortest.rootvolumefromsandbox  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhost  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhostsandboxmountpoint  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  [  failed  ] mesoscontainerizerlaunchtest.rootchangerootfs     9 failed tests    you have 10 disabled tests  ",5,train
MESOS-3294,Failing ROOT_ tests on CentOS 7.1 - UserCgroupIsolatorTest,"usercgroupisolatortest    this is one of several root failing tests: we want to track them individually and for each of them decide whether to:     fix;   remove; or    redesign.    (full verbose logs attached)    steps to reproduce    completely cleaned the build, removed directory, clean pull from master (sha: fb93d93)   same results, 9 failed tests:      [==========] 751 tests from 114 test cases ran. (231218 ms total)  [  passed  ] 742 tests.  [  failed  ] 9 tests, listed below:  [  failed  ] limitedcpuisolatortest.rootcgroupspidsandtids  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  [  failed  ] containerizertest.rootcgroupsballoonframework  [  failed  ] linuxfilesystemisolatortest.rootchangerootfilesystem  [  failed  ] linuxfilesystemisolatortest.rootvolumefromsandbox  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhost  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhostsandboxmountpoint  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  [  failed  ] mesoscontainerizerlaunchtest.rootchangerootfs     9 failed tests    you have 10 disabled tests  ",5,train
MESOS-3295,Failing ROOT_ tests on CentOS 7.1 - ContainerizerTest,"containerizertest.rootcgroupsballoonframework    this is one of several root failing tests: we want to track them individually and for each of them decide whether to:     fix;   remove; or    redesign.    (full verbose logs attached)    steps to reproduce    completely cleaned the build, removed directory, clean pull from master (sha: fb93d93)   same results, 9 failed tests:      [==========] 751 tests from 114 test cases ran. (231218 ms total)  [  passed  ] 742 tests.  [  failed  ] 9 tests, listed below:  [  failed  ] limitedcpuisolatortest.rootcgroupspidsandtids  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  [  failed  ] containerizertest.rootcgroupsballoonframework  [  failed  ] linuxfilesystemisolatortest.rootchangerootfilesystem  [  failed  ] linuxfilesystemisolatortest.rootvolumefromsandbox  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhost  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhostsandboxmountpoint  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  [  failed  ] mesoscontainerizerlaunchtest.rootchangerootfs     9 failed tests    you have 10 disabled tests  ",5,train
MESOS-3296,Failing ROOT_ tests on CentOS 7.1 - LinuxFilesystemIsolatorTest,"linuxfilesystemisolatortest    this is one of several root failing tests: we want to track them individually and for each of them decide whether to:     fix;   remove; or    redesign.    (full verbose logs attached)    steps to reproduce    completely cleaned the build, removed directory, clean pull from master (sha: fb93d93)   same results, 9 failed tests:      [==========] 751 tests from 114 test cases ran. (231218 ms total)  [  passed  ] 742 tests.  [  failed  ] 9 tests, listed below:  [  failed  ] limitedcpuisolatortest.rootcgroupspidsandtids  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  [  failed  ] containerizertest.rootcgroupsballoonframework  [  failed  ] linuxfilesystemisolatortest.rootchangerootfilesystem  [  failed  ] linuxfilesystemisolatortest.rootvolumefromsandbox  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhost  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhostsandboxmountpoint  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  [  failed  ] mesoscontainerizerlaunchtest.rootchangerootfs     9 failed tests    you have 10 disabled tests  ",5,train
MESOS-3297,Failing ROOT_ tests on CentOS 7.1 - MesosContainerizerLaunchTest,"mesoscontainerizerlaunchtest    this is one of several root failing tests: we want to track them individually and for each of them decide whether to:     fix;   remove; or    redesign.    (full verbose logs attached)    steps to reproduce    completely cleaned the build, removed directory, clean pull from master (sha: fb93d93)   same results, 9 failed tests:      [==========] 751 tests from 114 test cases ran. (231218 ms total)  [  passed  ] 742 tests.  [  failed  ] 9 tests, listed below:  [  failed  ] limitedcpuisolatortest.rootcgroupspidsandtids  [  failed  ] usercgroupisolatortest/1.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  [  failed  ] containerizertest.rootcgroupsballoonframework  [  failed  ] linuxfilesystemisolatortest.rootchangerootfilesystem  [  failed  ] linuxfilesystemisolatortest.rootvolumefromsandbox  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhost  [  failed  ] linuxfilesystemisolatortest.rootvolumefromhostsandboxmountpoint  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  [  failed  ] mesoscontainerizerlaunchtest.rootchangerootfs     9 failed tests    you have 10 disabled tests  ",5,train
MESOS-3299,Add a protobuf to represent time with integer precision.,"existing timestamps in the protobufs use double to encode time.  generally, the field represents seconds (with the decimal component to represent smaller denominations of time).  this is less than ideal.    instead, we should use integers, so as to not lose data (and to be able to compare value reliably).    something like:    message time   ",1,train
MESOS-3304,Remove remnants of LIBPROCESS_STATISTICS_WINDOW,"as seen in mesos1283, libprocessstatisticswindow is no longer needed since metrics now require specification of a window size, and default to no history if not provided.    some commentedout code remnants associated with this environment variable still remain and should be removed.",1,train
MESOS-3307,Configurable size of completed task / framework history,"we try to make mesos work with multiple frameworks and mesosdns at the same time. the goal is to have set of frameworks per team / project on a single mesos cluster.    at this point our mesos state.json is at 4mb and it takes a while to assembly. 5 mesosdns instances hit state.json every 5 seconds, effectively pushing mesosmaster cpu usage through the roof. it's at 100%+ all the time.    here's the problem:      mesos  curl s http:/mesosmaster:5050/master/state.json  uniq c  jq c .  jq .frameworks[.tasks  wc        16      37  252774      i see four options that can improve the situation:    1. add query string param to exclude completed tasks from state.json and use it in mesosdns and similar tools. there is no need for mesosdns to know about completed tasks, it's just extra load on master and mesosdns.    2. make history size configurable.    3. make json serialization faster. with 10000s of tasks even without history it would take a lot of time to serialize tasks for mesosdns. doing it every 60 seconds instead of every 5 seconds isn't really an option.    4. create event bus for mesos master. marathon has it and it'd be nice to have it in mesos. this way mesosdns could avoid polling master state and switch to listening for events.    all can be done independently.    note to mesosphere folks: please start distributing debug symbols with your distribution. i was asking for it for a while and it is really helpful: https:/github.com/mesosphere/marathon/issues/1497#issuecomment 104182501    perf report for leading master:         i'm on 0.23.0.",3,train
MESOS-3308,Define the container rootfs directories within the slave work_dir.,"a few motivations:    1) given the design in mesos 3004 it became apparent that we need to support multiple images in a container and these images can be of different image types. (there are no sufficient reasons or major obstacles that force us not to allow it and it obviously gives the users more flexibility).    2) also, even though we currently allow only one backend for each provisioner, when we update a running slave there can be multiple backends left in each container that we need to launch tasks with, or at least recover. we should evaluate in the future whether to support multiple backends and choose among them dynamically based on image characteristics.    3) since the rootfs' lifecycle tie with the running containers and should be cleaned up after containers die, it fits into the pattern of word_dir and we can manage them inside the work dir without needing to ask the operator to specify more flags.  ",2,train
MESOS-3310,Support provisioning images specified in volumes.,this is related to mesos3095 and mesos3227.    the idea is that we should allow command executor to run under host filesystem and provision the filesystem for the user. the command line executor will then chroot into user's root filesystem.    this solves the issue that the command executor is not launchable in the user specified root filesystem.     the design doc is here:  https:/docs.google.com/document/d/16hylvrl0nzkbts1j5stgyxzpnifpbpbs7rzrqvch4/edit?usp=sharing,3,train
MESOS-3311,SlaveTest.HTTPSchedulerSlaveRestart,"observed on asf ci      [ run      ] slavetest.httpschedulerslaverestart  using temporary directory '/tmp/slavetesthttpschedulerslaverestartcxydra'  i0825 22:07:36.809872 27610 leveldb.cpp:176] opened db in 3.751801ms  i0825 22:07:36.811115 27610 leveldb.cpp:183] compacted db in 1.2194ms  i0825 22:07:36.811175 27610 leveldb.cpp:198] created db iterator in 30669ns  i0825 22:07:36.811197 27610 leveldb.cpp:204] seeked to beginning of db in 7829ns  i0825 22:07:36.811208 27610 leveldb.cpp:273] iterated through 0 keys in the db in 6017ns  i0825 22:07:36.811245 27610 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0825 22:07:36.811722 27638 recover.cpp:449] starting replica recovery  i0825 22:07:36.811980 27638 recover.cpp:475] replica is in empty status  i0825 22:07:36.813033 27641 replica.cpp:641] replica in empty status received a broadcasted recover request  i0825 22:07:36.813355 27635 recover.cpp:195] received a recover response from a replica in empty status  i0825 22:07:36.813756 27628 recover.cpp:566] updating replica status to starting  i0825 22:07:36.814434 27636 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 570160ns  i0825 22:07:36.814471 27636 replica.cpp:323] persisted replica status to starting  i0825 22:07:36.814743 27642 recover.cpp:475] replica is in starting status  i0825 22:07:36.814965 27638 master.cpp:378] master 201508252207362348855485121927610 (09c6504e3a31) started on 172.17.0.14:51219  i0825 22:07:36.814999 27638 master.cpp:380] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/slavetesthttpschedulerslaverestartcxydra/credentials"" frameworksorter=""drf"" help=""false"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.25.0/inst/share/mesos/webui"" workdir=""/tmp/slavetesthttpschedulerslaverestartcxydra/master"" zksessiontimeout=""10secs""  i0825 22:07:36.815347 27638 master.cpp:425] master only allowing authenticated frameworks to register  i0825 22:07:36.815371 27638 master.cpp:430] master only allowing authenticated slaves to register  i0825 22:07:36.815402 27638 credentials.hpp:37] loading credentials for authentication from '/tmp/slavetesthttpschedulerslaverestartcxydra/credentials'  i0825 22:07:36.815634 27632 replica.cpp:641] replica in starting status received a broadcasted recover request  i0825 22:07:36.815752 27638 master.cpp:469] using default 'crammd5' authenticator  i0825 22:07:36.815904 27638 master.cpp:506] authorization enabled  i0825 22:07:36.815979 27643 recover.cpp:195] received a recover response from a replica in starting status  i0825 22:07:36.816185 27637 whitelistwatcher.cpp:79] no whitelist given  i0825 22:07:36.816186 27641 hierarchical.hpp:346] initialized hierarchical allocator process  i0825 22:07:36.816519 27630 recover.cpp:566] updating replica status to voting  i0825 22:07:36.817258 27639 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 475231ns  i0825 22:07:36.817296 27639 replica.cpp:323] persisted replica status to voting  i0825 22:07:36.817420 27637 master.cpp:1525] the newly elected leader is master@172.17.0.14:51219 with id 201508252207362348855485121927610  i0825 22:07:36.817467 27637 master.cpp:1538] elected as the leading master!  i0825 22:07:36.817483 27637 master.cpp:1308] recovering from registrar  i0825 22:07:36.817509 27635 recover.cpp:580] successfully joined the paxos group  i0825 22:07:36.817708 27633 registrar.cpp:311] recovering registrar  i0825 22:07:36.817844 27635 recover.cpp:464] recover process terminated  i0825 22:07:36.818439 27631 log.cpp:661] attempting to start the writer  i0825 22:07:36.819694 27636 replica.cpp:477] replica received implicit promise request with proposal 1  i0825 22:07:36.820133 27636 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 421255ns  i0825 22:07:36.820168 27636 replica.cpp:345] persisted promised to 1  i0825 22:07:36.820804 27630 coordinator.cpp:231] coordinator attemping to fill missing position  i0825 22:07:36.822105 27638 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0825 22:07:36.822597 27638 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 468065ns  i0825 22:07:36.822625 27638 replica.cpp:679] persisted action at 0  i0825 22:07:36.823737 27637 replica.cpp:511] replica received write request for position 0  i0825 22:07:36.823796 27637 leveldb.cpp:438] reading position from leveldb took 39603ns  i0825 22:07:36.824267 27637 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 446655ns  i0825 22:07:36.824296 27637 replica.cpp:679] persisted action at 0  i0825 22:07:36.824961 27634 replica.cpp:658] replica received learned notice for position 0  i0825 22:07:36.825340 27634 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 362236ns  i0825 22:07:36.825369 27634 replica.cpp:679] persisted action at 0  i0825 22:07:36.825388 27634 replica.cpp:664] replica learned nop action at position 0  i0825 22:07:36.825975 27642 log.cpp:677] writer started with ending position 0  i0825 22:07:36.826997 27628 leveldb.cpp:438] reading position from leveldb took 56us  i0825 22:07:36.829946 27639 registrar.cpp:344] successfully fetched the registry (0b) in 12.187136ms  i0825 22:07:36.830077 27639 registrar.cpp:443] applied 1 operations in 40874ns; attempting to update the 'registry'  i0825 22:07:36.832870 27635 log.cpp:685] attempting to append 174 bytes to the log  i0825 22:07:36.833088 27641 coordinator.cpp:341] coordinator attempting to write append action at position 1  i0825 22:07:36.833845 27636 replica.cpp:511] replica received write request for position 1  i0825 22:07:36.834293 27636 leveldb.cpp:343] persisting action (193 bytes) to leveldb took 425175ns  i0825 22:07:36.834324 27636 replica.cpp:679] persisted action at 1  i0825 22:07:36.835077 27643 replica.cpp:658] replica received learned notice for position 1  i0825 22:07:36.835500 27643 leveldb.cpp:343] persisting action (195 bytes) to leveldb took 404831ns  i0825 22:07:36.835532 27643 replica.cpp:679] persisted action at 1  i0825 22:07:36.835574 27643 replica.cpp:664] replica learned append action at position 1  i0825 22:07:36.836545 27643 registrar.cpp:488] successfully updated the 'registry' in 6.393088ms  i0825 22:07:36.836707 27643 registrar.cpp:374] successfully recovered registrar  i0825 22:07:36.836874 27639 log.cpp:704] attempting to truncate the log to 1  i0825 22:07:36.837174 27632 master.cpp:1335] recovered 0 slaves from the registry (135b) ; allowing 10mins for slaves to reregister  i0825 22:07:36.837291 27634 coordinator.cpp:341] coordinator attempting to write truncate action at position 2  i0825 22:07:36.838249 27639 replica.cpp:511] replica received write request for position 2  i0825 22:07:36.838685 27639 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 412214ns  i0825 22:07:36.838716 27639 replica.cpp:679] persisted action at 2  i0825 22:07:36.839735 27628 replica.cpp:658] replica received learned notice for position 2  i0825 22:07:36.840304 27628 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 547841ns  i0825 22:07:36.840375 27628 leveldb.cpp:401] deleting ~1 keys from leveldb took 51256ns  i0825 22:07:36.840401 27628 replica.cpp:679] persisted action at 2  i0825 22:07:36.840428 27628 replica.cpp:664] replica learned truncate action at position 2  i0825 22:07:36.849371 27610 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i0825 22:07:36.856500 27633 slave.cpp:190] slave started on 286)@172.17.0.14:51219  i0825 22:07:36.856541 27633 slave.cpp:191] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/slavetesthttpschedulerslaverestartukka8l/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/slavetesthttpschedulerslaverestartukka8l/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.25.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resourcemonitoringinterval=""1secs"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/slavetesthttpschedulerslaverestartukka8l""  i0825 22:07:36.857074 27633 credentials.hpp:85] loading credential for authentication from '/tmp/slavetesthttpschedulerslaverestartukka8l/credential'  i0825 22:07:36.857275 27633 slave.cpp:321] slave using credential for: testprincipal  i0825 22:07:36.857822 27633 slave.cpp:354] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0825 22:07:36.857936 27633 slave.cpp:384] slave hostname: 09c6504e3a31  i0825 22:07:36.857959 27633 slave.cpp:389] slave checkpoint: true  i0825 22:07:36.858886 27637 state.cpp:54] recovering state from '/tmp/slavetesthttpschedulerslaverestartukka8l/meta'  i0825 22:07:36.859130 27638 statusupdatemanager.cpp:202] recovering status update manager  i0825 22:07:36.859465 27636 containerizer.cpp:379] recovering containerizer  i0825 22:07:36.860631 27634 slave.cpp:4069] finished recovery  i0825 22:07:36.861034 27634 slave.cpp:4226] querying resource estimator for oversubscribable resources  i0825 22:07:36.861239 27643 statusupdatemanager.cpp:176] pausing sending status updates  i0825 22:07:36.861240 27634 slave.cpp:684] new master detected at master@172.17.0.14:51219  i0825 22:07:36.861322 27634 slave.cpp:747] authenticating with master master@172.17.0.14:51219  i0825 22:07:36.861343 27634 slave.cpp:752] using default crammd5 authenticatee  i0825 22:07:36.861450 27634 slave.cpp:720] detecting new master  i0825 22:07:36.861495 27628 authenticatee.cpp:115] creating new client sasl connection  i0825 22:07:36.861569 27634 slave.cpp:4240] received oversubscribable resources  from the resource estimator  i0825 22:07:36.861716 27632 master.cpp:4694] authenticating slave(286)@172.17.0.14:51219  i0825 22:07:36.861799 27629 authenticator.cpp:407] starting authentication session for crammd5authenticatee(665)@172.17.0.14:51219  i0825 22:07:36.862045 27642 authenticator.cpp:92] creating new server sasl connection  i0825 22:07:36.862308 27635 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0825 22:07:36.862337 27635 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0825 22:07:36.862421 27629 authenticator.cpp:197] received sasl authentication start  i0825 22:07:36.862478 27629 authenticator.cpp:319] authentication requires more steps  i0825 22:07:36.862579 27633 authenticatee.cpp:252] received sasl authentication step  i0825 22:07:36.862679 27628 authenticator.cpp:225] received sasl authentication step  i0825 22:07:36.862707 27628 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: '09c6504e3a31' server fqdn: '09c6504e3a31' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0825 22:07:36.862717 27628 auxprop.cpp:174] looking up auxiliary property 'userpassword'  i0825 22:07:36.862754 27628 auxprop.cpp:174] looking up auxiliary property 'cmusaslsecretcrammd5'  i0825 22:07:36.862785 27628 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: '09c6504e3a31' server fqdn: '09c6504e3a31' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0825 22:07:36.862797 27628 auxprop.cpp:124] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0825 22:07:36.862802 27628 auxprop.cpp:124] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0825 22:07:36.862817 27628 authenticator.cpp:311] authentication success  i0825 22:07:36.862884 27629 authenticatee.cpp:292] authentication success  i0825 22:07:36.862921 27630 master.cpp:4724] successfully authenticated principal 'testprincipal' at slave(286)@172.17.0.14:51219  i0825 22:07:36.862969 27642 authenticator.cpp:425] authentication session cleanup for crammd5authenticatee(665)@172.17.0.14:51219  i0825 22:07:36.863139 27639 slave.cpp:815] successfully authenticated with master master@172.17.0.14:51219  i0825 22:07:36.863256 27639 slave.cpp:1209] will retry registration in 15.028678ms if necessary  i0825 22:07:36.863382 27643 master.cpp:3636] registering slave at slave(286)@172.17.0.14:51219 (09c6504e3a31) with id 201508252207362348855485121927610s0  i0825 22:07:36.863899 27610 sched.cpp:164] version: 0.25.0  i0825 22:07:36.863940 27636 registrar.cpp:443] applied 1 operations in 94492ns; attempting to update the 'registry'  i0825 22:07:36.864670 27632 sched.cpp:262] new master detected at master@172.17.0.14:51219  i0825 22:07:36.864790 27632 sched.cpp:318] authenticating with master master@172.17.0.14:51219  i0825 22:07:36.864821 27632 sched.cpp:325] using default crammd5 authenticatee  i0825 22:07:36.865095 27637 authenticatee.cpp:115] creating new client sasl connection  i0825 22:07:36.865453 27643 master.cpp:4694] authenticating scheduler6c5ddcdb9dd14b38b0515f714d3c1c55@172.17.0.14:51219  i0825 22:07:36.865603 27629 authenticator.cpp:407] starting authentication session for crammd5authenticatee(666)@172.17.0.14:51219  i0825 22:07:36.865840 27638 authenticator.cpp:92] creating new server sasl connection  i0825 22:07:36.866217 27630 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0825 22:07:36.866260 27630 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0825 22:07:36.866433 27639 authenticator.cpp:197] received sasl authentication start  i0825 22:07:36.866513 27639 authenticator.cpp:319] authentication requires more steps  i0825 22:07:36.866710 27630 authenticatee.cpp:252] received sasl authentication step  i0825 22:07:36.866999 27638 authenticator.cpp:225] received sasl authentication step  i0825 22:07:36.867051 27638 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: '09c6504e3a31' server fqdn: '09c6504e3a31' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0825 22:07:36.867077 27638 auxprop.cpp:174] looking up auxiliary property 'userpassword'  i0825 22:07:36.867130 27638 auxprop.cpp:174] looking up auxiliary property 'cmusaslsecretcrammd5'  i0825 22:07:36.867162 27638 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: '09c6504e3a31' server fqdn: '09c6504e3a31' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0825 22:07:36.867175 27638 auxprop.cpp:124] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0825 22:07:36.867183 27638 auxprop.cpp:124] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0825 22:07:36.867202 27638 authenticator.cpp:311] authentication success  i0825 22:07:36.867426 27636 authenticatee.cpp:292] authentication success  i0825 22:07:36.867434 27633 authenticator.cpp:425] authentication session cleanup for crammd5authenticatee(666)@172.17.0.14:51219  i0825 22:07:36.867627 27630 master.cpp:4724] successfully authenticated principal 'testprincipal' at scheduler6c5ddcdb9dd14b38b0515f714d3c1c55@172.17.0.14:51219  i0825 22:07:36.867951 27641 sched.cpp:407] successfully authenticated with master master@172.17.0.14:51219  i0825 22:07:36.867986 27641 sched.cpp:713] sending subscribe call to master@172.17.0.14:51219  i0825 22:07:36.868114 27641 sched.cpp:746] will retry registration in 1.352726078secs if necessary  i0825 22:07:36.868233 27634 log.cpp:685] attempting to append 344 bytes to the log  i0825 22:07:36.868268 27638 master.cpp:2094] received subscribe call for framework 'default' at scheduler6c5ddcdb9dd14b38b0515f714d3c1c55@172.17.0.14:51219  i0825 22:07:36.868305 27638 master.cpp:1564] authorizing framework principal 'testprincipal' to receive offers for role ''  i0825 22:07:36.868373 27631 coordinator.cpp:341] coordinator attempting to write append action at position 3  i0825 22:07:36.868614 27642 master.cpp:2164] subscribing framework default with checkpointing enabled and capabilities [  ]  i0825 22:07:36.868999 27643 hierarchical.hpp:391] added framework 2015082522073623488554851219276100000  i0825 22:07:36.869030 27643 hierarchical.hpp:1010] no resources available to allocate!  i0825 22:07:36.869046 27643 hierarchical.hpp:910] performed allocation for 0 slaves in 34654ns  i0825 22:07:36.869215 27631 sched.cpp:640] framework registered with 2015082522073623488554851219276100000  i0825 22:07:36.869215 27643 replica.cpp:511] replica received write request for position 3  i0825 22:07:36.869268 27631 sched.cpp:654] scheduler::registered took 29976ns  i0825 22:07:36.869453 27643 leveldb.cpp:343] persisting action (363 bytes) to leveldb took 181689ns  i0825 22:07:36.869477 27643 replica.cpp:679] persisted action at 3  i0825 22:07:36.870075 27629 replica.cpp:658] replica received learned notice for position 3  i0825 22:07:36.870542 27629 leveldb.cpp:343] persisting action (365 bytes) to leveldb took 469081ns  i0825 22:07:36.870589 27629 replica.cpp:679] persisted action at 3  i0825 22:07:36.870622 27629 replica.cpp:664] replica learned append action at position 3  i0825 22:07:36.872133 27632 registrar.cpp:488] successfully updated the 'registry' in 8.113152ms  i0825 22:07:36.872354 27639 log.cpp:704] attempting to truncate the log to 3  i0825 22:07:36.872470 27632 coordinator.cpp:341] coordinator attempting to write truncate action at position 4  i0825 22:07:36.872879 27637 slave.cpp:3058] received ping from slaveobserver(274)@172.17.0.14:51219  i0825 22:07:36.873015 27636 master.cpp:3699] registered slave 201508252207362348855485121927610s0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0825 22:07:36.873180 27637 slave.cpp:859] registered with master master@172.17.0.14:51219; given slave id 201508252207362348855485121927610s0  i0825 22:07:36.873219 27637 fetcher.cpp:77] clearing fetcher cache  i0825 22:07:36.873410 27634 statusupdate_manager.cpp:183] resuming sending status updates  i0825 22:07:36.873379 27628 hierarchical.hpp:542] added slave 201508252207362348855485121927610s0 (09c6504e3a31) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0825 22:07:36.873482 27642 replica.cpp:511] replica received write request for position 4  i0825 22:0...",2,train
MESOS-3312,Factor out JSON to repeated protobuf conversion,"in general, we have the collection of protobuf messages as another protobuf message, which makes json > protobuf conversion straightforward. this is not always the case, for example, resources class is not a protobuf, though protobufconvertible.    to facilitate conversions like json > resources and avoid writing code for each particular case, we propose to introduce json::array > repeated protobuf conversion. with this in place, json::array > resources boils down to json::array > repeated resource > (extra ctor call)  > resources.",2,train
MESOS-3313,Rework Jenkins build script,"mesos jenkins build script needs to be reworked to support the following:     wider test coverage (libevent, libssl, root tests, docker tests).   more os/compiler docker images for testing mesos.   excluding tests on perimage basis.    reproducing the test image locally.  ",3,train
MESOS-3319,Mesos will not build when configured with gperftools enabled,"mesos configured with enableperftools currently will not build on osx 10.10.4 or ubuntu 14.04, possibly because the bundled gperftools2.0 is not current. the stable release is now 2.4, which builds successfully on both of these platforms.    this issue is resolved when mesos will build successfully out of the box with gperftools enabled. after this ticket is resolved, the libprocess profiler should be tested to confirm that it still works and if not, it should be fixed.",2,train
MESOS-3321,Spurious fetcher message about extracting an archive,"the fetcher emits a spurious log message about not extracting an archive with "".tgz"" extension, even though the tarball is extracted correctly.      i0826 19:02:08.304914  2109 logging.cpp:172] info level logging started!  i0826 19:02:08.305253  2109 fetcher.cpp:413] fetcher info: }],""sandbox_directory"":""\/tmp\/mesos\/slaves\/2015082618571625166276450501s0\/frameworks\/20150826185716251662764505010000\/executors\/sampleflaskapp.f222d2024c2411e5a6280242ac110011\/runs\/e71f50b8816d46d5bcc6f9850a0402ed"",""user"":""root""}  i0826 19:02:08.306834  2109 fetcher.cpp:368] fetching uri 'file:/mesos/sampleflaskapp.tgz'  i0826 19:02:08.306864  2109 fetcher.cpp:242] fetching directly into the sandbox directory  i0826 19:02:08.306884  2109 fetcher.cpp:179] fetching uri 'file:/mesos/sampleflaskapp.tgz'  i0826 19:02:08.306900  2109 fetcher.cpp:159] copying resource with command:cp '/mesos/sampleflaskapp.tgz' '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6f9850a0402ed/sampleflaskapp.tgz'  i0826 19:02:08.309063  2109 fetcher.cpp:76] extracting with command: tar c '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6f9850a0402ed' xf '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6f9850a0402ed/sampleflaskapp.tgz'  i0826 19:02:08.315313  2109 fetcher.cpp:84] extracted '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6f9850a0402ed/sampleflaskapp.tgz' into '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6f9850a0402ed'  w0826 19:02:08.315381  2109 fetcher.cpp:264] copying instead of extracting resource from uri with 'extract' flag, because it does not seem to be an archive: file:/mesos/sampleflaskapp.tgz  i0826 19:02:08.315604  2109 fetcher.cpp:445] fetched 'file:/mesos/sampleflaskapp.tgz' to '/tmp/mesos/slaves/2015082618571625166276450501s0/frameworks/20150826185716251662764505010000/executors/sampleflaskapp.f222d2024c2411e5a6280242ac110011/runs/e71f50b8816d46d5bcc6 f9850a0402ed/sampleflaskapp.tgz'  ",1,train
MESOS-3323,Auto-generate protos for stout tests,stout protobufs (afaik right now it's just a single file protobuf_tests.proto) are not generated automatically. including proto generation step would be cleaner and more convenient.,2,train
MESOS-3326,Make use of C++11 atomics,"now that we require c11, we can make use of std::atomic. for example:     libprocess/process.cpp uses a bare int + syncsynchronize() for ""running""   syncsynchronize() is used in logging.hpp in libprocess and fork.hpp in stout   sched/sched.cpp uses a volatile int for ""running""  this is wrong, ""volatile"" is not sufficient to ensure safe concurrent access   ""volatile"" is used in a few other places  most are probably dubious but i haven't looked closely",2,train
MESOS-3332,Support HTTP Pipelining in libprocess (http::post),"currently , http::post in libprocess, does not support http pipelining. each call as of know sends in the connection: close header, thereby, signaling to the server to close the tcp socket after the response.    we either need to create a new interface for supporting http pipelining , or modify the existing http::post to do so.    this is needed for the scheduler/executor library implementations to make sure ""calls"" are sent in order to the master. currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.    ",8,train
MESOS-3337,Refactored libprocess SSL tests., refactor ssl test fixture to be available for reuse by other projects. currently the fixture class and its the symbols it depends on are not present in libproces's include files.,3,train
MESOS-3338,Dynamic reservations are not counted as used resources in the master,dynamically reserved resources should be considered used or allocated and hence reflected in mesos bookkeeping structures and state.json.    i expanded the reservationtest.reservethenunreserve test with the following section:      / check that the master counts the reservation as a used resource.        and got    ../../../src/tests/reservationtests.cpp:168: failure  value of: (cpus).get()    actual: 0  expected: json::number(1)  which is: 1      idea for new resources states: https:/docs.google.com/drawings/d/1aquviqpy8d_mrcqjzuwz5nnn3cyp3jxqeguhl kzc/edit,3,train
MESOS-3339,Implement filtering mechanism for (Scheduler API Events) Testing,"currently, our testing infrastructure does not have a mechanism of filtering/dropping http events of a particular type from the scheduler api response stream.  we need a drophttpcalls abstraction that can help us to filter a particular event type.      / enqueues all received events into a libprocess queue.  actionp(enqueue, queue)   else       events.pop();    }  }      this helper code is duplicated in at least two places currently, scheduler library/maintenance primitives tests.    the solution can be as trivial as moving this helper function to a common testheader.   implement a drophttpcalls similar to what we do for other protobufs via dropcalls.",3,train
MESOS-3340,Command-line flags should take precedence over OS Env variables,"currently, it appears that redefining a flag on the commandline that was already defined via a os env var (mesos ) causes the master to fail with a not very helpful message.    for example, if one has mesosquorum defined, this happens:    $ ./mesosmaster zk=zk:/192.168.1.4/mesos quorum=1 hostname=192.168.1.4 ip=192.168.1.4  duplicate flag 'quorum' on command line      which is not very helpful.    ideally, we would parse the flags with a ""wellknown"" priority (commandline first, environment last)  but at the very least, the error message should be more helpful in explaining what the issue is.",2,train
MESOS-3343,Rate Limiting functionality for HTTP Frameworks,we need to build rate limiting functionality for frameworks connecting via the scheduler http api similar to the pid based frameworks.    link to the ratelimiting section from design doc:  https:/docs.google.com/document/d/1pniyhckimknvpqhkrhbc9esitwnftprixhurrt0/edit#heading=h.kzgdk4d5fmba     this ticket deals with refactoring the existing pid based framework functionality and extend it for http frameworks.   the second part of notifying the framework when ratelimiting is active i.e. returning a status of 429 can be undertook as part of mesos 1664,5,train
MESOS-3345,Expand the range of integer precision when converting into/out of json.,"for [mesos3299], we added some protobufs to represent time with integer precision.  however, this precision is not maintained through protobuf / json conversion, because of how our json encoders/decoders convert numbers to floating point.    to maintain precision, we can try one of the following:   try using a long double to represent a number.   add logic to stringify/parse numbers without loss when possible.   try representing int64t as a string and parse it as such?   update picojson and add a compiler flag, i.e.  dpicojsonuseint64     in all cases, we'll need to make sure that:   integers are properly stringified without loss.   the json decoder parses the integer without loss.    we have some unit tests for big (close to int32max/int64_max) and small integers.",5,train
MESOS-3346,Add filter support for inverse offers,"a filter attached to the inverse offer can be used by the framework to control when it wants to be contacted again with the inverse offer, since future circumstances may change the viability of the maintenance schedule.  the filter? for inverseoffers is identical to the existing mechanism for re offering offers to frameworks.",5,train
MESOS-3348,Add either log rotation or capped-size logging (for tasks),"tasks currently log their output (i.e. stdout/stderr) to files (the ""sandbox"") on an agent's disk.  in some cases, the accumulation of these logs can completely fill up the agent's disk and thereby kill the task or machine.    to prevent this, we should either implement a log rotation mechanism or cappedsize logging.  this would be used by executors to control the amount of logs they keep.      master/agent logs will not be affected.    we will first scope out several possible approaches for log rotation/capping in a design document (see [mesos3356]).  once an approach is chosen, this story will be broken down into some corresponding issues.",13,train
MESOS-3349,Removing mount point fails with EBUSY in LinuxFilesystemIsolator.,"when running the tests as root, we found persistentvolumetest.accesspersistentvolume fails consistently on some platforms.      [ run      ] persistentvolumetest.accesspersistentvolume  i0901 02:17:26.435140 39432 exec.cpp:133] version: 0.25.0  i0901 02:17:26.442129 39461 exec.cpp:207] executor registered on slave 2015090102172618286599785210232604s0  registered executor on hostname  starting task d8ff1f00e7204a61b440e111009dfdc3  sh c 'echo abc > path1/file'  forked command at 39484  command exited with status 0 (pid: 39484)  ../../src/tests/persistentvolumetests.cpp:579: failure  value of: os::exists(path::join(directory, ""path1""))    actual: true  expected: false  [  failed  ] persistentvolumetest.accesspersistentvolume (777 ms)      turns out that the 'rmdir' after the 'umount' fails with ebusy because there's still some references to the mount.    fyi [jieyu] [mcypark]",5,train
MESOS-3352,Problem Statement Summary for Systemd Cgroup Launcher,"there have been many reports of cgroups related issues when running mesos on systemd.  many of these issues are rooted in the manual manipulation of the cgroups filesystem by mesos.  this task is to describe the problem in a 1 page summary, and elaborate on the suggested 2 part solution:  1. using the delegate=true flag for the slave  2. implementing a systemd launcher to run executors with tighter systemd integration.",5,train
MESOS-3356,Scope out approaches to deal with logging to finite disks (i.e. log rotation|capped-size logging).,"for the background, see the parent story [mesos 3348].    for the work/design/discussion, see the linked design document (below).    ",5,train
MESOS-3357,Update quota design doc based on user comments and offline syncs,"we got plenty of feedback from different parties, which we would like to persist in the design doc for posterity.",3,train
MESOS-3365,Export per container SNMP statistics,"we need to export the per container snmp statistics too, from its /proc/net/snmp.",5,train
MESOS-3366,Allow resources/attributes discovery,"in heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. the current solution is to use custom resources and attributes on the agents. detecting nonstandard resources/attributes requires wrapping the ""mesosslave"" binary behind a script and use custom code to probe the agent. unfortunately, this approach doesn't allow composition. the solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.    please review the detailed document below:  https:/docs.google.com/document/d/15okebdezfxzeylsyqou0upb0eovecalzekeg0hqax9w    feel free to express comments/concerns by annotating the document or by replying to this issue.  ",3,train
MESOS-3368,Add device support in cgroups abstraction,"add support for https:/ v1/devices.txt to aid isolators controlling access to devices.    in the future, we could think about how to numerate and control access to devices as resource or task/container policy",3,train
MESOS-3375,Add executor protobuf to v1,"a new protobuf for executor was introduced in mesos for the http api, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. this protobuf is ought to be changed as the executor http api design evolves.",1,train
MESOS-3378,Document a test pattern for expediting event firing,we use clock::advance() extensively in tests to expedite event firing and minimize overall make check time. document this pattern for posterity.,3,train
MESOS-3393,Remove unused executor protobuf,"the executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",1,train
MESOS-3399,Rewrite perf events code,"our current code base invokes and parses `perf stat`, which sucks, because cmdline output is not a stable abi at all, it can break our code at any time, for example mesos 2834.    we should use the stable api perfeventopen(2). with this patch https:/reviews.apache.org/r/37540/, we already have the infrastructure for the implementation, so it should not be hard to rewrite all the perf events code.",5,train
MESOS-3402,mesos-execute does not support credentials,mesos execute does not appear to support passing credentials. this makes it impossible to use on a cluster where framework authentication is required.,2,train
MESOS-3409,Refactor the plain JSON parsing in the docker containerizer,two functions in the dockerrelated code take a string and parse it to json:   docker::container::create in src/docker/docker.cpp   token::create in src/slave/containerizer/provisioners/docker/token_manager.cpp    this json is then validated (lots of ifelses) and used via the json::value accessors.  we could instead use a protobuf and the related stout json >protobuf conversion function.,3,train
MESOS-3413,Docker containerizer does not symlink persistent volumes into sandbox,"for the arangodb framework i am trying to use the persistent primitives. nearly all is working, but i am missing a crucial piece at the end: i have successfully created a persistent disk resource and have set the persistence and volume information in the diskinfo message. however, i do not see any way to find out what directory on the host the mesos slave has reserved for us. i know it is $/volumes/roles/_/ but we have no way to query this information anywhere. the docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. therefore, i have essentially no access to it. note that the mesos containerizer (which i cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. with that, i could mount the volume into our docker container and all would be well.",5,train
MESOS-3416,Publish egg for 0.24.0 to PyPI,"0.24.0 was released, but the python egg has not been published.",1,train
MESOS-3417,Log source address replicated log recieved broadcasts,"currently mesos doesn't log what machine a replicated log status broadcast was recieved from:    sep 11 21:41:14 master01 mesosmaster[15625]: i0911 21:41:14.320164 15637 replica.cpp:641] replica in empty status received a broadcasted recover request  sep 11 21:41:14 master01 mesosdns[15583]: i0911 21:41:14.321097   15583 detect.go:118] ignoring childrenchanged event, leader has not changed: /mesos  sep 11 21:41:14 master01 mesosmaster[15625]: i0911 21:41:14.353914 15639 replica.cpp:641] replica in empty status received a broadcasted recover request  sep 11 21:41:14 master01 mesos master[15625]: i0911 21:41:14.479132 15639 replica.cpp:641] replica in empty status received a broadcasted recover request      it would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from",2,train
MESOS-3418,Factor out V1 API test helper functions,"we currently have some helper functionality for v1 api tests. this is copied in a few test files.  factor this out into a common place once the api is stabilized.    / helper class for using expectcall since the mesos scheduler api    / is callback based.    class callbacks    ;      / enqueues all received events into a libprocess queue.  / todo(jmlvanre): factor this common code out of tests into v1  / helper.  actionp(enqueue, queue)   else       events.pop();    }  }      we can also update the helpers in /tests/mesos.hpp to support the v1 api.  this would let us get ride of lines like:    v1::taskinfo taskinfo = evolve(createtask(devolve(offer), """", defaultexecutorid));    in favor of:    v1::taskinfo taskinfo = createtask(offer, """", defaultexecutor_id);  ",2,train
MESOS-3423,Perf event isolator stops performing sampling if a single timeout occurs.,"currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:          duration timeout = flags.perf_duration + seconds(2);      this should be based on the reap interval maximum.    also, the code stops sampling altogether when a single timeout occurs. we've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. it may also make sense to continue sampling in the case of errors, since these may be transient.",3,train
MESOS-3424,Support fetching AppC images into the store,"so far appc store is read only and depends on out of band mechanisms to get the images. we need to design a way to support fetching in a native way.    as commented on mesos 2824:  it's unacceptable to have either have:   the slave to be blocked for extended period of time (minutes) which delays the communication between the executor and scheduler, or   the first task that uses this image to be blocked for a long time to wait for the container image to be ready.    the solution needs to enable the operator to prefetch a list of ""preferred images"" without introducing the above problems.",5,train
MESOS-3425,Modify LinuxLauncher to support Systemd,"implement the solution described in mesos3352 in the linuxlauncher    in order to avoid the migration of cgroup pids by systemd we can use the delegate=true flag. this guards systemd from migrating the pids that are descendants of the process launched by a systemd unit.    in order for this strategy to work, the delegate flag must be supported by the systemd version. support for this was introduced in systemd v218; however, it has also been backported to v208 for rhel7 and centos7 http:/centoserrata.nagater.net/item/ceba20150037centos7.i386.x8664.html with the package https:/rhn.redhat.com/errata/rhba20151155.html. it is highly recommended to upgrade to this package if running those operating systems.    once the delegate=true flag has been set, the cgroups that are manually manipulated by the agent will no longer be migrated during the lifetime of the agent.    this still leaves the problem of tasks being migrated after the agent has stopped running_ (voluntarily or not). in order to deal with the problem we propose the following solution:    if an agent is running on a systemd initialized machine, then the agent will create a systemd slice with a lifetime that is independent of the agent and delegate=true. the linux launcher (used when cgroups isolators are enabled) will then assign the cgroup name for any executor that is launched to this separate slice. the consequence of this is that when the agent unit is terminated, the separate slice will continue to delegate the cgroups preventing systemd from migrating the pids. a side benefit of this is that we can maintain the killmode=controlgroup flag on the agent and terminate all agent specific services such as the fetcher without terminating the tasks. this provides for a nice cleanup.    this solution will still require that the agent unit be launched with the delegate=true flag such that there is no race during the transition of the pids from the agent to the separate slice.    the agent will be responsible for verifying the slice is still available upon recovery, and warning the operator if it notices that the tasks it is recovering are no longer associated with this separate slice, as this can cause silent loss of isolation of existing tasks.",8,train
MESOS-3426,process::collect and process::await do not perform discard propagation.,"when aggregating futures with collect, one may discard the outer future:      promise/ p1;  promise/ p2;    future/ collect = process::collect(p1.future(), p2.future());    collect.discard();    / collect will transition to discarded    / however, p.future().hasdiscard() remains false  / as there is no discard propagation!      discard requests should propagate down into the inner futures being collected.",3,train
MESOS-3428,Support running filesystem isolation with Command Executor in MesosContainerizer,nan,4,train
MESOS-3430,LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem fails on CentOS 7.1,"just ran root tests on centos 7.1 and had the following failure (clean build, just pulled from master):    [ run      ] linuxfilesystemisolatortest.rootpersistentvolumewithoutrootfilesystem  ../../src/tests/containerizer/filesystemisolatortests.cpp:498: failure  (wait).failure(): failed to clean up an isolator when destroying container '366b6d37b3264ed18a5f43d483dbbace' :failed to unmount volume '/tmp/linuxfilesystemisolatortestrootpersistentvolumewithoutrootfilesystemkxgvoh/sandbox/volume': failed to unmount '/tmp/linuxfilesystemisolatortestrootpersistentvolumewithoutrootfilesystemkxgvoh/sandbox/volume': invalid argument  ../../src/tests/utils.cpp:75: failure  os::rmdir(sandbox.get()): device or resource busy  [  failed  ] linuxfilesystemisolatortest.rootpersistentvolumewithoutrootfilesystem (1943 ms)  [] 1 test from linuxfilesystemisolatortest (1943 ms total)    [] global test environment tear down  [==========] 1 test from 1 test case ran. (1951 ms total)  [  passed  ] 0 tests.  [  failed  ] 1 test, listed below:  [  failed  ] linuxfilesystemisolatortest.root_persistentvolumewithoutrootfilesystem  ",2,train
MESOS-3432,Unify the implementations of the image provisioners.,"the current design uses separate provisioner implementation for each type of image (e.g., appc, docker).    this creates a lot of code duplications. since we already have a unified provisioner backend (e.g., copy, bind, overlayfs), we should be able to unify the implementations of image provisioners and hide the image specific logics in the corresponding 'store' implementation.",5,train
MESOS-3433,Unmount irrelevant host mounts in the new container's mount namespace.,as described in this https:/github.com/apache/mesos/blob/e601e469c64594dd8339352af405cbf26a574ea8/src/slave/containerizer/isolators/filesystem/linux.cpp#l418:      / todo(jieyu): try to unmount work directory mounts and persistent    / volume mounts for other containers to release the extra    / references to those mounts.      this will a best effort attempt to alleviate the race condition between provisioner's container cleanup and new containers copying host mount table.,3,train
MESOS-3443,Windows: Port protobuf_tests.hpp,"we have ported `stout/protobuf.hpp`, but to make the `protobuf_tests.cpp` file to work, we need to port `stout/uuid.hpp`.",2,train
MESOS-3449,Expand the range of integer precision in json <-> protobuf conversions to include unsigned integers,"the previous changes (mesos3345) to support integer precision when converting json / protobuf did not support precision for unsigned integers between int64max and uint64max.  (there's some loss, but the conversion is still as good/bad as it was with doubles.)    this problem is due to a limitation in the json parsing library we use (picojson), which parses integers as int64t.    some possible solutions or things to investigate:   we can patch picojson to parse some large values as uint64t.   we can investigate using another parsing library.    if we want extra precision beyond 64 or 80 bits per double, one possibility is the https:/gmplib.org/.  we'd still need to change the parsing library though.",5,train
MESOS-3455,Higher level construct for expressing process dispatch,"since mesos code is based on the actor model and dispatching an interface    asynchronously is a large part of the code base, generalizing the concept of    asynchronously dispatching an interface would eliminate the need to manual    programming of the dispatch boilerplate.    an example usage:    for a simple interface like:      class interface                                                                    ;             today the developer has to do the following:    a. write a wrapper class that implements the same interface to add the    dispatching boilerplate.    b. spend precious time in reviews.    c. risk introducing bugs.    none of the above steps add any value to the executable binary.    the wrapper class would look like:      /  hpp file                                                                     class interfaceprocess;                                                              class interfaceimpl : public interface                                             ;                                                                                   /  cpp file                                                                     try/> create(const flags& flags)                                                                                                                   future future/ interfaceimpl::writetofile(const char  data)                                                                                                     interfaceimpl::interfaceimpl()                                                                                                                                         interfaceimpl::~interfaceimpl()                                                             at the caller/client site, the code would look like:      try/> in = interfaceimpl::create(flags);                           future/ result =                                                              in>writetofile(data);                                                                                 proposal    we should use c's rich language semnatics to express the intent and avoid    the boilerplate we write manually.    the basic intent of the code that leads to all the boilerplate above is:    a. an interface that provides a set of functionality.    b. an implementation of the interface.    c. ability to dispatch that interface asynchronously using actor.    c has a rich set of generics that can be used to express above.    components    processdispatcher    this component will ""dispatch"" an interface implementation asychronously using   the process framework.    this component can be expressed as:      processdispatcher/         dispatchinterface    any interface that provides an implementation that can be ""dispatched"" can be    expressed using this component.    this component can be expressed as:      dispatchable/          usage:    simple usage      try/>> dispatcher =                                     processdispatcher/::create(flags);                        future/ result =                                                              dispatcher >dispatch(                                                                interface::writetofile,                                                            data);                                                                             collecting the interface in a container      vector/>> dispatchcollection;                           try/>> dispatcher1 =                                  processdispatcher/::create(flags);                       try/>> dispatcher2 =                                  processdispatcher/::create(""test"");                      dispatchcollection.pushback(dispatcher1);                                         dispatchcollection.push_back(dispatcher2);          the advantages of using the generic dispatcher:    saves time by avoiding to write all the boilerplate and going through review  cycles.  less bugs.  focus on real problem and not boilerplate.  ",6,train
MESOS-3457,Add flag to disable hostname lookup,"in testing / buildinging dcos we've found that we need to set hostname explicitly on the masters. for our uses ip and `hostname` must always be the same thing.     more in general, under certain circumstances, dynamic lookup of hostname, while successful, provides undesirable results; we would also like, in those circumstances, be able to just set the hostname to the chosen  ip address (possibly set via the  ipdiscoverycommand method).    we suggest adding a nohostnamelookup.   note that we can introduce this flag as hostnamelookup with a default to 'true' (which is the current semantics) and that way someone can do nohostnamelookup or hostnamelookup=false.  ",3,train
MESOS-3458,Segfault when accepting or declining inverse offers,discovered while writing a test for filters (in regards to inverse offers).    fix here: https:/reviews.apache.org/r/38470/,1,train
MESOS-3459,Change /machine/up and /machine/down endpoints to take an array,"with [mesos 3312] committed, the /machine/up and /machine/down endpoints should also take an input as an array.    it is important to change this before maintenance primitives are released:  https:/reviews.apache.org/r/38011/    also, a minor change to the error message from these endpoints:  https:/reviews.apache.org/r/37969/",1,train
MESOS-3466,Add metrics for filesystem isolation and image provisioning.,we need to know about:    1) errors encountered while provisioning root filesystems  2) errors encountered while cleaning up root filesystems  3) number of containers changing root filesystem  ...,2,train
MESOS-3467,Provide the users with a fully writable filesystem,"in the first phase of filesystem provisioning and isolation we are disallowing (or at least should, especially in the case of copybackend) users to write outside the sandbox without explicitly mounting specific volumes into the container. we do this even when overlaybackend can potentially support a empty writable top layer.    however in the real world use of containers (and for people coming from the vm world), users and applications often are used to being able to write to the full filesystem (restricted by plain file system permissions) with reasons ranging from applications being nonportable (filesystemwise) to the need to do custom installs at run time to system directories (inside its container).    in general, it's a good practice to restrict the application to write to confined locations and software dependencies can be managed through pre packaged layers but these often introduce a high entry barrier for users.    we should discuss a solution that gives the users the option to write to a full filesystem with a filesystem layer on top of provisioned images and optionally enable persistence of that layer through persistent volumes. this has implication in the management of user namespaces and resource reservations and requires a thorough design.",13,train
MESOS-3468,Improve apply_reviews.sh script to apply chain of reviews,currently the support/apply review.sh script allows an user (typically committer) to apply a single review on top the head. since mesos contributors typically submit a chain of reviews for a given issue it makes sense for the script to apply the whole chain recursively.,8,train
MESOS-3470,UserCgroupIsolatorTest failed on CentOS 6.6,usercgroupisolatortest use /sys/fs/cgroup as cgroupshierarchy. but centos 6.6 cgroupshierarchy is /cgroup. need change to follow the way in containerizertest.,1,train
MESOS-3472,RegistryTokenTest.ExpiredToken test is flaky,"registrytokentest.expiredtoken test is flaky. here is the error i got on osx after running it for several times:      [ run      ] registrytokentest.expiredtoken  ../../src/tests/containerizer/provisionerdockertests.cpp:167: failure  value of: token.iserror()    actual: false  expected: true  libcabi.dylib: terminating with uncaught exception of type testing::internal::googletestfailureexception: ../../src/tests/containerizer/provisionerdockertests.cpp:167: failure  value of: token.iserror()    actual: false  expected: true   aborted at 1442708631 (unix time) try ""date  d @1442708631"" if you are using gnu date   pc: @     0x7fff925fd286 pthreadkill   sigabrt (@0x7fff925fd286) received by pid 7082 (tid 0x7fff7d7ad300) stack trace:       @     0x7fff9041af1a sigtramp      @     0x7fff59759968 (unknown)      @     0x7fff9bb429b3 abort      @     0x7fff90ce1a21 abortmessage      @     0x7fff90d099b9 defaultterminatehandler()      @     0x7fff994767eb objcterminate()      @     0x7fff90d070a1 std::terminate()      @     0x7fff90d06d48 cxarethrow      @        0x10781bb16 testing::internal::handleexceptionsinmethodifsupported/()      @        0x1077e9d30 testing::unittest::run()      @        0x106d59a91 runalltests()      @        0x106d55d47 main      @     0x7fff8fc395c9 start      @                0x3 (unknown)  abort trap: 6  ~/src/mesos/build ((3ee82e3...)) $  ",3,train
MESOS-3476,Refactor Status Update method on Agent to handle HTTP based Executors,"currently, receiving a status update sent from slave to itself , runtask , killtask and status updates from executors are handled by the slave::statusupdate method on slave. the signature of the method is void slave::statusupdate(statusupdate update, const upid& pid).     we need to create another overload of it that can also handle http based executors which the previous pid based function can also call into. the signature of the new function could be:    void slave::statusupdate(statusupdate update, executor  executor)    the http executor would also call into this new function via src/slave/http.cpp",8,train
MESOS-3480,Refactor Executor struct in Slave to handle HTTP based executors,"currently, the struct executor in slave only supports executors connected via message passing (driver). we should refactor it to add support for http based executors similar to what was done for the scheduler api struct framework in src/master/master.hpp",3,train
MESOS-3481,Add const accessor to Master flags,"it would make sense to have an accessor to the master's flags, especially for tests.    for example, see https:/github.com/apache/mesos/blob/2876b8c918814347dd56f6f87d461e414a90650a/src/tests/mastermaintenancetests.cpp#l1231 l1235.",2,train
MESOS-3483,LinuxFilesystemIsolator should make the slave's work_dir a shared mount.,"so that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. if we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.      e0921 17:35:57.268159 47010 bind.cpp:182] failed to remove rootfs mount point '/var/lib/mesos/provisioner/containers/07eb666025ff4e838b2f06955567e04a/backends/bind/rootfses/30f7e5e255d04d4da662f8aad0d56b33': device or resource busy  e0921 17:35:57.268349 47010 provisioner.cpp:403] failed to remove the provisioned container directory at '/var/lib/mesos/provisioner/containers/07eb666025ff4e838b2f06955567e04a': device or resource busy  ",3,train
MESOS-3485,Make hook execution order deterministic,"currently, when using multiple hooks of the same type, the execution order is implementationdefined.     this is because in src/hook/manager.cpp, the list of available hooks is stored in a hashmap/. a hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. a data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. i suggest that the execution order should be the order in which hooks are specified with hooks when starting an agent/master.    this will be useful when combining multiple hooks after mesos3366 is done.",3,train
MESOS-3489,Add support for exposing Accept/Decline responses for inverse offers,current implementation of maintenance primitives does not support exposing accept/decline responses of frameworks to the cluster operators.     this functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.,2,train
MESOS-3490,Mesos UI fails to represent JSON entities,"the mesos ui is broken, it seems to fail to represent json from /state.  this may have been introduced with https:/reviews.apache.org/r/38028 ",1,train
MESOS-3491,Enable ubuntu builds in ASF CI,"i've disabled ubuntu:14.04 builds on asf ci because the job randomly fails on fetching packages.      get:406 http:/archive.ubuntu.com/ubuntu/ trustyupdates/main gdisk amd64 0.8.81ubuntu0.1 [185 kb]  err http:/archive.ubuntu.com/ubuntu/ trustysecurity/main libldap2.42 amd64 2.4.311nmu2ubuntu8.1    404  not found [ip: 91.189.91.15 80]  err http:/archive.ubuntu.com/ubuntu/ trustysecurity/main libfreetype6 amd64 2.5.21ubuntu2.4    404  not found [ip: 91.189.91.15 80]  err http:/archive.ubuntu.com/ubuntu/ trustysecurity/main libicu52 amd64 52.13ubuntu0.3    404  not found [ip: 91.189.91.15 80]  fetched 213 mb in 1min 57s (1812 kb/s)   [91me [0m [91m: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/o/openldap/libldap2.422.4.311nmu2ubuntu8.1amd64.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/f/freetype/libfreetype62.5.21ubuntu2.4amd64.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/i/icu/libicu5252.13ubuntu0.3amd64.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfscommon1.20.30ubuntu1.1all.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfslibs1.20.30ubuntu1.1amd64.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfsdaemons1.20.30ubuntu1.1amd64.deb  404  not found [ip: 91.189.91.15 80]    e: failed to fetch http:/archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs1.20.30ubuntu1.1amd64.deb  404  not found [ip: 91.189.91.15 80]    e: unable to fetch some archives, maybe run aptget update or try with fixmissing?   [0mthe command '/bin/sh c aptget y install buildessential clang git maven autoconf libtool' returned a non zero code: 100      we need to figure out what the problem is and fix it before enabling testing on ubuntu.",1,train
MESOS-3492,Expose maintenance user doc via the documentation home page,"the committed docs can be found here:  http:/mesos.apache.org/documentation/latest/maintenance/    we need to add a link to docs/home.md  also, the doc needs some minor formatting tweaks.",1,train
MESOS-3496,Create interface for digest verifier,"add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",2,train
MESOS-3497,Add implementation for sha256 based file content verification.,https:/reviews.apache.org/r/38747/,3,train
MESOS-3499,Add a test for os::realpath(),nan,1,train
MESOS-3501,configure cannot find libevent headers in CentOS 6,"if libevent is installed via sudo yum install libeventheaders, running ../configure enablelibevent will fail to discover the libevent headers:      checking event2/event.h usability... no  checking event2/event.h presence... no  checking for event2/event.h... no  configure: error: cannot find libevent headers    libevent is required for libprocess to build.    ",2,train
MESOS-3504,Introduce MESOS_SANDBOX environment variable in Mesos containerizer.,"similar to docker containerizer, if a container changes rootfs, we'll have two environment variables:    mesosdirectory: the path in the host filesystem  mesossandbox: the path in the container filesystem",3,train
MESOS-3506,Build instructions for CentOS 6.6 should include `sudo yum update`,neglecting to run sudo yum update on centos 6.6 currently causes the build to break when building mesos 0.25.0.jar. the build instructions for this platform on the getting started page should be changed accordingly.,1,train
MESOS-3510,Synchronize V1 helper functions with pre-v1,nan,5,train
MESOS-3512,Don't retry close() on EINTR.,"on linux, retrying close on eintr is dangerous because the fd is already released and we may accidentally close a newly opened fd (from another thread), see:    http:/ewontfix.com/4/  http:/lwn.net/articles/576478/  http:/lwn.net/articles/576591/    it appears that other oses, like hpux, require a retry of close on eintr. the austin group recently proposed changes to posix to require that the eintr case need a retry, but einprogress be used for when a retry should not occur:    http:/austingroupbugs.net/view.php?id=529    however, linux does not follow this and so we need to remove our eintr retries.    some more links for posterity:    https:/github.com/wahern/cqueues/issues/56#issuecomment 108656004  https:/code.google.com/p/chromium/issues/detail?id=269623  https:/codereview.chromium.org/23455051/  ",1,train
MESOS-3513,Cgroups Test Filters aborts tests on Centos 6.6 ,running make check on centos 6.6 causes all tests to abort due to checksome test in cgroupsfilter:      build directory: /home/jenkins/workspace/mesosconfigcentos6/build  f0923 23:00:49.748896 27362 environment.cpp:132] checksome(hierarchies): failed to determine canonical path of /sys/fs/cgroup/freezer: no such file or directory    check failure stack trace:       @     0x7fb786ca0c4d  google::logmessage::fail()      @     0x7fb786ca298c  google::logmessage::sendtolog()      @     0x7fb786ca083c  google::logmessage::flush()      @     0x7fb786ca3289  google::logmessagefatal::~logmessagefatal()      @           0x58e66c  mesos::internal::tests::cgroupsfilter::cgroupsfilter()      @           0x58712f  mesos::internal::tests::environment::environment()      @           0x4c882f  main      @     0x7fb782767d5d  libcstart_main      @           0x4d6331  (unknown)  make[3]:   [check local] aborted  ,1,train
MESOS-3515,Support Subscribe Call for HTTP based Executors,we need to add a subscribe(...) method in src/slave/slave.cpp to introduce the ability for http based executors to subscribe and then receive events on the persistent http connection. most of the functionality needed would be similar to master::subscribe in src/master/master.cpp.,5,train
MESOS-3516,Add user doc for networking support in Mesos 0.25.0,nan,2,train
MESOS-3519,Fix file descriptor leakage / double close in the code base,nan,3,train
MESOS-3520,Add an abstraction to manage the life cycle of file descriptors.,"in order to avoid missing close() calls on file descriptors, or double closing file descriptors, it would be nice to add a reference counted filedescriptor in a similar way to what we've done for socket. this will be closed automatically when the last reference goes away, and double closes can be prevented via internal state.",5,train
MESOS-3525,Figure out how to enforce 64-bit builds on Windows.,we need to make sure people don't try to compile mesos on 32bit architectures. we don't want a windows repeat of something like this:    https:/issues.apache.org/jira/browse/mesos267,3,train
MESOS-3539,Validate that slave's work_dir is a shared mount in its own peer group when LinuxFilesystemIsolator is used.,"to address this todo in the code:      src/slave/containerizer/isolators/filesystem/linux.cpp +122      / todo(jieyu): currently, we don't check if the slave's work_dir  / mount is a shared mount or not. we just assume it is. we cannot  / simply mark the slave as shared again because that will create a  / new peer group for the mounts. this is a temporary workaround for  / now while we are thinking about fixes.  ",3,train
MESOS-3540,Libevent termination triggers Broken Pipe,"when the libevent loop terminates and we unblock the sigpipe signal, the pending sigpipe instantly triggers and causes a broken pipe when the test binary stops running.    program received signal sigpipe, broken pipe.  [switching to thread 0x7ffff18b4700 (lwp 16270)]  pthreadsigmask (how=1, newmask=/, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthreadsigmask.c:53  53 ../sysdeps/unix/sysv/linux/pthreadsigmask.c: no such file or directory.  (gdb) bt  #0  pthreadsigmask (how=1, newmask=/, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthreadsigmask.c:53  #1  0x00000000006fd9a4 in unblock () at ../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp:90  #2  0x00000000007d7915 in run () at ../../../3rdparty/libprocess/src/libevent.cpp:125  #3  0x00000000007950cb in minvoke/(void) () at /usr/include/c/4.9/functional:1700  #4  0x0000000000795000 in operator() () at /usr/include/c/4.9/functional:1688  #5  0x0000000000794f6e in mrun () at /usr/include/c/4.9/thread:115  #6  0x00007ffff668de30 in ?? () from /usr/lib/x8664linuxgnu/libstdc.so.6  #7  0x00007ffff79a16aa in startthread (arg=0x7ffff18b4700) at pthreadcreate.c:333  #8  0x00007ffff5df1eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.s:109  ",2,train
MESOS-3550,Create a Executor Library based on the new Executor HTTP API,"similar to the scheduler library src/scheduler/scheduler.cpp , we would need a executor library that speaks the new executor http api. ",5,train
MESOS-3551,Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.,"strerror() is not required to be thread safe by posix and is listed as unsafe on linux:    http:/pubs.opengroup.org/onlinepubs/9699919799/  http:/man7.org/linux/man pages/man3/strerror.3.html    i don't believe we've seen any issues reported due to this. we should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites.",3,train
MESOS-3552,CHECK failure due to floating point precision on reservation request,result.cpus() == cpus() check is failing due to ( double == double ) comparison problem.       root cause :     framework requested 0.1 cpu reservation for the first task. so far so good. next reserve operation  lead to double operations resulting in following double values :     results.cpus() : 23.9999999999999964472863211995 cpus() : 24    and the check ( result.cpus() == cpus() ) failed.      the double arithmetic operations caused results.cpus() value to be :  23.9999999999999964472863211995 and hence ( 23.9999999999999964472863211995 == 24 ) failed.      ,3,train
MESOS-3553,LIBPROCESS_IP not passed when executor's environment is specified,"when the executor's environment is specified explicitly via executorenvironmentvariables, libprocess_ip will not be passed, leading to errors in some cases   for example, when no dns is available.",2,train
MESOS-3554,Allocator changes trigger large re-compiles,"due to the templatized nature of the allocator, even small changes trigger large recompiles of the code base. this make iterating on changes expensive for developers.",3,train
MESOS-3556,mesos.cli broken in 0.24.x,"the issue was initially reported on the mailing list: http:/    the format of the master data stored in zookeeper has changed but the mesos.cli does not reflect these changes causing tools like mesostail and mesosps to fail.    example error from mesostail:      mesosmaster ~$ mesos tail f n 50 service  traceback (most recent call last):    file ""/usr/local/bin/mesostail"", line 11, in /      sys.exit(main())    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/cli.py"", line 61, in   wrapper      return fn(args, kwargs)    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/cmds/tail.py"", line   55, in main      args.task, args.file, fail=(not args.follow)):    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/cluster.py"", line 27,   in files      tlist = master.tasks(fltr)    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/master.py"", line 174,   in tasks      self.tasklist(activeonly))))    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/master.py"", line 153,   in tasklist      [util.merge(x, keys) for x in self.frameworks(activeonly)])    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/master.py"", line 185,   in frameworks      return util.merge(self.state, keys)    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/util.py"", line 58, in   get      value = self.fget(inst)    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/master.py"", line 123,   in state      return self.fetch(""/master/state.json"").json()    file ""/usr/local/lib/python2.7/distpackages/mesos/cli/master.py"", line 64,   in fetch      return requests.get(urlparse.urljoin(self.host, url), kwargs)    file ""/usr/local/lib/python2.7/distpackages/requests/api.py"", line 69, in get      return request('get', url, params=params, kwargs)    file ""/usr/local/lib/python2.7/distpackages/requests/api.py"", line 50, in   request      response = session.request(method=method, url=url, kwargs)    file ""/usr/local/lib/python2.7/distpackages/requests/sessions.py"", line 451,   in request      prep = self.preparerequest(req)    file ""/usr/local/lib/python2.7/distpackages/requests/sessions.py"", line 382,   in preparerequest      hooks=mergehooks(request.hooks, self.hooks),    file ""/usr/local/lib/python2.7/distpackages/requests/models.py"", line 304,   in prepare      self.prepareurl(url, params)    file ""/usr/local/lib/python2.7/distpackages/requests/models.py"", line 357,   in prepare_url      raise invalidurl( e.args)  requests.exceptions.invalidurl: failed to parse:   10.100.1.100:5050"",""port"":5050,""version"":""0.24.1""}      the problem exists in https:/github.com/mesosphere/mesoscli/blob/master/mesos/cli/master.py#l107. the code should be along the lines of:                  try:                  parsed =  json.loads(val)                  return parsed[""address""][""ip""]  "":""  str(parsed[""address""][""port""])              except exception:                  return val.split(""@"")[ 1]      this causes the master address to come back correctly.",1,train
MESOS-3558,Implement  HTTPCommandExecutor that uses the Executor Library ,"instead of using the mesosexecutordriver , we should make the commandexecutor in src/launcher/executor.cpp use the new executor http library that we create in mesos 3550.     this would act as a good validation of the http api implementation.",13,train
MESOS-3559,Make the Command Scheduler use the HTTP Scheduler Library,we should make the command scheduler in src/cli/executor.cpp use the scheduler library src/scheduler/scheduler.cpp instead of the scheduler driver.,3,train
MESOS-3560,JSON-based credential files do not work correctly,"specifying the following credentials file:        ]  }      then hitting a master endpoint with:    curl i u user:password? ...      does not work. this is contrary to the textbased credentials file which works:    user password      currently, the password in a jsonbased credentials file needs to be base64 encoded in order for it to work:        ]  }  ",1,train
MESOS-3563,Revocable task CPU shows as zero in /state.json,"the slave's state.json reports revocable task resources as zero:      resources: ,      also, there is no indication that a task uses revocable cpu. it would be great to have this type of info.",2,train
MESOS-3567,Support TCP checks in Mesos health check program,"in marathon we have the ability to specify health checks for:   command (mesos supports this)   http (see progress in mesos2533)   tcp missing    see here for reference: https:/mesosphere.github.io/marathon/docs/health checks.html    since we made good experiences with those 3 options in marathon, i see a lot of value, if mesos would also support them.  ",8,train
MESOS-3570,Make Scheduler Library use HTTP Pipelining Abstraction in Libprocess,"currently, the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call. this was done because there was no http pipelining abstraction in libprocess process::post.    however once mesos 3332 is resolved, we should be now able to use the new abstraction.",8,train
MESOS-3571,Refactor registry_client,refactor registry client component to:     make methods shorter for readability   pull out structs not related to registry client into common namespace.,5,train
MESOS-3573,Mesos does not kill orphaned docker containers,"after upgrade to 0.24.0 we noticed hanging containers appearing. looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.    here's how to trigger this bug:    1. deploy app in docker container.  2. kill corresponding mesosdockerexecutor process  3. observe hanging container    here are the logs after kill:      slave1     i1002 12:12:59.362284  7791 docker.cpp:1374] destroying container 'f083aaa2d5c343c1b6ba342de8829fa8'  slave1     i1002 12:12:59.363876  7791 slave.cpp:3399] executor 'sleepy.87eb619168fe11e594448eb895523b9c' of framework 201509231221302153451692505010000 terminated with signal terminated  slave1     i1002 12:12:59.367842  7791 slave.cpp:5094] terminating task sleepy.87eb619168fe11e594448eb895523b9c  slave1     i1002 12:12:59.368671  7791 statusupdatemanager.cpp:322] received status update taskfailed (uuid: 4a1b2387a4694f01bfcb0d1cccbde550) for task sleepy.87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000  slave1     i1002 12:12:59.370636  7791 statusupdatemanager.cpp:376] forwarding update taskfailed (uuid: 4a1b2387a4694f01bfcb0d1cccbde550) for task sleepy.87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 to the slave  slave1     i1002 12:12:59.371908  7791 slave.cpp:2899] status update manager successfully handled status update taskfailed (uuid: 4a1b2387a4694f01bfcb0d1cccbde550) for task sleepy.87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000  master1    i1002 12:12:59.372534    11 master.cpp:4108] forwarding status update taskfailed (uuid: 4a1b2387a4694f01bfcb0d1cccbde550) for task sleepy.87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000  master1    i1002 12:12:59.373447    11 hierarchical.hpp:814] recovered cpus():0.1; mem():16; ports():[3168531685] (total: cpus():4; mem():1001; disk():52869; ports():[3100032000], allocated: cpus():8.32667e17) on slave 20151002120829215345169250501s0 from framework 201509231221302153451692505010000      another issue: if you restart mesos slave on the host with orphaned docker containers, they are not getting killed. this was the case before and i hoped for this trick to kill hanging containers, but it doesn't work now.    marking this as critical because it hoards cluster resources and blocks scheduling.",5,train
MESOS-3575,V1 API java/python protos are not generated,"the java/python protos for the v1 api should be generated according to the makefile; however, they do not show up in the generated build directory.",2,train
MESOS-3579,FetcherCacheTest.LocalUncachedExtract is flaky,"from asf ci:  https:/builds.apache.org/job/mesos/866/compiler=clang,configuration=verbose,os=ubuntu:14.04,labelexp=docker%7c%7chadoop/console      [ run      ] fetchercachetest.localuncachedextract  using temporary directory '/tmp/fetchercachetestlocaluncachedextractjhbfea'  i0925 19:15:39.541198 27410 leveldb.cpp:176] opened db in 3.43934ms  i0925 19:15:39.542362 27410 leveldb.cpp:183] compacted db in 1.136184ms  i0925 19:15:39.542428 27410 leveldb.cpp:198] created db iterator in 35866ns  i0925 19:15:39.542448 27410 leveldb.cpp:204] seeked to beginning of db in 8807ns  i0925 19:15:39.542459 27410 leveldb.cpp:273] iterated through 0 keys in the db in 6325ns  i0925 19:15:39.542505 27410 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0925 19:15:39.543143 27438 recover.cpp:449] starting replica recovery  i0925 19:15:39.543393 27438 recover.cpp:475] replica is in empty status  i0925 19:15:39.544373 27436 replica.cpp:641] replica in empty status received a broadcasted recover request  i0925 19:15:39.544791 27433 recover.cpp:195] received a recover response from a replica in empty status  i0925 19:15:39.545284 27433 recover.cpp:566] updating replica status to starting  i0925 19:15:39.546155 27436 master.cpp:376] master c8bf1c9550f44832a570c560f0b466ae (f57fd4291168) started on 172.17.1.195:41781  i0925 19:15:39.546257 27433 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 747249ns  i0925 19:15:39.546288 27433 replica.cpp:323] persisted replica status to starting  i0925 19:15:39.546483 27434 recover.cpp:475] replica is in starting status  i0925 19:15:39.546187 27436 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/fetchercachetestlocaluncachedextractjhbfea/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.26.0/inst/share/mesos/webui"" workdir=""/tmp/fetchercachetestlocaluncachedextractjhbfea/master"" zksessiontimeout=""10secs""  i0925 19:15:39.546567 27436 master.cpp:423] master only allowing authenticated frameworks to register  i0925 19:15:39.546617 27436 master.cpp:428] master only allowing authenticated slaves to register  i0925 19:15:39.546632 27436 credentials.hpp:37] loading credentials for authentication from '/tmp/fetchercachetestlocaluncachedextractjhbfea/credentials'  i0925 19:15:39.546931 27436 master.cpp:467] using default 'crammd5' authenticator  i0925 19:15:39.547044 27436 master.cpp:504] authorization enabled  i0925 19:15:39.547276 27441 whitelistwatcher.cpp:79] no whitelist given  i0925 19:15:39.547320 27434 hierarchical.hpp:468] initialized hierarchical allocator process  i0925 19:15:39.547471 27438 replica.cpp:641] replica in starting status received a broadcasted recover request  i0925 19:15:39.548318 27443 recover.cpp:195] received a recover response from a replica in starting status  i0925 19:15:39.549067 27435 recover.cpp:566] updating replica status to voting  i0925 19:15:39.549115 27440 master.cpp:1603] the newly elected leader is master@172.17.1.195:41781 with id c8bf1c9550f44832a570c560f0b466ae  i0925 19:15:39.549162 27440 master.cpp:1616] elected as the leading master!  i0925 19:15:39.549190 27440 master.cpp:1376] recovering from registrar  i0925 19:15:39.549342 27434 registrar.cpp:309] recovering registrar  i0925 19:15:39.549666 27430 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 418187ns  i0925 19:15:39.549753 27430 replica.cpp:323] persisted replica status to voting  i0925 19:15:39.550089 27442 recover.cpp:580] successfully joined the paxos group  i0925 19:15:39.550320 27442 recover.cpp:464] recover process terminated  i0925 19:15:39.550904 27430 log.cpp:661] attempting to start the writer  i0925 19:15:39.551955 27434 replica.cpp:477] replica received implicit promise request with proposal 1  i0925 19:15:39.552351 27434 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 380746ns  i0925 19:15:39.552372 27434 replica.cpp:345] persisted promised to 1  i0925 19:15:39.552896 27436 coordinator.cpp:231] coordinator attemping to fill missing position  i0925 19:15:39.554003 27432 replica.cpp:378] replica received explicit promise request for position 0 with proposal 2  i0925 19:15:39.554534 27432 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 510572ns  i0925 19:15:39.554558 27432 replica.cpp:679] persisted action at 0  i0925 19:15:39.555516 27443 replica.cpp:511] replica received write request for position 0  i0925 19:15:39.555595 27443 leveldb.cpp:438] reading position from leveldb took 65355ns  i0925 19:15:39.556177 27443 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 542757ns  i0925 19:15:39.556200 27443 replica.cpp:679] persisted action at 0  i0925 19:15:39.556813 27429 replica.cpp:658] replica received learned notice for position 0  i0925 19:15:39.557251 27429 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 422272ns  i0925 19:15:39.557281 27429 replica.cpp:679] persisted action at 0  i0925 19:15:39.557315 27429 replica.cpp:664] replica learned nop action at position 0  i0925 19:15:39.558061 27442 log.cpp:677] writer started with ending position 0  i0925 19:15:39.559294 27434 leveldb.cpp:438] reading position from leveldb took 56536ns  i0925 19:15:39.560333 27432 registrar.cpp:342] successfully fetched the registry (0b) in 10.936064ms  i0925 19:15:39.560469 27432 registrar.cpp:441] applied 1 operations in 41340ns; attempting to update the 'registry'  i0925 19:15:39.561244 27441 log.cpp:685] attempting to append 176 bytes to the log  i0925 19:15:39.561378 27436 coordinator.cpp:341] coordinator attempting to write append action at position 1  i0925 19:15:39.562126 27439 replica.cpp:511] replica received write request for position 1  i0925 19:15:39.562515 27439 leveldb.cpp:343] persisting action (195 bytes) to leveldb took 364968ns  i0925 19:15:39.562539 27439 replica.cpp:679] persisted action at 1  i0925 19:15:39.563160 27438 replica.cpp:658] replica received learned notice for position 1  i0925 19:15:39.563699 27438 leveldb.cpp:343] persisting action (197 bytes) to leveldb took 455933ns  i0925 19:15:39.563730 27438 replica.cpp:679] persisted action at 1  i0925 19:15:39.563753 27438 replica.cpp:664] replica learned append action at position 1  i0925 19:15:39.564749 27434 registrar.cpp:486] successfully updated the 'registry' in 4.214016ms  i0925 19:15:39.564893 27434 registrar.cpp:372] successfully recovered registrar  i0925 19:15:39.564950 27442 log.cpp:704] attempting to truncate the log to 1  i0925 19:15:39.565039 27429 coordinator.cpp:341] coordinator attempting to write truncate action at position 2  i0925 19:15:39.565172 27430 master.cpp:1413] recovered 0 slaves from the registry (137b) ; allowing 10mins for slaves to reregister  i0925 19:15:39.565946 27429 replica.cpp:511] replica received write request for position 2  i0925 19:15:39.566349 27429 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 375473ns  i0925 19:15:39.566371 27429 replica.cpp:679] persisted action at 2  i0925 19:15:39.566994 27431 replica.cpp:658] replica received learned notice for position 2  i0925 19:15:39.567440 27431 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 437095ns  i0925 19:15:39.567483 27431 leveldb.cpp:401] deleting 1 keys from leveldb took 31954ns  i0925 19:15:39.567498 27431 replica.cpp:679] persisted action at 2  i0925 19:15:39.567514 27431 replica.cpp:664] replica learned truncate action at position 2  i0925 19:15:39.576660 27410 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  w0925 19:15:39.577055 27410 backend.cpp:50] failed to create 'bind' backend: bindbackend requires root privileges  i0925 19:15:39.583020 27443 slave.cpp:190] slave started on 46)@172.17.1.195:41781  i0925 19:15:39.583062 27443 slave.cpp:191] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/fetchercachetestlocaluncachedextractlwfzk4/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/fetchercachetestlocaluncachedextractlwfzk4/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.26.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resourcemonitoringinterval=""1secs"" resources=""cpus():1000; mem():1000"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/fetchercachetestlocaluncachedextractlwfzk4""  i0925 19:15:39.583472 27443 credentials.hpp:85] loading credential for authentication from '/tmp/fetchercachetestlocaluncachedextractlwfzk4/credential'  i0925 19:15:39.583752 27443 slave.cpp:321] slave using credential for: testprincipal  i0925 19:15:39.584249 27443 slave.cpp:354] slave resources: cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000]  i0925 19:15:39.584344 27443 slave.cpp:390] slave hostname: f57fd4291168  i0925 19:15:39.584362 27443 slave.cpp:395] slave checkpoint: true  i0925 19:15:39.585180 27428 state.cpp:54] recovering state from '/tmp/fetchercachetestlocaluncachedextractlwfzk4/meta'  i0925 19:15:39.585383 27440 statusupdatemanager.cpp:202] recovering status update manager  i0925 19:15:39.585636 27435 containerizer.cpp:386] recovering containerizer  i0925 19:15:39.586380 27438 slave.cpp:4110] finished recovery  i0925 19:15:39.586845 27438 slave.cpp:4267] querying resource estimator for oversubscribable resources  i0925 19:15:39.587059 27430 statusupdatemanager.cpp:176] pausing sending status updates  i0925 19:15:39.587064 27438 slave.cpp:705] new master detected at master@172.17.1.195:41781  i0925 19:15:39.587139 27438 slave.cpp:768] authenticating with master master@172.17.1.195:41781  i0925 19:15:39.587163 27438 slave.cpp:773] using default crammd5 authenticatee  i0925 19:15:39.587321 27438 slave.cpp:741] detecting new master  i0925 19:15:39.587357 27434 authenticatee.cpp:115] creating new client sasl connection  i0925 19:15:39.587574 27438 slave.cpp:4281] received oversubscribable resources  from the resource estimator  i0925 19:15:39.587739 27442 master.cpp:5138] authenticating slave(46)@172.17.1.195:41781  i0925 19:15:39.587853 27441 authenticator.cpp:407] starting authentication session for crammd5authenticatee(139)@172.17.1.195:41781  i0925 19:15:39.588052 27439 authenticator.cpp:92] creating new server sasl connection  i0925 19:15:39.588248 27431 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0925 19:15:39.588297 27431 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0925 19:15:39.588443 27437 authenticator.cpp:197] received sasl authentication start  i0925 19:15:39.588506 27437 authenticator.cpp:319] authentication requires more steps  i0925 19:15:39.588677 27443 authenticatee.cpp:252] received sasl authentication step  i0925 19:15:39.588814 27436 authenticator.cpp:225] received sasl authentication step  i0925 19:15:39.588855 27436 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'f57fd4291168' server fqdn: 'f57fd4291168' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0925 19:15:39.588876 27436 auxprop.cpp:174] looking up auxiliary property 'userpassword'  i0925 19:15:39.588937 27436 auxprop.cpp:174] looking up auxiliary property 'cmusaslsecretcrammd5'  i0925 19:15:39.588979 27436 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'f57fd4291168' server fqdn: 'f57fd4291168' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0925 19:15:39.588997 27436 auxprop.cpp:124] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0925 19:15:39.589011 27436 auxprop.cpp:124] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0925 19:15:39.589036 27436 authenticator.cpp:311] authentication success  i0925 19:15:39.589126 27443 authenticatee.cpp:292] authentication success  i0925 19:15:39.589192 27437 master.cpp:5168] successfully authenticated principal 'testprincipal' at slave(46)@172.17.1.195:41781  i0925 19:15:39.589238 27433 authenticator.cpp:425] authentication session cleanup for crammd5authenticatee(139)@172.17.1.195:41781  i0925 19:15:39.589412 27440 slave.cpp:836] successfully authenticated with master master@172.17.1.195:41781  i0925 19:15:39.589540 27440 slave.cpp:1230] will retry registration in 13.562027ms if necessary  i0925 19:15:39.589745 27436 master.cpp:3862] registering slave at slave(46)@172.17.1.195:41781 (f57fd4291168) with id c8bf1c9550f44832a570c560f0b466aes0  i0925 19:15:39.590121 27438 registrar.cpp:441] applied 1 operations in 70627ns; attempting to update the 'registry'  i0925 19:15:39.590831 27430 log.cpp:685] attempting to append 345 bytes to the log  i0925 19:15:39.590927 27439 coordinator.cpp:341] coordinator attempting to write append action at position 3  i0925 19:15:39.591809 27430 replica.cpp:511] replica received write request for position 3  i0925 19:15:39.592072 27430 leveldb.cpp:343] persisting action (364 bytes) to leveldb took 221734ns  i0925 19:15:39.592099 27430 replica.cpp:679] persisted action at 3  i0925 19:15:39.592643 27442 replica.cpp:658] replica received learned notice for position 3  i0925 19:15:39.593215 27442 leveldb.cpp:343] persisting action (366 bytes) to leveldb took 560946ns  i0925 19:15:39.593237 27442 replica.cpp:679] persisted action at 3  i0925 19:15:39.593255 27442 replica.cpp:664] replica learned append action at position 3  i0925 19:15:39.594663 27433 registrar.cpp:486] successfully updated the 'registry' in 4.472832ms  i0925 19:15:39.594874 27431 log.cpp:704] attempting to truncate the log to 3  i0925 19:15:39.595407 27429 slave.cpp:3138] received ping from slaveobserver(45)@172.17.1.195:41781  i0925 19:15:39.595450 27433 coordinator.cpp:341] coordinator attempting to write truncate action at position 4  i0925 19:15:39.596017 27442 replica.cpp:511] replica received write request for position 4  i0925 19:15:39.596029 27429 hierarchical.hpp:675] added slave c8bf1c9550f44832a570c560f0b466aes0 (f57fd4291168) with cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000] (allocated: )  i0925 19:15:39.595952 27441 master.cpp:3930] registered slave c8bf1c9550f44832a570c560f0b466aes0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with cpus():1000; mem():1000; disk():3.70122e06; ports():[3100032000]  i0925 19:15:39.596240 27429 hierarchical.hpp:1326] no resources available to allocate!  i0925 19:15:39.596263 27439 slave.cpp:880] registered with master master@172.17.1.195:41781; given slave id c8bf1c9550f44832a570c560f0b466aes0  i0925 19:15:39.596341 27439 fetcher.cpp:77] clearing fetcher cache  i0925 19:15:39.596345 27429 hierarchical.hpp:1421] no inverse offers to send out!  i0925 19:15:39.596367 27429 hierarchical.hpp:1239] performed allocation for slave c8bf1c9550f44832a570c560f0b466aes0 in 299337ns  i0925 19:15:39.596524 27434 statusupdatemanager.cpp:183] resuming sending status updates  i0925 19:15:39.596571 27442 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 575374ns  i0925 19:15:39.596662 27442 replica.cpp:679] persisted action at 4  i0925 19:15:39.596984 27439 slave.cpp:903] checkpointing slaveinfo to '/tmp/fetchercachetestlocaluncachedextractlwfzk4/meta/slaves/c8bf1c9550f44832a570c560f0b466aes0/slave.info'  i0925 19:15:39.597522 27434 replica.cpp:658] replica received learned notice for position 4  i0925 19:15:39.597553 27410 sched.cpp:164] version: 0.26.0  i0925 19:15:39.597746 27439 slave.cpp:939] forwarding total oversubscribed resources   i0925 19:15:39.598021 27429 master.cpp:4272] received update of slave c8bf1c9550f44832a570c560f0b466aes0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with total oversubscribed resources   i0925 19:15:39.598070 27434 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 531503ns  i0925 19:15:39.598162 27434 leveldb.cpp:401] deleting 2 keys from leveldb took 79081ns  i0925 19:15:39.598170 27428 sched.cpp:262] new master detected at master@172.17.1.195:41781  i0925 19:15:39.598206 27434 replica.cpp:679] persisted action at 4  i0925 19:15:39.598238 27434 replica.cpp:664] replica learned truncate action at position 4  i0925 19:15:39.598276 27428 sched.cpp:318] authenticating with master master@172.17.1.195:41781  i0925 19:15:39.598296 27428 sched.cpp:325] using default crammd5 authenticatee  i0925 19:15:39.598950 27430 hierarchical.hpp:735] slave c8bf1c9550f44832a570c560f0b466aes0 (f57fd4291168) updated with oversubscribed resources  (total: cpus():1000; mem():1000; disk():3.70122e06; ports( ):[3100032000], allocated: )  i0925 19:15:39.599242 27430 hierarchical.hpp:1326] no resources available to allocate!  i0925 19:15:39.599282 27430 hierarchical.hpp:1421] no inverse offers to send out!  i0925 19:15:39.599341 27430 hierarchical.hpp:1239] performed allocation for slave c8bf1c9550f44832a570c560f0b466aes0 in 327742ns  i0925 19:15:39.599632 27437 authenticatee.cpp:115] creating new client sasl connection  i0925 19:15:39.600005 27428 master.cpp:5138] authenticating schedulerdda30e8e47b74b1d9a9632364754be63@172.17.1.195:41781  i0925 19:15:39.600170 27435 authenticator.cpp:407] starting authentication session for crammd5_authenticatee(140)@172.17.1.195:41781  i0925 19:15:39.600518 27433 authenticator.cpp:92] creating new server sasl connection  i0925 19:15:39.600788 27436 authenticatee.cpp:206] received sasl authentication mechanisms: crammd5  i0925 19:15:39.600831 27436 authenticatee.cpp:232] attempting to authenticate with mechanism 'crammd5'  i0925 19:15:39.600944 27433 authenticator.cpp:197] received sasl authentication start  i0925 19:15:39.601019 27433 authenticator.cpp:319] authentication requires more steps  i0925 19:15:39.601150 27436 authenticatee.cpp:252] received sasl authentication step  i0925 19:15:39.601284 27436 authenticator.cpp:225] received sasl authentication step  i0925 19:15:39.601326 27436 auxprop.cpp:102] request to lookup properties for user: 'testprincipal' realm: 'f57fd4291168' server fqdn: 'f...",2,train
MESOS-3581,License headers show up all over doxygen documentation.,"currently license headers are commented in something resembling javadoc style,      /   licensed ...      since we use javadoc style comment blocks for doxygen documentation all license headers appear in the generated documentation, potentially and likely hiding the actual documentation.    using / to start the comment blocks would be enough to hide them from doxygen, but would likely also result in a largish (though mostly uninteresting) patch.",2,train
MESOS-3583,Introduce stream IDs in HTTP Scheduler API,"currently, the http scheduler api has no concept of sessions aka sessionid or a tokenid. this is useful in some failure scenarios. as of now, if a framework fails over and then subscribes again with the same frameworkid with the force option set, the mesos master would subscribe it.    if the previous instance of the framework/scheduler tries to send a call , e.g. call::kill with the same previous frameworkid set, it would be still accepted by the master leading to erroneously killing a task.    this is possible because we do not have a way currently of distinguishing connections. it used to work in the previous driver implementation due to the master also performing a upid check to verify if they matched and only then allowing the call. following the design process, we will implemented ""stream ids"" for mesos http schedulers; each id will be associated with a single subscription connection, and the scheduler must include it as a header in all non subscribe calls sent to the master.",5,train
MESOS-3584,"rename libprocess tests to ""libprocess-tests""","stout tests are in a binary named stouttests, mesos tests are in mesostests, but libprocess tests are just tests. it would be helpful to name them libprocess tests ",1,train
MESOS-3585,Add a test module for ip-per-container support,"with the addition of networkinfo to allow frameworks to request ippercontainer for their tasks, we should add a simple module that mimics the behavior of a real networkisolation module for testing purposes. we can then add this module in src/examples and write some tests against it.  this module can also serve as a template module for thirdparty network isolation provides for building their own network isolator modules.",3,train
MESOS-3586,MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky,"i am install mesos 0.24.0 on 4 servers which have very similar hardware and software configurations.     after performing ../configure, make, and make check some servers have completed successfully and other failed on test [ run      ] memorypressuremesostest.cgroupsrootstatistics.    is there something i should check in this test?       performed make check node001  [ run      ] memorypressuremesostest.cgroupsrootstatistics  i1005 14:37:35.585067 38479 exec.cpp:133] version: 0.24.0  i1005 14:37:35.593789 38497 exec.cpp:207] executor registered on slave 2015100514373523937682023510627900s0  registered executor on svdidac038.techlabs.accenture.com  starting task 010b2fe94eac41368a8a6ce7665488b0  forked command at 38510  sh c 'while true; do dd count=512 bs=1m if=/dev/zero of=./temp; done'      performed make check node002  [ run      ] memorypressuremesostest.cgroupsrootstatistics  i1005 14:38:58.794112 36997 exec.cpp:133] version: 0.24.0  i1005 14:38:58.802851 37022 exec.cpp:207] executor registered on slave 2015100514385723602137705042726325s0  registered executor on svdidac039.techlabs.accenture.com  starting task 9bb317ba41cb44a4b507d1c85ceabc28  sh c 'while true; do dd count=512 bs=1m if=/dev/zero of=./temp; done'  forked command at 37028  ../../src/tests/containerizer/memorypressuretests.cpp:145: failure  expected: (usage.get().memmediumpressurecounter()) >= (usage.get().memcriticalpressurecounter()), actual: 5 vs 6  20151005 14:39:00,130:26325(0x2af08cc78700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:37198] zk retcode= 4, errno=111(connection refused): server refused to accept the client  [  failed  ] memorypressuremesostest.cgroupsrootstatistics (4303 ms)  ",1,train
MESOS-3587,Framework failover when framework is 'active' does not trigger allocation.,"fwict, this is just a consequence of some technical debt in the master code. when an active framework fails over, we do not go through the deactivation>activation code paths, and so:    (1) the framework's filters in the allocator remain after the failover.  (2) the failed over framework does not receive an immediate allocation (it has to wait for the next allocation interval).    if the framework had disconnected first, then the failover goes through the deactivation>activation code paths.    this also means that some tests take longer to run than necessary.",5,train
MESOS-3593,Propagate Isolator::prepare() failures to the framework,"currently, if isolator::prepare fails for some isolator(s), we simply return a generic message about container being destroyed during launch.    it would be especially helpful if a third party isolator modules could report the error back to the framework.",2,train
MESOS-3595,Framework process hangs after master failover when number frameworks > libprocess thread pool size,"when running multi framework instances per process, if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock. e.g. on a machine with 24 cpus, if the framework instance count exceeds 24 ( per process)  then when the master fails over all the libprocess threads block updating the cache ( groupprocess) leading to deadlock. below is the stack trace of one the libprocess thread :      thread 101 (thread 0x7f42821f1700 (lwp 5974)):  #0  0x000000314100b5bc in pthreadcondwait@@glibc2.3.2 () from /lib64/libpthread.so.0  #1  0x00007f42870d1637 in gate::arrive(long) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #2  0x00007f42870be87c in process::processmanager::wait(process::upid const&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.eg  g/mesos/native/mesos.so  #3  0x00007f42870c25f7 in process::wait(process::upid const&, duration const&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.e  gg/mesos/native/mesos.so  #4  0x00007f428708e294 in process::latch::await(duration const&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/nativ  e/mesos.so  #5  0x00007f4286b67dea in process::future/::await(duration const&) const () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg  /mesos/native/mesos.so  #6  0x00007f4286b5a0df in process::future/::get() const () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/me  sos.so  #7  0x00007f4286ff0508 in zookeeper::getchildren(std::basicstring/, std::allocator/ > const&, bool, std::vector/, std::allocator/ >, std::allocator/, std::allocator/ > > >) () from /users/mchadha/venv/lib/python2.7/site  packages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #8  0x00007f4286cb394e in zookeeper::groupprocess::cache() () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mes  os.so  #9  0x00007f4286cb1e63 in zookeeper::groupprocess::updated(long, std::basicstring/, std::allocator/ > const&) () from /users/mchadha/venv/lib/py  thon2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #10 0x00007f4286ce027a in std::tr1::memfn/, std::allocator/ > const&)>::operator()(zo  okeeper::groupprocess, long, std::basicstring/, std::allocator/ > const&) const () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.n  ative0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #11 0x00007f4286ce0067 in std::tr1::resultof/, std::allocator/ > con  st&)> ()(std::tr1::resultof/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/)>::type, std::tr1::res  ultof/ ()(long, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))  >::type, std::tr1::resultof/, std::allocator/ >, false, false> ()(std::basicstring/  , std::allocator/ >, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))>::type)>::type std::tr1  ::bind/, std::allocator/ > const&)> ()(std::tr1::placeholder/, lo  ng, std::basicstring/, std::allocator/ >)>::call/(std::tr1::mu/, false, true> ( c  onst&)(std::tr1::placeholder/, std::tr1::tuple/), std::tr1::indextuple/) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.nati  ve0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #12 0x00007f4286cdfd16 in std::tr1::resultof/, std::allocator/ > con  st&)> ()(std::tr1::resultof/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/)>::type, std::tr1::resu  ltof/ ()(long, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))>:  :type, std::tr1::resultof/, std::allocator/ >, false, false> ()(std::basicstring/,  std::allocator/ >, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))>::type)>::type std::tr1::  bind/, std::allocator/ > const&)> ()(std::tr1::placeholder/, long,   std::basicstring/, std::allocator/ >)>::operator()/(zookeeper::groupprocess&) () from /users/mchadha/venv/lib/python2  .7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #13 0x00007f4286cdf8be in std::tr1::functionhandler/, std::allocator/ > const&)> ()(std::tr1::placeholder/, long, std::basicstring/, std::allocator/ >)> >::  minvoke(std::tr1::anydata const&, zookeeper::groupprocess) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/  mesos.so  #14 0x00007f4286cc2394 in std::tr1::function/::operator()(zookeeper::groupprocess) const () from /users/mchadha/venv/lib/python2.7/sitepackage  s/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #15 0x00007f4286cbc3a2 in void process::internal::vdispatcher/(process::processbase, std::tr1::sharedptr/ >) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #16 0x00007f4286ccdca5 in std::tr1::resultof/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/)>::type, std::tr1::resultof/ >, false, false> ()(std::tr1::sharedp  tr/ >, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))>::type))(process::processbase, std::tr1::sharedptr/ >)>::type std::tr1::bind/,  std::tr1::sharedptr/ >))(process::processbase, std::tr1::sharedptr/ >  )>::call/(std::tr1::mu/, false, true> ( const&)(std::tr1::placeholder/, std::tr1::tuple/), std:  :tr1::indextuple/) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #17 0x00007f4286cc7a5a in std::tr1::resultof/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/)>::type, std::tr1::resultof/ >, false, false> ()(std::tr1::sharedpt  r/ >, std::tr1::mu/, false, true> ()(std::tr1::placeholder/, std::tr1::tuple/))>::type))(process::processbase, std::tr1::sharedptr/ >)>::type std::tr1::bind/, st  d::tr1::sharedptr/ >))(process::processbase, std::tr1::sharedptr/ >)>  ::operator()/(process::processbase&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/me  sos.so  #18 0x00007f4286cc2480 in std::tr1::functionhandler/, std::tr1::sharedptr/ >))(process::processbase, std::tr1::sharedptr/ >)> >::minvoke(std::tr1::anydata con  st&, process::processbase) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #19 0x00007f42870db546 in std::tr1::function/::operator()(process::processbase) const () from /users/mchadha/venv/lib/python2.7/sitepackages/meso  s.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #20 0x00007f42870c1013 in process::processbase::visit(process::dispatchevent const&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8  664.egg/mesos/native/mesos.so  #21 0x00007f42870c5582 in process::dispatchevent::visit(process::eventvisitor) const () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx  8664.egg/mesos/native/mesos.so  #22 0x00007f428666680e in process::processbase::serve(process::event const&) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg  /mesos/native/mesos.so  #23 0x00007f42870bd88f in process::processmanager::resume(process::processbase) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664  .egg/mesos/native/mesos.so  #24 0x00007f42870b1cb9 in process::schedule(void ) () from /users/mchadha/venv/lib/python2.7/sitepackages/mesos.native0.22.1003py2.7linuxx8664.egg/mesos/native/mesos.so  #25 0x00000031410079d1 in start_thread () from /lib64/libpthread.so.0  #26 0x00000031408e88fd in clone () from /lib64/libc.so.6      solution:    create master detector per url instead of per framework.  will send the review request.     ",3,train
MESOS-3603,Test build failure due to comparison between signed and unsigned integers,"compilation fails on opensuse tumbleweed (linux 4.1.6, gcc 5.1.1, glibc 2.22) with the following errors:      in file included from ../../src/tests/valuestests.cpp:22:0:   ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h: in instantiatio  n of testing::assertionresult testing::internal::cmphelpereq(const char, const char,   const t1&, const t2&) [with t1 = int; t2 = long unsigned int]:   ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h:1484:23:   requi  red from static testing::assertionresult testing::internal::eqhelper/::compare(const char, const char, const t1&, const t2&) [with t1 = int; t2 = long un  signed int; bool lhsisnullliteral = false]   ../../src/tests/valuestests.cpp:287:3:   required from here   ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h:1448:16: error:   comparison between signed and unsigned integer expressions [werror=signcompare]     if (expected == actual) ",1,train
MESOS-3604,ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan,"the example persistent volume framework test does not pass in os x el capitan. it seems to be executing the /src/.libs/mesosexecutor directly while it should be executing the wrapper script at /src/mesosexecutor instead. the noexecutor framework passes however, which seem to have a very similar configuration with the persistent volume framework. the following is the output that shows the dyld load error:      i1008 01:22:52.280140 4284416 launcher.cpp:132] forked child with pid '1706' for contain  er 'b6d3bd962ebd47b1a16aa22ffba992aa'  i1008 01:22:52.280300 4284416 containerizer.cpp:873] checkpointing executor's forked pid   1706 to '/var/folders/p6/nfxknpz52dzfc6zqnz23tq180000gn/t/mesosxxxxxx.5oz3locb/0/meta/  slaves/34d6329e69cb4a72aee4fe892bf1c70bs2/frameworks/34d6329e69cb4a72aee4fe892b  f1c70b0000/executors/dec188d4d2dc40c5ac4d881adc3d81c0/runs/b6d3bd962ebd47b1a16a  a22ffba992aa/pids/forked.pid'  dyld: library not loaded: /usr/local/lib/libmesos0.26.0.dylib    referenced from: /users/mpark/projects/mesos/build/src/.libs/mesosexecutor    reason: image not found  dyld: library not loaded: /usr/local/lib/libmesos0.26.0.dylib    referenced from: /users/mpark/projects/mesos/build/src/.libs/mesosexecutor    reason: image not found  dyld: library not loaded: /usr/local/lib/libmesos0.26.0.dylib    referenced from: /users/mpark/projects/mesos/build/src/.libs/mesosexecutor    reason: image not found  i1008 01:22:52.365397 3211264 containerizer.cpp:1284] executor for container '06b649be88c840478fb5e89bdd096b66' has exited  i1008 01:22:52.365433 3211264 containerizer.cpp:1097] destroying container '06b649be88c840478fb5e89bdd096b66'  ",3,train
MESOS-3613,Port slave/paths.cpp to Windows,"important subset of dependency tree of changes necessary:    slave/paths.cpp: os, path",1,train
MESOS-3615,Port slave/state.cpp,"important subset of changes this depends on:    slave/state.cpp: pid, os, path, protobuf, paths, state  pid.hpp: address.hpp, ip.hpp  address.hpp: ip.hpp, net.hpp  net.hpp: ip, networking stuff  state: typeutils, pid, os, path, protobuf, uuid  typeutils.hpp: uuid.hpp",3,train
MESOS-3618,Port slave/containerizer/fetcher.cpp,"important subset of the dependency tree follows:    slave/containerizer/fetcher.cpp: slave, fetcher, collect, dispatch, net  collect: future, defer, process  fetcher: typeutils, future, process, subprocess  dispatch.hpp: process.hpp  net.hpp: ip, networking stuff  future.hpp: pid.hpp  defer.hpp: deferred.hpp, dispatch.hpp  deferred.hpp: dispatch.hpp, pid.hpp  typeutils.hpp: uuid.hpp  subprocess: os, future",3,train
MESOS-3619,Port slave/containerizer/isolator.cpp to Windows,"important subset of the dependency tree follows:    isolator.hpp: dispatch.hpp, path.hpp  isolator: process  dispatch.hpp: process.hpp  ",3,train
MESOS-3620,Create slave/containerizer/isolators/filesystem/windows.cpp,"should look a lot like the posix.cpp flavor. important subset of the dependency tree follows for the posix flavor:    slave/containerizer/isolators/filesystem/posix.cpp: filesystem/posix, fs, os, path  filesystem/posix: flags, isolator",3,train
MESOS-3623,Port slave/containerizer/mesos/containerizer.cpp to Windows,"important subset of the dependency tree follows:    slave/containerizer/mesos/containerizer.cpp: isolator, collect, defer, io, metrics, reap, subprocess, fs, os, path, protobuf_utils, paths, slave, containerizer, fetcher, launcher, posix, disk, containerizer, launch, provisioner",3,train
MESOS-3624,Port slave/containerizer/mesos/launch.cpp to Windows,"important subset of the dependency tree follows:    slave/containerizer/mesos/launch.cpp: os, protobuf, launch  launch: subcommand  subcommand: flags  flags.hpp: os.hpp, path.hpp, fetch.hpp",3,train
MESOS-3625,Add support for github and variable base URLs to apply-reviews.py,"from adam's email on dev@ list:    i have used the 'g' feature for github prs in the past, and we should  continue to support that model, so that new mesos contributors don't have  to create new rb accounts and learn a new process just for quick  documentation changes, etc.    as a side note, now that the myriad incubator project has migrated to  apache git and we can no longer merge prs directly, we were hoping to take  advantage of a tool like applyreviews to apply our pr patches. it looks  like applyreviews.sh only specifies 'mesos' in the githuburl/apiurl.  would applyreviews.py be just as easy to reuse for another project (i.e.  myriad)?",3,train
MESOS-3639,Implement stout/os/windows/killtree.hpp,killtree() is implemented using windows job objects. the processes created by the  executor are associated with a job object using `createjob'. killtree() is simply terminating the job object.     helper functions:  `createjob` function creates a job object whose name is derived from the `pid` and associates the `pid` process with the job object. every process started by the process which is part of the job object becomes part of the job object. the job name should match the name used in `killjob`. the jobs should be create with jobobjectlimitkillonjobclose and allow the caller to decide how to handle the returned handle.     `killjob` function assumes the process identified by `pid` is associated with a job object whose name is derive from it. every process started by the process which is part of the job object becomes part of the job object. destroying the task will close all such processes.,5,train
MESOS-3640,Implement stout/os/windows/ls.hpp,nan,3,train
MESOS-3641,Implement stout/os/windows/read.hpp and write.hpp,nan,2,train
MESOS-3645,Implement stout/os/windows/stat.hpp,nan,8,train
MESOS-3683,Port slave/containerizer/isolator.hpp to Windows,nan,3,train
MESOS-3688,Get Container Name information when launching a container task,"we want to get the docker name (or docker id, or both) when launching a container task with mesos. the container name is generated by mesos itself (i.e. mesos77e5fde683e74618a2dd d5b10f2b4d25, obtained with ""docker ps"") and it would be nice to expose this information to frameworks so that this information can be used, for example by marathon to give this information to users via a rest api.   to go a bit in depth with our use case, we have files created by fluentd logdriver that are named with docker name or docker id (full or short) and we need a mapping for the users of the rest api and thus the first step is to make this information available from mesos. ",3,train
MESOS-3692,Clarify error message 'could not chown work directory',when deploying a framework i encountered the error message 'could not chown work directory'.    it took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the docker container and the agent was running as root.    i suggest to clarify this message by pointing out to either set switch user  to false or to run the framework as the same user as the agent.,1,train
MESOS-3694,Enable building mesos.apache.org locally in a Docker container.,we should make it easy for everyone to modify the website and be able to generate it locally before pushing to upstream. ,3,train
MESOS-3698,JSON parsing allows non-whitespace trailing characters,"picojson supports a streaming mode in which a stream containing a series of json values can be repeatedly parsed. for this reason, it does not return an error when passed a string containing a valid json value followed by nonwhitespace trailing characters.    however, in addition to the fourargument picojson::parse() that we're using, picojson contains a twoargument parse() function (https:/github.com/kazuho/picojson/blob/master/picojson.h#l938l942) which accepts a std::string and should probably validate its input to ensure it doesn't contain trailing characters. a pull request has been filed for this change at https:/github.com/kazuho/picojson/pull/70 and if it's merged, we can switch to the two argument function call. in the meantime, we should provide such input validation ourselves in json::parse().",1,train
MESOS-3700,Deprecate resource_monitoring_interval flag,this parameter should be deprecated after 0.23.0 release as it has no use now. ,1,train
MESOS-3704,Allow easier detection when hook signature changes,"currently, if the signature of a hook function changes, we don't get any compile time errors if the hook implementation is not updated. this results in a hook that is never called.",2,train
MESOS-3705,HTTP Pipelining doesn't keep order of requests,"https:/en.wikipedia.org/wiki/httppipelining describes a mechanism by which multiple http request can be performed over a single socket. the requirement here is that responses should be send in the same order as requests are being made.    libprocess has some mechanisms built in to deal with pipelining when multiple http requests are made, it is still, however, possible to create a situation in which responses are scrambled respected to the requests arrival.    consider the situation in which there are two libprocess processes, processa and processb, each running in a different thread, thread2 and thread3 respectively. the https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l374 runs in thread1.    processa is of type processa which looks roughly as follows:      class processa : public processbase/        future/ foo(const http::request&)     protected:    virtual void initialize()   }      processb is from type processb which is just like processa but routes ""bar"" instead of ""foo"".    the situation in which the bug arises is the following:    # two requests, one for ""http:/serveruri/(1)/foo"" and one for ""http:/serveruri/(2)/bar"" are made over the same socket.  # the first request arrives to https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l2202 which is still running in thread1. this one creates an httpevent and delivers to the handler, in this case processa.  # https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l2361 enqueues the http event in to the processa queue. this happens in thread1.  # the second request arrives to https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l2202 which is still running in thread1. another httpevent is created and delivered to the handler, in this case processb.  # https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l2361 enqueues the http event in to the processb queue. this happens in thread1.  # thread2 is blocked, so processa cannot handle the first request, it is stuck in the queue.  # thread3 is idle, so it picks up the request to processb immediately.  # https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l3073 is called in thread3, this one in turn https:/github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#l3106 the response's future to the httpproxy associated with the socket where the request came.    at the last point, the bug is evident, the request to processb will be send before the request to processa even if the handler takes a long time and the processa::bar() actually finishes before. the responses are not send in the order the requests are done.    reproducer    the following is a test which successfully reproduces the issue:      #include /(&get2),                      return(http::ok(""2""))));      expectcall(server2.process, get())      .willonce(doall(futurearg/(&get3),                      return(http::ok(""3""))));        auto url1 = http::url(        ""http"",        server1.process>self().address.ip,        server1.process>self().address.port,        server1.process>self().id  ""/get"");      auto url2 = http::url(        ""http"",        server1.process>self().address.ip,        server1.process>self().address.port,        server2.process>self().id  ""/get"");      / create a connection to the server for http pipelining.    future/ connect = http::connect(url1);      awaitready(connect);      http::connection connection = connect.get();      http::request request1;    request1.method = ""get"";    request1.url = url1;    request1.keepalive = true;    request1.body = ""1"";    future/ response1 = connection.send(request1);      http::request request2 = request1;    request2.body = ""2"";    future/ response2 = connection.send(request2);      http::request request3;    request3.method = ""get"";    request3.url = url2;    request3.keepalive = true;    request3.body = ""3"";    future/ response3 = connection.send(request3);      / verify that request1 arrived at server1 and it is the right request.    / now server1 is blocked processing request1 and cannot pick up more events    / in the queue.    awaitready(get1);    expecteq(request1.body, get1>body);      / verify that request3 arrived at server2 and it is the right request.    awaitready(get3);    expecteq(request3.body, get3>body);      / request2 hasn't been picked up since server1 is still blocked serving    / request1.    expecttrue(get2.ispending());      / free server1 so it can serve request2.    latch.trigger();      / verify that request2 arrived at server1 and it is the right request.    awaitready(get2);    expecteq(request2.body, get2>body);      / wait for all responses.    awaitready(response1);    awaitready(response2);    awaitready(response3);      / if pipelining works as expected, even though server2 finished processing    / its request before server1 even began with request2, the responses should    / arrive in the order they were made.    expecteq(request1.body, response1>body);    expecteq(request2.body, response2>body);    expecteq(request3.body, response3>body);      awaitready(connection.disconnect());    awaitready(connection.disconnected());  }  ",3,train
MESOS-3716,Update Allocator interface to support quota,"an allocator should be notified when a quota is being set/updated or removed. also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",3,train
MESOS-3717,Master recovery in presence of quota,quota complicates master failover in several ways. the new master should determine if it is possible to satisfy the total quota and notify an operator in case it's not (imagine simultaneous failovers of multiple agents). the new master should hint the allocator how many agents might reconnect in the future to help it decide how to satisfy quota before the majority of agents reconnect.,5,train
MESOS-3718,Implement Quota support in allocator,"the builtin hierarchical drf allocator should support quota. this includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to nonquota'ed roles in presence of master failover.    a https:/issues.apache.org/jira/browse/mesos 2937 provides an overview of a feature set required to be implemented.",5,train
MESOS-3720,Tests for Quota support in master,allocator agnostic tests for quota support in the master. they can be divided into several groups:   heuristic check;   master failover;    functionality and quota guarantees.,5,train
MESOS-3722,Prototype quota request authentication,quota requests need to be authenticated.    this ticket will authenticate quota requests using credentials provided by the `authorization` field of the http request. this is similar to how authentication is implemented in `master::http`.,5,train
MESOS-3723,Prototype quota request authorization,when quotas are requested they should authorize their roles.    this ticket will authorize quota requests with acls. the existing authorization support that has been implemented in mesos 1342 will be extended to add a `request_quotas` acl.,5,train
MESOS-3732,Speed up FaultToleranceTest.FrameworkReregister test,faulttolerancetest.frameworkreregister test takes more than one second to complete:    [ run      ] faulttolerancetest.frameworkreregister  [       ok ] faulttolerancetest.frameworkreregister (1056 ms)      there must be a 1s timeout somewhere which we should mitigate via clock::advance().,1,train
MESOS-3734,Incorrect sed syntax for Mac OSX,"the build currently fails on osx:      ../3rdparty/libprocess/3rdparty/protobuf2.5.0/src/protoc i../../mesos/include/mesos/containerizer  \    i../../mesos/include i../../mesos/src      \    pythonout=python/interface/src/mesos/interface ../../mesos/include/mesos/containerizer/containerizer.proto  ../../mesos/installsh c d python/interface/src/mesos/v1/interface  sed i 's/mesos\.mesospb2/mesospb2/' python/interface/src/mesos/interface/containerizerpb2.py  sed: 1: ""python/interface/src/me ..."": extra characters at the end of p command  make[1]:   [python/interface/src/mesos/interface/containerizer_pb2.py] error 1      this is because the sed command uses the wrong syntax for osx: you need sed  i """" to instruct sed to not use a backup file.",2,train
MESOS-3736,Support docker local store pull same image simultaneously ,"the current local store implements get() using the local puller. for all requests of pulling same docker image at the same time, the local puller just untar the image tarball as many times as those requests are, and cp all of them to the same directory, which wastes time and bear high demand of computation. we should be able to support the local store/puller only do these for the first time, and the simultaneous pulling request should wait for the promised future and get it once the first pulling finishes. ",3,train
MESOS-3739,Mesos does not set Content-Type for 400 Bad Request,"while integrating with the http scheduler api i encountered the following scenario.    the message below was serialized to protobuf and sent as the post body    call ,      taskid:     }  }      post /api/v1/scheduler http/1.1  contenttype: application/xprotobuf  accept: application/xprotobuf  contentlength: 73  host: localhost:5050  useragent: rxnetty client      i received the following response    http/1.1 400 bad request  date: wed, 14 oct 2015 23:21:36 gmt  contentlength: 74    failed to validate scheduler::call: expecting 'framework_id' to be present      even though my accept header made no mention of text/plain the message body returned to me is text/plain. additionally, there is no content type header set on the response so i can't even do anything intelligently in my response handler.",2,train
MESOS-3740,LIBPROCESS_IP not passed to Docker containers,"docker containers aren't currently passed all the same environment variables that mesos containerizer tasks are. see: https:/github.com/apache/mesos/blob/master/src/slave/containerizer/containerizer.cpp#l254 for all the environment variables explicitly set for mesos containers.    while some of them don't necessarily make sense for docker containers, when the docker has inside of it a libprocess process (a mesos framework scheduler) and is using net=host the task needs to have libprocess_ip set otherwise the same sort of problems that happen because of mesos 3553 can happen (libprocess will try to guess the machine's ip address with likely bad results in a number of operating environment).",3,train
MESOS-3743,Provide diagnostic output in agent log when fetching fails,"when fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. it may even be impossible to get to if one only has the agent log available and no more access to the sandbox. this is for instance the case when looking at output from a ci run.    the fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. when it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.    this is similar to this patch: https:/reviews.apache.org/r/37813/    the difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",2,train
MESOS-3748,HTTP scheduler library does not gracefully parse invalid resource identifiers,"if you pass a nonsense string for ""master"" into a framework using the c http scheduler library, the framework segfaults.    for example, using the example frameworks:      build/src/testframework master=""asdf:/127.0.0.1:5050""    results in:    failed to create a master detector for 'asdf:/127.0.0.1:5050': failed to parse 'asdf:/127.0.0.1:5050'        export defaultprincipal=root  build/src/eventcallframework master=""asdf:/127.0.0.1:5050""    results in    i1015 16:18:45.432075 2062201600 scheduler.cpp:157] version: 0.26.0  segmentation fault: 11         thread #2: tid = 0x28b6bb, 0x0000000100ad03ca libmesos0.26.0.dylib`mesos::v1::scheduler::mesosprocess::initialize(this=0x00000001076031a0)  42 at scheduler.cpp:213, stop reason = excbadaccess (code=1, address=0x0)     frame #0: 0x0000000100ad03ca libmesos0.26.0.dylib`mesos::v1::scheduler::mesosprocess::initialize(this=0x00000001076031a0)  42 at scheduler.cpp:213      frame #1: 0x0000000100ad05f2 libmesos0.26.0.dylib`virtual thunk to mesos::v1::scheduler::mesosprocess::initialize(this=0x00000001076031a0)  34 at scheduler.cpp:210      frame #2: 0x00000001022b60f3 libmesos0.26.0.dylib`::resume()  931 at process.cpp:2449      frame #3: 0x00000001022c131c libmesos0.26.0.dylib`::operator()()  268 at process.cpp:2174      frame #4: 0x00000001022c0fa2 libmesos0.26.0.dylib`::threadproxy/ > > > >() [inlined] invoke/ &>  27 at functionalbase:415      frame #5: 0x00000001022c0f87 libmesos0.26.0.dylib`::threadproxy/ > > > >() [inlined] applyfunctor/ > >, 0, std::1::tuple/ >  55 at functional:2060      frame #6: 0x00000001022c0f50 libmesos0.26.0.dylib`::threadproxy/ > > > >() [inlined] operator()/  41 at functional:2123      frame #7: 0x00000001022c0f27 libmesos0.26.0.dylib`::threadproxy/ > > > >() [inlined] invoke/ > >>  14 at functionalbase:415      frame #8: 0x00000001022c0f19 libmesos0.26.0.dylib`::threadproxy/ > > > >() [inlined] threadexecute/ > >>  25 at thread:337      frame #9: 0x00000001022c0f00 libmesos0.26.0.dylib`::threadproxy/ > > > >()  368 at thread:347      frame #10: 0x00007fff964c705a libsystempthread.dylib`pthreadbody  131      frame #11: 0x00007fff964c6fd7 libsystempthread.dylib`pthreadstart  176      frame #12: 0x00007fff964c43ed libsystempthread.dylib`threadstart  13  ",1,train
MESOS-3749,Configuration docs are missing --enable-libevent and --enable-ssl,"the enablelibevent and enablessl config flags are currently not documented in the ""configuration"" docs with the rest of the flags. they should be added.",1,train
MESOS-3751,MESOS_NATIVE_JAVA_LIBRARY not set on MesosContainerize tasks with --executor_environmnent_variables,"when using executorenvironmentvariables, and having mesosnativejavalibrary in the environment of mesosslave, the mesos containerizer does not set mesosnativejavalibrary itself.    relevant code: https:/github.com/apache/mesos/blob/14f7967ef307f3d98e3a4b93d92d6b3a56399b20/src/slave/containerizer/containerizer.cpp#l281    it sees that the variable is in the mesosslave's environment (os::getenv), rather than checking if it is set in the environment variable set.",2,train
MESOS-3752,CentOS 6 dependency install fails at Maven,it seems the apache maven dependencies have changed such that following the getting started docs for centos 6.6 will fail at maven installation:      > package apachemaven.noarch 0:3.3.32.el6 will be installed  > processing dependency: javadevel >= 1:1.7.0 for package: apachemaven3.3.32.el6.noarch  > finished dependency resolution  error: package: apachemaven3.3.32.el6.noarch (epelapachemaven)             requires: javadevel >= 1:1.7.0             available: java1.5.0gcjdevel1.5.0.029.1.el6.x8664 (base)                 javadevel = 1.5.0             available: 1:java1.6.0openjdkdevel1.6.0.351.13.7.1.el66.x8664 (base)                 javadevel = 1:1.6.0             available: 1:java1.6.0openjdkdevel1.6.0.361.13.8.1.el67.x86_64 (updates)                 javadevel = 1:1.6.0   you could try using skipbroken to work around the problem   you could try running: rpm va nofiles  nodigest  ,1,train
MESOS-3753,Test the HTTP Scheduler library with SSL enabled,"currently, the http scheduler library does not support sslenabled mesos.    (you can manually test this by spinning up an sslenabled master and attempt to run the eventcall framework example against it.)    we need to add tests that check the http scheduler library against sslenabled mesos:   with downgrade support,   with required framework/clientside certifications,   with/without verification of certificates (masterside),   with/without verification of certificates (frameworkside),    with a custom certificate authority (ca)    these options should be controlled by the same environment variables found on the http:/mesos.apache.org/documentation/latest/ssl/.    note: this issue will be broken down into smaller subissues as bugs/problems are discovered.",13,train
MESOS-3756,Generalized HTTP Authentication Modules,libprocess is going to factor out an authentication interface: mesos 3231    here we propose that mesos can provide implementations for this interface as mesos modules.,13,train
MESOS-3758,0.26.0 Release,"manage the release of apache mesos version 0.26.0.     the mesos 0.26.0 release will aim at being timely and at improving robustness. it will not be gated by new features. however, there may be blockers when it comes to bugs or incompleteness of existing features.    once these blockers are resolved, we will start deferring unresolved issues by priority and status until we are ready to make the first cut.    here is how you can stay informed and help out.    users   note the ""is blocked by"" links in this ticket for major targeted features.   check out the 0.26.0 https:/issues.apache.org/jira/secure/dashboard.jspa?selectpageid=12327111 for status indicators.   see the inprogress https:/issues.apache.org/jira/secure/releasenote.jspa?version=12333528&stylename=html&projectid=12311242&create=create&atl_token=a5kq2qavt4jafded%7c675829c365428965ebec16702c62d3637db57d84%7clin to see what's committed so far.   add comments to issues describing your problems or use cases.    issue reporters   set target version to 0.26.0, if appropriate.   set priority for fixing in 0.26.0.   ask around on irc or dev@ for a shepherd!    developers   newbies: check out [accepted, unassigned, 'newbie'] issues.   looking for something meatier to work on? [accepted, unassigned for 0.26]   for shepherdless issues, find a shepherd before diving too deep!!   update target version and priority, as needed.   discuss your intended design on the jira, perhaps sharing a design doc.   update the status to ""in progress"" and ""reviewable"" as you go.   assign yourself if you are working on it. unassign yourself in case you stop before finishing.    committers   accept and shepherd all relevant [shepherdless issues].   update target version and priority, as needed.   add 'newbie' label to any easy ones.    important jira fields   target version: set to 0.26.0 if you want the issue to be addressed in 0.26.   priority: indicates how important it is for the issue to be fixed in the next release (0.26.0 in this case). if you want to update a priority, please add a comment explaining your reason, and only change the priority up/down one level.   blockedby links: major features and critical tickets can be linked as blockers to this ticket to give a highlevel overview of what we plan to land in 0.26. noncritical issues should just set the ""target version"".  ",5,train
MESOS-3759,Document messages.proto,the messages we pass between mesos components are largely undocumented.  see this https:/github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#l23.,3,train
MESOS-3762,Refactor SSLTest fixture such that MesosTest can use the same helpers.,"in order to write tests that exercise ssl with other components of mesos, such as the http scheduler library, we need to use the setup/teardown logic found in the ssltest fixture.    currently, the test fixtures have separate inheritance structures like this:    ssltest < ::testing::test  mesostest < temporarydirectorytest < ::testing::test    where ::testing::test is a gtest class.    the plan is the following:  # change ssltest to inherit from temporarydirectorytest.  this will require moving the setup (generation of keys and certs) from setuptestcase to setup.  at the same time, some of the cleanup logic in the ssltest will not be needed.  # move the logic of generating keys/certs into helpers, so that individual tests can call them when needed, much like mesostest.  # write a child class of ssltest which has the same functionality as the existing ssltest, for use by the existing tests that rely on ssltest or the registryclienttest.  # have mesostest inherit from ssltest (which might be renamed during the refactor).  if mesos is not compiled with enablessl, then ssltest could be #ifdef'd into any empty class.    the resulting structure should be like:    mesostest < ssltest < temporarydirectorytest <  ::testing::test  childofssltest /  ",3,train
MESOS-3763,Need for http::put request method,as we decided to create a more restful api for managing quota request.  therefore we also want to use the http put request and hence need to enable the libprocess/http to send put request besides get and post requests.,1,train
MESOS-3771,Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling,"spark encodes some binary data into the executorinfo.data field.  this field is sent as a ""bytes"" protobuf value, which can have arbitrary nonutf8 data.    if you have such a field, it seems that it is splatted out into json without any regards to proper character encoding:      0006b0b0  2e 73 70 61 72 6b 2e 65  78 65 63 75 74 6f 72 2e    0006b0c0  4d 65 73 6f 73 45 78 65  63 75 74 6f 72 42 61 63    0006b0d0  6b 65 6e 64 22 7d 2c 22  64 61 74 61 22 3a 22 ac    0006b0e0  ed 5c 75 30 30 30 30 5c  75 30 30 30 35 75 72 5c    0006b0f0  75 30 30 30 30 5c 75 30  30 30 66 5b 4c 73 63 61  la.tuple2;..\u00|      i suspect this is because the http api emits the executorinfo.data directly:      json::object model(const executorinfo& executorinfo)        i think this may be because the custom json processing library in stout seems to not have any idea of what a byte array is.  i'm guessing that some implicit conversion makes it get written as a string instead, but:      inline std::ostream& operator<<(std::ostream& out, const string& string)        thank you for any assistance here.  our cluster is currently entirely down  the frameworks cannot handle parsing the invalid json produced (it is not even valid utf8)  ",2,train
MESOS-3772,Consistency of quoted strings in error messages,"example log output:      i1020 18:56:02.933956  1790 slave.cpp:1270] got assigned task 13 for framework 496620b943684a71b74168216f3d909f0000  i1020 18:56:02.934185  1790 slave.cpp:1386] launching task 13 for framework 496620b943684a71b74168216f3d909f0000  i1020 18:56:02.934408  1790 slave.cpp:1618] queuing task '13' for executor default of framework '496620b943684a71b74168216f3d909f0000  i1020 18:56:02.935417  1790 slave.cpp:1760] sending queued task '13' to executor 'default' of framework 496620b943684a71b74168216f3d909f0000      aside from the typo (unmatched quote) in the third line, these log messages using quoting inconsistently: sometimes task, executor, and framework ids are quoted, other times they are not.    we should probably adopt a general rule, a la http:/ . my proposal: when interpolating a variable, only use quotes if it is possible that the value might contain whitespace or punctuation (in the latter case, the punctuation should probably be escaped).",3,train
MESOS-3773,RegistryClientTest.SimpleGetBlob is flaky,"registryclienttest.simplegetblob fails about 1/5 times.  this was encountered on osx.      bin/mesostests.sh gtestfilter=""registryclienttest.simplegetblob"" gtestrepeat=10 gtestbreakonfailure        [ run      ] registryclienttest.simplegetblob  ../../src/tests/containerizer/provisionerdockertests.cpp:946: failure  value of: blobresponse    actual: ""20151020 20:58:59.57939302400:00""  expected: blob.get()  which is: ""\x15\x3\x3\00(p~\xca&\xc6<\x4\x16\xe\xb2\xff\b1a\xb9z      [ run      ] registryclienttest.simplegetblob  ../../src/tests/containerizer/provisionerdockertests.cpp:926: failure  (socket).failure(): failed accept: connection error: error:00000000:lib(0):func(0):reason(0)  ",4,train
MESOS-3775,MasterAllocatorTest.SlaveLost is slow.,"the masterallocatortest.slavelost takes more that 5s to complete. a brief look into the code hints that the stopped agent does not quit immediately (and hence its resources are not released by the allocator) because https:/github.com/apache/mesos/blob/master/src/tests/masterallocatortests.cpp#l717. 5s timeout comes from executorshutdowngraceperiod agent constant.    possible solutions:   do not wait until the stopped agent quits (can be flaky, needs deeper analysis).   decrease the agent's executorshutdowngraceperiod flag.    terminate the executor faster (this may require some refactoring since the executor driver is created in the testcontainerizer and we do not have direct access to it.  ",1,train
MESOS-3781,Replace Master/Slave Terminology Phase I - Rename flag names and deprecate old ones,nan,3,train
MESOS-3785,Use URI content modification time to trigger fetcher cache updates.,"instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. http and hdfs.    proposal: instead of just fetching the content size, we fetch both size  and mtime together. as before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox.     assuming a size is given, we compare the mtime from the fetch uri with the mtime known to the cache. if it differs, we update the cache. (as a defensive measure, a difference in size should also trigger an update.)     not having an mtime available at the fetch uri is simply treated as a unique valid mtime value that differs from all others. this means that when initially there is no mtime, cache content remains valid until there is one. thereafter,  anew lack of an mtime invalidates the cache once. in other words: any change from no mtime to having one or back is the same as encountering a new mtime.    note that this scheme does not require any new protobuf fields.  ",5,train
MESOS-3786,Backticks are not mentioned in Mesos C++ Style Guide,"as far as i can tell, current practice is to quote code excerpts and object names with backticks when writing comments. for example:      / you know, `sadpanda` seems extra sad lately.  std::string sadpanda;  sadpanda = ""   :'(   "";      however, i don't see this documented in our c style guide at all. it should be added.",1,train
MESOS-3793,Cannot start mesos local on a Debian GNU/Linux 8 docker machine,"we updated the mesos version to 0.25.0 in our marathon docker image, that runs our integration tests.  we use mesos local for those tests. this fails with this message:      root@a06e4b4eb776:/marathon# mesos local  i1022 18:42:26.852485   136 leveldb.cpp:176] opened db in 6.103258ms  i1022 18:42:26.853302   136 leveldb.cpp:183] compacted db in 765740ns  i1022 18:42:26.853343   136 leveldb.cpp:198] created db iterator in 9001ns  i1022 18:42:26.853355   136 leveldb.cpp:204] seeked to beginning of db in 1287ns  i1022 18:42:26.853366   136 leveldb.cpp:273] iterated through 0 keys in the db in 1111ns  i1022 18:42:26.853406   136 replica.cpp:744] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1022 18:42:26.853775   141 recover.cpp:449] starting replica recovery  i1022 18:42:26.853862   141 recover.cpp:475] replica is in empty status  i1022 18:42:26.854751   138 replica.cpp:641] replica in empty status received a broadcasted recover request  i1022 18:42:26.854856   140 recover.cpp:195] received a recover response from a replica in empty status  i1022 18:42:26.855002   140 recover.cpp:566] updating replica status to starting  i1022 18:42:26.855655   138 master.cpp:376] master a3f398181bda4710b96b2a60ed4d12b8 (a06e4b4eb776) started on 172.17.0.14:5050  i1022 18:42:26.855680   138 master.cpp:378] flags at startup: allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticateslaves=""false"" authenticators=""crammd5"" authorizers=""local"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""5secs"" registrystrict=""false"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/share/mesos/webui"" workdir=""/tmp/mesos/local/ak0xpg"" zksessiontimeout=""10secs""  i1022 18:42:26.855790   138 master.cpp:425] master allowing unauthenticated frameworks to register  i1022 18:42:26.855803   138 master.cpp:430] master allowing unauthenticated slaves to register  i1022 18:42:26.855815   138 master.cpp:467] using default 'crammd5' authenticator  w1022 18:42:26.855829   138 authenticator.cpp:505] no credentials provided, authentication requests will be refused  i1022 18:42:26.855840   138 authenticator.cpp:512] initializing server sasl  i1022 18:42:26.856442   136 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i1022 18:42:26.856943   140 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 1.888185ms  i1022 18:42:26.856987   140 replica.cpp:323] persisted replica status to starting  i1022 18:42:26.857115   140 recover.cpp:475] replica is in starting status  i1022 18:42:26.857270   140 replica.cpp:641] replica in starting status received a broadcasted recover request  i1022 18:42:26.857312   140 recover.cpp:195] received a recover response from a replica in starting status  i1022 18:42:26.857368   140 recover.cpp:566] updating replica status to voting  i1022 18:42:26.857781   140 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 371121ns  i1022 18:42:26.857841   140 replica.cpp:323] persisted replica status to voting  i1022 18:42:26.857895   140 recover.cpp:580] successfully joined the paxos group  i1022 18:42:26.857928   140 recover.cpp:464] recover process terminated  i1022 18:42:26.862455   137 master.cpp:1603] the newly elected leader is master@172.17.0.14:5050 with id a3f398181bda4710b96b2a60ed4d12b8  i1022 18:42:26.862498   137 master.cpp:1616] elected as the leading master!  i1022 18:42:26.862511   137 master.cpp:1376] recovering from registrar  i1022 18:42:26.862560   137 registrar.cpp:309] recovering registrar  failed to create a containerizer: could not create mesoscontainerizer: failed to create launcher: failed to create linux launcher: failed to mount cgroups hierarchy at '/sys/fs/cgroup/freezer': 'freezer' is already attached to another hierarchy      the setup worked with mesos 0.24.0.  the dockerfile is here: https:/github.com/mesosphere/marathon/blob/mv/mesos0.25/dockerfile          root@a06e4b4eb776:/marathon# ls /sys/fs/cgroup/  root@a06e4b4eb776:/marathon#         root@a06e4b4eb776:/marathon# cat /proc/mounts   none / aufs rw,relatime,si=6e7ac87f36042e03,dio,dirperm1 0 0  proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0  tmpfs /dev tmpfs rw,nosuid,mode=755 0 0  devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0  shm /dev/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0  mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0  sysfs /sys sysfs ro,nosuid,nodev,noexec,relatime 0 0  /dev/sda1 /etc/resolv.conf ext4 rw,relatime,data=ordered 0 0  /dev/sda1 /etc/hostname ext4 rw,relatime,data=ordered 0 0  /dev/sda1 /etc/hosts ext4 rw,relatime,data=ordered 0 0  devpts /dev/console devpts rw,relatime,mode=600,ptmxmode=000 0 0  proc /proc/bus proc ro,nosuid,nodev,noexec,relatime 0 0  proc /proc/fs proc ro,nosuid,nodev,noexec,relatime 0 0  proc /proc/irq proc ro,nosuid,nodev,noexec,relatime 0 0  proc /proc/sys proc ro,nosuid,nodev,noexec,relatime 0 0  proc /proc/sysrqtrigger proc ro,nosuid,nodev,noexec,relatime 0 0  tmpfs /proc/kcore tmpfs rw,nosuid,mode=755 0 0  tmpfs /proc/timer_stats tmpfs rw,nosuid,mode=755 0 0       can you please assign to the correct person?",3,train
MESOS-3794,Master should not store arbitrarily sized data in ExecutorInfo,"from a comment in https:/github.com/apache/mesos/blob/master/src/master/master.hpp#l262l271, which means master would be at high risk of ooming if a bunch of executors were started with big data blobs.   master should scrub out unneeded bloat from executorinfo before storing it.   we can use an alternate internal object, like we do for taskinfo vs task; see https:/github.com/apache/mesos/blob/master/src/messages/messages.proto#l39l41.",3,train
MESOS-3800,Containerizer attempts to create Linux launcher by default ,mesos containerizer attempts to create a linux launcher by default without verifying whether the necessary prerequisites (such as availability of cgroups) are met.,3,train
MESOS-3814,Add checks to make sure isolators and the launcher are compatible.,"there's a recent change regarding the picking of which launcher (linux or posix) to use  https:/reviews.apache.org/r/39604    in our environment, cgroups are not automounted after reboot. we rely on mesos itself to mount all relevant cgroups hierachies.    after the reboot, the above patch detects that 'freezer' hierarchy is not mounted, therefore, decided to use the posix launcher (if  launcher is not specified explictly).    port mapping isolator requires network namespace to be created for each container (thus requires linux launcher). but we don't have a check to verify that launcher and isolators are compatible.    slave thus starts fine and task failed with weird error like:    collect failed: failed to create the ingress qdisc on mesos61099: link 'mesos61099' is not found.      it does take us quite a few time to figure out the root cause.",2,train
MESOS-3819,"Add documentation explaining ""roles""","docs currently talk about resources, static/dynamic reservations, but don't explain what a ""role"" concept is to begin with.",2,train
MESOS-3820,Test-only libprocess reinitialization,"background  libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests.  some properties of the server socket are configured via environment variables, such as the ip and port or the ssl configuration.    in the case of tests, libprocess is initialized once per test binary.  this means that testing different configurations (ssl in particular) is cumbersome as a separate process would be needed for every test case.    proposal  # add some optional code between some tests like:    / cleanup all of libprocess's state, as if we're starting anew.  process::finalize();     / for tests that need to test ssl connections with the master:  openssl::reinitialize();    process::initialize();    see [mesos 3863] for more on process::finalize.",3,train
MESOS-3825,Enable mesos-reviewbot project on jenkins to use SSL,currently mesosreviewbot project does  not support parameterized configuration. this limits the project from building using enablessl (and others) configuration arguments for building mesos.   ,3,train
MESOS-3831,Document operator HTTP endpoints,"these are not exhaustively documented; they probably should be.    some endpoints have docs: e.g., /reserve and /unreserve are described in the reservation doc page. but it would be good to have a single page that lists all the endpoints and their semantics.",3,train
MESOS-3833,/help endpoints do not work for nested paths,"mesos displays the list of all supported endpoints starting at a given path prefix using the /help suffix, e.g. master:5050/help.    it seems that the help functionality is broken for url's having nested paths e.g. master:5050/help/master/machine/down. the response returned is:    malformed url, expecting '/help/id/name/'  ",2,train
MESOS-3837,Rootfs in provisioner test doesn't handle symlink directories properly,"currently rootfs doesn't fully copy the directory structure over, and also doesn't create the symlinks in the new rootfs and will cause shell and other binaries that rely on the symlinks to no longer function.",4,train
MESOS-3839,Update documentation for FetcherCache mtime-related changes,nan,1,train
MESOS-3847,Root tests for LinuxFilesystemIsolatorTest are broken,"the refactor in [mesos3762] ended up exposing some differences in the temporarydirectorytest classes (one in stout, one in mesosproper).    the tests that broke (during tear down):    linuxfilesystemisolatortest.rootpersistentvolumewithrootfilesystem  linuxfilesystemisolatortest.rootpersistentvolumewithoutrootfilesystem  linuxfilesystemisolatortest.root_multiplecontainers      as per an offline discussion between [jvanremoortere] and [jieyu], the solution is to merge the two temporarydirectorytest classes and to fix the tear down of linuxfilesystemisolatortest.",2,train
MESOS-3848,Refactor Environment::mkdtemp into TemporaryDirectoryTest.,"as part of https:/github.com/apache/mesos/blob/master/src/tests/environment.cpp#l494.  we would like the naming, which is valuable for debugging, to be available for a majority of tests.  (a majority of tests inherit from temporarydirectorytest in some way.)    note:   any additional directories created via environment>mkdtemp are cleaned up after the test.   we don't want mesos specific logic in stout, like the umount shell command in environment::teardown.    proposed change:  move the temporary directory logic from environment::mkdtemp to temporarydirectorytest.    tests that need to change   logzookeepertest    mesostest::createslaveflags    testscript      ",3,train
MESOS-3849,Corrected style in Makefiles,order of files in makefiles is not strictly alphabetic,1,train
MESOS-3851,Investigate recent crashes in Command Executor,"post https:/reviews.apache.org/r/38900 i.e. updating commandexecutor to support rootfs. there seem to be some tests showing frequent crashes due to assert violations.    fetchercachetest.simpleeviction failed due to the following log:      i1107 19:36:46.360908 30657 slave.cpp:1793] sending queued task '3' to executor ''3' of framework 7d94c7fb89504bcf80c146112292dcd60000 at executor(1)@172.17.5.200:33871'  i1107 19:36:46.363682  1236 exec.cpp:297]     i1107 19:36:46.373569  1245 exec.cpp:210] executor registered on slave 7d94c7fb89504bcf80c146112292dcd6s0      @     0x7f9f5a7db3fa  google::logmessage::fail()  i1107 19:36:46.394081  1245 exec.cpp:222] executor::registered took 395411ns      @     0x7f9f5a7db359  google::logmessage::sendtolog()      @     0x7f9f5a7dad6a  google::logmessage::flush()      @     0x7f9f5a7dda9e  google::logmessagefatal::logmessagefatal()      @           0x48d00a  checkfatal::checkfatal()      @           0x49c99d  mesos::internal::commandexecutorprocess::launchtask()      @           0x4b3dd7  zzn7process8dispatchin5mesos8internal22commandexecutorprocessepns114executordrivererkns18taskinfoes5s6eevrkns3piditeemsafvt0t1et2t3enkulpns11processbaseeeclesl      @           0x4c470c  znst17functionhandlerifvpn7process11processbaseeezns08dispatchin5mesos8internal22commandexecutorprocessepns514executordrivererkns58taskinfoes9saeevrkns03piditeemsefvt0t1et2t3euls2ee9minvokeerkst9anydatas2      @     0x7f9f5a761b1b  std::function/::operator()()      @     0x7f9f5a749935  process::processbase::visit()      @     0x7f9f5a74d700  process::dispatchevent::visit()      @           0x48e004  process::processbase::serve()      @     0x7f9f5a745d21  process::processmanager::resume()      @     0x7f9f5a742f52  zzn7process14processmanager12initthreadsevenkulrkst11atomicboolecles3      @     0x7f9f5a74cf2c  znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eee6callivieilm0eeeetost5tupleiidpt0eest12indextupleiixspt1eee      @     0x7f9f5a74cedc  znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eeecliieveet0dpot      @     0x7f9f5a74ce6e  znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeevee9minvokeiieeevst12indextupleiixspteee      @     0x7f9f5a74cdc5  znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeeveeclev      @     0x7f9f5a74cd5e  znst6thread5implist12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis6eeeveee6mrunev      @     0x7f9f5624f1e0  (unknown)      @     0x7f9f564a8df5  startthread      @     0x7f9f559b71ad  clone  i1107 19:36:46.551370 30656 containerizer.cpp:1257] executor for container '6553a6176b4a418d97595681f45ff854' has exited  i1107 19:36:46.551429 30656 containerizer.cpp:1074] destroying container '6553a6176b4a418d97595681f45ff854'  i1107 19:36:46.553869 30656 containerizer.cpp:1257] executor for container 'd2c1f924c92a453e82b1c294d09c4873' has exited      the reason seems to be a race between the executor receiving a runtaskmessage before executorregisteredmessage leading to the checksome(executorinfo) failure.    link to complete log: https:/issues.apache.org/jira/browse/mesos2831?focusedcommentid=14995535&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment14995535    another related failure from examplestest.persistentvolumeframework          @     0x7f4f71529cbd  google::logmessage::sendtolog()  i1107 13:15:09.949987 31573 slave.cpp:2337] status update manager successfully handled status update acknowledgement (uuid: 721c731655804636a83a098e3bd4ed1f) for task ad90531fd3d843f696f2c81c4548a12d of framework ac4ea54a7d194e419ee31a761f8e5b0f0000      @     0x7f4f715296ce  google::logmessage::flush()      @     0x7f4f7152c402  google::logmessagefatal::logmessagefatal()      @           0x48d00a  checkfatal::checkfatal()      @           0x49c99d  mesos::internal::commandexecutorprocess::launchtask()      @           0x4b3dd7  zzn7process8dispatchin5mesos8internal22commandexecutorprocessepns114executordrivererkns18taskinfoes5s6eevrkns3piditeemsafvt0t1et2t3enkulpns11processbaseeeclesl      @           0x4c470c  znst17functionhandlerifvpn7process11processbaseeezns08dispatchin5mesos8internal22commandexecutorprocessepns514executordrivererkns58taskinfoes9saeevrkns03piditeemsefvt0t1et2t3euls2ee9minvokeerkst9anydatas2      @     0x7f4f714b047f  std::function/::operator()()      @     0x7f4f71498299  process::processbase::visit()      @     0x7f4f7149c064  process::dispatchevent::visit()      @           0x48e004  process::processbase::serve()      @     0x7f4f71494685  process::processmanager::resume()      full logs at:  https:/builds.apache.org/job/mesos/1191/compiler=gcc,configuration=verbose,os=centos:7,labelexp=docker%7c%7chadoop/consolefull",2,train
MESOS-3854,Finalize design for generalized Authorizer interface,finalize the structure of acls and achieve consensus on the design doc proposed in mesos 2949.,2,train
MESOS-3856,Add mtime-related fetcher tests,nan,2,train
MESOS-3857,Draft Design Doc for first Step External Volume MVP,"as part of the overall design doc for global resources we would like to introduce improvements for docker volume driver isolator module (https:/github.com/emccode/mesosmoduledvdi).  currently the isolator module is controlled by setting environment variables as follows:  ""env"":   we should develop a more structured way for passing these settings to the isolator module which is in line with the overall goal of global resources.",3,train
MESOS-3858,Draft quota limits design document,in the design documents for quota (https:/docs.google.com/document/d/16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq 9i/edit#) the proposed mvp does not include quota limits. quota limits represent an upper bound of resources that a role is allowed to use. the task of this ticket is to outline a design document on how to implement quota limits when the quota mvp is implemented.,5,train
MESOS-3859,Add github support to apply-reviews.py.,nan,3,train
MESOS-3861,Authenticate quota requests,quota requests need to be authenticated.  this ticket will authenticate quota requests using credentials provided by the authorization field of the http request. this is similar to how authentication is implemented in master::http.,3,train
MESOS-3862,Authorize set quota requests.,when quotas are requested they should authorize their roles.  this ticket will authorize quota requests with acls. the existing authorization support that has been implemented in mesos 1342 will be extended to add a `request_quotas` acl.,5,train
MESOS-3863,Investigate the requirements of programmatically re-initializing libprocess,"this issue is for investigating what needs to be added/changed in process::finalize such that process::initialize will start on a clean slate.  additional issues will be created once done.  also see mesos3820.    process::finalize should cover the following components:   s (the server socket)   delete should be sufficient.  this closes the socket and thereby prevents any further interaction from it.   processmanager   related prior work: https:/github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#l963, which currently leaks a pointer).   cleans up any other spawn 'd process.   manages the eventloop.   clock   the goal here is to clear any timers so that nothing can deference processmanager while we're finalizing/finalized.  it's probably not important to execute any remaining timers, since we're ""shutting down"" libprocess.  this means:   the clock should be paused and settled before the clean up of processmanager.   processes, which might interact with the clock, should be cleaned up next.   a new clock::finalize method would then clear timers, processspecific clocks, and tick s; and then resume the clock.   address (the advertised ip and port)   needs to be cleared after processmanager has been cleaned up.  processes use this to communicate events.  if cleared prematurely, terminateevents will not be sent correctly, leading to infinite waits.   socketmanager   the idea here is to close all sockets and deallocate any existing httpproxy or encoder objects.   all sockets are created via s_, so cleaning up the server socket prior will prevent any new activity.   mime   this is effectively a static map.   it should be possible to statically initialize it.   synchronization atomics initialized & initializing.   once cleanup is done, these should be reset.    summary:   implement clock::finalize.  [mesos3882]   implement ~socketmanager.  [mesos3910]   make sure the metricsprocess and reaperprocess are reinitialized.  [mesos3934]   (optional) clean up mime.    wrap everything up in process::finalize.",2,train
MESOS-3864,Simplify and/or document the libprocess initialization synchronization logic,"tracks this https:/github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#l749.    the https:/github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff128d3b56fc8c9ec0176fdbadcfd11fc2 https:/github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff0eebc8689450916990abe080d86c2acb like process::once, which is used in almost all other one time initialization blocks.      the logic should be documented.  it can also be simplified (see the https:/reviews.apache.org/r/39949/).  or it can be replaced with process::once.",1,train
MESOS-3867,Make `Resource.DiskInfo.Persistence.principal` a required field,"a `principal` field is being added to the `resource.diskinfo.persistence` message to facilitate authorization of persistent volume creation/deletion. in the long run it should be a required field, but it's being initially introduced as optional to avoid breaking existing frameworks. the field should be changed to required at the end of a deprecation cycle.",1,train
MESOS-3868,Make apply-review.sh use apply-reviews.py,nan,1,train
MESOS-3873,Enhance allocator interface with the recovery() method,there are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. introduce a method into the allocator interface that allows for this.,3,train
MESOS-3874,Investigate recovery for the Hierarchical allocator,the built in hierarchical allocator should implement the recovery (in the presence of quota).,3,train
MESOS-3875,Account dynamic reservations towards quota,"dynamic reservationswhether allocated or notshould be accounted towards role's quota. this requires update in at least two places:   the built in allocator, which actually satisfies quota;   the sanity check in the master.",3,train
MESOS-3877,Draft operator documentation for quota,draft an operator guide for quota which describes basic usage of the endpoints and few basic and advanced usage cases.,5,train
MESOS-3879,Incorrect and inconsistent include order for <gmock/gmock.h> and <gtest/gtest.h>.,"we currently have an inconsistent (and mostly incorrect) include order for / and / (see below). some files include them (incorrectly)  between the c and cpp standard header, while other correclt include them afterwards. according to the  https:/google.github.io/styleguide/cppguide.html#namesandorderofincludes the second include order is correct.        #include /    #include /    #include /        #include /    #include /  ",1,train
MESOS-3880,Propose a guideline for log messages,"we are rather inconsistent in the way we write log messages. it would be helpful to come up with a style and document various aspects of logs, including but not limited to:   usage of backticks and/or single quotes to quote interpolated variables;   usage of backticks and/or single quotes to quote types and other names;   usage of tenses and other grammatical forms;   proper way of nesting [error] messages;",5,train
MESOS-3881,Implement `stout/os/pstree.hpp` on Windows,nan,2,train
MESOS-3882,Libprocess: Implement process::Clock::finalize,"tracks this https:/github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#l974l975.    the clock is initialized with a callback that, among other things, will dereference the global processmanager object.    when libprocess is shutting down, the processmanager is cleaned up.  between cleanup and termination of libprocess, there is some chance that a timer will time out and result in dereferencing processmanager.    proposal    implement clock::finalize.  this would clear:   existing timers   processspecific clocks   ticks   change process::finalize.  # resume the clock.  (the clock is only paused during some tests.)  when the clock is not paused, the callback does not dereference processmanager.  # clean up process_manager.  this terminates all the processes that would potentially interact with clock.   # call clock::finalize.",3,train
MESOS-3883,Add support to apply-reviews.py to update SVN when necessary. ,"  that said, this can be automated as a step in applyreviews script. for  example, the script can check if something in site/ (or docs/ ?) is being  committed and if yes, also do an svn update. @artem do you want to take  this on as you revamp the applyreviews script?    on tue, nov 10, 2015 at 1:23 am, adam bordelon / wrote:    > since it's still a manual process, the website is usually only updated a)  > when we have a new release to announce, or b) when some other blogworthy  > content arises (e.g. mesoscon).      https:/mailarchives.apache.org/modmbox/mesos dev/201511.mbox/%3ccaakwvazqjq9kmdpcaqf%2bh1bnnzbrrknqzxkwjwztrihuf66fg%40mail.gmail.com%3e",3,train
MESOS-3884,Corrected style in hierarchical allocator,"the built in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",1,train
MESOS-3887,Add a flag to master to enable optimistic offers. ,nan,3,train
MESOS-3888,Support distinguishing revocable resources in the Resource protobuf.,"add enum type into revocableinfo:      framework need to assign revocableinfo when launching task; if its not assign, use reserved resources. framework need to identify which resources its using   oversubscription resources need to assign the type by agent (mesos3930)    update oversubscription document that oo has oversubscribe the allocation slack and recommend qos to handle the usage slack only. (mesos3889)      message resource        optional type type = 1;    }   ...    optional revocableinfo revocable = 9;   }    ",2,train
MESOS-3889,Modify Oversubscription documentation to explicitly forbid the QoS Controller from killing executors running on optimistically offered resources.,nan,2,train
MESOS-3890,Add notion of evictable task to RunTaskMessage,  message runtaskmessage   ,2,train
MESOS-3891,Add a helper function to the Agent to check available resources before launching a task. ,"launching a task using revocable resources should be funnelled through an accounting system:     if a task is launched using revocable resources, the resources must not be in use when launching the task.  if they are in use, then the task should fail to start.   if a task is launched using reserved resources, the resources must be made available.  this means potentially evicting tasks which are using revocable resources.    both cases could be implemented by adding a check in slave::runtask, like a new helper method:      class slave   ",5,train
MESOS-3892,"Add a helper function to the Agent to retrieve the list of executors that are using optimistically offered, revocable resources.","in the agent, add a helper function to get the list of the exeuctor using allocation_slack.    it's short term solution which is different the design document, because master did not have executor for command line executor. send evicatble executors from master to slave will addess in postmvp after mesos1718.      class slave     ",5,train
MESOS-3893,Implement tests for verifying allocator resource math.,write a test to ensure that the allocator performs the reservation slack calculations correctly.,8,train
MESOS-3894,Rebuild reservation slack allocator state during master failover.,nan,13,train
MESOS-3895,Update reservation slack allocator state during agent failover.,nan,13,train
MESOS-3896,Add accounting for reservation slack in the allocator.,"mesosxxx: optimsistic accounter          class hierarchicalallocatorprocess       ;                optimistic optimistic;        };      }      mesos4146: flatten & allocationslack for optimistic offer          class resources            mesosxxx: allocate the allocationslack resources to framework          void hierarchicalallocatorprocess::allocate(          const hashset/& slaveids)                  ...          offerable[frameworkid][slaveid] = resources  optimistic;            ...          slaves[slaveid].optimistic.allocated += optimistic;        }      }        here's some consideration about `allocationslack`:    1. 'old' resources (available/total) did not include allocationslack  2. after `quota`, `remainingclusterresources.contains` should not check allocationslack; if there no enough resources,  master can still offer allocationsalck resources.  3. in sorter, it'll not include allocationslack; as those resources are borrowed from other role/framework  4. if either normal resources or allocationslack resources are allocable/!filtered, it can be offered to framework  5. currently, allocator will assign all allocationsalck resources in slave to one framework    mesosxxx: update allocationslack for dynamic reservation (updateallocation)          void hierarchicalallocatorprocess::updateallocation(          const frameworkid& frameworkid,          const slaveid& slaveid,          const vector/& operations)                mesosxxx: add allocationslack when slaver register/reregister (addslave)          void hierarchicalallocatorprocess::addslave(          const slaveid& slaveid,          const slaveinfo& slaveinfo,          const option/& unavailability,          const resources& total,          const hashmap/& used)              no need to handle `removeslave`, it'll all related info from `slaves` including `optimistic`.    mesosxxx: return resources to allocator (recoverresources)          void hierarchicalallocatorprocess::recoverresources(          const frameworkid& frameworkid,          const slaveid& slaveid,          const resources& resources,          const option/& filters)            }  ",13,train
MESOS-3897,Identify and implement test cases for verifying eviction logic in the agent,nan,13,train
MESOS-3898,Identify and implement test cases for handling a race between optimistic lender and tenant offers.,an example is the when lender launches the task on an agent followed by a  borrower launching a task on the same agent before the optimistic offer is rescinded. ,13,train
MESOS-3899,Wrong syntax and inconsistent formatting of JSON examples in flag documentation,"the json examples in the documentation of the commandline flags (mesosmaster.sh help and mesosslave.sh help) don't have a consistent formatting. furthermore, some examples aren't even compliant json because they have trailing commas were they shouldn't.",1,train
MESOS-3900,Enable mesos-reviewbot project on jenkins to use docker,"as a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",3,train
MESOS-3903,Add authorization for '/create-volume' and '/destroy-volume' HTTP endpoints,"this is the fourth in a series of tickets that adds authorization support for persistent volumes.    we need to add acl authorization for the '/createvolume' and '/destroyvolume' http endpoints. in other complementary work, authorization for frameworks performing create and destroy operations is being added by mesos3065.    this will consist of adding authorization calls into the http endpoint code in src/master/http.cpp, as well as tests for both failed & successful calls to '/createvolumes' and '/destroy volumes' with authorization. we also must ensure that the principal field of resource.diskinfo.persistence is being populated correctly.",2,train
MESOS-3905,Five new docker-related slave flags are not covered by the configuration documentation.,"these flags were added to ""slave/flags.cpp"", but are not mentioned in ""docs/configuration.md"":      add(&flags::dockerauthserver,        ""dockerauthserver"",        ""docker authentication server"",        ""auth.docker.io"");      add(&flags::dockerauthserverport,        ""dockerauthserverport"",        ""docker authentication server port"",        ""443"");    add(&flags::dockerpullertimeoutsecs,        ""dockerpullertimeout"",        ""timeout value in seconds for pulling images from docker registry"",        ""60"");      add(&flags::dockerregistry,        ""dockerregistry"",        ""default docker image registry server host"",        ""registry 1.docker.io"");    add(&flags::dockerregistryport,        ""dockerregistry_port"",        ""default docker registry server port"",        ""443"");  ",1,train
MESOS-3909,isolator module headers depend on picojson headers,"when trying to build an isolator module, stout headers end up depending on picojson.hpp which is not installed.      in file included from /opt/mesos/include/mesos/module/isolator.hpp:25:  in file included from /opt/mesos/include/mesos/slave/isolator.hpp:30:  in file included from /opt/mesos/include/process/dispatch.hpp:22:  in file included from /opt/mesos/include/process/process.hpp:26:  in file included from /opt/mesos/include/process/event.hpp:21:  in file included from /opt/mesos/include/process/http.hpp:39:  /opt/mesos/include/stout/json.hpp:23:10: fatal error: 'picojson.h' file not found  #include /           ^  8 warnings and 1 error generated.  ",3,train
MESOS-3910,Libprocess: Implement cleanup of the SocketManager in process::finalize,"the socketmanager and processmanager are intricately tied together.  currently, only the processmanager is cleaned up by process::finalize.    to clean up the socketmanager, we must close all sockets and deallocate any existing httpproxy or encoder objects.  and we should prevent further objects from being created/tracked by the socketmanager.    proposal  # clean up all processes other than gc.  this will clear all links and delete all httpproxy s while socketmanager still exists.  # close all sockets via socketmanager::close.  all of socketmanager 's state is cleaned up via socketmanager::close, including termination of httpproxy (termination is idempotent, meaning that killing httpproxy s via processmanager is safe).  # at this point, socketmanager should be empty and only the gc process should be running.  (since we're finalizing, assume there are no threads trying to spawn processes.)  socketmanager can be deleted.  # gc can be deleted.  this is currently a leaked pointer, so we'll also need to track and delete that.  # process_manager should be devoid of processes, so we can proceed with cleanup (join threads, stop the eventloop, etc).",5,train
MESOS-3911,Add a `--force` flag to disable sanity check in quota,"there are use cases when an operator may want to disable the sanity check for quota endpoints (mesos3074), even if this renders the cluster under quota. for example, an operator sets quota before adding more agents in order to make sure that no nonquota allocations from new agents are made. ",1,train
MESOS-3912,Rescind offers in order to satisfy quota,"when a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. because resources are allocated in the allocator, there can be a race between rescinding and allocating. this race makes it hard to determine the exact amount of offers that should be rescinded in the master.",3,train
MESOS-3913,Disallow empty string roles,"having an empty role (empty string) looks like a terrible idea, but we do not prohibit it. i think we should add corresponding checks and update the docs to officially disallow empty roles.",3,train
MESOS-3916,MasterMaintenanceTest.InverseOffersFilters is flaky,"verbose logs:    [ run      ] mastermaintenancetest.inverseoffersfilters  i1113 16:43:58.486469  8728 leveldb.cpp:176] opened db in 2.360405ms  i1113 16:43:58.486935  8728 leveldb.cpp:183] compacted db in 407105ns  i1113 16:43:58.486995  8728 leveldb.cpp:198] created db iterator in 16221ns  i1113 16:43:58.487030  8728 leveldb.cpp:204] seeked to beginning of db in 10935ns  i1113 16:43:58.487046  8728 leveldb.cpp:273] iterated through 0 keys in the db in 999ns  i1113 16:43:58.487090  8728 replica.cpp:780] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1113 16:43:58.487735  8747 recover.cpp:449] starting replica recovery  i1113 16:43:58.488047  8747 recover.cpp:475] replica is in empty status  i1113 16:43:58.488977  8745 replica.cpp:676] replica in empty status received a broadcasted recover request from (58)@10.0.2.15:45384  i1113 16:43:58.489452  8746 recover.cpp:195] received a recover response from a replica in empty status  i1113 16:43:58.489712  8747 recover.cpp:566] updating replica status to starting  i1113 16:43:58.490706  8742 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 745443ns  i1113 16:43:58.490739  8742 replica.cpp:323] persisted replica status to starting  i1113 16:43:58.490859  8742 recover.cpp:475] replica is in starting status  i1113 16:43:58.491786  8747 replica.cpp:676] replica in starting status received a broadcasted recover request from (59)@10.0.2.15:45384  i1113 16:43:58.492542  8749 recover.cpp:195] received a recover response from a replica in starting status  i1113 16:43:58.493221  8743 recover.cpp:566] updating replica status to voting  i1113 16:43:58.493710  8743 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 331874ns  i1113 16:43:58.493767  8743 replica.cpp:323] persisted replica status to voting  i1113 16:43:58.493868  8743 recover.cpp:580] successfully joined the paxos group  i1113 16:43:58.494119  8743 recover.cpp:464] recover process terminated  i1113 16:43:58.504369  8749 master.cpp:367] master d59449fc546243c5b935e05563fdd4b6 (vagrantubuntuwily64) started on 10.0.2.15:45384  i1113 16:43:58.504438  8749 master.cpp:369] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/zb7css/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/zb7css/master"" zksessiontimeout=""10secs""  i1113 16:43:58.504717  8749 master.cpp:416] master allowing unauthenticated frameworks to register  i1113 16:43:58.504889  8749 master.cpp:419] master only allowing authenticated slaves to register  i1113 16:43:58.504922  8749 credentials.hpp:37] loading credentials for authentication from '/tmp/zb7css/credentials'  i1113 16:43:58.505497  8749 master.cpp:458] using default 'crammd5' authenticator  i1113 16:43:58.505759  8749 master.cpp:495] authorization enabled  i1113 16:43:58.507638  8746 master.cpp:1606] the newly elected leader is master@10.0.2.15:45384 with id d59449fc546243c5b935e05563fdd4b6  i1113 16:43:58.507693  8746 master.cpp:1619] elected as the leading master!  i1113 16:43:58.507720  8746 master.cpp:1379] recovering from registrar  i1113 16:43:58.507946  8749 registrar.cpp:309] recovering registrar  i1113 16:43:58.508561  8749 log.cpp:661] attempting to start the writer  i1113 16:43:58.510282  8747 replica.cpp:496] replica received implicit promise request from (60)@10.0.2.15:45384 with proposal 1  i1113 16:43:58.510867  8747 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 475696ns  i1113 16:43:58.510946  8747 replica.cpp:345] persisted promised to 1  i1113 16:43:58.511912  8745 coordinator.cpp:240] coordinator attempting to fill missing positions  i1113 16:43:58.513030  8749 replica.cpp:391] replica received explicit promise request from (61)@10.0.2.15:45384 for position 0 with proposal 2  i1113 16:43:58.513819  8749 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 739171ns  i1113 16:43:58.513867  8749 replica.cpp:715] persisted action at 0  i1113 16:43:58.522002  8745 replica.cpp:540] replica received write request for position 0 from (62)@10.0.2.15:45384  i1113 16:43:58.522114  8745 leveldb.cpp:438] reading position from leveldb took 33549ns  i1113 16:43:58.522599  8745 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 435729ns  i1113 16:43:58.522652  8745 replica.cpp:715] persisted action at 0  i1113 16:43:58.523291  8746 replica.cpp:694] replica received learned notice for position 0 from @0.0.0.0:0  i1113 16:43:58.523901  8746 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 538894ns  i1113 16:43:58.523983  8746 replica.cpp:715] persisted action at 0  i1113 16:43:58.524060  8746 replica.cpp:700] replica learned nop action at position 0  i1113 16:43:58.524775  8747 log.cpp:677] writer started with ending position 0  i1113 16:43:58.525902  8745 leveldb.cpp:438] reading position from leveldb took 39685ns  i1113 16:43:58.526852  8745 registrar.cpp:342] successfully fetched the registry (0b) in 18.832896ms  i1113 16:43:58.527084  8745 registrar.cpp:441] applied 1 operations in 24930ns; attempting to update the 'registry'  i1113 16:43:58.528020  8745 log.cpp:685] attempting to append 189 bytes to the log  i1113 16:43:58.528323  8748 coordinator.cpp:350] coordinator attempting to write append action at position 1  i1113 16:43:58.529465  8744 replica.cpp:540] replica received write request for position 1 from (63)@10.0.2.15:45384  i1113 16:43:58.530081  8744 leveldb.cpp:343] persisting action (208 bytes) to leveldb took 552812ns  i1113 16:43:58.530128  8744 replica.cpp:715] persisted action at 1  i1113 16:43:58.530781  8745 replica.cpp:694] replica received learned notice for position 1 from @0.0.0.0:0  i1113 16:43:58.531121  8745 leveldb.cpp:343] persisting action (210 bytes) to leveldb took 271774ns  i1113 16:43:58.531162  8745 replica.cpp:715] persisted action at 1  i1113 16:43:58.531188  8745 replica.cpp:700] replica learned append action at position 1  i1113 16:43:58.532064  8743 registrar.cpp:486] successfully updated the 'registry' in 4.9152ms  i1113 16:43:58.532402  8743 registrar.cpp:372] successfully recovered registrar  i1113 16:43:58.532768  8742 log.cpp:704] attempting to truncate the log to 1  i1113 16:43:58.532891  8743 master.cpp:1416] recovered 0 slaves from the registry (150b) ; allowing 10mins for slaves to reregister  i1113 16:43:58.532968  8742 coordinator.cpp:350] coordinator attempting to write truncate action at position 2  i1113 16:43:58.534010  8742 replica.cpp:540] replica received write request for position 2 from (64)@10.0.2.15:45384  i1113 16:43:58.534488  8742 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 420186ns  i1113 16:43:58.534533  8742 replica.cpp:715] persisted action at 2  i1113 16:43:58.535081  8748 replica.cpp:694] replica received learned notice for position 2 from @0.0.0.0:0  i1113 16:43:58.535482  8748 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 360618ns  i1113 16:43:58.535550  8748 leveldb.cpp:401] deleting 1 keys from leveldb took 23693ns  i1113 16:43:58.535575  8748 replica.cpp:715] persisted action at 2  i1113 16:43:58.535611  8748 replica.cpp:700] replica learned truncate action at position 2  i1113 16:43:58.550834  8746 slave.cpp:191] slave started on 5)@10.0.2.15:45384  i1113 16:43:58.550834  8746 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/mastermaintenancetestinverseoffersfilters2zc09g/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mastermaintenancetestinverseoffersfilters2zc09g/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostname=""maintenancehost"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/home/vagrant/buildmesos/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mastermaintenancetestinverseoffersfilters2zc09g""  i1113 16:43:58.551501  8746 credentials.hpp:85] loading credential for authentication from '/tmp/mastermaintenancetestinverseoffersfilters2zc09g/credential'  i1113 16:43:58.551703  8746 slave.cpp:322] slave using credential for: testprincipal  i1113 16:43:58.552422  8746 slave.cpp:392] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1113 16:43:58.552510  8746 slave.cpp:400] slave attributes: [  ]  i1113 16:43:58.552532  8746 slave.cpp:405] slave hostname: maintenancehost  i1113 16:43:58.552547  8746 slave.cpp:410] slave checkpoint: true  i1113 16:43:58.553520  8746 state.cpp:54] recovering state from '/tmp/mastermaintenancetestinverseoffersfilters2zc09g/meta'  i1113 16:43:58.553938  8746 statusupdatemanager.cpp:202] recovering status update manager  i1113 16:43:58.554251  8746 slave.cpp:4230] finished recovery  i1113 16:43:58.555016  8746 slave.cpp:729] new master detected at master@10.0.2.15:45384  i1113 16:43:58.555166  8746 slave.cpp:792] authenticating with master master@10.0.2.15:45384  i1113 16:43:58.555207  8746 slave.cpp:797] using default crammd5 authenticatee  i1113 16:43:58.555589  8746 slave.cpp:765] detecting new master  i1113 16:43:58.555076  8749 statusupdatemanager.cpp:176] pausing sending status updates  i1113 16:43:58.555719  8742 authenticatee.cpp:123] creating new client sasl connection  i1113 16:43:58.560645  8744 master.cpp:5150] authenticating slave(5)@10.0.2.15:45384  i1113 16:43:58.561305  8744 authenticator.cpp:100] creating new server sasl connection  i1113 16:43:58.566682  8744 authenticatee.cpp:214] received sasl authentication mechanisms: crammd5  i1113 16:43:58.566779  8744 authenticatee.cpp:240] attempting to authenticate with mechanism 'crammd5'  i1113 16:43:58.566872  8744 authenticator.cpp:205] received sasl authentication start  i1113 16:43:58.566936  8744 authenticator.cpp:327] authentication requires more steps  i1113 16:43:58.567602  8744 authenticatee.cpp:260] received sasl authentication step  i1113 16:43:58.567775  8744 authenticator.cpp:233] received sasl authentication step  i1113 16:43:58.568128  8744 authenticator.cpp:319] authentication success  i1113 16:43:58.568282  8742 authenticatee.cpp:300] authentication success  i1113 16:43:58.568320  8749 master.cpp:5180] successfully authenticated principal 'testprincipal' at slave(5)@10.0.2.15:45384  i1113 16:43:58.568701  8742 slave.cpp:860] successfully authenticated with master master@10.0.2.15:45384  i1113 16:43:58.569272  8747 master.cpp:3859] registering slave at slave(5)@10.0.2.15:45384 (maintenancehost) with id d59449fc546243c5b935e05563fdd4b6s0  i1113 16:43:58.570096  8747 registrar.cpp:441] applied 1 operations in 59195ns; attempting to update the 'registry'  i1113 16:43:58.570772  8748 log.cpp:685] attempting to append 362 bytes to the log  i1113 16:43:58.570772  8749 coordinator.cpp:350] coordinator attempting to write append action at position 3  i1113 16:43:58.572155  8745 replica.cpp:540] replica received write request for position 3 from (69)@10.0.2.15:45384  i1113 16:43:58.572801  8745 leveldb.cpp:343] persisting action (381 bytes) to leveldb took 563073ns  i1113 16:43:58.572854  8745 replica.cpp:715] persisted action at 3  i1113 16:43:58.573707  8745 replica.cpp:694] replica received learned notice for position 3 from @0.0.0.0:0  i1113 16:43:58.574255  8745 leveldb.cpp:343] persisting action (383 bytes) to leveldb took 485234ns  i1113 16:43:58.574311  8745 replica.cpp:715] persisted action at 3  i1113 16:43:58.574342  8745 replica.cpp:700] replica learned append action at position 3  i1113 16:43:58.575857  8747 master.cpp:3847] ignoring register slave message from slave(5)@10.0.2.15:45384 (maintenancehost) as admission is already in progress  i1113 16:43:58.576217  8744 log.cpp:704] attempting to truncate the log to 3  i1113 16:43:58.575887  8748 registrar.cpp:486] successfully updated the 'registry' in 5.682176ms  i1113 16:43:58.576400  8744 coordinator.cpp:350] coordinator attempting to write truncate action at position 4  i1113 16:43:58.577169  8746 master.cpp:3927] registered slave d59449fc546243c5b935e05563fdd4b6s0 at slave(5)@10.0.2.15:45384 (maintenancehost) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1113 16:43:58.577287  8745 hierarchical.cpp:344] added slave d59449fc546243c5b935e05563fdd4b6s0 (maintenancehost) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i1113 16:43:58.577472  8744 slave.cpp:904] registered with master master@10.0.2.15:45384; given slave id d59449fc546243c5b935e05563fdd4b6s0  i1113 16:43:58.577999  8745 statusupdatemanager.cpp:183] resuming sending status updates  i1113 16:43:58.578279  8748 replica.cpp:540] replica received write request for position 4 from (70)@10.0.2.15:45384  i1113 16:43:58.578346  8744 slave.cpp:963] forwarding total oversubscribed resources  i1113 16:43:58.578734  8744 master.cpp:4269] received update of slave d59449fc546243c5b935e05563fdd4b6s0 at slave(5)@10.0.2.15:45384 (maintenancehost) with total oversubscribed resources  i1113 16:43:58.578846  8748 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 304993ns  i1113 16:43:58.578889  8748 replica.cpp:715] persisted action at 4  i1113 16:43:58.578897  8744 hierarchical.cpp:400] slave d59449fc546243c5b935e05563fdd4b6s0 (maintenancehost) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports():[3100032000], allocated: )  i1113 16:43:58.579463  8744 replica.cpp:694] replica received learned notice for position 4 from @0.0.0.0:0  i1113 16:43:58.579888  8744 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 384596ns  i1113 16:43:58.579952  8744 leveldb.cpp:401] deleting 2 keys from leveldb took 27011ns  i1113 16:43:58.579977  8744 replica.cpp:715] persisted action at 4  i1113 16:43:58.580001  8744 replica.cpp:700] replica learned truncate action at position 4  i1113 16:43:58.584300  8743 slave.cpp:191] slave started on 6)@10.0.2.15:45384  i1113 16:43:58.584398  8743 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/mastermaintenancetestinverseoffersfilterscdfgvt/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mastermaintenancetestinverseoffersfilterscdfgvt/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostname=""maintenancehost2"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/home/vagrant/buildmesos/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mastermaintenancetestinverseoffersfilterscdfgvt""  i1113 16:43:58.584731  8743 credentials.hpp:85] loading credential for authentication from '/tmp/mastermaintenancetestinverseoffersfilterscdfgvt/credential'  i1113 16:43:58.584915  8743 slave.cpp:322] slave using credential for: testprincipal  i1113 16:43:58.585309  8743 slave.cpp:392] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1113 16:43:58.585482  8743 slave.cpp:400] slave attributes: [  ]  i1113 16:43:58.585566  8743 slave.cpp:405] slave hostname: maintenancehost2  i1113 16:43:58.585619  8743 slave.cpp:410] slave checkpoint: true  i1113 16:43:58.586431  8743 state.cpp:54] recovering state from '/tmp/mastermaintenancetestinverseoffersfilterscdfgvt/meta'  i1113 16:43:58.586890  8745 statusupdatemanager.cpp:202] recovering status update manager  i1113 16:43:58.587136  8745 slave.cpp:4230] finished recovery  i1113 16:43:58.587817  8745 slave.cpp:729] new master detected at master@10.0.2.15:45384  i1113 16:43:58.587836  8747 statusupdate_manager.cpp:176] pausing sending status updates  i1113 16:43:58.587908  8745 slave.cpp:792] authenticating with master master@10.0.2.15:45384  i1113 16:43:58.587934  8745 slave.cpp:797] using default crammd5 authenticatee  i1113 16:43:58.588043  8745 slave.cpp:765] detecting new master  i1113 16:43:58.588170  8745 authenticatee.cpp:123] creating new client sasl connection  i1113 16:43:58.592891  8745 master.cpp:5150] authenticating slave(6)@10.0.2.15:45384  i1113 16:43:58.594146  8745 authenticator.cpp:100] creating new server sasl connection  i1113 16:43:58.599606  8749 authenticatee.cpp:214] received sasl authentication mechanisms: crammd5  i1113 16:43:58.599684  8749 authenticatee.cpp:240] attempting to authenticate with mechanism 'cram md5'  i1113 16:43:58.599774  8749 authenticator.cpp:205] received sasl authentication start  i1113 16:43:58.599830  8749 authenticator.cpp:327] authentication requires more steps  i1113 ...",3,train
MESOS-3923,Implement AuthN handling in Master for the Scheduler endpoint,"if authentication(authn) is enabled on a master, frameworks attempting to use the http scheduler api can't register.      $ cat /tmp/subscribe943257503176798091.bin  note: binary data not shown in terminal |      http/1.1 401 unauthorized  date: fri, 13 nov 2015 20:00:45 gmt  wwwauthenticate: basic realm=""mesos master""  content length: 65    http schedulers are not supported when authentication is required      authorization(authz) is already supported for http based frameworks.",5,train
MESOS-3925,Add HDFS based URI fetcher plugin.,this plugin uses hdfs client to fetch artifacts. it can support schemes like hdfs/hftp/s3/s3n    it'll shell out the hadoop command to do the actual fetching.,3,train
MESOS-3926,Modularize URI fetcher plugin interface.  ,so that we can add custom uri fetcher plugins using modules.,3,train
MESOS-3928,ROOT tests fail on Mesos 0.26 on Ubuntu/CentOS,"running 0.26.0 rc1 on both centos 7.1 and ubuntu 14.04 with sudo privileges, causes segfaults when running docker tests.    logs attached.",2,train
MESOS-3929,Automate the process of landing commits for committers,this script should do the following things    1) apply a chain of reviews to a local branch  2) push the commits upstream  3) mark the reviews as submitted  4) optionally close any attached jira tickets    ,3,train
MESOS-3934,Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess,"related to this https:/github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#l949l950.    the metricsprocess and reaperprocess are global processes (singletons) which are initialized upon first use.  the two processes could be initialized alongside the gc, help, logging, profiler, and system (statistics) processes inside process::initialize.    this is also necessary for libprocess reinitialization.",3,train
MESOS-3936,Document possible task state transitions for framework authors,"we should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault tolerant schedulers and do task state reconciliation.",5,train
MESOS-3937,Test DockerContainerizerTest.ROOT_DOCKER_Launch_Executor fails.,"  ../configure  make check  sudo ./bin/mesostests.sh gtestfilter=""dockercontainerizertest.rootdockerlaunchexecutor"" verbose        [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from dockercontainerizertest  i1117 15:08:09.265943 26380 leveldb.cpp:176] opened db in 3.199666ms  i1117 15:08:09.267761 26380 leveldb.cpp:183] compacted db in 1.684873ms  i1117 15:08:09.267902 26380 leveldb.cpp:198] created db iterator in 58313ns  i1117 15:08:09.267966 26380 leveldb.cpp:204] seeked to beginning of db in 4927ns  i1117 15:08:09.267997 26380 leveldb.cpp:273] iterated through 0 keys in the db in 1605ns  i1117 15:08:09.268156 26380 replica.cpp:780] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1117 15:08:09.270148 26396 recover.cpp:449] starting replica recovery  i1117 15:08:09.272105 26396 recover.cpp:475] replica is in empty status  i1117 15:08:09.275640 26396 replica.cpp:676] replica in empty status received a broadcasted recover request from (4)@10.0.2.15:50088  i1117 15:08:09.276578 26399 recover.cpp:195] received a recover response from a replica in empty status  i1117 15:08:09.277600 26397 recover.cpp:566] updating replica status to starting  i1117 15:08:09.279613 26396 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 1.016098ms  i1117 15:08:09.279731 26396 replica.cpp:323] persisted replica status to starting  i1117 15:08:09.280306 26399 recover.cpp:475] replica is in starting status  i1117 15:08:09.282181 26400 replica.cpp:676] replica in starting status received a broadcasted recover request from (5)@10.0.2.15:50088  i1117 15:08:09.282552 26400 master.cpp:367] master 59c600f192ff49269c84073d9b81f68a (vagrantubuntutrusty64) started on 10.0.2.15:50088  i1117 15:08:09.283021 26400 master.cpp:369] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/40alt8/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/40alt8/master"" zksessiontimeout=""10secs""  i1117 15:08:09.283920 26400 master.cpp:414] master only allowing authenticated frameworks to register  i1117 15:08:09.283972 26400 master.cpp:419] master only allowing authenticated slaves to register  i1117 15:08:09.284032 26400 credentials.hpp:37] loading credentials for authentication from '/tmp/40alt8/credentials'  i1117 15:08:09.282944 26401 recover.cpp:195] received a recover response from a replica in starting status  i1117 15:08:09.284639 26401 recover.cpp:566] updating replica status to voting  i1117 15:08:09.285539 26400 master.cpp:458] using default 'crammd5' authenticator  i1117 15:08:09.285995 26401 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 1.075466ms  i1117 15:08:09.286062 26401 replica.cpp:323] persisted replica status to voting  i1117 15:08:09.286200 26401 recover.cpp:580] successfully joined the paxos group  i1117 15:08:09.286471 26401 recover.cpp:464] recover process terminated  i1117 15:08:09.287303 26400 authenticator.cpp:520] initializing server sasl  i1117 15:08:09.289371 26400 master.cpp:495] authorization enabled  i1117 15:08:09.296018 26399 master.cpp:1606] the newly elected leader is master@10.0.2.15:50088 with id 59c600f192ff49269c84073d9b81f68a  i1117 15:08:09.296115 26399 master.cpp:1619] elected as the leading master!  i1117 15:08:09.296187 26399 master.cpp:1379] recovering from registrar  i1117 15:08:09.296717 26397 registrar.cpp:309] recovering registrar  i1117 15:08:09.298842 26396 log.cpp:661] attempting to start the writer  i1117 15:08:09.301563 26394 replica.cpp:496] replica received implicit promise request from (6)@10.0.2.15:50088 with proposal 1  i1117 15:08:09.302561 26394 leveldb.cpp:306] persisting metadata (8 bytes) to leveldb took 922719ns  i1117 15:08:09.302635 26394 replica.cpp:345] persisted promised to 1  i1117 15:08:09.303755 26394 coordinator.cpp:240] coordinator attempting to fill missing positions  i1117 15:08:09.306161 26394 replica.cpp:391] replica received explicit promise request from (7)@10.0.2.15:50088 for position 0 with proposal 2  i1117 15:08:09.306972 26394 leveldb.cpp:343] persisting action (8 bytes) to leveldb took 711749ns  i1117 15:08:09.307034 26394 replica.cpp:715] persisted action at 0  i1117 15:08:09.308732 26401 replica.cpp:540] replica received write request for position 0 from (8)@10.0.2.15:50088  i1117 15:08:09.308830 26401 leveldb.cpp:438] reading position from leveldb took 46444ns  i1117 15:08:09.309710 26401 leveldb.cpp:343] persisting action (14 bytes) to leveldb took 779098ns  i1117 15:08:09.309754 26401 replica.cpp:715] persisted action at 0  i1117 15:08:09.311007 26397 replica.cpp:694] replica received learned notice for position 0 from @0.0.0.0:0  i1117 15:08:09.311652 26397 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 567289ns  i1117 15:08:09.311731 26397 replica.cpp:715] persisted action at 0  i1117 15:08:09.311771 26397 replica.cpp:700] replica learned nop action at position 0  i1117 15:08:09.313212 26397 log.cpp:677] writer started with ending position 0  i1117 15:08:09.315682 26399 leveldb.cpp:438] reading position from leveldb took 27974ns  i1117 15:08:09.318694 26395 registrar.cpp:342] successfully fetched the registry (0b) in 21.862144ms  i1117 15:08:09.319007 26395 registrar.cpp:441] applied 1 operations in 91867ns; attempting to update the 'registry'  i1117 15:08:09.321730 26395 log.cpp:685] attempting to append 193 bytes to the log  i1117 15:08:09.321935 26397 coordinator.cpp:350] coordinator attempting to write append action at position 1  i1117 15:08:09.323103 26399 replica.cpp:540] replica received write request for position 1 from (9)@10.0.2.15:50088  i1117 15:08:09.323917 26399 leveldb.cpp:343] persisting action (212 bytes) to leveldb took 735223ns  i1117 15:08:09.323983 26399 replica.cpp:715] persisted action at 1  i1117 15:08:09.324975 26398 replica.cpp:694] replica received learned notice for position 1 from @0.0.0.0:0  i1117 15:08:09.325695 26398 leveldb.cpp:343] persisting action (214 bytes) to leveldb took 668268ns  i1117 15:08:09.325741 26398 replica.cpp:715] persisted action at 1  i1117 15:08:09.325778 26398 replica.cpp:700] replica learned append action at position 1  i1117 15:08:09.327258 26396 registrar.cpp:486] successfully updated the 'registry' in 8.090112ms  i1117 15:08:09.327525 26396 registrar.cpp:372] successfully recovered registrar  i1117 15:08:09.328083 26400 log.cpp:704] attempting to truncate the log to 1  i1117 15:08:09.328251 26394 master.cpp:1416] recovered 0 slaves from the registry (154b) ; allowing 10mins for slaves to reregister  i1117 15:08:09.328814 26396 coordinator.cpp:350] coordinator attempting to write truncate action at position 2  i1117 15:08:09.330158 26401 replica.cpp:540] replica received write request for position 2 from (10)@10.0.2.15:50088  i1117 15:08:09.330994 26401 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 760471ns  i1117 15:08:09.331055 26401 replica.cpp:715] persisted action at 2  i1117 15:08:09.331583 26401 replica.cpp:694] replica received learned notice for position 2 from @0.0.0.0:0  i1117 15:08:09.332172 26401 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 497457ns  i1117 15:08:09.332500 26401 leveldb.cpp:401] deleting 1 keys from leveldb took 49327ns  i1117 15:08:09.332715 26401 replica.cpp:715] persisted action at 2  i1117 15:08:09.332964 26401 replica.cpp:700] replica learned truncate action at position 2  i1117 15:08:09.354073 26401 slave.cpp:191] slave started on 1)@10.0.2.15:50088  i1117 15:08:09.354316 26401 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/dockercontainerizertestrootdockerlaunchexecutorhakhaq/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/dockercontainerizertestrootdockerlaunchexecutorhakhaq/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/home/vagrant/mesos/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/dockercontainerizertestrootdockerlaunchexecutorhakhaq""  i1117 15:08:09.355077 26401 credentials.hpp:85] loading credential for authentication from '/tmp/dockercontainerizertestrootdockerlaunchexecutorhakhaq/credential'  i1117 15:08:09.355587 26401 slave.cpp:322] slave using credential for: testprincipal  i1117 15:08:09.357144 26401 slave.cpp:392] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1117 15:08:09.357477 26401 slave.cpp:400] slave attributes: [  ]  i1117 15:08:09.357719 26401 slave.cpp:405] slave hostname: vagrantubuntutrusty64  i1117 15:08:09.357936 26380 sched.cpp:166] version: 0.26.0  i1117 15:08:09.358010 26401 slave.cpp:410] slave checkpoint: true  i1117 15:08:09.359058 26400 sched.cpp:264] new master detected at master@10.0.2.15:50088  i1117 15:08:09.359216 26400 sched.cpp:320] authenticating with master master@10.0.2.15:50088  i1117 15:08:09.359277 26400 sched.cpp:327] using default crammd5 authenticatee  i1117 15:08:09.359856 26400 authenticatee.cpp:99] initializing client sasl  i1117 15:08:09.360539 26400 authenticatee.cpp:123] creating new client sasl connection  i1117 15:08:09.361399 26398 state.cpp:54] recovering state from '/tmp/dockercontainerizertestrootdockerlaunchexecutorhakhaq/meta'  i1117 15:08:09.361994 26398 statusupdatemanager.cpp:202] recovering status update manager  i1117 15:08:09.362191 26395 master.cpp:5150] authenticating scheduler38aa807a672a4e1eb82371f119980e86@10.0.2.15:50088  i1117 15:08:09.362565 26401 docker.cpp:536] recovering docker containers  i1117 15:08:09.362908 26395 authenticator.cpp:100] creating new server sasl connection  i1117 15:08:09.363533 26401 slave.cpp:4230] finished recovery  i1117 15:08:09.363675 26394 authenticatee.cpp:214] received sasl authentication mechanisms: crammd5  i1117 15:08:09.363950 26394 authenticatee.cpp:240] attempting to authenticate with mechanism 'crammd5'  i1117 15:08:09.364137 26394 authenticator.cpp:205] received sasl authentication start  i1117 15:08:09.364241 26394 authenticator.cpp:327] authentication requires more steps  i1117 15:08:09.364481 26394 authenticatee.cpp:260] received sasl authentication step  i1117 15:08:09.364667 26394 authenticator.cpp:233] received sasl authentication step  i1117 15:08:09.364828 26394 authenticator.cpp:319] authentication success  i1117 15:08:09.365039 26398 authenticatee.cpp:300] authentication success  i1117 15:08:09.365170 26398 master.cpp:5180] successfully authenticated principal 'testprincipal' at scheduler38aa807a672a4e1eb82371f119980e86@10.0.2.15:50088  i1117 15:08:09.365656 26398 sched.cpp:409] successfully authenticated with master master@10.0.2.15:50088  i1117 15:08:09.366044 26401 slave.cpp:729] new master detected at master@10.0.2.15:50088  i1117 15:08:09.366283 26398 master.cpp:2176] received subscribe call for framework 'default' at scheduler38aa807a672a4e1eb82371f119980e86@10.0.2.15:50088  i1117 15:08:09.366317 26401 slave.cpp:792] authenticating with master master@10.0.2.15:50088  i1117 15:08:09.366688 26401 slave.cpp:797] using default crammd5 authenticatee  i1117 15:08:09.366525 26395 statusupdatemanager.cpp:176] pausing sending status updates  i1117 15:08:09.366442 26398 master.cpp:1645] authorizing framework principal 'testprincipal' to receive offers for role ''  i1117 15:08:09.367207 26401 slave.cpp:765] detecting new master  i1117 15:08:09.367496 26395 master.cpp:2247] subscribing framework default with checkpointing disabled and capabilities [  ]  i1117 15:08:09.368417 26396 hierarchical.cpp:195] added framework 59c600f192ff49269c84073d9b81f68a0000  i1117 15:08:09.367250 26398 authenticatee.cpp:123] creating new client sasl connection  i1117 15:08:09.368506 26395 sched.cpp:643] framework registered with 59c600f192ff49269c84073d9b81f68a0000  i1117 15:08:09.369287 26398 master.cpp:5150] authenticating slave(1)@10.0.2.15:50088  i1117 15:08:09.370213 26401 authenticator.cpp:100] creating new server sasl connection  i1117 15:08:09.370846 26396 authenticatee.cpp:214] received sasl authentication mechanisms: crammd5  i1117 15:08:09.370964 26396 authenticatee.cpp:240] attempting to authenticate with mechanism 'crammd5'  i1117 15:08:09.371233 26396 authenticator.cpp:205] received sasl authentication start  i1117 15:08:09.371387 26396 authenticator.cpp:327] authentication requires more steps  i1117 15:08:09.371707 26398 authenticatee.cpp:260] received sasl authentication step  i1117 15:08:09.371835 26398 authenticator.cpp:233] received sasl authentication step  i1117 15:08:09.371944 26398 authenticator.cpp:319] authentication success  i1117 15:08:09.372195 26396 authenticatee.cpp:300] authentication success  i1117 15:08:09.372248 26398 master.cpp:5180] successfully authenticated principal 'testprincipal' at slave(1)@10.0.2.15:50088  i1117 15:08:09.373002 26396 slave.cpp:860] successfully authenticated with master master@10.0.2.15:50088  i1117 15:08:09.373566 26398 master.cpp:3859] registering slave at slave(1)@10.0.2.15:50088 (vagrantubuntutrusty64) with id 59c600f192ff49269c84073d9b81f68as0  i1117 15:08:09.374301 26401 registrar.cpp:441] applied 1 operations in 65094ns; attempting to update the 'registry'  i1117 15:08:09.376809 26400 log.cpp:685] attempting to append 374 bytes to the log  i1117 15:08:09.376994 26399 coordinator.cpp:350] coordinator attempting to write append action at position 3  i1117 15:08:09.377960 26397 replica.cpp:540] replica received write request for position 3 from (16)@10.0.2.15:50088  i1117 15:08:09.378844 26397 leveldb.cpp:343] persisting action (393 bytes) to leveldb took 805302ns  i1117 15:08:09.378904 26397 replica.cpp:715] persisted action at 3  i1117 15:08:09.379823 26400 replica.cpp:694] replica received learned notice for position 3 from @0.0.0.0:0  i1117 15:08:09.380592 26400 leveldb.cpp:343] persisting action (395 bytes) to leveldb took 691729ns  i1117 15:08:09.380666 26400 replica.cpp:715] persisted action at 3  i1117 15:08:09.380702 26400 replica.cpp:700] replica learned append action at position 3  i1117 15:08:09.382014 26398 registrar.cpp:486] successfully updated the 'registry' in 7.384064ms  i1117 15:08:09.382184 26400 log.cpp:704] attempting to truncate the log to 3  i1117 15:08:09.382380 26398 coordinator.cpp:350] coordinator attempting to write truncate action at position 4  i1117 15:08:09.383361 26399 master.cpp:3927] registered slave 59c600f192ff49269c84073d9b81f68as0 at slave(1)@10.0.2.15:50088 (vagrantubuntutrusty64) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1117 15:08:09.383437 26396 slave.cpp:904] registered with master master@10.0.2.15:50088; given slave id 59c600f192ff49269c84073d9b81f68as0  i1117 15:08:09.383741 26400 statusupdate_manager.cpp:183] resuming sending status updates  i1117 15:08:09.384004 26401 hierarchical.cpp:344] added slave 59c600f192ff49269c84073d9b81f68as0 (vagrantubuntutrusty64) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i1117 15:08:09.384101 26396 slave.cpp:963] forwarding total oversubscribed resources   i1117 15:08:09.384831 26396 master.cpp:4269] received update of slave 59c600f192ff49269c84073d9b81f68as0 at slave(1)@10.0.2.15:50088 (vagrantubuntutrusty64) with total oversubscribed resources   i1117 15:08:09.384466 26398 replica.cpp:540] replica received write request for position 4 from (17)@10.0.2.15:50088  i1117 15:08:09.385957 26397 master.cpp:4979] sending 1 offers to framework 59c600f192ff49269c84073d9b81f68a0000 (default) at scheduler38aa807a672a4e1eb82371f119980e86@10.0.2.15:50088  i1117 15:08:09.386066 26401 hierarchical.cpp:400] slave 59c600f192ff49269c84073d9b81f68as0 (vagrantubuntutrusty64) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports():[3100032000], allocated: cpus():2; mem():1024; disk():1024; ports():[3100032000])  i1117 15:08:09.386219 26398 leveldb.cpp:343] persisting action (16 bytes) to leveldb took 605641ns  i1117 15:08:09.386445 26398 replica.cpp:715] persisted action at 4  i1117 15:08:09.388450 26397 replica.cpp:694] replica received learned notice for position 4 from @0.0.0.0:0  i1117 15:08:09.389235 26397 leveldb.cpp:343] persisting action (18 bytes) to leveldb took 715846ns  i1117 15:08:09.389345 26397 leveldb.cpp:401] deleting 2 keys from leveldb took 40455ns  i1117 15:08:09.389402 26397 replica.cpp:715] persisted action at 4  i1117 15:08:09.389464 26397 replica.cpp:700] replica learned truncate action at position 4  i1117 15:08:09.390585 26394 master.cpp:2915] processing accept call for offers: [ 59c600f192ff49269c84073d9b81f68ao0 ] on slave 59c600f192ff49269c84073d9b81f68as0 at slave(1)@10.0.2.15:50088 (vagrantubuntutrusty64) for framework 59c600f192ff49269c84073d9b81f68a0000 (default) at scheduler38aa807a672a4e1eb82371f119980e86@10.0.2.15:50088  i1117 15:08:09.390805 26394 master.cpp:2711] authorizing framework principal 'testprincipal' to launch task 1 as user 'root'  w1117 15:08:09.393517 26396 validation.cpp:422] executor e1 for task 1 uses less cpus (none) than the minimum required (0.01). please update your executor, as this will be mandatory in future releases.  w1117 15:08:09.393632 26396 validation.cpp:434] executor e1 for task 1 uses less memory (none) than the minimum required (32mb). please update your executor, as this will be mandatory in future releases.  i1117 15:08:09.394270 26396 master.hpp:176] adding task 1 with resources cpus():2; mem():1024; disk():1024; ports():[3100032000] on slave 59c600f192ff49269c84073d9b81f68as0 (vagrantubuntutrusty64)  i1117 15:08:09.394580 26396 master.cpp:3245] launching task 1 of framework 59c600f192ff49269c840...",2,train
MESOS-3938,Allow setting quotas for the default '*' role,"investigate use cases and implications of the possibility to set quota for the '' role. for example, having quota for '' set can effectively reduce the scope of the quota capacity heuristic.",3,train
MESOS-3939,ubsan error in net::IP::create(sockaddr const&): misaligned address,"running ubsan from gcc 5.2 on the current mesos unit tests yields this, among other problems:      /mesos/3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp:230:56: runtime error: reference binding to misaligned address 0x00000199629c for type 'const struct sockaddrstorage', which requires 8 byte alignment  0x00000199629c: note: pointer points here    00 00 00 00 02 00 00 00  ff ff ff 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00                ^      #0 0x5950cb in net::ip::create(sockaddr const&) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x5950cb)      #1 0x5970cd in net::ipnetwork::fromlinkdevice(std::cxx11::basicstring/, std::allocator/ > const&, int) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x5970cd)      #2 0x58e006 in nettestlinkdevicetest::testbody() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x58e006)      #3 0x85abd5 in void testing::internal::handlesehexceptionsinmethodifsupported/(testing::test, void (testing::test::)(), char const) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x85abd5)      #4 0x848abc in void testing::internal::handleexceptionsinmethodifsupported/(testing::test, void (testing::test::)(), char const) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x848abc)      #5 0x7e2755 in testing::test::run() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x7e2755)      #6 0x7e44a0 in testing::testinfo::run() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x7e44a0)      #7 0x7e5ffa in testing::testcase::run() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x7e5ffa)      #8 0x7ffe21 in testing::internal::unittestimpl::runalltests() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x7ffe21)      #9 0x85d7a5 in bool testing::internal::handlesehexceptionsinmethodifsupported/(testing::internal::unittestimpl, bool (testing::internal::unittestimpl::)(), char const) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x85d7a5)      #10 0x84b37a in bool testing::internal::handleexceptionsinmethodifsupported/(testing::internal::unittestimpl, bool (testing::internal::unittestimpl::)(), char const) (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x84b37a)      #11 0x7f8a4a in testing::unittest::run() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x7f8a4a)      #12 0x608a96 in runalltests() (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x608a96)      #13 0x60896b in main (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stouttests0x60896b)      #14 0x7fd0f0c7fa3f in libcstartmain (/lib/x8664linuxgnu/libc.so.60x20a3f)      #15 0x4145c8 in _start (/home/vagrant/buildmesosubsan/3rdparty/libprocess/3rdparty/stout tests0x4145c8)  ",2,train
MESOS-3940,/reserve and /unreserve should be permissive under a master without authentication.,"currently, the /reserve and /unreserve endpoints do not work without authentication enabled on the master. when authentication is disabled on the master, these endpoints should just be permissive.",1,train
MESOS-3943,Support dynamic weight in allocator,this jira will focus on update the allocator api to support weight update of a role.,5,train
MESOS-3945,Add operator documentation for /weight endpoint,"this jira ticket will update the related doc to apply to dynamic weights, and add an new operator guide for dynamic weights which describes basic usage of the /weights endpoint.",2,train
MESOS-3949,User CGroup Isolation tests fail on Centos 6.,"usercgroupisolatortest/0.rootcgroupsusercgroup and usercgroupisolatortest/1.rootcgroupsusercgroup fail on centos 6.6 with similar output when libevent and ssl are enabled.      sudo ./bin/mesostests.sh gtestfilter=""usercgroupisolatortest/0.rootcgroupsusercgroup"" verbose      [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from usercgroupisolatortest/0, where typeparam = mesos::internal::slave::cgroupsmemisolatorprocess  userdel: user 'mesos.test.unprivileged.user' does not exist  [ run      ] usercgroupisolatortest/0.rootcgroupsusercgroup  i1118 16:53:35.273717 30249 mem.cpp:605] started listening for oom events for container 867a829e4a2643f586e0938bf1f47688  i1118 16:53:35.274538 30249 mem.cpp:725] started listening on low memory pressure events for container 867a829e4a2643f586e0938bf1f47688  i1118 16:53:35.275164 30249 mem.cpp:725] started listening on medium memory pressure events for container 867a829e4a2643f586e0938bf1f47688  i1118 16:53:35.275784 30249 mem.cpp:725] started listening on critical memory pressure events for container 867a829e4a2643f586e0938bf1f47688  i1118 16:53:35.276448 30249 mem.cpp:356] updated 'memory.softlimitinbytes' to 1gb for container 867a829e4a2643f586e0938bf1f47688  i1118 16:53:35.277331 30249 mem.cpp:391] updated 'memory.limitinbytes' to 1gb for container 867a829e4a2643f586e0938bf1f47688  bash: /sys/fs/cgroup/memory/mesos/867a829e4a2643f586e0938bf1f47688/cgroup.procs: no such file or directory  mkdir: cannot create directory `/sys/fs/cgroup/memory/mesos/867a829e4a2643f586e0938bf1f47688/user': no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1307: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/memory/mesos/867a829e4a2643f586e0938bf1f47688/user/cgroup.procs: no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1316: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  [  failed  ] usercgroupisolatortest/0.rootcgroupsusercgroup, where typeparam = mesos::internal::slave::cgroupsmemisolatorprocess (149 ms)        sudo ./bin/mesostests.sh gtestfilter=""usercgroupisolatortest/1.rootcgroupsusercgroup"" verbose      [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from usercgroupisolatortest/1, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess  userdel: user 'mesos.test.unprivileged.user' does not exist  [ run      ] usercgroupisolatortest/1.rootcgroupsusercgroup  i1118 17:01:00.550706 30357 cpushare.cpp:392] updated 'cpu.shares' to 1024 (cpus 1) for container e57f43431a974b44b347803be47ace80  bash: /sys/fs/cgroup/cpuacct/mesos/e57f43431a974b44b347803be47ace80/cgroup.procs: no such file or directory  mkdir: cannot create directory `/sys/fs/cgroup/cpuacct/mesos/e57f43431a974b44b347803be47ace80/user': no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1307: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpuacct/mesos/e57f43431a974b44b347803be47ace80/user/cgroup.procs: no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1316: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpu/mesos/e57f43431a974b44b347803be47ace80/cgroup.procs: no such file or directory  mkdir: cannot create directory `/sys/fs/cgroup/cpu/mesos/e57f43431a974b44b347803be47ace80/user': no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1307: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'mkdir ""  path::join(flags.cgroupshierarchy, usercgroup)  ""'"")    actual: 256  expected: 0  bash: /sys/fs/cgroup/cpu/mesos/e57f43431a974b44b347803be47ace80/user/cgroup.procs: no such file or directory  ../../src/tests/containerizer/isolatortests.cpp:1316: failure  value of: os::system( ""su  ""  unprivilegedusername  "" c 'echo $$ >""  path::join(flags.cgroupshierarchy, usercgroup, ""cgroup.procs"")  ""'"")    actual: 256  expected: 0  [  failed  ] usercgroupisolatortest/1.rootcgroups_usercgroup, where typeparam = mesos::internal::slave::cgroupscpushareisolatorprocess (116 ms)  ",3,train
MESOS-3951,Make HDFS tool wrappers asynchronous.,"the existing hdfs tool wrappers (src/hdfs/hdfs.hpp) are synchronous. they use os::shell to shell out the 'hadoop' commands. this makes it very hard to be reused at other locations in the code base.    the uri fetcher hdfs plugin will try to re use the existing hdfs tool wrappers. in order to do that, we need to make it asynchronous first.",5,train
MESOS-3960,Standardize quota endpoints,"to be consistent with other operator endpoints, require a single json object in the request as opposed to key value pairs encoded in a string.",3,train
MESOS-3964,LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.,"sudo ./bin/mesostest.sh  gtestfilter=""limitedcpuisolatortest.rootcgroupscfs""      ...  f1119 14:34:52.514742 30706 isolatortests.cpp:455] checksome(isolator): failed to find 'cpu.cfsquota_us'. your kernel might be too old to use the cfs cgroups feature.    ",2,train
MESOS-3965,Ensure resources in `QuotaInfo` protobuf do not contain `role`,"quotainfo protobuf currently stores per role quotas, including resource objects. these resources are neither statically nor dynamically reserved, hence they may not contain role field. we should ensure this field is unset, as well as update validation routine for quotainfo",3,train
MESOS-3967,Add integration tests for quota,these tests should verify whether quota implements declared functionality. this will require the whole pipeline: master harness code and an allocator implementation (in contrast to to isolated master and allocator tests).,8,train
MESOS-3969,"Failing 'make distcheck' on Debian 8, somehow SSL-related.","as nonroot: make distcheck.      /bin/mkdir p '/home/vagrant/mesos/build/mesos0.26.0/inst/bin'  /bin/bash ../libtool mode=install /usr/bin/install c mesoslocal mesoslog mesos mesosexecute mesosresolve '/home/vagrant/mesos/build/mesos0.26.0/inst/bin'  libtool: install: /usr/bin/install c .libs/mesoslocal /home/vagrant/mesos/build/mesos0.26.0/inst/bin/mesoslocal  libtool: install: /usr/bin/install c .libs/mesoslog /home/vagrant/mesos/build/mesos0.26.0/inst/bin/mesoslog  libtool: install: /usr/bin/install c .libs/mesos /home/vagrant/mesos/build/mesos0.26.0/inst/bin/mesos  libtool: install: /usr/bin/install c .libs/mesosexecute /home/vagrant/mesos/build/mesos0.26.0/inst/bin/mesosexecute  libtool: install: /usr/bin/install c .libs/mesosresolve /home/vagrant/mesos/build/mesos0.26.0/inst/bin/mesosresolve  traceback (most recent call last):  file ""/"", line 1, in /  file ""/home/vagrant/mesos/build/mesos0.26.0/build/3rdparty/pip1.5.6/pip/init.py"", line 11, in /  from pip.vcs import git, mercurial, subversion, bazaar # noqa  file ""/home/vagrant/mesos/build/mesos0.26.0/build/3rdparty/pip1.5.6/pip/vcs/mercurial.py"", line 9, in /  from pip.download import pathtourl  file ""/home/vagrant/mesos/build/mesos0.26.0/build/3rdparty/pip1.5.6/pip/download.py"", line 22, in /  from pip.vendor import requests, six  file ""/home/vagrant/mesos/build/mesos0.26.0/build/3rdparty/pip1.5.6/pip/vendor/requests/init.py"", line 53, in /  from .packages.urllib3.contrib import pyopenssl  file ""/home/vagrant/mesos/build/mesos0.26.0/build/3rdparty/pip1.5.6/pip/vendor/requests/packages/urllib3/contrib/pyopenssl.py"", line 70, in /  ssl.protocolsslv3: openssl.ssl.sslv3method,  attributeerror: 'module' object has no attribute 'protocolsslv3'  traceback (most recent call last):  file ""/"", line 1, in /  file ""/home/vagrant/mesos/build/mesos0.26.0/_build/3rd    ",3,train
MESOS-3973,"Failing 'make distcheck' on Mac OS X 10.10.5, also 10.11.","nonroot 'make distcheck.      ...  [] global test environment teardown  [==========] 826 tests from 113 test cases ran. (276624 ms total)  [  passed  ] 826 tests.      you have 6 disabled tests    making install in .  make[3]: nothing to be done for `installexecam'.   ../installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/lib/pkgconfig'   /usr/bin/install c m 644 mesos.pc '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/lib/pkgconfig'  making install in 3rdparty  /applications/xcode.app/contents/developer/usr/bin/make  installrecursive  making install in libprocess  making install in 3rdparty  /applications/xcode.app/contents/developer/usr/bin/make  installrecursive  making install in stout  making install in .  make[9]: nothing to be done for `installexecam'.  make[9]: nothing to be done for `installdataam'.  making install in include  make[9]: nothing to be done for `installexecam'.   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/abort.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/attributes.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/base64.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/bits.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/bytes.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/cache.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/dynamiclibrary.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/error.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/exit.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/foreach.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/format.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/fs.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/gtest.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/hashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/hashset.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/interval.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/json.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/lambda.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/linkedhashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/list.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/mac.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/multihashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/multimap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/net.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/none.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/nothing.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/numify.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/path.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/preprocessor.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/proc.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/protobuf.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/recordio.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/result.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/chdir.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/close.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/constants.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/environment.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/getcwd.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/linux.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/mkdir.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/mktemp.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/open.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/os.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/permissions.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/realpath.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/rename.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/rm.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/stat.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/sysctl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/touch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/utime.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/write.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/posix'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/posix/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/posix/os.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/posix'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/flags'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/fetch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/flag.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/flags.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/parse.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/flags'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/tests'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/tests/utils.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/tests'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/windows'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/stat.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/windows'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/posix'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/stat.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/posix'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/set.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/some.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/stopwatch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/stringify.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/strings.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/subcommand.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/svn.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/synchronized.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/threadlocal.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/unimplemented.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/unreachable.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/utils.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/uuid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/windows'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/format.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/os.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/windows'   ../../../../../../3rdparty/libprocess/3rdparty/stout/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/raw'   /usr/bin/install c m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/raw/environment.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/stout/os/raw'  make[8]: nothing to be done for `installexecam'.  make[8]: nothing to be done for `installdataam'.  making install in .  make[6]: nothing to be done for `installexecam'.  make[6]: nothing to be done for `installdataam'.  making install in include  make[6]: nothing to be done for `installexecam'.   ../../../../3rdparty/libprocess/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include'   ../../../../3rdparty/libprocess/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process'   /usr/bin/install c m 644  ../../../../3rdparty/libprocess/include/process/address.hpp ../../../../3rdparty/libprocess/include/process/async.hpp ../../../../3rdparty/libprocess/include/process/check.hpp ../../../../3rdparty/libprocess/include/process/clock.hpp ../../../../3rdparty/libprocess/include/process/collect.hpp ../../../../3rdparty/libprocess/include/process/defer.hpp ../../../../3rdparty/libprocess/include/process/deferred.hpp ../../../../3rdparty/libprocess/include/process/delay.hpp ../../../../3rdparty/libprocess/include/process/dispatch.hpp ../../../../3rdparty/libprocess/include/process/event.hpp ../../../../3rdparty/libprocess/include/process/executor.hpp ../../../../3rdparty/libprocess/include/process/filter.hpp ../../../../3rdparty/libprocess/include/process/firewall.hpp ../../../../3rdparty/libprocess/include/process/future.hpp ../../../../3rdparty/libprocess/include/process/gc.hpp ../../../../3rdparty/libprocess/include/process/gmock.hpp ../../../../3rdparty/libprocess/include/process/gtest.hpp ../../../../3rdparty/libprocess/include/process/help.hpp ../../../../3rdparty/libprocess/include/process/http.hpp ../../../../3rdparty/libprocess/include/process/id.hpp ../../../../3rdparty/libprocess/include/process/io.hpp ../../../../3rdparty/libprocess/include/process/latch.hpp ../../../../3rdparty/libprocess/include/process/limiter.hpp ../../../../3rdparty/libprocess/include/process/logging.hpp ../../../../3rdparty/libprocess/include/process/message.hpp ../../../../3rdparty/libprocess/include/process/mime.hpp ../../../../3rdparty/libprocess/include/process/mutex.hpp ../../../../3rdparty/libprocess/include/process/network.hpp ../../../../3rdparty/libprocess/include/process/once.hpp ../../../../3rdparty/libprocess/include/process/owned.hpp ../../../../3rdparty/libprocess/include/process/pid.hpp ../../../../3rdparty/libprocess/include/process/process.hpp ../../../../3rdparty/libprocess/include/process/profiler.hpp ../../../../3rdparty/libprocess/include/process/protobuf.hpp ../../../../3rdparty/libprocess/include/process/queue.hpp ../../../../3rdparty/libprocess/include/process/reap.hpp ../../../../3rdparty/libprocess/include/process/run.hpp ../../../../3rdparty/libprocess/include/process/sequence.hpp ../../../../3rdparty/libprocess/include/process/shared.hpp ../../../../3rdparty/libprocess/include/process/socket.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process'   ../../../../3rdparty/libprocess/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process'   /usr/bin/install c m 644  ../../../../3rdparty/libprocess/include/process/statistics.hpp ../../../../3rdparty/libprocess/include/process/system.hpp ../../../../3rdparty/libprocess/include/process/subprocess.hpp ../../../../3rdparty/libprocess/include/process/time.hpp ../../../../3rdparty/libprocess/include/process/timeout.hpp ../../../../3rdparty/libprocess/include/process/timer.hpp ../../../../3rdparty/libprocess/include/process/timeseries.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process'   ../../../../3rdparty/libprocess/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process/ssl'   /usr/bin/install c m 644  ../../../../3rdparty/libprocess/include/process/ssl/gtest.hpp ../../../../3rdparty/libprocess/include/process/ssl/utilities.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process/ssl'   ../../../../3rdparty/libprocess/installsh c d '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process/metrics'   /usr/bin/install c m 644  ../../../../3rdparty/libprocess/include/process/metrics/counter.hpp ../../../../3rdparty/libprocess/include/process/metrics/gauge.hpp ../../../../3rdparty/libprocess/include/process/metrics/metric.hpp ../../../../3rdparty/libprocess/include/process/metrics/metrics.hpp ../../../../3rdparty/libprocess/include/process/metrics/timer.hpp '/users/bernd/mesos/mesos/build/mesos0.26.0/inst/include/process/metrics'  make[5]: nothing to be done for `installexecam'.  make[5]: nothing to be done for `installdataam'.  making install in src  /applications/xcode.app/contents/developer/usr/bin/make  installam  test ""../.."" = ""..""    \    (../../installsh c d python/cli/src/mesos && cp pf ../../src/python/cli/src/mesos/cli.py python/cli/src/mesos/cli.py)  test ""../.."" = ""..""    \    (../../installsh c d python/cli/src/mesos && cp pf ../../src/python/cli/src/mesos/http.py python/cli/src/mesos/http.py)  test ""../.."" = ""..""    \    (../../installsh c d python/interface/src/mesos/interface && cp pf ../../src/python/interface/src/mesos/interface/init.py python/interface/src/mesos/interface/init.py)  test ""../.."" = ""..""    \    (../../installsh c d python/interface/src/mesos/v1/interface && cp pf ../../src/python/interface/src/mesos/v1/interface/init.py python/interface/src/mesos/v1/interface/init_.py)  test ""../.."" = ""..""    \    (.....",2,train
MESOS-3975,SSL build of mesos causes flaky testsuite.,"when running the tests of an ssl build of mesos on centos 7.1, i see spurious test failures that are, so far, not reproducible.    the following tests did fail for me in complete runs but did seem fine when running them individually, in repetition.        dockertest.rootdockercheckportresource        containerizertest.rootcgroupsballoonframework        [ run      ] linuxfilesystemisolatortest.rootchangerootfilesystemcommandexecutor  20151120 19:08:38,826:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client   /home/vagrant/mesos/build/src/mesoscontainerizer mount help=false operation=makerslave path=/   grep e /tmp/linuxfilesystemisolatortestrootchangerootfilesystemcommandexecutortz7p8c/. /proc/self/mountinfo   grep v 2b98025c74f141d2b35ace2cdfae347e   cut 'd ' f5   xargs norunifempty umount l  + mount n rbind /tmp/linuxfilesystemisolatortestrootchangerootfilesystemcommandexecutortz7p8c/provisioner/containers/2b98025c74f141d2b35ace2cdfae347e/backends/copy/rootfses/bed11080474b4c698e7f0ab85e895b0d /tmp/linuxfilesystemisolatortestrootchangerootfilesystemcommandexecutortz7p8c/slaves/830e842ec36a4e4cbff45b9568d7df12s0/frameworks/830e842ec36a4e4cbff45b9568d7df120000/executors/c735be54c47f4645bfc12f4647e2cddb/runs/2b98025c74f141d2b35ace2cdfae347e/.rootfs  could not load cert file  ../../src/tests/containerizer/filesystemisolatortests.cpp:354: failure  value of: statusrunning.get().state()    actual: taskfailed  expected: taskrunning  20151120 19:08:42,164:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client  20151120 19:08:45,501:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client  20151120 19:08:48,837:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client  20151120 19:08:52,174:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client  ../../src/tests/containerizer/filesystemisolatortests.cpp:355: failure  failed to wait 15secs for statusfinished  ../../src/tests/containerizer/filesystemisolatortests.cpp:349: failure  actual function call count doesn't match expectcall(sched, statusupdate(&driver, ))...           expected: to be called twice             actual: called once  unsatisfied and active  20151120 19:08:55,511:21380(0x7fa10d5f2700):zooerror@handlesocketerrormsg@1697: socket [127.0.0.1:53444] zk retcode=4, errno=111(connection refused): server refused to accept the client   aborted at 1448046536 (unix time) try ""date d @1448046536"" if you are using gnu date   pc: @                0x0 (unknown)   sigsegv (@0x0) received by pid 21380 (tid 0x7fa1549e68c0) from pid 0; stack trace:       @     0x7fa141796fbb (unknown)      @     0x7fa14179b341 (unknown)      @     0x7fa14f096130 (unknown)        vagrantfile generator:    cat / vagrantfile  #  mode: ruby "" >  # vi: set ft=ruby :  vagrant.configure(2) do     # disable shared folder to prevent certain kernel module dependencies.    config.vm.syncedfolder ""."", ""/vagrant"", disabled: true      config.vm.hostname = ""centos71""      config.vm.box = ""bento/centos7.1""      config.vm.provider ""virtualbox"" do       vb.memory = 16384      vb.cpus = 8    end      config.vm.provider ""vmwarefusion"" do       vb.memory = 9216      vb.cpus = 4    end      config.vm.provision ""shell"", inline: <<shell         sudo yum y update systemd         sudo yum install y tar wget       sudo wget http:/repos.fedorapeople.org/repos/dchen/apachemaven/epelapachemaven.repo o /etc/yum.repos.d/epelapachemaven.repo         sudo yum groupinstall y ""development tools""       sudo yum install y apachemaven pythondevel java1.7.0openjdkdevel zlibdevel libcurldevel openssldevel cyrussasldevel cyrussaslmd5 aprdevel subversiondevel aprutildevel         sudo yum install libeventdevel         sudo yum install y git         sudo yum install y docker       sudo service docker start       sudo docker info         #sudo wget qo https:/get.docker.com/ | sh      shell  end  eof    vagrant up  vagrant reload    vagrant ssh c ""  git clone  https:/github.com/apache/mesos.git mesos  cd mesos  git checkout b 0.26.0rc1 0.26.0rc1    ./bootstrap  mkdir build  cd build    ../configure enablelibevent enablessl  gtest_filter="""" make check  sudo ./bin/mesos tests.sh  ""  ",5,train
MESOS-3976,C++ HTTP Scheduler Library does not work with SSL enabled,"the c http scheduler library does not work against mesos when ssl is enabled (without downgrade).    the fix should be simple:   the library should detect if ssl is enabled.   if ssl is enabled, connections should be made with https instead of http.",2,train
MESOS-3979,Replace `QuotaInfo` with `Quota` in allocator interface,"after introduction of c wrapper `quota` for `quotainfo`, all allocator methods using `quotainfo` should be updated.",3,train
MESOS-3981,Implement recovery in the Hierarchical allocator,the built in hierarchical allocator should implement the recovery (in the presence of quota).,3,train
MESOS-3983,Tests for quota request validation,tests should include:   json validation;   absence of irrelevant fields;    semantic validation.,3,train
MESOS-3984,Tests for quota support in `allocate()` function.,nan,3,train
MESOS-3985,Tests for rescinding offers for quota,nan,1,train
MESOS-3986,Tests for allocator recovery,nan,5,train
MESOS-3994,Refactor registry client/puller to avoid JSON and struct.,"we should get rid of all json and struct for message passing as function returned type. by using the methods provided by spec.hpp to refactor all unnecessary json message and struct in registry client and registry puller. also, remove all redundant check in registry client that are already checked by spec validation. ",3,train
MESOS-3996,"libprocess: document when, why defer() is necessary","current rules around this are pretty confusing and undocumented, as evidenced by some recent bugs in this area.    some example snippets in the mesos source code that were a result of this confusion and are indeed bugs:    1. https:/github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/provisioner/docker/registryclient.cpp#l754    return dohttpget(bloburl, none(), true, true, none())      .then([this, bloburlpath, digest, filepath](          const http::response& response)  > future/   ",1,train
MESOS-4000,Implicit roles: Design Doc,nan,2,train
MESOS-4002,ReservationEndpointsTest.UnreserveAvailableAndOfferedResources is flaky,"showed up on asf ci: ( test kept looping on and on and ultimately failing the build after 300 minutes )  https:/builds.apache.org/job/mesos/compiler=gcc,configuration=verbose,os=ubuntu%3a14.04,labelexp=docker%7c%7chadoop/1269/changes      [ run      ] reservationendpointstest.unreserveavailableandofferedresources  i1124 01:07:20.050729 30260 leveldb.cpp:174] opened db in 107.434842ms  i1124 01:07:20.099630 30260 leveldb.cpp:181] compacted db in 48.82312ms  i1124 01:07:20.099722 30260 leveldb.cpp:196] created db iterator in 29905ns  i1124 01:07:20.099738 30260 leveldb.cpp:202] seeked to beginning of db in 3145ns  i1124 01:07:20.099750 30260 leveldb.cpp:271] iterated through 0 keys in the db in 279ns  i1124 01:07:20.099804 30260 replica.cpp:778] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1124 01:07:20.100637 30292 recover.cpp:447] starting replica recovery  i1124 01:07:20.100934 30292 recover.cpp:473] replica is in empty status  i1124 01:07:20.103240 30288 replica.cpp:674] replica in empty status received a broadcasted recover request from (6305)@172.17.18.107:37993  i1124 01:07:20.103672 30292 recover.cpp:193] received a recover response from a replica in empty status  i1124 01:07:20.104142 30292 recover.cpp:564] updating replica status to starting  i1124 01:07:20.114534 30284 master.cpp:365] master ad27bc6016d142399a65235a991f9600 (9f2f81738d5e) started on 172.17.18.107:37993  i1124 01:07:20.114558 30284 master.cpp:367] flags at startup: acls="""" allocationinterval=""1000secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/i60i5f/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" roles=""role"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.26.0/inst/share/mesos/webui"" workdir=""/tmp/i60i5f/master"" zksessiontimeout=""10secs""  i1124 01:07:20.114809 30284 master.cpp:412] master only allowing authenticated frameworks to register  i1124 01:07:20.114820 30284 master.cpp:417] master only allowing authenticated slaves to register  i1124 01:07:20.114825 30284 credentials.hpp:35] loading credentials for authentication from '/tmp/i60i5f/credentials'  i1124 01:07:20.115067 30284 master.cpp:456] using default 'crammd5' authenticator  i1124 01:07:20.115320 30284 master.cpp:493] authorization enabled  i1124 01:07:20.115792 30285 hierarchical.cpp:162] initialized hierarchical allocator process  i1124 01:07:20.115855 30285 whitelistwatcher.cpp:77] no whitelist given  i1124 01:07:20.118755 30285 master.cpp:1625] the newly elected leader is master@172.17.18.107:37993 with id ad27bc6016d142399a65235a991f9600  i1124 01:07:20.118788 30285 master.cpp:1638] elected as the leading master!  i1124 01:07:20.118809 30285 master.cpp:1383] recovering from registrar  i1124 01:07:20.119078 30285 registrar.cpp:307] recovering registrar  i1124 01:07:20.143256 30292 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 38.787419ms  i1124 01:07:20.143347 30292 replica.cpp:321] persisted replica status to starting  i1124 01:07:20.143717 30292 recover.cpp:473] replica is in starting status  i1124 01:07:20.145454 30286 replica.cpp:674] replica in starting status received a broadcasted recover request from (6307)@172.17.18.107:37993  i1124 01:07:20.145979 30292 recover.cpp:193] received a recover response from a replica in starting status  i1124 01:07:20.146654 30292 recover.cpp:564] updating replica status to voting  i1124 01:07:20.182672 30286 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 35.422256ms  i1124 01:07:20.182747 30286 replica.cpp:321] persisted replica status to voting  i1124 01:07:20.182929 30286 recover.cpp:578] successfully joined the paxos group  i1124 01:07:20.183115 30286 recover.cpp:462] recover process terminated  i1124 01:07:20.183831 30286 log.cpp:659] attempting to start the writer  i1124 01:07:20.185907 30285 replica.cpp:494] replica received implicit promise request from (6308)@172.17.18.107:37993 with proposal 1  i1124 01:07:20.225256 30285 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 39.291288ms  i1124 01:07:20.225344 30285 replica.cpp:343] persisted promised to 1  i1124 01:07:20.226850 30286 coordinator.cpp:238] coordinator attempting to fill missing positions  i1124 01:07:20.228394 30293 replica.cpp:389] replica received explicit promise request from (6309)@172.17.18.107:37993 for position 0 with proposal 2  i1124 01:07:20.266371 30293 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 37.874181ms  i1124 01:07:20.266456 30293 replica.cpp:713] persisted action at 0  i1124 01:07:20.267927 30293 replica.cpp:538] replica received write request for position 0 from (6310)@172.17.18.107:37993  i1124 01:07:20.268002 30293 leveldb.cpp:436] reading position from leveldb took 37139ns  i1124 01:07:20.308117 30293 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 39.961976ms  i1124 01:07:20.308205 30293 replica.cpp:713] persisted action at 0  i1124 01:07:20.309033 30290 replica.cpp:692] replica received learned notice for position 0 from @0.0.0.0:0  i1124 01:07:20.343257 30290 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 34.175337ms  i1124 01:07:20.343343 30290 replica.cpp:713] persisted action at 0  i1124 01:07:20.343377 30290 replica.cpp:698] replica learned nop action at position 0  i1124 01:07:20.344446 30282 log.cpp:675] writer started with ending position 0  i1124 01:07:20.346143 30291 leveldb.cpp:436] reading position from leveldb took 56896ns  i1124 01:07:20.347618 30291 registrar.cpp:340] successfully fetched the registry (0b) in 228.495104ms  i1124 01:07:20.347862 30291 registrar.cpp:439] applied 1 operations in 41164ns; attempting to update the 'registry'  i1124 01:07:20.348794 30279 log.cpp:683] attempting to append 178 bytes to the log  i1124 01:07:20.349081 30279 coordinator.cpp:348] coordinator attempting to write append action at position 1  i1124 01:07:20.350244 30294 replica.cpp:538] replica received write request for position 1 from (6311)@172.17.18.107:37993  i1124 01:07:20.385246 30294 leveldb.cpp:341] persisting action (197 bytes) to leveldb took 34.872508ms  i1124 01:07:20.385323 30294 replica.cpp:713] persisted action at 1  i1124 01:07:20.386814 30294 replica.cpp:692] replica received learned notice for position 1 from @0.0.0.0:0  i1124 01:07:20.425163 30294 leveldb.cpp:341] persisting action (199 bytes) to leveldb took 38.282493ms  i1124 01:07:20.425262 30294 replica.cpp:713] persisted action at 1  i1124 01:07:20.425298 30294 replica.cpp:698] replica learned append action at position 1  i1124 01:07:20.427994 30287 registrar.cpp:484] successfully updated the 'registry' in 79.949056ms  i1124 01:07:20.428141 30283 log.cpp:702] attempting to truncate the log to 1  i1124 01:07:20.428738 30287 registrar.cpp:370] successfully recovered registrar  i1124 01:07:20.429306 30290 master.cpp:1435] recovered 0 slaves from the registry (139b) ; allowing 10mins for slaves to reregister  i1124 01:07:20.429592 30290 hierarchical.cpp:174] allocator recovery is not supported yet  i1124 01:07:20.430083 30294 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i1124 01:07:20.431411 30294 replica.cpp:538] replica received write request for position 2 from (6312)@172.17.18.107:37993  i1124 01:07:20.467258 30294 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 35.661978ms  i1124 01:07:20.467342 30294 replica.cpp:713] persisted action at 2  i1124 01:07:20.468842 30290 replica.cpp:692] replica received learned notice for position 2 from @0.0.0.0:0  i1124 01:07:20.502264 30290 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 33.367074ms  i1124 01:07:20.502426 30290 leveldb.cpp:399] deleting ~1 keys from leveldb took 80765ns  i1124 01:07:20.502452 30290 replica.cpp:713] persisted action at 2  i1124 01:07:20.502488 30290 replica.cpp:698] replica learned truncate action at position 2  i1124 01:07:20.510509 30260 containerizer.cpp:141] using isolation: posix/cpu,posix/mem,filesystem/posix  w1124 01:07:20.511119 30260 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i1124 01:07:20.516801 30288 slave.cpp:189] slave started on 219)@172.17.18.107:37993  i1124 01:07:20.516839 30288 slave.cpp:190] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.26.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr""  i1124 01:07:20.517670 30288 credentials.hpp:83] loading credential for authentication from '/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr/credential'  i1124 01:07:20.517982 30288 slave.cpp:320] slave using credential for: testprincipal  i1124 01:07:20.518334 30288 resources.cpp:472] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i1124 01:07:20.518815 30260 resources.cpp:472] parsing resources as json failed: cpus:1;mem:128  trying semicolondelimited string format instead  i1124 01:07:20.518975 30288 slave.cpp:390] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1124 01:07:20.519104 30288 slave.cpp:398] slave attributes: [  ]  i1124 01:07:20.519124 30288 slave.cpp:403] slave hostname: 9f2f81738d5e  i1124 01:07:20.519136 30288 slave.cpp:408] slave checkpoint: true  i1124 01:07:20.519407 30260 resources.cpp:472] parsing resources as json failed: mem:384  trying semicolondelimited string format instead  i1124 01:07:20.522702 30288 state.cpp:52] recovering state from '/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr/meta'  i1124 01:07:20.523265 30288 statusupdatemanager.cpp:200] recovering status update manager  i1124 01:07:20.523531 30288 containerizer.cpp:383] recovering containerizer  i1124 01:07:20.524998 30288 slave.cpp:4258] finished recovery  i1124 01:07:20.525802 30288 slave.cpp:4430] querying resource estimator for oversubscribable resources  i1124 01:07:20.526753 30288 slave.cpp:727] new master detected at master@172.17.18.107:37993  i1124 01:07:20.527292 30288 slave.cpp:790] authenticating with master master@172.17.18.107:37993  i1124 01:07:20.528240 30288 slave.cpp:795] using default crammd5 authenticatee  i1124 01:07:20.527003 30286 statusupdatemanager.cpp:174] pausing sending status updates  i1124 01:07:20.528955 30285 authenticatee.cpp:121] creating new client sasl connection  i1124 01:07:20.529469 30285 master.cpp:5169] authenticating slave(219)@172.17.18.107:37993  i1124 01:07:20.529729 30283 authenticator.cpp:413] starting authentication session for crammd5authenticatee(515)@172.17.18.107:37993  i1124 01:07:20.530287 30283 authenticator.cpp:98] creating new server sasl connection  i1124 01:07:20.530764 30285 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i1124 01:07:20.530903 30285 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i1124 01:07:20.531096 30285 authenticator.cpp:203] received sasl authentication start  i1124 01:07:20.531241 30285 authenticator.cpp:325] authentication requires more steps  i1124 01:07:20.531388 30285 authenticatee.cpp:258] received sasl authentication step  i1124 01:07:20.531616 30285 authenticator.cpp:231] received sasl authentication step  i1124 01:07:20.531668 30285 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '9f2f81738d5e' server fqdn: '9f2f81738d5e' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1124 01:07:20.531690 30285 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i1124 01:07:20.531774 30285 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i1124 01:07:20.531834 30285 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '9f2f81738d5e' server fqdn: '9f2f81738d5e' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1124 01:07:20.531855 30285 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1124 01:07:20.531867 30285 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1124 01:07:20.531903 30285 authenticator.cpp:317] authentication success  i1124 01:07:20.532016 30283 authenticatee.cpp:298] authentication success  i1124 01:07:20.532331 30281 master.cpp:5199] successfully authenticated principal 'testprincipal' at slave(219)@172.17.18.107:37993  i1124 01:07:20.532652 30291 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(515)@172.17.18.107:37993  i1124 01:07:20.533113 30288 slave.cpp:763] detecting new master  i1124 01:07:20.533628 30288 slave.cpp:4444] received oversubscribable resources  from the resource estimator  i1124 01:07:20.546396 30288 slave.cpp:858] successfully authenticated with master master@172.17.18.107:37993  i1124 01:07:20.547111 30287 master.cpp:3878] registering slave at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with id ad27bc6016d142399a65235a991f9600s0  i1124 01:07:20.547886 30287 registrar.cpp:439] applied 1 operations in 91121ns; attempting to update the 'registry'  i1124 01:07:20.550647 30287 log.cpp:683] attempting to append 347 bytes to the log  i1124 01:07:20.550935 30279 coordinator.cpp:348] coordinator attempting to write append action at position 3  i1124 01:07:20.551534 30288 slave.cpp:1252] will retry registration in 3.399312ms if necessary  i1124 01:07:20.551868 30291 replica.cpp:538] replica received write request for position 3 from (6324)@172.17.18.107:37993  i1124 01:07:20.557605 30281 slave.cpp:1252] will retry registration in 16.296866ms if necessary  i1124 01:07:20.557891 30293 master.cpp:3866] ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress  i1124 01:07:20.574681 30279 slave.cpp:1252] will retry registration in 73.52632ms if necessary  i1124 01:07:20.575078 30293 master.cpp:3866] ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress  i1124 01:07:20.586236 30291 leveldb.cpp:341] persisting action (366 bytes) to leveldb took 34.301173ms  i1124 01:07:20.586287 30291 replica.cpp:713] persisted action at 3  i1124 01:07:20.587509 30289 replica.cpp:692] replica received learned notice for position 3 from @0.0.0.0:0  i1124 01:07:20.611263 30289 leveldb.cpp:341] persisting action (368 bytes) to leveldb took 23.677211ms  i1124 01:07:20.611352 30289 replica.cpp:713] persisted action at 3  i1124 01:07:20.611387 30289 replica.cpp:698] replica learned append action at position 3  i1124 01:07:20.613580 30279 registrar.cpp:484] successfully updated the 'registry' in 65.490944ms  i1124 01:07:20.613802 30288 log.cpp:702] attempting to truncate the log to 3  i1124 01:07:20.613993 30288 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i1124 01:07:20.615281 30289 replica.cpp:538] replica received write request for position 4 from (6325)@172.17.18.107:37993  i1124 01:07:20.615883 30279 master.cpp:3946] registered slave ad27bc6016d142399a65235a991f9600s0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i1124 01:07:20.616261 30282 slave.cpp:902] registered with master master@172.17.18.107:37993; given slave id ad27bc6016d142399a65235a991f9600s0  i1124 01:07:20.616883 30282 fetcher.cpp:79] clearing fetcher cache  i1124 01:07:20.617261 30280 statusupdatemanager.cpp:181] resuming sending status updates  i1124 01:07:20.617766 30282 slave.cpp:925] checkpointing slaveinfo to '/tmp/reservationendpointstestunreserveavailableandofferedresourcescszecr/meta/slaves/ad27bc6016d142399a65235a991f9600s0/slave.info'  i1124 01:07:20.616550 30284 hierarchical.cpp:380] added slave ad27bc6016d142399a65235a991f9600s0 (9f2f81738d5e) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i1124 01:07:20.618670 30282 slave.cpp:961] forwarding total oversubscribed resources   i1124 01:07:20.618932 30282 slave.cpp:3197] received ping from slaveobserver(216)@172.17.18.107:37993  i1124 01:07:20.619288 30285 master.cpp:4288] received update of slave ad27bc6016d142399a65235a991f9600s0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with total oversubscribed resources   i1124 01:07:20.619446 30284 hierarchical.cpp:1066] no resources available to allocate!  i1124 01:07:20.619526 30284 hierarchical.cpp:1159] no inverse offers to send out!  i1124 01:07:20.619568 30284 hierarchical.cpp:977] performed allocation for slave ad27bc6016d142399a65235a991f9600s0 in 1.108641ms  i1124 01:07:20.620057 30284 hierarchical.cpp:436] slave ad27bc6016d142399a65235a991f9600s0 (9f2f81738d5e) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports( ):[3100032000], allocated: )  i1124 01:07:20.620393 30284 hierarchical.cpp:1066] no resources available to allocate!  i1124 01:07:20.620462 30284 hierarchical.cpp:1159] no inverse offers to send out!  i1124 01:07:20.620507 30284 hierarchical.cpp:977] performed allocation for slave ad27bc6016d142399a65235a991f9600 s0 in 395959ns  i1124 01:07:20.624356 30285 process.cpp:3067] handling http event for process 'master' with path: '/master/reserve'  i1124 01:07:20.624418 30285 http.cpp:336] http post for /master/reserve from 172.17.18.107:48995  i1124 01:07:20.626936 30285 master.cpp:6224] sending checkpointed resources cpus(role, te...",1,train
MESOS-4003,Pass agent work_dir to isolator modules,"some isolator modules can benefit from access to the agent's workdir. for example, the dvd isolator (https:/github.com/emccode/mesosmoduledvdi) is currently forced to mount external volumes in a hard coded directory. making the workdir accessible to the isolator via isolator::recover() would allow the isolator to mount volumes within the agent's workdir. this can be accomplished by simply adding an overloaded signature for isolator::recover() which includes the workdir as a parameter.",1,train
MESOS-4004,Support default entrypoint and command runtime config in Mesos containerizer,we need to use the entrypoint and command runtime configuration returned from image to be used in mesos containerizer.,3,train
MESOS-4005,Support workdir runtime configuration from image ,we need to support workdir runtime configuration returned from image such as dockerfile.,2,train
MESOS-4009,RegistryClientTest.SimpleRegistryPuller doesn't compile with GCC 5.1.1,gcc 5.1.1 has werror=signcompare in  wall and stumbles over a comparison between signed and unsigned int in provisionerdockertests.cpp.,1,train
MESOS-4013,Introduce status endpoint for quota,this endpoint is for querying quota status via the get method.,5,train
MESOS-4014,Introduce remove endpoint for quota,this endpoint is for removing quotas via the delete method.,3,train
MESOS-4020,Introduce filter for non-revocable resources in `Resources`,"resources class defines some handy filters, like revocable(), unreserved(), and so on. this ticket proposes to add one more: nonrevocable().",1,train
MESOS-4021,Remove quota from Registry for quota remove request,"when a remove quota requests hits the endpoint and passes validation, quota should be removed from the registry before the allocator is notified about the change.",1,train
MESOS-4026,RegistryClientTest.SimpleRegistryPuller is flaky,"from asf ci:  https:/builds.apache.org/job/mesos/1289/compiler=gcc,configuration=verbose%20enablelibevent%20enablessl,os=centos:7,labelexp=docker%7c%7chadoop/console      [ run      ] registryclienttest.simpleregistrypuller  i1127 02:51:40.235900   362 registryclient.cpp:511] response status for url 'https:/localhost:57828/v2/library/busybox/manifests/latest': 401 unauthorized  i1127 02:51:40.249766   360 registryclient.cpp:511] response status for url 'https:/localhost:57828/v2/library/busybox/manifests/latest': 200 ok  i1127 02:51:40.251137   361 registrypuller.cpp:195] downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'  i1127 02:51:40.258514   354 registryclient.cpp:511] response status for url 'https:/localhost:57828/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 temporary redirect  i1127 02:51:40.264171   367 libeventsslsocket.cpp:1023] socket error: connection reset by peer  ../../src/tests/containerizer/provisionerdockertests.cpp:1210: failure  (socket).failure(): failed accept: connection error: connection reset by peer  [  failed  ] registryclienttest.simpleregistrypuller (349 ms)      logs from a previous run that passed:    [ run      ] registryclienttest.simpleregistrypuller  i1126 18:49:05.306396   349 registryclient.cpp:511] response status for url 'https:/localhost:53492/v2/library/busybox/manifests/latest': 401 unauthorized  i1126 18:49:05.321362   347 registryclient.cpp:511] response status for url 'https:/localhost:53492/v2/library/busybox/manifests/latest': 200 ok  i1126 18:49:05.322720   352 registrypuller.cpp:195] downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'  i1126 18:49:05.331317   350 registryclient.cpp:511] response status for url 'https:/localhost:53492/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 temporary redirect  i1126 18:49:05.370625   352 registryclient.cpp:511] response status for url 'https:/127.0.0.1:53492/': 200 ok  i1126 18:49:05.372102   355 registrypuller.cpp:294] untarring layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' downloaded from registry to directory 'outputdir'  [       ok ] registryclienttest.simpleregistrypuller (353 ms)  ",4,train
MESOS-4029,ContentType/SchedulerTest is flaky.,"ssl build, https:/github.com/tillt/mesosvagrantci/blob/master/ubuntu14/setup.sh, nonroot test run.      [] 22 tests from contenttype/schedulertest  [ run      ] contenttype/schedulertest.subscribe/0  [       ok ] contenttype/schedulertest.subscribe/0 (48 ms)   aborted at 1448928007 (unix time) try ""date d @1448928007"" if you are using gnu date   [ run      ] contenttype/schedulertest.subscribe/1  pc: @          0x1451b8e testing::internal::untypedfunctionmockerbase::untypedinvokewith()   sigsegv (@0x100000030) received by pid 21320 (tid 0x2b549e5d4700) from pid 48; stack trace:       @     0x2b54c95940b7 os::linux::chainedhandler()      @     0x2b54c9598219 jvmhandlelinuxsignal      @     0x2b5496300340 (unknown)      @          0x1451b8e testing::internal::untypedfunctionmockerbase::untypedinvokewith()      @           0xe2ea6d zn7testing8internal18functionmockerbaseifvrkst5queuein5mesos2v19scheduler5eventest5dequeis6sais6eeeee10invokewitherkst5tupleijscee      @           0xe2b1bc testing::internal::functionmocker/::invoke()      @          0x1118aed mesos::internal::tests::schedulertest::callbacks::received()      @          0x111c453 znkst7memfnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins02v19scheduler5eventest5dequeis8sais8eeeeeclijseeveevrs4dpot      @          0x111c001 znst5bindifst7memfnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins12v19scheduler5eventest5dequeis9sais9eeeeest17referencewrapperis5est12placeholderili1eeee6callivjsfejlm0elm1eeeetost5tupleijdpt0eest12indextupleijxspt1eee      @          0x111b90d znst5bindifst7memfnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins12v19scheduler5eventest5dequeis9sais9eeeeest17referencewrapperis5est12placeholderili1eeeeclijsfeveet0dpot      @          0x111ae09 std::functionhandler/::minvoke()      @     0x2b5493c6da09 std::function/::operator()()      @     0x2b5493c688ee process::asyncexecutorprocess::execute/()      @     0x2b5493c6db2a zzn7process8dispatchi7nothingns20asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeis8sais8eeeeescpvsgscsjeens6futureiteerkns3pidit0eemsofslt1t2t3et4t5t6enkulpns11processbaseeecles11      @     0x2b5493c765a4 znst17functionhandlerifvpn7process11processbaseeezns08dispatchi7nothingns020asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeiscsaisceeeeesgpvsksgsneens06futureiteerkns03pidit0eemssfspt1t2t3et4t5t6euls2ee9minvokeerkst9anydatas2      @     0x2b54946b1201 std::function/::operator()()      @     0x2b549469960f process::processbase::visit()      @     0x2b549469d480 process::dispatchevent::visit()      @           0x9dc0ba process::processbase::serve()      @     0x2b54946958cc process::processmanager::resume()      @     0x2b5494692a9c zzn7process14processmanager12initthreadsevenkulrkst11atomicboolecles3      @     0x2b549469ccac znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eee6callivieilm0eeeetost5tupleiidpt0eest12indextupleiixspt1eee      @     0x2b549469cc5c znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eeecliieveet0dpot      @     0x2b549469cbee znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeevee9minvokeiieeevst12indextupleiixspteee      @     0x2b549469cb45 znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeeveeclev      @     0x2b549469cade znst6thread5implist12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis6eeeveee6mrunev      @     0x2b5495b81a40 (unknown)      @     0x2b54962f8182 start_thread      @     0x2b549660847d (unknown)  make[3]:  [checklocal] segmentation fault  make[3]: leaving directory `/home/vagrant/mesos/build/src'  make[2]:  [checkam] error 2  make[2]: leaving directory `/home/vagrant/mesos/build/src'  make[1]:  [check] error 2  make[1]: leaving directory `/home/vagrant/mesos/build/src'  make:  [check recursive] error 1  ",2,train
MESOS-4036,Install instructions for CentOS 6.6 lead to errors running `perf`,"after using the current installation instructions in the getting started documentation, perf will not run on centos 6.6 because the version of elfutils included in devtoolset2 is not compatible with the version of perf installed by yum. installing and using devtoolset3, however (http:/linux.web.cern.ch/linux/scientific6/docs/softwarecollections.shtml) fixes this issue. this could be resolved by updating the getting started documentation to recommend installing devtoolset 3.",1,train
MESOS-4046,Enable `Env` specified in docker image can be returned from docker pull,"currently docker pull only return an image structure, which only contains entrypoint info. we have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. we should be able to support returning environment variables information from the image.",3,train
MESOS-4047,MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky,"  [] 1 test from memorypressuremesostest  10 records in  10 records out  1048576 bytes (1.0 mb) copied, 0.000430889 s, 2.4 gb/s  [ run      ] memorypressuremesostest.cgroupsrootslaverecovery  i1202 11:09:14.319327  5062 exec.cpp:134] version: 0.27.0  i1202 11:09:14.333317  5079 exec.cpp:208] executor registered on slave bea15b359aa14b5796fb29b5f70638acs0  registered executor on ubuntu  starting task 4e62294ccfcf4a13b699c6a4b7ac5162  sh c 'while true; do dd count=512 bs=1m if=/dev/zero of=./temp; done'  forked command at 5085  i1202 11:09:14.391739  5077 exec.cpp:254] received reconnect request from slave bea15b359aa14b5796fb29b5f70638acs0  i1202 11:09:14.398598  5082 exec.cpp:231] executor reregistered on slave bea15b359aa14b5796fb29b5f70638acs0  reregistered executor on ubuntu  shutting down  sending sigterm to process tree at pid 5085  killing the following process trees:  [    5085 sh c while true; do dd count=512 bs=1m if=/dev/zero of=./temp; done    \ 5086 dd count=512 bs=1m if=/dev/zero of=./temp   ]  [       ok ] memorypressuremesostest.cgroupsrootslaverecovery (1096 ms)        [] 1 test from memorypressuremesostest  10 records in  1+0 records out  1048576 bytes (1.0 mb) copied, 0.000404489 s, 2.6 gb/s  [ run      ] memorypressuremesostest.cgroupsrootslaverecovery  i1202 11:09:15.509950  5109 exec.cpp:134] version: 0.27.0  i1202 11:09:15.568183  5123 exec.cpp:208] executor registered on slave 88734acc718e45b095b9d8f07cea8a9es0  registered executor on ubuntu  starting task 14b6bab99f604130bdc444efba262bc6  forked command at 5132  sh c 'while true; do dd count=512 bs=1m if=/dev/zero of=./temp; done'  i1202 11:09:15.665498  5129 exec.cpp:254] received reconnect request from slave 88734acc718e45b095b9d8f07cea8a9es0  i1202 11:09:15.670995  5123 exec.cpp:381] executor asked to shutdown  shutting down  sending sigterm to process tree at pid 5132  ../../src/tests/containerizer/memorypressuretests.cpp:283: failure  (usage).failure(): unknown container: ebe90e1572fa4519837b62f43052c913   aborted at 1449083355 (unix time) try ""date d @1449083355"" if you are using gnu date       notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",1,train
MESOS-4053,MemoryPressureMesosTest tests fail on CentOS 6.6,"memorypressuremesostest.cgroupsrootstatistics and memorypressuremesostest.cgroupsrootslaverecovery fail on centos 6.6. it seems that mounted cgroups are not properly cleaned up after previous tests, so multiple hierarchies are detected and thus an error is produced:      [ run      ] memorypressuremesostest.cgroupsrootstatistics  ../../src/tests/mesos.cpp:849: failure  value of: basehierarchy.get()    actual: ""/cgroup""  expected: basehierarchy  which is: ""/tmp/mesostestcgroup""    multiple cgroups base hierarchies detected:    '/tmp/mesostestcgroup'    '/cgroup'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    ../../src/tests/mesos.cpp:932: failure  (cgroups::destroy(hierarchy, cgroup)).failure(): failed to remove cgroup '/tmp/mesostestcgroup/perfevent/mesostest': device or resource busy  [  failed  ] memorypressuremesostest.cgroupsrootstatistics (12 ms)  [ run      ] memorypressuremesostest.cgroupsrootslaverecovery  ../../src/tests/mesos.cpp:849: failure  value of: basehierarchy.get()    actual: ""/cgroup""  expected: basehierarchy  which is: ""/tmp/mesostestcgroup""    multiple cgroups base hierarchies detected:    '/tmp/mesostestcgroup'    '/cgroup'  mesos does not support multiple cgroups base hierarchies.  please unmount the corresponding (or all) subsystems.    ../../src/tests/mesos.cpp:932: failure  (cgroups::destroy(hierarchy, cgroup)).failure(): failed to remove cgroup '/tmp/mesostestcgroup/perfevent/mesostest': device or resource busy  [  failed  ] memorypressuremesostest.cgroupsrootslaverecovery (7 ms)  ",3,train
MESOS-4056,Respond with `MethodNotAllowed` if a request uses an unsupported method.,we are inconsistent right now in how we respond to endpoint requests with unsupported methods: both methodnotallowed and badrequest are used. we are also not consistent in the error message we include in the body.    this ticket proposes use methodnotallowed with standardized message text.,1,train
MESOS-4058,Do not use `Resource.role` for resources in quota request.,"to be consistent with other operator endpoints and to adhere to the principal of least surprise, move role from each resource in quota set request to the request itself.     resource.role is used for reserved resources. since quota is not a direct reservation request, to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved.    food for thought: shall we try to keep internal storage protobufs as close as possible to operator's json to provide some sort of a schema or decouple those two for the sake of flexibility?",1,train
MESOS-4059,Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters,"per comments in mesos3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur  should be investigated.    flakiness in task acknowledgment    i1203 18:25:04.609817 28732 statusupdatemanager.cpp:392] received status update acknowledgement (uuid: 6afd012e8e8841b28239a9b852d07ca1) for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000  w1203 18:25:04.610076 28732 statusupdatemanager.cpp:762] unexpected status update acknowledgement (received 6afd012e8e8841b28239a9b852d07ca1, expecting 82fc7a7be64a4f4dab7476abac42b4e6) for update taskrunning (uuid: 82fc7a7be64a4f4dab7476abac42b4e6) for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000  e1203 18:25:04.610339 28736 slave.cpp:2339] failed to handle status update acknowledgement (uuid: 6afd012e8e8841b28239a9b852d07ca1) for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000: duplicate acknowledgemen      this is a race between https:/github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/mastermaintenance_tests.cpp#l1486l1517.  the status updates for each task are not necessarily received in the same order as launching the tasks.    flakiness in first inverse offer filter  see https:/issues.apache.org/jira/browse/mesos3916?focusedcommentid=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment 15027478 for the explanation.  the related logs are above the comment.",1,train
MESOS-4064,Add ContainerInfo to internal Task protobuf.,"in what seems like an oversight, when containerinfo was added to taskinfo, it was not added to our internal task protobuf.    also, unlike the agent, it appears that the master does not use protobuf::createtask. we should try remove the manual construction in the master in favor of construction through protobuf::createtask.    partial contents of containerinfo should be exposed through state endpoints on the master and the agent.  ",3,train
MESOS-4066,Agent should not return partial state when a request is made to /state endpoint during recovery.,"currently when a user is hitting /state.json on the agent, it may return partial state if the agent has failed over and is recovering. there is currently no clear way to tell if this is the case when looking at a response, so the user may incorrectly interpret the agent as being empty of tasks.    we could consider exposing the 'state' enum of the agent in the endpoint:        enum state     state;      this may be a bit tricky to maintain as far as backwardscompatibility of the endpoint, if we were to alter this enum.    exposing this would allow users to be more informed about the state of the agent.",3,train
MESOS-4067,ReservationTest.ACLMultipleOperations is flaky,"observed from the ci: https:/builds.apache.org/job/mesos/compiler=gcc,configuration=verbose%20enablelibevent%20enablessl,os=ubuntu%3a14.04,label_exp=docker%7c%7chadoop/1319/changes",2,train
MESOS-4069,libevent_ssl_socket assertion fails ,"have been seeing the following socket  receive error frequently:      f1204 11:12:47.301839 54104 libeventsslsocket.cpp:245] check failed: length > 0    check failure stack trace:       @     0x7f73227fe5a6  google::logmessage::fail()      @     0x7f73227fe4f2  google::logmessage::sendtolog()      @     0x7f73227fdef4  google::logmessage::flush()      @     0x7f7322800e08  google::logmessagefatal::logmessagefatal()      @     0x7f73227b93e2  process::network::libeventsslsocketimpl::recvcallback()      @     0x7f73227b9182  process::network::libeventsslsocketimpl::recvcallback()      @     0x7f731cbc75cc  buffereventrundeferredcallbackslocked      @     0x7f731cbbdc5d  eventbaseloop      @     0x7f73227d9ded  process::eventloop::run()      @     0x7f73227a3101  znst12bindsimpleifpfvvevee9minvokeijeeevst12indextupleijxspteee      @     0x7f73227a305b  std::bindsimple/::operator()()      @     0x7f73227a2ff4  std::thread::impl/::mrun()      @     0x7f731e0d1a40  (unknown)      @     0x7f731de0a182  startthread      @     0x7f731db3730d  (unknown)      @              (nil)  (unknown)        in this case this was a http get over ssl. the url being:    https:/dseasb33srnrn.cloudfront.net:443/registryv2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?expires=1449259252&signature=q4cqdr1lbxsiyyvebmetrxlqdgqfhvkgxpbmm3poisn6r07dxizbx6tl1izx9uxdfr5awh8kxwhy8b0dtv3mltzavlnezlhbhbax9qbymd180qvuvrfezwolsmx4b3idvozk0caruu3ev1hbjz5y3olwe2zc~rxhewzkq&keypairid=apkajech5m7vwis5yz6q      steps to reproduce:    1. run master  2. run slave from your build directory as  as:       glogv=1;sslenabled=1;sslkeyfile=/;sslcertfile=/;sudo e ./bin/mesosslave.sh \        master=127.0.0.1:5050 \                                                          executorregistrationtimeout=5mins \                                            containerizers=mesos  \                                                          isolation=filesystem/linux \                                                     imageproviders=docker  \                                                        dockerpullertimeout=600 \                                                      launcherdir=$mesosbuilddir/src/.libs \                                        switchuser=""false"" \                                                            dockerpuller=""registry""                 3. run mesosexecute from your build directory as :                                                                  ./src/mesosexecute \                                                              master=127.0.0.1:5050 \                                                          command=""uname a""  \                                                            name=test \                                                                       dockerimage=ubuntu   ",8,train
MESOS-4073,Expose recovery parameters from Hierarchical allocator,"while implementing recovery in the hierarchical allocator, we introduced some internal constants that influence the recovery process: allocationholdoffrecoverytimeout and agentrecoveryfactor. we should expose these parameters for operators to configure.    however, i am a bit reluctant to expose them as master flags, because they are implementation specific. it would be nice to combine all hierarchical allocator related flags into one (maybe json) file, similar to how we do it for modules.",3,train
MESOS-4074,Tests for master failover in presence of quota,nan,5,train
MESOS-4075,Continue test suite execution across crashing tests.,"currently, mesostests.sh exits when a test crashes. this is inconvenient when trying to find out all tests that fail.     mesostests.sh should rate a test that crashes as failed and continue the same way as if the test merely returned with a failure result and exited properly.",8,train
MESOS-4082,Add tests for quota authentication and authorization.,nan,3,train
MESOS-4085,Implement implicit roles,see also design doc: mesos 4000.,5,train
MESOS-4087,Introduce a module for logging executor/task output,"existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).    a logger for executor/task logs has the following requirements:   the logger is given a command to run and must handle the stdout/stderr of the command.   the handling of stdout/stderr must be resilient across agent failover.  logging should not stop if the agent fails.    logs should be readable, presumably via the web ui, or via some other module specific ui.",5,train
MESOS-4088,Modularize existing plain-file logging for executor/task logs launched with the Mesos Containerizer,"once a module for executor/task output logging has been introduced, the default module will mirror the existing behavior.  executor/task stdout/stderr is piped into files within the executor's sandbox directory.    the files are exposed in the web ui, via the /files endpoint.",2,train
MESOS-4090,Create light-weight executor only and scheduler only mesos eggs,"currently, when running tasks in docker containers, if the executor uses the mesos.native python library, the execution environment inside the container (os, native libs, etc) must match the execution environment outside the container fairly closely in order to load the mesos.so library.    the solution here can be to introduce a much lighter weight python egg, mesos.executor, which only includes code (and dependencies) needed to create and run an mesosexecutordriver.  executors can then use this native library instead of mesos.native.",5,train
MESOS-4098,Allow interactive terminal for mesos containerizer,today mesos containerizer does not have a way to run tasks that require interactive sessions. an example use case is running a task that requires a manual password entry from an operator. another use case could be debugging (gdb). ,10,train
MESOS-4099,parallel make tests does not build all test targets,when inside 3rdparty/libprocess:  running make j8 tests from a clean build does not yield the libprocesstests binary.  running it a subsequent time triggers more compilation and ends up yielding the libprocess tests binary.  this suggests the test target is not being built correctly.,1,train
MESOS-4102,Quota doesn't allocate resources on slave joining,"see attached patch. framework1 is not allocated any resources, despite the fact that the resources on agent2 can safely be allocated to it without risk of violating quota1. if i understand the intended quota behavior correctly, this doesn't seem intended.    note that if the framework is added after the slaves are added, the resources on agent2 are allocated to framework1.",5,train
MESOS-4104,Design document for interactive terminal for mesos containerizer,"as a first step to address the use cases, propose a design document covering the requirement, design and implementation details.",4,train
MESOS-4107,`os::strerror_r` breaks the Windows build,`os::strerror_r` does not exist on windows.,1,train
MESOS-4108,Implement `os::mkdtemp` for Windows,"used basically exclusively for testing, this insecure and otherwisenotquitesuitablefor prod function needs to work to run what will eventually become the fs tests.",5,train
MESOS-4109,HTTPConnectionTest.ClosingResponse is flaky,"output of the test:    [ run      ] httpconnectiontest.closingresponse  i1210 01:20:27.048532 26671 process.cpp:3077] handling http event for process '(22)' with path: '/(22)/get'  ../../../3rdparty/libprocess/src/tests/httptests.cpp:919: failure  actual function call count doesn't match expectcall( http.process, get(_))...           expected: to be called twice             actual: called once   unsatisfied and active  [  failed  ] httpconnectiontest.closingresponse (43 ms)  ",1,train
MESOS-4110,Implement `WindowsError` to correspond with `ErrnoError`.,"in the c standard library, `errno` records the last error on a thread. you can prettyprint it with `strerror`.    in stout, we report these errors with `errnoerror`.    the windows api has something similar, called `getlasterror()`. the way to prettyprint this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `errnoerror`, maybe called `windowserror`.",5,train
MESOS-4112,Clean up libprocess gtest macros,"this ticket is regarding the libprocess gtest helpers in 3rdparty/libprocess/include/process/gtest.hpp.    the pattern in this file seems to be a set of macros:     awaitassert/for   awaitassert/  default of 15 seconds   await/\for  alias for awaitassert/for   await/  alias for awaitassert/   awaitexpect/for   awaitexpect/  default of 15 seconds    (1) awaiteqfor should be added for completeness.    (2) in gtest, we've got expecteq as well as the boolspecific versions: expecttrue and expectfalse.    we should adopt this pattern in these helpers as well. keeping the pattern above in mind, the following are missing:     awaitasserttruefor   awaitasserttrue   awaitassertfalsefor   awaitassertfalse   awaitexpecttruefor   awaitexpectfalsefor    (3) there are http response related macros at the bottom of the file, e.g. awaitexpectresponsestatuseq, however these are missing their assert counterparts.    (4) the reason for (3) presumably is because we reach for expect over assert in general due to the test suite crashing behavior of assert. if this is the case, it would be worthwhile considering whether macros such as awaitready should alias awaitexpectready rather than awaitassertready.     (5) there are a few more missing macros, given awaiteqfor and awaiteq which aliases to awaitasserteqfor and awaitasserteq respectively, we should also add awaittruefor, awaittrue, awaitfalsefor, and awaitfalse as well.",2,train
MESOS-4114,Add field VIP to message Port,"we would like to extend the mesos protocol buffer 'port' to include an optional repeated string named ""vip""   to map it to a well known virtual ip, or virtual hostname for discovery purposes.    we also want this field exposed in discoveryinfo in state.json.",2,train
MESOS-4115,Fix possible race conditions in registry client tests.,registryclient tests show flakiness which manifests as socket timeouts or unexpected buffer showing up in the blobs. investigate them for possible race conditions.,5,train
MESOS-4116,Add tests for quotas + empty roles (no registered frameworks),nan,2,train
MESOS-4126,Construct the error string in `MethodNotAllowed`.,"consider constructing the error string in methodnotallowed rather than at the invocation site. currently we want all error messages follow the same pattern, so instead of writing    return methodnotallowed(, ""expecting 'post', received '""  request.method  ""'"");    we can write something like    methodnotallowed(, request.method)`    ",1,train
MESOS-4127,Ensure `Content-Type` field is set for some responses.,"as pointed out by  in https:/reviews.apache.org/r/40905/, we should make sure we set the content type files for some responses.",3,train
MESOS-4128,Refactor sorter factories in allocator and improve comments around them,for clarity we want to refactor the factory section in the allocator and explain the purpose (and necessity) of all sorters.,3,train
MESOS-4130,Document how the fetcher can reach across a proxy connection.,"the fetcher uses libcurl for downloading content from http, https, etc. there is no source code in the pertinent parts of ""net.hpp"" that deals with proxy settings. however, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. see ""man libcurl tutorial"" for details. see section ""proxies"", subsection ""environment variables"". if you follow this recipe in your mesos agent startup script, you can use a proxy.     we should document this in the fetcher (cache) doc (http:/mesos.apache.org/documentation/latest/fetcher/).  ",1,train
MESOS-4136,Add a ContainerLogger module that restrains log sizes,"one of the major problems this logger module aims to solve is overflowing executor/task log files.  log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).    we should add a containerlogger module that truncates logs as it reaches a configurable maximum size.  additionally, we should determine if the web ui's pailer needs to be changed to deal with logs that are not appendonly.    this will be a nondefault module which will also serve as an example for how to implement the module.",3,train
MESOS-4137,Modularize plain-file logging for executor/task logs launched with the Docker Containerizer,adding a hook inside the docker containerizer is slightly more involved than the mesos containerizer.    docker executors/tasks perform plainfile logging in different places depending on whether the agent is in a docker container itself   code    dockercontainerizerprocess::launchexecutorprocess  in container     this means a containerlogger will need to be loaded or hooked into the mesosdockerexecutor.  or we will need to change how piping in done in mesosdocker executor.,3,train
MESOS-4143,Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles,"when working with dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the roles flag on the master.  however, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available.     per the mailing list, changing roles after the fact is not possible at this time. (that may be another jira), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by roles.  ",2,train
MESOS-4149,Clean up authentication implementation for quota,"to authenticate quota requests we allowed quotahandler to call private http::authenticate() function. once mesos 3231 lands we do not need neither this injection, nor authenticate() calls in the quotahandler.",1,train
MESOS-4150,Implement container logger module metadata recovery,"the containerloggers are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.    for default containerlogger s, like the sandboxcontainerlogger and the (tentatively named) truncatingsandboxcontainerlogger, the log files are exposed during agent recovery regardless.    for non default containerlogger s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  this can be implemented as part of containerizer::recover.",3,train
MESOS-4154,Rename shutdown_frameworks to teardown_framework,"the mesos is now using teardown framework to shutdown a framework but the acls are still using shutdownframework, it is better to rename shutdownframework to teardown_framework for acl to keep consistent.    this is a post review request for https:/reviews.apache.org/r/40829/",2,train
MESOS-4160,Log recover tests are slow.,"on mac os 10.10.4, some tests take longer than 1s to finish:    recovertest.autoinitialization (1003 ms)  recovertest.autoinitializationretry (1000 ms)  ",1,train
MESOS-4164,MasterTest.RecoverResources is slow.,the mastertest.recoverresources test takes more than 1s to finish on my mac os 10.10.4:    mastertest.recoverresources (1018 ms)  ,1,train
MESOS-4165,MasterTest.MasterInfoOnReElection is slow.,the mastertest.masterinfoonreelection test takes more than 1s to finish on my mac os 10.10.4:    mastertest.masterinfoonreelection (1024 ms)  ,1,train
MESOS-4166,MasterTest.LaunchCombinedOfferTest is slow.,the mastertest.launchcombinedoffertest test takes more than 2s to finish on my mac os 10.10.4:    mastertest.launchcombinedoffertest (2023 ms)  ,1,train
MESOS-4167,MasterTest.OfferTimeout is slow.,the mastertest.offertimeout test takes more than 1s to finish on my mac os 10.10.4:    mastertest.offertimeout (1053 ms)  ,1,train
MESOS-4170,OversubscriptionTest.UpdateAllocatorOnSchedulerFailover is slow.,the oversubscriptiontest.updateallocatoronschedulerfailover test takes more than 1s to finish on my mac os 10.10.4:    oversubscriptiontest.updateallocatoronschedulerfailover (1018 ms)  ,1,train
MESOS-4171,OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover is slow.,the oversubscriptiontest.removecapabilitiesonschedulerfailover test takes more than 1s to finish on my mac os 10.10.4:    oversubscriptiontest.removecapabilitiesonschedulerfailover (1018 ms)  ,1,train
MESOS-4172,GarbageCollectorIntegrationTest.Restart is slow,the garbagecollectorintegrationtest.restart test takes more than 5s to finish on my mac os 10.10.4:    garbagecollectorintegrationtest.restart (5102 ms)  ,3,val
MESOS-4174,HookTest.VerifySlaveLaunchExecutorHook is slow.,the hooktest.verifyslavelaunchexecutorhook test takes more than 5s to finish on my mac os 10.10.4:    hooktest.verifyslavelaunchexecutorhook (5061 ms)  ,1,val
MESOS-4175,ContentType/SchedulerTest.Decline is slow.,the contenttype/schedulertest.decline test takes more than 1s to finish on my mac os 10.10.4:    contenttype/schedulertest.decline/0 (1022 ms)  ,1,val
MESOS-4177,Create a user doc for Executor HTTP API,we need a user doc similar to the corresponding one for the scheduler http api.,3,val
MESOS-4178,Add persistent volume support to the Authorizer,"this ticket is the first in a series that adds authorization support for persistent volume creation and destruction.    persistent volumes should be authorized with the principal of the reserving entity (framework or master). the idea is to introduce create and destroy into the acl.        message create       message destroy       acls for volume creation and destruction must be added to authorizer.proto, and the appropriate function overloads must be added to the authorizer.",1,val
MESOS-4179,Extend `Master` to authorize persistent volumes,this ticket is the second in a series that adds authorization support for persistent volumes.    methods master::authorizecreatevolume() and master::authorizedestroyvolume must be added to allow the master to authorize these operations.,1,val
MESOS-4183,Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.,"we often include complex headers like / in "".hpp"" files to define operator<<() inline (e.g. ""mesos/authorizer/authorizer.hpp""). instead, we can move definitions to corresponding "".cpp"" files and replace stream headers with iosfwd, for example, this is partially done for uri in ""mesos/uri/uri.hpp"".",3,val
MESOS-4184,Jenkins builds for Centos fail with missing 'which' utility and incorrect 'java.home',"jenkins builds are now consistently failing for centos 7, withe the failure:    checking value of java system property 'java.home'...  /usr/lib/jvm/java1.8.0openjdk1.8.0.653.b17.el7.x8664/jre  configure: error: could not guess javahome    they also fail early on during 'bootstrap' with a missing 'which' command.    the solution is to update support/docker_build.sh to install 'which' as well as make sure the proper versions of java are installed during the installation process.    the problem here is that we install maven before installing java1.7.0openjdkdevel, causing maven to pull in a dependency on java1.8.0openjdk. this causes problems with finding the proper java.home in our mesos/configure script because of the mismatch between the most up to date jre (1.8.0) and the most up to date development tools (1.7.0).  we can either update the script to pull in the 1.8 devel tools or move our dependence on maven until after our installation of java1.7.0openjdkdevel.  unclear what the best solution is.",3,val
MESOS-4186,Serialize docker v1 image spec as protobuf,"currently we only support v2 docker manifest serialization method. when we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info.",2,val
MESOS-4187,Avoid using absolute URLs in documentation pages,"links from one documentation page to another should not use absolute urls (e.g., http:/mesos.apache.org/documentation/latest/...) for several good reasons. for instance, absolute urls break when the docs are generated/previewed locally.",1,val
MESOS-4190,Create a Design Doc for dynamic weights.,"a short design doc for dynamic weights, it will focus on /weights api and the changes to the allocator api.",3,val
MESOS-4191,Design doc for fixed point resources,nan,5,val
MESOS-4192,Add documentation for API Versioning,"currently, we don't have any documentation for:     how mesos implements api versioning ?   how are protobufs versioned and how does mesos handle them internally ?   what do contributors need to do when they make a change to a external user facing protobuf ?    the relevant design doc:  https:/docs.google.com/document/d/1iqjo6778hfu1zi_yk6szg8qj wqygvgnx7u3h6ou/edit#heading=h.2gkbjz6amn7b  ",3,val
MESOS-4193,Port `process/file.hpp`,nan,3,val
MESOS-4194,MesosContainerizer* tests leak FDs (pipes),"if you run:  bin/mesostests.sh gtestfilter=""mesoscontainerizer"" gtestrepeat=1 gtestbreakon_failure    and then check:  lsof | grep mesos    the number of open pipes will grow linearly with the number of test repetitions.",2,val
MESOS-4195,Add dynamic reservation tests with no principal,"currently, there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal. this should be added in order to more comprehensively test the dynamic reservation code.",1,val
MESOS-4196,Enable running tests without authorizer.,"we do not support creating master instance without an authorizer in tests: https:/github.com/apache/mesos/blob/aa497e81c945677c570484a8aa1a8c8b2e979dfd/src/tests/cluster.cpp#l217. this leads to a segfault when masterflags.acls = none(); is used in a test, while it's a valid use case and should be allowed.    alternatively, we use masterflags.acls = acls();, which triggers creation of localauthorizer with emtpy acls, which seems to be semantically equal to the absence of an authorizer, given permissive flag is true. this equivalence should be verified by a test.",3,val
MESOS-4198,Disk Resource Reservation is NOT Enforced for Persistent Volumes,"if i create a persistent volume on a reserved disk resource, i am able to write data in excess of my reserved size.    disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",3,val
MESOS-4200,Test case(s) for weights + allocation behavior,"as far as i can see, we currently have no test cases for behavior when weights are defined.",2,val
MESOS-4202,Race in SSL socket shutdown ,"libprocess socket shares the ownership of the file descriptor with libevent. in  the destructor of the libprocess libeventssl socket, we call ssl shutdown which  is executed asynchronously. this causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. since we set the shutdown options as sslreceived_shutdown, we leave the any write operations to continue with possibly closed file descriptor.    this issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by os) that has the above issue.",5,val
MESOS-4204,Document that frameworks that participate in a role should cooperate,nan,2,val
MESOS-4206,Write new log-related documentation,"this should include:   default logging behavior for master, agent, framework, executor, task.   master/agent:   a summary of log related flags.   glog specific options.   separation of master/agent logs from container logs.   the containerlogger module.",3,val
MESOS-4207,Add an example bug due to a lack of defer() to the defer() documentation,"in the past, some bugs have been introduced into the codebase due to a lack of defer() where it should have been used. it would be useful to add an example of this to the defer() documentation.",2,val
MESOS-4208,PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky,"  [ run      ] persistentvolumetest.badacldropcreateanddestroy  i1219 09:51:32.623245 31878 leveldb.cpp:174] opened db in 4.393596ms  i1219 09:51:32.624084 31878 leveldb.cpp:181] compacted db in 709447ns  i1219 09:51:32.624186 31878 leveldb.cpp:196] created db iterator in 21252ns  i1219 09:51:32.624290 31878 leveldb.cpp:202] seeked to beginning of db in 11391ns  i1219 09:51:32.624378 31878 leveldb.cpp:271] iterated through 0 keys in the db in 611ns  i1219 09:51:32.624505 31878 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1219 09:51:32.625195 31904 recover.cpp:447] starting replica recovery  i1219 09:51:32.625641 31904 recover.cpp:473] replica is in empty status  i1219 09:51:32.627305 31904 replica.cpp:673] replica in empty status received a broadcasted recover request from (6740)@172.17.0.3:36408  i1219 09:51:32.627749 31904 recover.cpp:193] received a recover response from a replica in empty status  i1219 09:51:32.628330 31904 recover.cpp:564] updating replica status to starting  i1219 09:51:32.629068 31906 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 410494ns  i1219 09:51:32.629169 31906 replica.cpp:320] persisted replica status to starting  i1219 09:51:32.629598 31906 recover.cpp:473] replica is in starting status  i1219 09:51:32.630782 31912 replica.cpp:673] replica in starting status received a broadcasted recover request from (6741)@172.17.0.3:36408  i1219 09:51:32.631166 31901 recover.cpp:193] received a recover response from a replica in starting status  i1219 09:51:32.632467 31902 recover.cpp:564] updating replica status to voting  i1219 09:51:32.633600 31907 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 311370ns  i1219 09:51:32.633627 31907 replica.cpp:320] persisted replica status to voting  i1219 09:51:32.633719 31907 recover.cpp:578] successfully joined the paxos group  i1219 09:51:32.633874 31907 recover.cpp:462] recover process terminated  i1219 09:51:32.636409 31909 master.cpp:365] master bded856d1c7f4fada8bc3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408  i1219 09:51:32.636593 31909 master.cpp:367] flags at startup: acls=""createvolumes     volumetypes   }  createvolumes     volumetypes   }  "" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/sppf7b/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" roles=""role1"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.27.0/inst/share/mesos/webui"" workdir=""/tmp/sppf7b/master"" zksessiontimeout=""10secs""  i1219 09:51:32.637055 31909 master.cpp:414] master allowing unauthenticated frameworks to register  i1219 09:51:32.637068 31909 master.cpp:417] master only allowing authenticated slaves to register  i1219 09:51:32.637094 31909 credentials.hpp:35] loading credentials for authentication from '/tmp/sppf7b/credentials'  i1219 09:51:32.637403 31909 master.cpp:456] using default 'crammd5' authenticator  i1219 09:51:32.637555 31909 master.cpp:493] authorization enabled  w1219 09:51:32.637575 31909 master.cpp:553] the 'roles' flag is deprecated. this flag will be removed in the future. see the mesos 0.27 upgrade notes for more information  i1219 09:51:32.637806 31897 whitelistwatcher.cpp:77] no whitelist given  i1219 09:51:32.637820 31910 hierarchical.cpp:147] initialized hierarchical allocator process  i1219 09:51:32.639677 31909 master.cpp:1629] the newly elected leader is master@172.17.0.3:36408 with id bded856d1c7f4fada8bc3629ba8c59d3  i1219 09:51:32.639768 31909 master.cpp:1642] elected as the leading master!  i1219 09:51:32.639892 31909 master.cpp:1387] recovering from registrar  i1219 09:51:32.640136 31907 registrar.cpp:307] recovering registrar  i1219 09:51:32.640929 31901 log.cpp:659] attempting to start the writer  i1219 09:51:32.642199 31912 replica.cpp:493] replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1  i1219 09:51:32.642719 31912 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 445876ns  i1219 09:51:32.642755 31912 replica.cpp:342] persisted promised to 1  i1219 09:51:32.643478 31904 coordinator.cpp:238] coordinator attempting to fill missing positions  i1219 09:51:32.645009 31909 replica.cpp:388] replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2  i1219 09:51:32.645356 31909 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 310064ns  i1219 09:51:32.645382 31909 replica.cpp:712] persisted action at 0  i1219 09:51:32.646662 31909 replica.cpp:537] replica received write request for position 0 from (6744)@172.17.0.3:36408  i1219 09:51:32.646721 31909 leveldb.cpp:436] reading position from leveldb took 29298ns  i1219 09:51:32.647047 31909 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 283424ns  i1219 09:51:32.647073 31909 replica.cpp:712] persisted action at 0  i1219 09:51:32.647722 31909 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i1219 09:51:32.648052 31909 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 300825ns  i1219 09:51:32.648077 31909 replica.cpp:712] persisted action at 0  i1219 09:51:32.648095 31909 replica.cpp:697] replica learned nop action at position 0  i1219 09:51:32.655295 31899 log.cpp:675] writer started with ending position 0  i1219 09:51:32.656543 31905 leveldb.cpp:436] reading position from leveldb took 32788ns  i1219 09:51:32.658164 31905 registrar.cpp:340] successfully fetched the registry (0b) in 0ns  i1219 09:51:32.658604 31905 registrar.cpp:439] applied 1 operations in 38183ns; attempting to update the 'registry'  i1219 09:51:32.660102 31905 log.cpp:683] attempting to append 170 bytes to the log  i1219 09:51:32.660538 31906 coordinator.cpp:348] coordinator attempting to write append action at position 1  i1219 09:51:32.661872 31906 replica.cpp:537] replica received write request for position 1 from (6745)@172.17.0.3:36408  i1219 09:51:32.662719 31906 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 483018ns  i1219 09:51:32.663054 31906 replica.cpp:712] persisted action at 1  i1219 09:51:32.664008 31902 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i1219 09:51:32.664330 31902 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 287310ns  i1219 09:51:32.664355 31902 replica.cpp:712] persisted action at 1  i1219 09:51:32.664376 31902 replica.cpp:697] replica learned append action at position 1  i1219 09:51:32.665365 31902 registrar.cpp:484] successfully updated the 'registry' in 0ns  i1219 09:51:32.665493 31902 registrar.cpp:370] successfully recovered registrar  i1219 09:51:32.665894 31902 master.cpp:1439] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i1219 09:51:32.665990 31902 hierarchical.cpp:165] skipping recovery of hierarchical allocator: nothing to recover  i1219 09:51:32.666266 31902 log.cpp:702] attempting to truncate the log to 1  i1219 09:51:32.666424 31902 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i1219 09:51:32.667181 31907 replica.cpp:537] replica received write request for position 2 from (6746)@172.17.0.3:36408  i1219 09:51:32.667768 31907 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 335947ns  i1219 09:51:32.668067 31907 replica.cpp:712] persisted action at 2  i1219 09:51:32.668942 31906 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i1219 09:51:32.669240 31906 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 266566ns  i1219 09:51:32.669292 31906 leveldb.cpp:399] deleting ~1 keys from leveldb took 27852ns  i1219 09:51:32.669314 31906 replica.cpp:712] persisted action at 2  i1219 09:51:32.669334 31906 replica.cpp:697] replica learned truncate action at position 2  i1219 09:51:32.691251 31878 containerizer.cpp:141] using isolation: posix/cpu,posix/mem,filesystem/posix  w1219 09:51:32.691759 31878 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i1219 09:51:32.697428 31901 slave.cpp:191] slave started on 228)@172.17.0.3:36408  i1219 09:51:32.697459 31901 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.27.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk(role1):2048"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc""  i1219 09:51:32.697963 31901 credentials.hpp:83] loading credential for authentication from '/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc/credential'  i1219 09:51:32.698210 31901 slave.cpp:322] slave using credential for: testprincipal  i1219 09:51:32.698449 31901 resources.cpp:478] parsing resources as json failed: cpus:2;mem:1024;disk(role1):2048  trying semicolondelimited string format instead  i1219 09:51:32.699065 31901 slave.cpp:392] slave resources: cpus():2; mem():1024; disk(role1):2048; ports():[3100032000]  i1219 09:51:32.699137 31901 slave.cpp:400] slave attributes: [  ]  i1219 09:51:32.699151 31901 slave.cpp:405] slave hostname: 60ab6e727501  i1219 09:51:32.699161 31901 slave.cpp:410] slave checkpoint: true  i1219 09:51:32.699364 31878 sched.cpp:164] version: 0.27.0  i1219 09:51:32.700614 31911 sched.cpp:262] new master detected at master@172.17.0.3:36408  i1219 09:51:32.700703 31911 sched.cpp:272] no credentials provided. attempting to register without authentication  i1219 09:51:32.700724 31911 sched.cpp:714] sending subscribe call to master@172.17.0.3:36408  i1219 09:51:32.700839 31911 sched.cpp:747] will retry registration in 620.399428ms if necessary  i1219 09:51:32.701244 31903 master.cpp:2197] received subscribe call for framework 'default' at scheduler0333dddc4b4140ed8853a1aadf1f1879@172.17.0.3:36408  i1219 09:51:32.701313 31903 master.cpp:1668] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i1219 09:51:32.701625 31903 master.cpp:2268] subscribing framework default with checkpointing disabled and capabilities [  ]  i1219 09:51:32.702308 31903 hierarchical.cpp:260] added framework bded856d1c7f4fada8bc3629ba8c59d30000  i1219 09:51:32.702386 31903 hierarchical.cpp:1329] no resources available to allocate!  i1219 09:51:32.702422 31903 hierarchical.cpp:1423] no inverse offers to send out!  i1219 09:51:32.702448 31903 hierarchical.cpp:1079] performed allocation for 0 slaves in 114358ns  i1219 09:51:32.702638 31903 sched.cpp:641] framework registered with bded856d1c7f4fada8bc3629ba8c59d30000  i1219 09:51:32.702688 31903 sched.cpp:655] scheduler::registered took 25558ns  i1219 09:51:32.703553 31901 state.cpp:58] recovering state from '/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc/meta'  i1219 09:51:32.704118 31897 statusupdatemanager.cpp:200] recovering status update manager  i1219 09:51:32.704407 31907 containerizer.cpp:383] recovering containerizer  i1219 09:51:32.705373 31907 slave.cpp:4427] finished recovery  i1219 09:51:32.705991 31907 slave.cpp:4599] querying resource estimator for oversubscribable resources  i1219 09:51:32.706277 31907 slave.cpp:4613] received oversubscribable resources  from the resource estimator  i1219 09:51:32.706666 31907 slave.cpp:729] new master detected at master@172.17.0.3:36408  i1219 09:51:32.706738 31907 slave.cpp:792] authenticating with master master@172.17.0.3:36408  i1219 09:51:32.706760 31907 slave.cpp:797] using default crammd5 authenticatee  i1219 09:51:32.706886 31899 statusupdatemanager.cpp:174] pausing sending status updates  i1219 09:51:32.706941 31907 slave.cpp:765] detecting new master  i1219 09:51:32.707036 31899 authenticatee.cpp:121] creating new client sasl connection  i1219 09:51:32.707291 31910 master.cpp:5423] authenticating slave(228)@172.17.0.3:36408  i1219 09:51:32.707479 31910 authenticator.cpp:413] starting authentication session for crammd5authenticatee(510)@172.17.0.3:36408  i1219 09:51:32.707849 31910 authenticator.cpp:98] creating new server sasl connection  i1219 09:51:32.708082 31910 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i1219 09:51:32.708112 31910 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i1219 09:51:32.708196 31910 authenticator.cpp:203] received sasl authentication start  i1219 09:51:32.708395 31910 authenticator.cpp:325] authentication requires more steps  i1219 09:51:32.708611 31902 authenticatee.cpp:258] received sasl authentication step  i1219 09:51:32.708773 31910 authenticator.cpp:231] received sasl authentication step  i1219 09:51:32.708889 31910 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '60ab6e727501' server fqdn: '60ab6e727501' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i1219 09:51:32.708976 31910 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i1219 09:51:32.709096 31910 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i1219 09:51:32.709200 31910 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '60ab6e727501' server fqdn: '60ab6e727501' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i1219 09:51:32.709285 31910 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i1219 09:51:32.709363 31910 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i1219 09:51:32.709452 31910 authenticator.cpp:317] authentication success  i1219 09:51:32.709707 31910 authenticatee.cpp:298] authentication success  i1219 09:51:32.710252 31910 slave.cpp:860] successfully authenticated with master master@172.17.0.3:36408  i1219 09:51:32.710525 31910 slave.cpp:1254] will retry registration in 17.44437ms if necessary  i1219 09:51:32.709839 31908 master.cpp:5453] successfully authenticated principal 'testprincipal' at slave(228)@172.17.0.3:36408  i1219 09:51:32.710985 31908 master.cpp:4132] registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d1c7f4fada8bc3629ba8c59d3s0  i1219 09:51:32.711645 31908 registrar.cpp:439] applied 1 operations in 83191ns; attempting to update the 'registry'  i1219 09:51:32.709908 31912 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(510)@172.17.0.3:36408  i1219 09:51:32.713407 31908 log.cpp:683] attempting to append 343 bytes to the log  i1219 09:51:32.713646 31912 coordinator.cpp:348] coordinator attempting to write append action at position 3  i1219 09:51:32.714884 31911 replica.cpp:537] replica received write request for position 3 from (6758)@172.17.0.3:36408  i1219 09:51:32.715221 31911 leveldb.cpp:341] persisting action (362 bytes) to leveldb took 288909ns  i1219 09:51:32.715250 31911 replica.cpp:712] persisted action at 3  i1219 09:51:32.716145 31912 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i1219 09:51:32.716689 31912 leveldb.cpp:341] persisting action (364 bytes) to leveldb took 512217ns  i1219 09:51:32.716716 31912 replica.cpp:712] persisted action at 3  i1219 09:51:32.716737 31912 replica.cpp:697] replica learned append action at position 3  i1219 09:51:32.718426 31911 registrar.cpp:484] successfully updated the 'registry' in 0ns  i1219 09:51:32.719441 31902 slave.cpp:3371] received ping from slaveobserver(228)@172.17.0.3:36408  i1219 09:51:32.719843 31909 log.cpp:702] attempting to truncate the log to 3  i1219 09:51:32.719908 31911 master.cpp:4200] registered slave bded856d1c7f4fada8bc3629ba8c59d3s0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus():2; mem():1024; disk(role1):2048; ports():[3100032000]  i1219 09:51:32.720064 31911 slave.cpp:904] registered with master master@172.17.0.3:36408; given slave id bded856d1c7f4fada8bc3629ba8c59d3s0  i1219 09:51:32.720088 31911 fetcher.cpp:81] clearing fetcher cache  i1219 09:51:32.720491 31911 slave.cpp:927] checkpointing slaveinfo to '/tmp/persistentvolumetestbadacldropcreateanddestroygwltnc/meta/slaves/bded856d1c7f4fada8bc3629ba8c59d3s0/slave.info'  i1219 09:51:32.720844 31909 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i1219 09:51:32.720929 31911 slave.cpp:963] forwarding total oversubscribed resources   i1219 09:51:32.721017 31903 statusupdate_manager.cpp:181] resuming sending status updates  i1219 09:51:32.721099 31911 master.cpp:4542] received update of slave bded856d1c7f4fada8bc3629ba8c59d3s0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources   i1219 09:51:32.721141 31905 hierarchical.cpp:465] added slave bded856d1c7f4fada8bc3629ba8c59d3s0 (60ab6e727501) with cpus():2; mem():1024; disk(role1):2048; ports():[3100032000] (allocated: )  i1219 09:51:32.721879 31911 replica.cpp:537] replica received write request for position 4 from (6759)@172.17.0.3:36408  i1219 09:51:32.722293 31905 hierarchical.cpp:1423] no inverse offers to send out!  i1219 09:51:32.722337 31905 hierarchical.cpp:1101] performed allocation for slave bded856d1c7f4fada8bc3629ba8c59d3s0 in 1.155563ms  i1219 09:51:32.722681 31905 hierarchical.cpp:521] slave bded856d1c7f4fada8bc3629ba8c59d3s0 (60ab6e727501) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk(role1):2048; ports():[3100032000], allocated: cpus():2; mem():1024; ports():[3100032000]; disk(role1):2048)  i1219 09:51:32.722713 31909 master.cpp:5252] sending 1 offers to framework bded856d ...",1,val
MESOS-4209,"Document ""how to program with dynamic reservations and persistent volumes""","specifically, some of the gotchas around:     retrying reservation attempts after a timeout   fuzzy matching resources to determine whether a reservation/pv is successful    represent client state as a state machine and repeatedly move ""toward"" successful terminate stats    should also point to persistent volume example framework. we should also ask gabriel and others (arango?) who have built frameworks with pvs/drs for feedback.",3,val
MESOS-4214,Introduce HTTP endpoint /weights for updating weight,nan,5,val
MESOS-4218,Test for Quota Status Endpoint,nan,3,val
MESOS-4222,Document containerizer from user perspective.,"add documentation that covers:     purpose of containerizers from a use case perspective.   what purpose does each containerizer (mesos. docker, compose) serve.    what criteria could be used to choose a containerizer.",3,val
MESOS-4223,Document isolators from user perspective.,the documentation should cover:     purpose of isolators (business/user perspective).   what is the criteria for choosing/picking any set of isolators.,4,val
MESOS-4224,Document isolator internals.,"document isolators from developer perspective, possibly covering:     linux isolators   posix isolators    filesystem, network isolators",4,val
MESOS-4225,Exposed docker/appc image manifest to mesos containerizer.,"collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner.",2,val
MESOS-4226,Enable passing docker image environment variables runtime config to provisioner,"collect environment variables runtime config information from a docker image, and save as a map. pass it back to provisioner, and handling environment variables merge issue.",1,val
MESOS-4227,Enable passing docker image cmd runtime config to provisioner,"cmd is the command to run when starting a container. we should be able to collect cmd config information from a docker image, and pass it back to provisioner.",1,val
MESOS-4240,Pull provisioner from linux filesystem isolator to Mesos containerizer.,"the rationale behind this change is that many of the image specifications (e.g., docker/appc) are not just for filesystems. they also specify runtime configurations (e.g., environment variables, volumes, etc) for the container.    provisioner should return those runtime configurations to the mesos containerizer and mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator.    here is what it will be look like eventually. we could do those changes in phases:  1) provisioner will return a provisioninfo which includes a 'rootfs' and image specific runtime configurations (could be the docker/appc manifest).  2) then, the mesos containerizer will generate a containerconfig (a protobuf which includes rootfs, sandbox, docker/appc manifest, similar to oci's host independent config.json) and pass that to each isolator in 'prepare'. imaging in the future, a dockerruntimeisolator takes the docker manifest from containerconfig and prepare the container.  3) the isolator's prepare function will return a containerlaunchinfo (contains environment variables, namespaces, etc.) which will be used by mesos containerize to launch containers. imaging that information will be passed to the launcher in the future.    we can do the renaming (containerprepareinfo  > containerlaunchinfo) later.    ",5,val
MESOS-4241,Consolidate docker store slave flags,currently there are too many slave flags for configuring the docker store/puller.  we can remove the following flags:    dockerauthserverport  dockerlocalarchivesdir  dockerregistryport  docker_puller    and consolidate them into the existing flags.,3,val
MESOS-4255,Add mechanism for testing recovery of HTTP based executors,"currently, the slave process generates a process id every time it is initialized via process::id::generate function call. this is a problem for testing http executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented.       agent pid before:  slave(1)@127.0.0.1:43915    agent pid after restart:  slave(2)@127.0.0.1:43915      there are a couple of ways to fix this:   add a constructor to slave exclusively for testing that passes on a fixed id instead of relying on id::generate.   currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the url in libprocess i.e. 127.0.0.1:43915/api/v1/executor would delegate to slave(1)@127.0.0.1:43915/api/v1/executor. instead of defaulting to (1), we can default to the last known active id.",3,val
MESOS-4257,ExamplesTest.NoExecutorFramework runs forever.,"  [ run      ] examplestest.noexecutorframework  i1221 23:10:02.721617 32528 exec.cpp:444] ignoring exited event because the driver is aborted!  using temporary directory '/tmp/examplestestnoexecutorframeworkfcmfln'  i1221 23:10:02.721675 32539 exec.cpp:444] ignoring exited event because the driver is aborted!  i1221 23:10:02.722024 32554 exec.cpp:444] ignoring exited event because the driver is aborted!  warning: logging before initgooglelogging() is written to stderr  i1221 23:10:05.179466 32569 resources.cpp:478] parsing resources as json failed: cpus:0.1;mem:32;disk:32  trying semicolondelimited string format instead  i1221 23:10:05.180269 32569 logging.cpp:172] logging to stderr  i1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus  i1221 23:10:05.200728 32569 leveldb.cpp:174] opened db in 4.184362ms  i1221 23:10:05.202234 32569 leveldb.cpp:181] compacted db in 1.459268ms  i1221 23:10:05.202353 32569 leveldb.cpp:196] created db iterator in 73761ns  i1221 23:10:05.202383 32569 leveldb.cpp:202] seeked to beginning of db in 3382ns  i1221 23:10:05.202405 32569 leveldb.cpp:271] iterated through 0 keys in the db in 633ns  i1221 23:10:05.202674 32569 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i1221 23:10:05.205301 32604 recover.cpp:447] starting replica recovery  i1221 23:10:05.206414 32569 local.cpp:239] using 'local' authorizer  i1221 23:10:05.206405 32604 recover.cpp:473] replica is in empty status  i1221 23:10:05.209595 32594 replica.cpp:673] replica in empty status received a broadcasted recover request from (4)@172.17.0.2:40874  i1221 23:10:05.210916 32596 recover.cpp:193] received a recover response from a replica in empty status  i1221 23:10:05.211515 32597 master.cpp:365] master 3931c1a81cd649eb94c8d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874  i1221 23:10:05.211699 32605 recover.cpp:564] updating replica status to starting  i1221 23:10:05.211539 32597 master.cpp:367] flags at startup: acls=""permissive: false  registerframeworks     roles   }  runtasks     users   }  "" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticateslaves=""false"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/examplestestnoexecutorframeworkfcmfln/credentials"" frameworksorter=""drf"" help=""true"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""5secs"" registrystrict=""false"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.27.0/src/webui"" workdir=""/tmp/mesosotpdch"" zksessiontimeout=""10secs""  i1221 23:10:05.212323 32597 master.cpp:412] master only allowing authenticated frameworks to register  i1221 23:10:05.212337 32597 master.cpp:419] master allowing unauthenticated slaves to register  i1221 23:10:05.212347 32597 credentials.hpp:35] loading credentials for authentication from '/tmp/examplestestnoexecutorframeworkfcmfln/credentials'  w1221 23:10:05.212442 32597 credentials.hpp:50] permissions on credentials file '/tmp/examplestestnoexecutorframeworkfcmfln/credentials' are too open. it is recommended that your credentials file is not accessible by others.  i1221 23:10:05.212606 32600 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 656857ns  i1221 23:10:05.212620 32597 master.cpp:456] using default 'crammd5' authenticator  i1221 23:10:05.212631 32600 replica.cpp:320] persisted replica status to starting  i1221 23:10:05.212893 32597 authenticator.cpp:518] initializing server sasl  i1221 23:10:05.213091 32608 recover.cpp:473] replica is in starting status  i1221 23:10:05.213958 32595 replica.cpp:673] replica in starting status received a broadcasted recover request from (5)@172.17.0.2:40874  i1221 23:10:05.214323 32594 recover.cpp:193] received a recover response from a replica in starting status  i1221 23:10:05.214689 32595 recover.cpp:564] updating replica status to voting  i1221 23:10:05.215353 32596 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 487419ns  i1221 23:10:05.215384 32596 replica.cpp:320] persisted replica status to voting  i1221 23:10:05.215481 32605 recover.cpp:578] successfully joined the paxos group  i1221 23:10:05.215867 32605 recover.cpp:462] recover process terminated  i1221 23:10:05.216111 32569 containerizer.cpp:141] using isolation: filesystem/posix,posix/cpu,posix/mem  w1221 23:10:05.217021 32569 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i1221 23:10:05.221482 32608 slave.cpp:191] slave started on 1)@172.17.0.2:40874  i1221 23:10:05.221521 32608 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""filesystem/posix,posix/cpu,posix/mem"" launcher=""posix"" launcherdir=""/mesos/mesos0.27.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mesosotpdch/0""  i1221 23:10:05.222578 32608 resources.cpp:478] parsing resources as json failed: cpus:2;mem:10240  trying semicolondelimited string format instead  i1221 23:10:05.223465 32608 slave.cpp:392] slave resources: cpus():2; mem():10240; disk():3.70122e06; ports():[3100032000]  i1221 23:10:05.223621 32569 containerizer.cpp:141] using isolation: filesystem/posix,posix/cpu,posix/mem  i1221 23:10:05.223610 32608 slave.cpp:400] slave attributes: [  ]  i1221 23:10:05.223677 32608 slave.cpp:405] slave hostname: 6ccf2ee56b13  i1221 23:10:05.223697 32608 slave.cpp:410] slave checkpoint: true  w1221 23:10:05.224143 32569 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i1221 23:10:05.226668 32604 slave.cpp:191] slave started on 2)@172.17.0.2:40874  i1221 23:10:05.226692 32604 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""filesystem/posix,posix/cpu,posix/mem"" launcher=""posix"" launcherdir=""/mesos/mesos0.27.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mesosotpdch/1""  i1221 23:10:05.227520 32604 resources.cpp:478] parsing resources as json failed: cpus:2;mem:10240  trying semicolondelimited string format instead  i1221 23:10:05.228037 32604 slave.cpp:392] slave resources: cpus():2; mem():10240; disk():3.70122e06; ports():[3100032000]  i1221 23:10:05.228148 32604 slave.cpp:400] slave attributes: [  ]  i1221 23:10:05.228169 32604 slave.cpp:405] slave hostname: 6ccf2ee56b13  i1221 23:10:05.228184 32604 slave.cpp:410] slave checkpoint: true  i1221 23:10:05.229123 32569 containerizer.cpp:141] using isolation: filesystem/posix,posix/cpu,posix/mem  i1221 23:10:05.229641 32605 state.cpp:58] recovering state from '/tmp/mesosotpdch/0/meta'  w1221 23:10:05.229645 32569 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i1221 23:10:05.229636 32595 state.cpp:58] recovering state from '/tmp/mesosotpdch/1/meta'  i1221 23:10:05.230242 32605 statusupdatemanager.cpp:200] recovering status update manager  i1221 23:10:05.230254 32598 statusupdatemanager.cpp:200] recovering status update manager  i1221 23:10:05.230515 32601 containerizer.cpp:383] recovering containerizer  i1221 23:10:05.230562 32602 containerizer.cpp:383] recovering containerizer  i1221 23:10:05.232681 32597 auxprop.cpp:71] initialized inmemory auxiliary property plugin  i1221 23:10:05.232803 32597 master.cpp:493] authorization enabled  i1221 23:10:05.232867 32600 slave.cpp:4427] finished recovery  i1221 23:10:05.232980 32598 slave.cpp:191] slave started on 3)@172.17.0.2:40874  i1221 23:10:05.233039 32594 slave.cpp:4427] finished recovery  i1221 23:10:05.233376 32599 whitelistwatcher.cpp:77] no whitelist given  i1221 23:10:05.233428 32601 hierarchical.cpp:147] initialized hierarchical allocator process  i1221 23:10:05.233003 32598 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""auth.docker.io"" dockerauthserverport=""443"" dockerkillorphans=""true"" dockerlocalarchivesdir=""/tmp/mesos/images/docker"" dockerpuller=""local"" dockerpullertimeout=""60"" dockerregistry=""registry1.docker.io"" dockerregistryport=""443"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mesos/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""filesystem/posix,posix/cpu,posix/mem"" launcher=""posix"" launcherdir=""/mesos/mesos0.27.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""1secs"" resources=""cpus:2;mem:10240"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mesosotpdch/2""  i1221 23:10:05.233744 32600 slave.cpp:4599] querying resource estimator for oversubscribable resources  i1221 23:10:05.233749 32598 resources.cpp:478] parsing resources as json failed: cpus:2;mem:10240  trying semicolondelimited string format instead  i1221 23:10:05.234222 32598 slave.cpp:392] slave resources: cpus():2; mem():10240; disk():3.70122e+06; ports():[3100032000]  i1221 23:10:05.234284 32598 slave.cpp:400] slave attributes: [  ]  i1221 23:10:05.234299 32598 slave.cpp:405] slave hostname: 6ccf2ee56b13  i1221 23:10:05.234311 32598 slave.cpp:410] slave checkpoint: true  i1221 23:10:05.234338 32600 slave.cpp:729] new master detected at master@172.17.0.2:40874  i1221 23:10:05.234376 32604 statusupdatemanager.cpp:174] pausing sending status updates  i1221 23:10:05.234424 32600 slave.cpp:754] no credentials provided. attempting to register without authentication  i1221 23:10:05.234522 32600 slave.cpp:765] detecting new master  i1221 23:10:05.234616 32569 sched.cpp:164] version: 0.27.0  i1221 23:10:05.234658 32600 slave.cpp:4613] received oversubscribable resources  from the resource estimator  i1221 23:10:05.234671 32594 slave.cpp:4599] querying resource estimator for oversubscribable resources  i1221 23:10:05.234884 32606 slave.cpp:4613] received oversubscribable resources  from the resource estimator  i1221 23:10:05.235038 32595 statusupdatemanager.cpp:174] pausing sending status updates  i1221 23:10:05.235043 32606 slave.cpp:729] new master detected at master@172.17.0.2:40874  i1221 23:10:05.235111 32606 slave.cpp:754] no credentials provided. attempting to register without authentication  i1221 23:10:05.235147 32606 slave.cpp:765] detecting new master  i1221 23:10:05.235240 32594 state.cpp:58] recovering state from '/tmp/mesosotpdch/2/meta'  i1221 23:10:05.235443 32608 statusupdatemanager.cpp:200] recovering status update manager  i1221 23:10:05.235625 32594 containerizer.cpp:383] recovering containerizer  i1221 23:10:05.236549 32599 slave.cpp:4427] finished recovery  i1221 23:10:05.236984 32593 sched.cpp:262] new master detected at master@172.17.0.2:40874  i1221 23:10:05.237004 32599 slave.cpp:4599] querying resource estimator for oversubscribable resources  i1221 23:10:05.237221 32593 sched.cpp:318] authenticating with master master@172.17.0.2:40874  i1221 23:10:05.237277 32593 sched.cpp:325] using default crammd5 authenticatee  i1221 23:10:05.237285 32604 statusupdatemanager.cpp:174] pausing sending status updates  i1221 23:10:05.237288 32599 slave.cpp:729] new master detected at master@172.17.0.2:40874  i1221 23:10:05.237361 32599 slave.cpp:754] no credentials provided. attempting to register without authentication  i1221 23:10:05.237433 32599 slave.cpp:765] detecting new master  i1221 23:10:05.237565 32599 slave.cpp:4613] received oversubscribable resources  from the resource estimator  i1221 23:10:05.238154 32605 authenticatee.cpp:97] initializing client sasl  i1221 23:10:05.238315 32605 authenticatee.cpp:121] creating new client sasl connection  i1221 23:10:05.239640 32597 master.cpp:1200] dropping 'mesos.internal.authenticatemessage' message since not elected yet  i1221 23:10:05.239765 32597 master.cpp:1629] the newly elected leader is master@172.17.0.2:40874 with id 3931c1a81cd649eb94c8d01b33bb008e  i1221 23:10:05.239794 32597 master.cpp:1642] elected as the leading master!  i1221 23:10:05.239843 32597 master.cpp:1387] recovering from registrar  i1221 23:10:05.240056 32600 registrar.cpp:307] recovering registrar  i1221 23:10:05.241477 32608 log.cpp:659] attempting to start the writer  i1221 23:10:05.244540 32600 replica.cpp:493] replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1  i1221 23:10:05.245358 32600 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 776937ns  i1221 23:10:05.245393 32600 replica.cpp:342] persisted promised to 1  i1221 23:10:05.246625 32601 coordinator.cpp:238] coordinator attempting to fill missing positions  i1221 23:10:05.248757 32605 replica.cpp:388] replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2  i1221 23:10:05.249214 32605 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 366567ns  i1221 23:10:05.249246 32605 replica.cpp:712] persisted action at 0  i1221 23:10:05.250998 32599 replica.cpp:537] replica received write request for position 0 from (41)@172.17.0.2:40874  i1221 23:10:05.251111 32599 leveldb.cpp:436] reading position from leveldb took 66773ns  i1221 23:10:05.251734 32599 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 379612ns  i1221 23:10:05.251759 32599 replica.cpp:712] persisted action at 0  i1221 23:10:05.252555 32601 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i1221 23:10:05.253010 32601 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 381858ns  i1221 23:10:05.253036 32601 replica.cpp:712] persisted action at 0  i1221 23:10:05.253068 32601 replica.cpp:697] replica learned nop action at position 0  i1221 23:10:05.254043 32595 log.cpp:675] writer started with ending position 0  i1221 23:10:05.256741 32595 leveldb.cpp:436] reading position from leveldb took 48607ns  i1221 23:10:05.260617 32601 registrar.cpp:340] successfully fetched the registry (0b) in 20.47616ms  i1221 23:10:05.260988 32601 registrar.cpp:439] applied 1 operations in 103123ns; attempting to update the 'registry'  i1221 23:10:05.264700 32604 log.cpp:683] attempting to append 170 bytes to the log  i1221 23:10:05.265138 32601 coordinator.cpp:348] coordinator attempting to write append action at position 1  i1221 23:10:05.266208 32603 replica.cpp:537] replica received write request for position 1 from (42)@172.17.0.2:40874  i1221 23:10:05.266829 32603 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 551087ns  i1221 23:10:05.266861 32603 replica.cpp:712] persisted action at 1  i1221 23:10:05.267918 32605 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i1221 23:10:05.268442 32605 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 453416ns  i1221 23:10:05.268470 32605 replica.cpp:712] persisted action at 1  i1221 23:10:05.268506 32605 replica.cpp:697] replica learned append action at position 1  i1221 23:10:05.270512 32606 registrar.cpp:484] successfully updated the 'registry' in 9.375232ms  i1221 23:10:05.270705 32606 registrar.cpp:370] successfully recovered registrar  i1221 23:10:05.271045 32602 log.cpp:702] attempting to truncate the log to 1  i1221 23:10:05.271178 32603 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i1221 23:10:05.271695 32605...",3,val
MESOS-4261,Remove docker auth server flag,we currently use a configured docker auth server from a slave flag to get token auth for docker registry. however this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.    we should remove docker auth server flag completely and ask the docker registry for auth server.,3,val
MESOS-4262,Enable net_cls subsytem in cgroup infrastructure,"currently the control group infrastructure within mesos supports only the memory and cpu subsystems. we need to enhance this infrastructure to support the netcls subsystem as well. details of the netcls subsystem and its usecases can be found here:  https:/    enabling the netcls will allow us to provide operators to, potentially, regulate framework traffic on a percontainer basis.  ",5,val
MESOS-4263,Report volume usage through ResourceStatistics.,posix disk isolator does not currently report volume usage through resourcestatistics. posixdiskisolatorprocess::usage() should be amended to take into account volume usage as well. ,3,val
MESOS-4279,Docker executor truncates task's output when the task is killed.,"i'm implementing a graceful restarts of our mesosmarathondocker setup and i came to a following issue:    (it was already discussed on https:/github.com/mesosphere/marathon/issues/2876 and guys form mesosphere got to a point that its probably a docker containerizer problem...)  to sum it up:    when i deploy simple python script to all mesosslaves:    #!/usr/bin/python    from time import sleep  import signal  import sys  import datetime    def sigtermhandler(signo, stackframe):      print ""got %i"" % signo      print datetime.datetime.now().time()      sys.stdout.flush()      sleep(2)      print datetime.datetime.now().time()      print ""ending""      sys.stdout.flush()      sys.exit(0)    signal.signal(signal.sigterm, sigtermhandler)  signal.signal(signal.sigint, sigterm_handler)    try:      print ""hello""      i = 0      while true:          i += 1          print datetime.datetime.now().time()          print ""iteration #%i"" % i          sys.stdout.flush()          sleep(1)  finally:      print ""goodbye""      and i run it through marathon like    data =       during the app restart i get expected result  the task receives sigterm and dies peacefully (during my scriptspecified 2 seconds period)    but when i wrap this python script in a docker:    from node:4.2    run mkdir /app  add . /app  workdir /app  entrypoint []    and run appropriate application by marathon:    data = ,    forcepullimage: yes   },   cpus: 0.1,   mem: 256,   instances: 1,   id: ""marathontest api""  }      the task during restart (issued from marathon) dies immediately without having a chance to do any cleanup.  ",5,val
MESOS-4281,Correctly handle disk quota usage when volumes are bind mounted into the container.,in its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when linux filesystem isolator is used).,3,val
MESOS-4282,Update isolator prepare function to use ContainerLaunchInfo,"currently we have the isolator's prepare function returning containerprepareinfo protobuf. we should enable containerlaunchinfo (contains environment variables, namespaces, etc.) to be returned which will be used by mesos containerize to launch containers.     by doing this (containerprepareinfo  > containerlaunchinfo), we can select any necessary information and passing then to launcher.",2,val
MESOS-4284,Draft design doc for multi-role frameworks,create a document that describes the problems with having only single role frameworks and proposes an mvp solution and implementation approach.,8,val
MESOS-4285,Mesos command task doesn't support volumes with image,currently volumes are stripped when an image is specified running a command task with mesos containerizer. ,3,val
MESOS-4289,Design doc for simple appc image discovery,create a design document describing the following:     model and abstraction of the discoverer   workflow of the discovery process  ,5,val
MESOS-4291,fs::enter(rootfs) does not work if 'rootfs' is read only.,"i noticed this when i was testing the unified containerizer with the bind mount backend and no volumes.    the current implementation of fs::enter will put the old root under /tmp/.oldroot.xxxxxx in the new rootfs. it assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.    to solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivotroot.",2,val
MESOS-4292,Tests for quota with implicit roles.,"with the introduction of implicit roles (mesos 3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state.",3,val
MESOS-4294,Protobuf parse should support parsing JSON object containing JSON Null.,"(this bug was exposed by mesos4184, when serializing docker v1 image manifest as protobuf).    currently protobuf::parse returns failures when parsing any json containing json::null. if we have any protobuf field set as `json::null`, any other nonrepeated field cannot capture their value. for example, assuming we have a protobuf message:    message nested       if there exists any field containing json::null, like below:              when we do protobuf::parse, it would return the following failure:    failure parse: not expecting a json null  ",1,val
MESOS-4295,"Change documentation links to ""*.md""","right now, links either use the form [label](/documentation/latest/foo/) or [label](foo.md). we should probably switch to using the latter form consistently  it previews better on github, and it will make it easier to have multiple versions of the docs on the website at once in the future.",3,val
MESOS-4296,Add docker URI fetcher plugin based on curl.,"the existing registry client for docker assumes that mesos is built using ssl support and ssl is enabled. that means mesos built with libev (or if ssl is disabled) won't be able to use docker registry client to provision docker images.    given the new uri fetcher (mesos 3918) work has been committed, we can add a new uri fetcher plugin for docker. the plugin will be based on curl so that https and 3xx redirects will be handled automatically. the docker registry puller will just use the uri fetcher to get docker images.",8,val
MESOS-4298,Sync up configuration.md and flags.cpp,"the https:/reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.",1,val
MESOS-4300,Add AuthN and AuthZ to maintenance endpoints.,maintenance endpoints are currently only restricted by firewall settings.  they should also support authentication/authorization like other http endpoints.,3,val
MESOS-4301,Accepting an inverse offer prints misleading logs,"whenever a scheduler accepts an inverse offer, mesos will print a line like this in the master logs:    w1125 10:05:53.155109 29362 master.cpp:2897] accept call used invalid offers '[ 932f7d7bf2d442c79391222c19b9d35bo2 ]': offer 932f7d7bf2d442c79391222c19b9d35bo2 is no longer valid      inverse offers should not trigger this warning.",1,val
MESOS-4304,hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.,"this bug was resolved for the hdfs protocol for mesos3602 but since the process checks for the ""hdfs"" protocol at the beginning of the uri, the fix does not extend itself to nonhdfs hadoop clients.    i0107 01:22:01.259490 17678 logging.cpp:172] info level logging started!  i0107 01:22:01.259856 17678 fetcher.cpp:422] fetcher info: },}],""sandboxdirectory"":""\/mnt\/data\/mesos\/slaves\/530dda5a481a411781543aee637d3b38s3\/frameworks\/530dda5a481a411781543aee637d3b380000\/executors\/wordcount11452129714\/runs\/4443d5acd03449b3bf1208fb9b0d92d0"",""user"":""root""}  i0107 01:22:01.262171 17678 fetcher.cpp:377] fetching uri 'maprfs:/mesos/stormmesos0.9.3.tgz'  i0107 01:22:01.262212 17678 fetcher.cpp:248] fetching directly into the sandbox directory  i0107 01:22:01.262243 17678 fetcher.cpp:185] fetching uri 'maprfs:/mesos/stormmesos0.9.3.tgz'  i0107 01:22:01.671777 17678 fetcher.cpp:110] downloading resource with hadoop client from 'maprfs:/mesos/stormmesos0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a481a411781543aee637d3b38s3/frameworks/530dda5a481a411781543aee637d3b380000/executors/wordcount11452129714/runs/4443d5acd03449b3bf1208fb9b0d92d0/stormmesos0.9.3.tgz'  copytolocal: java.net.urisyntaxexception: expected schemespecific part at index 7: maprfs:  usage: java fsshell [copytolocal [ignorecrc] [crc] / /]  e0107 01:22:02.435556 17678 shell.hpp:90] command 'hadoop fs copytolocal '/maprfs:/mesos/stormmesos0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a481a411781543aee637d3b38s3/frameworks/530dda5a481a411781543aee637d3b380000/executors/wordcount11452129714/runs/4443d5acd03449b3bf1208fb9b0d92d0/stormmesos0.9.3.tgz'' failed; this is the output:  failed to fetch 'maprfs:/mesos/stormmesos0.9.3.tgz': hdfs copytolocal failed: failed to execute 'hadoop fs copytolocal '/maprfs:/mesos/stormmesos0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a481a411781543aee637d3b38s3/frameworks/530dda5a481a411781543aee637d3b380000/executors/wordcount11452129714/runs/4443d5acd03449b3bf1208fb9b0d92d0/stormmesos0.9.3.tgz''; the command was either not found or exited with a nonzero exit status: 255  failed to synchronize with slave (it's probably exited)      after a brief chat with , it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",1,val
MESOS-4307,"Expand the ""Getting Started"" installation instructions","the ""getting started"" documentation currently contains basic instructions to prepare several platforms for compilation and installation of mesos. however, these instructions are not sufficient to run and pass all tests in the test suite, using all configuration options. the installation instructions should be made comprehensive in this respect.    it may also be desirable to provide scripts that have been verified to prepare a particular base os to build, install, and test mesos. this would be very useful for both developers and users of mesos.    note that using some features on some platforms requires the installation of software packages from sources that may not be completely reliable in the long term; for example, packages which are maintained as personal projects of individuals. this should be noted in the instructions accordingly.",5,val
MESOS-4308,Reliably report executor terminations to framework schedulers.,"now that executor terminations are reported (unreliably), we should investigate queuing up these messages (on the agent?) and resending them periodically until we get an acknowledgement, much like status updates do.    from mesos 313: the scheduler interface has a callback for executorlost, but currently it is never called.",5,val
MESOS-4311,Protobuf parse should pass error messages when parsing nested JSON.,"currently when protobuf::parse handles nested json objects, it cannot pass any error message out. we should enable showing those error messages.",1,val
MESOS-4314,Publish Quota Documentation,publish and finish the operator guide draft  for quota which describes basic usage of the endpoints and few basic and advanced usage cases.,3,val
MESOS-4316,Support get non-default weights by /weights,"like /quota, we should also add query logic for /weights to keep consistent. then /roles no longer needs to show weight information.",5,val
MESOS-4318,PersistentVolumeTest.BadACLNoPrincipal is flaky,"https:/builds.apache.org/job/mesos/1457/compiler=gcc,configuration=verbose%20enablelibevent%20enablessl,os=centos:7,labelexp=docker%7c%7chadoop/consolefull      [ run      ] persistentvolumetest.badaclnoprincipal  i0108 01:13:16.117883  1325 leveldb.cpp:174] opened db in 2.614722ms  i0108 01:13:16.118650  1325 leveldb.cpp:181] compacted db in 706567ns  i0108 01:13:16.118702  1325 leveldb.cpp:196] created db iterator in 24489ns  i0108 01:13:16.118723  1325 leveldb.cpp:202] seeked to beginning of db in 2436ns  i0108 01:13:16.118738  1325 leveldb.cpp:271] iterated through 0 keys in the db in 397ns  i0108 01:13:16.118793  1325 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0108 01:13:16.119627  1348 recover.cpp:447] starting replica recovery  i0108 01:13:16.120352  1348 recover.cpp:473] replica is in empty status  i0108 01:13:16.121750  1357 replica.cpp:673] replica in empty status received a broadcasted recover request from (7084)@172.17.0.2:32801  i0108 01:13:16.122297  1353 recover.cpp:193] received a recover response from a replica in empty status  i0108 01:13:16.122747  1350 recover.cpp:564] updating replica status to starting  i0108 01:13:16.123625  1354 master.cpp:365] master 773d31e8383d4e4baa68f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801  i0108 01:13:16.123946  1347 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 728242ns  i0108 01:13:16.123999  1347 replica.cpp:320] persisted replica status to starting  i0108 01:13:16.123708  1354 master.cpp:367] flags at startup: acls=""createvolumes     volumetypes   }  createvolumes     volumetypes   }  "" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/f2ra75/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" roles=""role1"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.27.0/inst/share/mesos/webui"" workdir=""/tmp/f2ra75/master"" zksessiontimeout=""10secs""  i0108 01:13:16.124219  1354 master.cpp:414] master allowing unauthenticated frameworks to register  i0108 01:13:16.124236  1354 master.cpp:417] master only allowing authenticated slaves to register  i0108 01:13:16.124248  1354 credentials.hpp:35] loading credentials for authentication from '/tmp/f2ra75/credentials'  i0108 01:13:16.124294  1358 recover.cpp:473] replica is in starting status  i0108 01:13:16.124644  1354 master.cpp:456] using default 'crammd5' authenticator  i0108 01:13:16.124820  1354 master.cpp:493] authorization enabled  w0108 01:13:16.124843  1354 master.cpp:553] the 'roles' flag is deprecated. this flag will be removed in the future. see the mesos 0.27 upgrade notes for more information  i0108 01:13:16.125154  1348 hierarchical.cpp:147] initialized hierarchical allocator process  i0108 01:13:16.125334  1345 whitelistwatcher.cpp:77] no whitelist given  i0108 01:13:16.126065  1346 replica.cpp:673] replica in starting status received a broadcasted recover request from (7085)@172.17.0.2:32801  i0108 01:13:16.126806  1348 recover.cpp:193] received a recover response from a replica in starting status  i0108 01:13:16.128237  1354 recover.cpp:564] updating replica status to voting  i0108 01:13:16.128402  1359 master.cpp:1629] the newly elected leader is master@172.17.0.2:32801 with id 773d31e8383d4e4baa68f9a3fb9f1fc2  i0108 01:13:16.128489  1359 master.cpp:1642] elected as the leading master!  i0108 01:13:16.128523  1359 master.cpp:1387] recovering from registrar  i0108 01:13:16.128756  1355 registrar.cpp:307] recovering registrar  i0108 01:13:16.129259  1344 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 531437ns  i0108 01:13:16.129292  1344 replica.cpp:320] persisted replica status to voting  i0108 01:13:16.129425  1358 recover.cpp:578] successfully joined the paxos group  i0108 01:13:16.129680  1358 recover.cpp:462] recover process terminated  i0108 01:13:16.130187  1358 log.cpp:659] attempting to start the writer  i0108 01:13:16.131613  1352 replica.cpp:493] replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1  i0108 01:13:16.131983  1352 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 333646ns  i0108 01:13:16.132004  1352 replica.cpp:342] persisted promised to 1  i0108 01:13:16.132627  1348 coordinator.cpp:238] coordinator attempting to fill missing positions  i0108 01:13:16.133896  1349 replica.cpp:388] replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2  i0108 01:13:16.134289  1349 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 349652ns  i0108 01:13:16.134317  1349 replica.cpp:712] persisted action at 0  i0108 01:13:16.135470  1351 replica.cpp:537] replica received write request for position 0 from (7088)@172.17.0.2:32801  i0108 01:13:16.135537  1351 leveldb.cpp:436] reading position from leveldb took 36181ns  i0108 01:13:16.135901  1351 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 308752ns  i0108 01:13:16.135924  1351 replica.cpp:712] persisted action at 0  i0108 01:13:16.136529  1347 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0108 01:13:16.136889  1347 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 327106ns  i0108 01:13:16.136916  1347 replica.cpp:712] persisted action at 0  i0108 01:13:16.136943  1347 replica.cpp:697] replica learned nop action at position 0  i0108 01:13:16.137707  1359 log.cpp:675] writer started with ending position 0  i0108 01:13:16.138844  1348 leveldb.cpp:436] reading position from leveldb took 31371ns  i0108 01:13:16.139878  1356 registrar.cpp:340] successfully fetched the registry (0b) in 0ns  i0108 01:13:16.140012  1356 registrar.cpp:439] applied 1 operations in 42063ns; attempting to update the 'registry'  i0108 01:13:16.140797  1355 log.cpp:683] attempting to append 170 bytes to the log  i0108 01:13:16.140974  1345 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0108 01:13:16.141744  1354 replica.cpp:537] replica received write request for position 1 from (7089)@172.17.0.2:32801  i0108 01:13:16.142226  1354 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 441971ns  i0108 01:13:16.142251  1354 replica.cpp:712] persisted action at 1  i0108 01:13:16.142860  1351 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0108 01:13:16.143198  1351 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 305928ns  i0108 01:13:16.143223  1351 replica.cpp:712] persisted action at 1  i0108 01:13:16.143241  1351 replica.cpp:697] replica learned append action at position 1  i0108 01:13:16.144271  1354 registrar.cpp:484] successfully updated the 'registry' in 0ns  i0108 01:13:16.144435  1354 registrar.cpp:370] successfully recovered registrar  i0108 01:13:16.144567  1359 log.cpp:702] attempting to truncate the log to 1  i0108 01:13:16.144780  1359 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0108 01:13:16.144989  1348 hierarchical.cpp:165] skipping recovery of hierarchical allocator: nothing to recover  i0108 01:13:16.144928  1354 master.cpp:1439] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0108 01:13:16.145690  1357 replica.cpp:537] replica received write request for position 2 from (7090)@172.17.0.2:32801  i0108 01:13:16.146072  1357 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 345113ns  i0108 01:13:16.146097  1357 replica.cpp:712] persisted action at 2  i0108 01:13:16.146667  1358 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0108 01:13:16.147060  1358 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 283648ns  i0108 01:13:16.147116  1358 leveldb.cpp:399] deleting ~1 keys from leveldb took 32174ns  i0108 01:13:16.147135  1358 replica.cpp:712] persisted action at 2  i0108 01:13:16.147153  1358 replica.cpp:697] replica learned truncate action at position 2  i0108 01:13:16.166832  1325 containerizer.cpp:139] using isolation: posix/cpu,posix/mem,filesystem/posix  w0108 01:13:16.167556  1325 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i0108 01:13:16.170526  1349 slave.cpp:191] slave started on 231)@172.17.0.2:32801  i0108 01:13:16.170718  1349 slave.cpp:192] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/persistentvolumetestbadaclnoprincipalyqjjly/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/persistentvolumetestbadaclnoprincipalyqjjly/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.27.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk(role1):2048"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/persistentvolumetestbadaclnoprincipalyqjjly""  i0108 01:13:16.171269  1349 credentials.hpp:83] loading credential for authentication from '/tmp/persistentvolumetestbadaclnoprincipalyqjjly/credential'  i0108 01:13:16.171505  1349 slave.cpp:322] slave using credential for: testprincipal  i0108 01:13:16.171747  1349 resources.cpp:481] parsing resources as json failed: cpus:2;mem:1024;disk(role1):2048  trying semicolondelimited string format instead  i0108 01:13:16.172266  1349 slave.cpp:392] slave resources: cpus():2; mem():1024; disk(role1):2048; ports():[3100032000]  i0108 01:13:16.172327  1349 slave.cpp:400] slave attributes: [  ]  i0108 01:13:16.172340  1349 slave.cpp:405] slave hostname: d9632dd1c41e  i0108 01:13:16.172353  1349 slave.cpp:410] slave checkpoint: true  i0108 01:13:16.173418  1353 state.cpp:58] recovering state from '/tmp/persistentvolumetestbadaclnoprincipalyqjjly/meta'  i0108 01:13:16.173521  1325 sched.cpp:164] version: 0.27.0  i0108 01:13:16.174054  1345 statusupdatemanager.cpp:200] recovering status update manager  i0108 01:13:16.174289  1353 containerizer.cpp:387] recovering containerizer  i0108 01:13:16.174295  1356 sched.cpp:268] new master detected at master@172.17.0.2:32801  i0108 01:13:16.174387  1356 sched.cpp:278] no credentials provided. attempting to register without authentication  i0108 01:13:16.174409  1356 sched.cpp:722] sending subscribe call to master@172.17.0.2:32801  i0108 01:13:16.174515  1356 sched.cpp:755] will retry registration in 1.699889272secs if necessary  i0108 01:13:16.174653  1349 master.cpp:2197] received subscribe call for framework 'noprincipal' at schedulerbf0ed267b4c4412d9fb084c85cd2fbce@172.17.0.2:32801  i0108 01:13:16.174823  1349 master.cpp:1668] authorizing framework principal '' to receive offers for role 'role1'  i0108 01:13:16.175250  1347 master.cpp:2268] subscribing framework noprincipal with checkpointing disabled and capabilities [  ]  i0108 01:13:16.175359  1353 slave.cpp:4429] finished recovery  i0108 01:13:16.175715  1345 hierarchical.cpp:260] added framework 773d31e8383d4e4baa68f9a3fb9f1fc20000  i0108 01:13:16.175734  1351 sched.cpp:649] framework registered with 773d31e8383d4e4baa68f9a3fb9f1fc20000  i0108 01:13:16.175792  1345 hierarchical.cpp:1329] no resources available to allocate!  i0108 01:13:16.175833  1345 hierarchical.cpp:1423] no inverse offers to send out!  i0108 01:13:16.175853  1353 slave.cpp:4601] querying resource estimator for oversubscribable resources  i0108 01:13:16.175869  1345 hierarchical.cpp:1079] performed allocation for 0 slaves in 127881ns  i0108 01:13:16.175923  1351 sched.cpp:663] scheduler::registered took 27956ns  i0108 01:13:16.176110  1353 slave.cpp:729] new master detected at master@172.17.0.2:32801  i0108 01:13:16.176187  1353 slave.cpp:792] authenticating with master master@172.17.0.2:32801  i0108 01:13:16.176216  1353 slave.cpp:797] using default crammd5 authenticatee  i0108 01:13:16.176398  1357 statusupdatemanager.cpp:174] pausing sending status updates  i0108 01:13:16.176404  1353 slave.cpp:765] detecting new master  i0108 01:13:16.176463  1358 authenticatee.cpp:121] creating new client sasl connection  i0108 01:13:16.176553  1353 slave.cpp:4615] received oversubscribable resources  from the resource estimator  i0108 01:13:16.176709  1353 master.cpp:5445] authenticating slave(231)@172.17.0.2:32801  i0108 01:13:16.176823  1359 authenticator.cpp:413] starting authentication session for crammd5authenticatee(516)@172.17.0.2:32801  i0108 01:13:16.177135  1348 authenticator.cpp:98] creating new server sasl connection  i0108 01:13:16.177373  1356 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0108 01:13:16.177399  1356 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0108 01:13:16.177502  1344 authenticator.cpp:203] received sasl authentication start  i0108 01:13:16.177563  1344 authenticator.cpp:325] authentication requires more steps  i0108 01:13:16.177680  1346 authenticatee.cpp:258] received sasl authentication step  i0108 01:13:16.177848  1354 authenticator.cpp:231] received sasl authentication step  i0108 01:13:16.177883  1354 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'd9632dd1c41e' server fqdn: 'd9632dd1c41e' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0108 01:13:16.177894  1354 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0108 01:13:16.177944  1354 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0108 01:13:16.177994  1354 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'd9632dd1c41e' server fqdn: 'd9632dd1c41e' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0108 01:13:16.178014  1354 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0108 01:13:16.178040  1354 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0108 01:13:16.178066  1354 authenticator.cpp:317] authentication success  i0108 01:13:16.178256  1355 authenticatee.cpp:298] authentication success  i0108 01:13:16.178315  1354 master.cpp:5475] successfully authenticated principal 'testprincipal' at slave(231)@172.17.0.2:32801  i0108 01:13:16.178356  1355 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(516)@172.17.0.2:32801  i0108 01:13:16.178710  1354 slave.cpp:860] successfully authenticated with master master@172.17.0.2:32801  i0108 01:13:16.178865  1354 slave.cpp:1254] will retry registration in 13.009431ms if necessary  i0108 01:13:16.179138  1350 master.cpp:4154] registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8383d4e4baa68f9a3fb9f1fc2s0  i0108 01:13:16.179628  1345 registrar.cpp:439] applied 1 operations in 71663ns; attempting to update the 'registry'  i0108 01:13:16.180505  1356 log.cpp:683] attempting to append 343 bytes to the log  i0108 01:13:16.180711  1352 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0108 01:13:16.181499  1350 replica.cpp:537] replica received write request for position 3 from (7103)@172.17.0.2:32801  i0108 01:13:16.182080  1350 leveldb.cpp:341] persisting action (362 bytes) to leveldb took 537757ns  i0108 01:13:16.182112  1350 replica.cpp:712] persisted action at 3  i0108 01:13:16.182749  1351 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0108 01:13:16.183120  1351 leveldb.cpp:341] persisting action (364 bytes) to leveldb took 340999ns  i0108 01:13:16.183151  1351 replica.cpp:712] persisted action at 3  i0108 01:13:16.183177  1351 replica.cpp:697] replica learned append action at position 3  i0108 01:13:16.184787  1348 registrar.cpp:484] successfully updated the 'registry' in 0ns  i0108 01:13:16.185287  1348 log.cpp:702] attempting to truncate the log to 3  i0108 01:13:16.185484  1349 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0108 01:13:16.186043  1353 slave.cpp:3371] received ping from slaveobserver(230)@172.17.0.2:32801  i0108 01:13:16.186074  1345 master.cpp:4222] registered slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus():2; mem():1024; disk(role1):2048; ports():[3100032000]  i0108 01:13:16.186224  1353 slave.cpp:904] registered with master master@172.17.0.2:32801; given slave id 773d31e8383d4e4baa68f9a3fb9f1fc2s0  i0108 01:13:16.186441  1353 fetcher.cpp:81] clearing fetcher cache  i0108 01:13:16.186486  1349 hierarchical.cpp:465] added slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 (d9632dd1c41e) with cpus():2; mem():1024; disk(role1):2048; ports():[3100032000] (allocated: )  i0108 01:13:16.186658  1346 statusupdatemanager.cpp:181] resuming sending status updates  i0108 01:13:16.186885  1353 slave.cpp:927] checkpointing slaveinfo to '/tmp/persistentvolumetestbadaclnoprincipal_yqjjly/meta/slaves/773d31e8383d4e4baa68f9a3fb9f1fc2s0/slave.info'  i0108 01:13:16.186905  1350 replica.cpp:537] replica received write request for position 4 from (7104)@172.17.0.2:32801  i0108 01:13:16.187595  1350 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 645704ns  i0108 01:13:16.187628  1350 replica.cpp:712] persisted action at 4  i0108 01:13:16.188347  1349 hierarchical.cpp:1423] no inverse offers to send out!  i0108 01:13:16.188475  1349 hierarchical.cpp:1101] performed allocation for slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 in 1.861833ms  i0108 01:13:16.188560  1348 replica.cpp:691] replica received learned notice for position 4 from @0.0.0.0:0  i0108 01:13:16.188385  1353 slave.cpp:963] forwarding total oversubscribed resources   i0108 01:13:16.189275  1344 master.cpp:5274] sending 1 offers to framework 773d31e8383d4e4baa68f9a3fb9f1fc20000 (noprincipal) at schedulerbf0ed267b4c4412d9fb084c85cd2fbce@172.17.0.2:32801  i0108 01:13:16.189792  1344 master.cpp:4564] received update of slave 773d31e8383d4e4baa68f9a3fb9f1fc2 s0 at slave(231)@1...",1,val
MESOS-4329,SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation,"executing slavetest.launchtaskinfowithcontainerinfo from 468b8ec under os x 10.10.5 in isolation fails due to missing cleanup,    % ./bin/mesostests.sh gtestfilter=slavetest.launchtaskinfowithcontainerinfo  source directory: /abc/def/src/mesos  build directory: /abc/def/src/mesos/build    we cannot run any docker tests because:  docker tests not supported on nonlinux systems    /usr/bin/nc  /usr/bin/curl  note: google test filter = slavetest.launchtaskinfowithcontainerinfohealthchecktest.rootdockerdockerhealthytask:healthchecktest.rootdockerdockerhealthstatuschange:hierarchicalallocatorbenchmarktest.declineoffers:hooktest.rootdockerverifyslaveprelaunchdockerhook:slavetest.rootruntaskwithcommandinfowithoutuser:slavetest.disabledrootruntaskwithcommandinfowithuser:dockercontainerizertest.rootdockerlaunch:dockercontainerizertest.rootdockerkill:dockercontainerizertest.rootdockerusage:dockercontainerizertest.rootdockerrecover:dockercontainerizertest.rootdockerskiprecovernondocker:dockercontainerizertest.rootdockerlogs:dockercontainerizertest.rootdockerdefaultcmd:dockercontainerizertest.rootdockerdefaultcmdoverride:dockercontainerizertest.rootdockerdefaultcmdargs:dockercontainerizertest.rootdockerslaverecoverytaskcontainer:dockercontainerizertest.disabledrootdockerslaverecoveryexecutorcontainer:dockercontainerizertest.rootdockerncportmapping:dockercontainerizertest.rootdockerlaunchsandboxwithcolon:dockercontainerizertest.rootdockerdestroywhilefetching:dockercontainerizertest.rootdockerdestroywhilepulling:dockercontainerizertest.rootdockerexecutorcleanupwhenlaunchfailed:dockercontainerizertest.rootdockerfetchfailure:dockercontainerizertest.rootdockerdockerpullfailure:dockercontainerizertest.rootdockerdockerinspectdiscard:dockertest.rootdockerinterface:dockertest.rootdockerparsingversion:dockertest.rootdockercheckcommandwithshell:dockertest.rootdockercheckportresource:dockertest.rootdockercancelpull:dockertest.rootdockermountrelative:dockertest.rootdockermountabsolute:copybackendtest.rootcopybackend:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/0:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/1:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/2:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/3:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/4:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/5:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/6:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/7:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/8:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/9:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/10:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/11:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/12:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/13:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/14:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/15:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/16:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/17:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/18:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/19:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/20:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/21:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/22:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/23:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/24:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/25:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/26:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/27:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/28:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/29:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/30:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/31:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/32:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/33:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/34:slaveandframeworkcount/hierarchicalallocatorbenchmarktest.addandupdateslave/35:slavecount/registrarbenchmarktest.performance/0:slavecount/registrarbenchmarktest.performance/1:slavecount/registrarbenchmarktest.performance/2:slavecount/registrarbenchmarktest.performance/3  [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from slavetest  [ run      ] slavetest.launchtaskinfowithcontainerinfo  [       ok ] slavetest.launchtaskinfowithcontainerinfo (79 ms)  [] 1 test from slavetest (79 ms total)    [] global test environment teardown  ../../src/tests/environment.cpp:569: failure  failed  tests completed with child processes remaining:  + 54487 /abc/def/src/mesos/build/src/.libs/mesostests gtestfilter=slavetest.launchtaskinfowithcontainerinfo   \ 54503 /bin/sh /abc/def/src/mesos/build/src/mesoscontainerizer launch command= commands= directory=/tmp help=false piperead=10 pipewrite=13  user=test  [==========] 1 test from 1 test case ran. (87 ms total)  [  passed  ] 1 test.  [  failed  ] 0 tests, listed below:     0 failed tests    ",1,val
MESOS-4333,Refactor Appc provisioner tests  ,current tests can be refactored so that we can reuse some common tasks like test image creation. this will benefit future tests like appc image puller tests.,2,val
MESOS-4336,Document supported file types for archive extraction by fetcher,"the mesos fetcher extracts specified uris if requested to do so by the scheduler. however, the documentation at http:/mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.    https:/github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#l63 specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match.",1,val
MESOS-4337,"Implement a simple Windows version of dirent.hpp, for compatibility.",nan,5,val
MESOS-4338,Create utilities for common shell commands used. ,"we spawn shell for command line utilities like tar, untar, sha256 etc. would be great for resuse if we can create a common utilities class/file for all these utilities.    ",5,val
MESOS-4342,Add parameters to apply patches quiet,added a parameters to apply the patches quiet; so it's easy for contributor to apply patches with  c.,1,val
MESOS-4344,Allow operators to assign net_cls major handles to mesos agents,the netcls cgroup allows operators to assign a 16bit major and 16bit minor network handle to tasks associated with a specific netcls cgroup. in mesos we need to give the operator the ability to fix the 16 bit major handle used in an agent. fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say deny all) for all container traffic till the container is allocated a minor handle.     a simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. ,1,val
MESOS-4345,Implement a network-handle manager for net_cls cgroup subsystem,"as part of implementing the netcls cgroup isolator we need a mechanism to manage the minor handles that will allocated to containers when they are associated with a netcls cgroup. the network handle manager needs to provide the following functionality:    a) during normal operation keep track of the free and allocated network handles. there can be a total of 64k such network handles.  b) on startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. ",8,val
MESOS-4347,GMock warning in ReservationTest.ACLMultipleOperations,"  [ run      ] reservationtest.aclmultipleoperations    gmock warning:  uninteresting mock function call  returning directly.      function call: shutdown(0x7fa2a311b300)  stack trace:  [       ok ] reservationtest.aclmultipleoperations (174 ms)  [] 1 test from reservationtest (174 ms total)      seems to occur nondeterministically for me, maybe once per 50 runs or so. osx 10.10",1,val
MESOS-4348,"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator",  [ run      ] hooktest.verifyslaveruntaskhook    gmock warning:  uninteresting mock function call  returning directly.      function call: shutdown(0x7ff079cb2420)  stack trace:  [       ok ] hooktest.verifyslaveruntaskhook (51 ms)  [ run      ] hooktest.verifyslavetaskstatusdecorator    gmock warning:  uninteresting mock function call  returning directly.      function call: shutdown(0x7ff079cbb790)  stack trace:  [       ok ] hooktest.verifyslavetaskstatusdecorator (54 ms)      occurs non deterministically for me. osx 10.10.,1,val
MESOS-4349,GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor,"  [ run      ] slavetest.containerupdatedbeforetaskreachesexecutor    gmock warning:  uninteresting mock function call  returning directly.      function call: shutdown(0x7fe189cae850)  stack trace:  [       ok ] slavetest.containerupdatedbeforetaskreachesexecutor (51 ms)      occurs nondeterministically for me on osx 10.10, perhaps one run in ten.",1,val
MESOS-4350,GMock warning on `offerRescinded` in `ReservationTest` fixture,"several tests involving checkpointing of resources in the reservationtest fixture are throwing gmock warnings occasionally. here is the output of gtestfilter=""reservationtest. "" bin/mesostests.sh gtestrepeat=10000 gtestbreakon_failure=1 | grep b 3 a 6 warning:        we cannot run any docker tests because:  docker tests not supported on nonlinux systems    [       ok ] reservationtest.masterfailover (89 ms)  [ run      ] reservationtest.compatiblecheckpointedresources    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec320fab0 65537c10285c419eb89f191283402d85o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresources (52 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (45 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (46 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec796f220 bf4e1b5202db47638be03c759c80f1bao1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (63 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (45 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (42 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec7ad92b0 42a9f1ff122e4df79530a96126e36f84o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (65 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (46 ms)      [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (49 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec7af4310 d5e1005fabb84bfd92e03976ee150fbfo1)  stack trace:  [       ok ] reservationtest.incompatiblecheckpointedresources (94 ms)  [ run      ] reservationtest.goodaclreservethenunreserve  [       ok ] reservationtest.goodaclreservethenunreserve (57 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (43 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec7cdadc0 36e15f52329946fa850d970097fef8e2o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (62 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (46 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (47 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feec8c1b580 c8dd35ab736340e08e208c7dc76a8497o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (62 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (45 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (47 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feecbd9b5b0 031c21488a204532b77fb6200c3791c8o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (62 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (46 ms)      [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (47 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feecd52adb0 edc5a322b2204b13a39b99a523b172bao1)  stack trace:  [       ok ] reservationtest.incompatiblecheckpointedresources (76 ms)  [ run      ] reservationtest.goodaclreservethenunreserve  [       ok ] reservationtest.goodaclreservethenunreserve (63 ms)      [       ok ] reservationtest.sendingcheckpointresourcesmessage (45 ms)  [ run      ] reservationtest.resourcescheckpointing    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f015df8, @0x7feecfe16f00 09a90e67a40f4e4288021a5644733a06o1)  stack trace:  [       ok ] reservationtest.resourcescheckpointing (60 ms)  [ run      ] reservationtest.masterfailover  [       ok ] reservationtest.masterfailover (89 ms)      [       ok ] reservationtest.compatiblecheckpointedresources (43 ms)  [ run      ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes    gmock warning:  uninteresting mock function call  returning directly.      function call: offerrescinded(0x7fff5f014960, @0x7feecacceba0 8496598428cd4bc8b25b746583477d09o1)  stack trace:  [       ok ] reservationtest.compatiblecheckpointedresourceswithpersistentvolumes (58 ms)  [ run      ] reservationtest.incompatiblecheckpointedresources  [       ok ] reservationtest.incompatiblecheckpointedresources (68 ms)  ",2,val
MESOS-4353,Limit the number of processes created by libprocess,"currently libprocess will create max(8, number of cpu cores) processes during the initialization, see https:/github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#l2146 for details. this should be ok for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an ibm power machine may have 192 cores), this will cause too much worker threads which are not necessary.    and since libprocess is widely used in mesos (master, agent, scheduler, executor), it may also cause some performance issue. for example, when user creates a docker container via mesos in a mesos agent which is running on a powerful machine with 192 cores, the dockercontainerizer in mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. and if user creates 1000 docker containers in that machine, then there will be 1000 executors, i.e., 1000   192 worker threads which is a large number and may thrash the os.    ",1,val
MESOS-4357,GMock warning in RoleTest.ImplicitRoleStaticReservation,  [ run      ] roletest.implicitrolestaticreservation    gmock warning:  uninteresting mock function call   returning directly.      function call: shutdown(0x7fe37a4752f0)  stack trace:  [       ok ] roletest.implicitrolestaticreservation (52 ms)  ,1,val
MESOS-4358,Expose net_cls network handles in agent's state endpoint,"we need to expose netcls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy.     in order to achieve the above we need to add a new field in the `networkinfo` protobuf (say nethandles) and update this field when a container gets assigned to a netcls cgroup. the `containerstatus` protobuf already has the `networkinfo` protobuf as a nested message, and the `containerstatus` itself is exposed to operators as part of taskinfo (for tasks associated with the container) in an agent's state.json. ",2,val
MESOS-4359,GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard,"the following gmock warning was seen on centos 7.1:      [ run      ] dockercontainerizertest.rootdockerdockerinspectdiscard    gmock warning:  uninteresting mock function call  returning directly.      function call: executorlost(0x7ffdd74f73e0, @0x7f3e3c00fa20 e1, @0x7f3e3c00f4b0 cf212bb4c8c54a43b71fc17b27458627s0,  1)  stack trace:  [       ok ] dockercontainerizertest.rootdockerdockerinspectdiscard (405 ms)  ",2,val
MESOS-4360,Create common tar/untar utility function.,"as part of refactoring and creating a common place to add all command utilities, add tar and untar as the first poc.",3,val
MESOS-4362,Formating issues and broken links in documentation.,"the online documentation has a number of bad formatting issues and broken links (e.g., mesos provider.md).",1,val
MESOS-4363,Add a roles field to FrameworkInfo,to represent multiple roles per framework a new repeated string field for roles is needed.,1,val
MESOS-4364,Add roles validation code to master,a frameworkinfo can only have one of role or roles. a natural location for this appears to be under validation::operation::validate.,5,val
MESOS-4365,Add internal migration from role to roles to master,"if only the role field is given, add it as single entry to roles. add a note to changelog/release notes on deprecation of the existing role field. file a jira issue for removal of that migration code once the deprecation cycle is over.  ",3,val
MESOS-4366,Migrate all existing uses of FrameworkInfo.role to FrameworkInfo.roles,nan,3,val
MESOS-4367,Add tracking of the role a Resource was offered for,"if a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation).",5,val
MESOS-4368,Make HierarchicalAllocatorProcess set a Resource's active role during allocation,the concrete implementation here depends on the implementation strategy used to solve mesos 4367.,3,val
MESOS-4376,Document semantics of `slaveLost`,"we should clarify the semantics of this callback:     is it always invoked, or just a hint?   can a slave ever come back from `slavelost`?    what happens to persistent resources on a lost slave?    the new ha framework development guide might be a good place to put (some of?) this information.  ",2,val
MESOS-4377,Document units associated with resource types,we should document the units associated with memory and disk resources.,1,val
MESOS-4378,Add Source to Resource.DiskInfo.,source is used to describe the extra information about the source of a disk resource. we will support 'path' type first and then 'block' later.      message source           message path           message block           required type type = 1;        optional path path = 2;        optional block block = 3;      }  }  ,1,val
MESOS-4379,Design doc for reservation IDs,nan,3,val
MESOS-4380,Adjust Resource arithmetics for DiskInfo.Source.,"since we added the source for diskinfo, we need to adjust the resource arithmetics for that. that includes equality check, addable check, subtractable check, etc.",2,val
MESOS-4381,Improve upgrade compatibility documentation.,investigate and document upgrade compatibility for 0.27 release.,3,val
MESOS-4382,Change the `principal` in `ReservationInfo` to optional,"with the addition of http endpoints for /reserve and /unreserve, it is now desirable to allow dynamic reservations without a principal, in the case where http authentication is disabled. to allow for this, we will change the principal field in reservationinfo from required to optional. for backwards compatibility, however, the master should currently invalidate any reservationinfo messages that do not have this field set.",1,val
MESOS-4383,Support docker runtime configuration env var from image.,we need to support env var configuration returned from docker image in mesos containerizer.,2,val
MESOS-4385,Offers and InverseOffers cannot be accepted in the same ACCEPT call,"problem   in master::accept, validation::offer::validate returns an error when an inverseoffer is included in the list of offerids in an accept call.   if an offer is part of the same accept, the master sees error.issome() and returns a tasklost for normal offers.  (https:/github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#l3117)    here's a regression test:  https:/reviews.apache.org/r/42092/    proprosal  the question is whether we want to allow the mixing of offers and inverseoffers.    arguments for mixing:   the design/structure of the maintenance originally intended to overload accept and decline to take inverse offers.   enforcing non mixing may require breaking changes to scheduler.proto.    arguments against mixing:   some semantics are difficult to explain.  what does it mean to supply inverseoffers with offer::operations?  what about decline with offers and inverseoffers, including a ""reason""?   what happens if we presumably add a third type of offer?    does it make sense to tasklost valid normal offers if inverseoffers are invalid?",2,val
MESOS-4386,Deprecate 'authenticate' master flag in favor of 'authenticate_frameworks' flag,"to be consistent with `authenticateslaves` and `authenticatehttp` flags, we should rename `authenticate` to `authenticateframeworks` flag.    this should be done via deprecation cycle.     1) release x supports both `authenticate` and `authenticateframeworks` flags    2)  release x + n supports only `authenticate_frameworks` flag.   ",1,val
MESOS-4390,Shared Volumes Design Doc,review & approve design doc,3,val
MESOS-4393,Draft design document for resource revocability by default.,"create a design document for setting offered resources as ""revocable by default"". greedy frameworks can then temporarily use resources set aside to satisfy quota.  ",8,val
MESOS-4395,Add persistent volume endpoint tests with no principal,there are currently no persistent volume endpoint tests that do not use a principal; they should be added.,1,val
MESOS-4397,Rename ContainerPrepareInfo to ContainerLaunchInfo for isolators.,"the name ""containerprepareinfo"" does not really capture the purpose of this struct. containerlaunchinfo better captures the purpose of this struct. containerlaunchinfo is returned by the isolator 'prepare' function. it contains information about how a container should be launched (e.g., environment variables, namespaces, commands, etc.). the information will be used by the mesos containerizer when launching the container.",2,val
MESOS-4398,Synchronously handle AuthZ errors for the Scheduler endpoint.,"currently, any authz errors for the /scheduler endpoint are handled asynchronously as frameworkerrormessage. here is an example:        if (authorizationerror.issome())       we would like to handle such errors synchronously when the request is received similar to what other endpoints like /reserve/quota do. we already have the relevant functions authorizexxx etc in master.cpp. we should just make the requests pass through once the relevant future from the authorizexxx function is fulfilled.",5,val
MESOS-4400,Create persistent volume directories based on DiskInfo.Source.,"currently, we always create persistent volumes from root disk, and the persistent volumes are directories. with diskinfo.source being added, we should create the persistent volume accordingly based on the information in diskinfo.source.    this ticket handles the case where diskinfo.source.type is path. in that case, we should create sub directories and use the same layout as slave.workdir.    see the relevant code here:    void slave::checkpointresources(...)      }  }  ",2,val
MESOS-4402,Update filesystem isolators to look for persistent volume directories from the correct location.,this is related to mesos 4400.    since persistent volume directories can be created from non root disk now. we need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in diskinfo.source.    see relevant code in:    future/ posixfilesystemisolatorprocess::update(..);  future/ linuxfilesystemisolatorprocess::update(..);  ,2,val
MESOS-4403,Check paths in DiskInfo.Source.Path exist during slave initialization.,we have two options here. we can either check and fail if it does not exists. or we can create if it does not exist like we did for slave.work_dir.,2,val
MESOS-4410,Introduce protobuf for quota set request.,"to document quota request json schema and simplify request processing, introduce a quotarequest protobuf wrapper.",3,val
MESOS-4411,Traverse all roles for quota allocation.,there might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. we need to investigate this behavior.,3,val
MESOS-4415,Implement stout/os/windows/rmdir.hpp,nan,5,val
MESOS-4417,Prevent allocator from crashing on successful recovery.,"there might be a bug that may crash the master as pointed out by  in https:/reviews.apache.org/r/42222/:    it looks like if we trip the resume call in addslave, this delayed resume will crash the master   due to the check(paused) that currently resides in resume.  ",3,val
MESOS-4421,"Document that /reserve, /create-volumes endpoints can return misleading ""success""","the docs for the /reserve endpoint say:      200 ok: success (the requested resources have been reserved).      this is not true: the master returns 200 when the request has been validated and a checkpointresourcesmessage has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. same behavior applies to /unreserve, /createvolumes, and /destroyvolumes.    we should either:    1. accurately document what 200 return code means.  2. change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the http client.",3,val
MESOS-4425,Introduce filtering test abstractions for HTTP events to libprocess,"we need a test abstraction for httpevent similar to the already existing one's for dispatchevent, messageevent in libprocess.    the abstraction can look similar in semantics to the already existing futuredispatch/futuremessage.",3,val
MESOS-4433,Implement a callback testing interface for the Executor Library,"currently, we do not have a mocking based callback interface for the executor library. this should look similar to the ongoing work for mesos 3339 i.e. the corresponding issue for the scheduler library.    the interface should allow us to set expectations like we do for the driver. an example:      expect_call(executor, connected())    .times(1)  ",3,val
MESOS-4434,"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos",mesos modules depend on having these packages installed with the exact version as mesos was compiled with.,3,val
MESOS-4435,Update `Master::Http::stateSummary` to use `jsonify`.,update state summary to use jsonify to stay consistent with state http endpoint.,3,val
MESOS-4437,Disable the test RegistryClientTest.BadTokenServerAddress.,"as we are retiring registry client, disable this test which looks flaky.",1,val
MESOS-4438,Add 'dependency' message to 'AppcImageManifest' protobuf.,appcimagemanifest protobuf currently lacks 'dependencies' which is necessary for image discovery.,1,val
MESOS-4439,Fix appc CachedImage image validation,currently image validation is done assuming that the image's filename will have  digest (sha 512) information. this is not part of the spec      (https:/github.com/appc/spec/blob/master/spec/discovery.md).            the spec specifies the tuple / as unique identifier for  discovering an image.  ,1,val
MESOS-4441,Allocate revocable resources beyond quota guarantee.,"status quo  currently resources allocated to frameworks in a role with quota (aka quota'ed role) beyond quota guarantee are marked nonrevocable. this impacts our flexibility for revoking them if we decide so in the future.    proposal  once quota guarantee is satisfied we must not necessarily further allocate resources as nonrevocable. instead we can mark all offers resources beyond guarantee as revocable. when in the future revocableinfo evolves frameworks will get additional information about ""revocability"" of the resource (i.e. allocation slack)    caveats  though it seems like a simple change, it has several implications.    fairness  currently the hierarchical allocator considers revocable resources as regular resources when doing fairness calculations. this may prevent frameworks getting nonrevocable resources as part of their role's quota guarantee if they accept some revocable resources as well.    consider the following scenario. a single framework in a role with quota set to 10 cpus is allocated 10 cpus as nonrevocable resources as part of its quota and additionally 2 revocable cpus. now a task using 2 nonrevocable cpus finishes and its resources are returned. total allocation for the role is 8 nonrevocable + 2 revocable. however, the role may not be offered additional 2 nonrevocable since its total allocation satisfies quota.    resource math  if we allocate nonrevocable resources as revocable, we should make sure we do accounting right: either we should update total agent resources and mark them as revocable as well, or bookkeep resources as nonrevocable and convert them to revocable when necessary.    coarsegrained nature of allocation  the hierarchical allocator performs ""coarsegrained"" allocation, meaning it always allocates the entire remaining agent resources to a single framework. this may lead to overallocating some resources as non revocable beyond quota guarantee.    quotas smaller than fair share  if a quota set for a role is smaller than its fair share, it may reduce the amount of resources offered to this role, if frameworks in it do not accept revocable resources. this is probably the most important consequence of the proposed change. operators may set quota to get guarantees, but may observe a decrease in amount of resources a role gets, which is not intuitive.",8,val
MESOS-4443,Refactor allocator recovery.,allocator recovery code can be improved for readability.  left some thoughts about it in https:/reviews.apache.org/r/42222/.,3,val
MESOS-4444,Design doc for reservation labels,nan,3,val
MESOS-4445,Labels equality behavior is wrong,"  test(revocableresourcetest, labelsemantics)        output:    [ run      ] revocableresourcetest.labelsemantics  i0120 13:15:25.207223 2078158848 resources_tests.cpp:1990] equal? true  [       ok ] revocableresourcetest.labelsemantics (0 ms)      this behavior seems pretty problematic.",5,val
MESOS-4449,SegFault on agent during executor startup,when repeatedly performing our system tests we have found that we get a segfault on one of the agents. it probably occurs about one time in ten. i have attached the full log from that agent. i've attached the log from the agent that failed and the master (although i think this is less helpful).    to reproduce    i have no idea. it seems to occur at certain times. e.g. like if a packet is created right on a minute boundary or something. but i don't think it's something caused by our code because the timestamps are stamped by mesos. i was surprised not to find a bug already open.,1,val
MESOS-4452,"Improve documentation around roles, principals, authz, and reservations"," what is the difference between a role and a principal?   why do some acl entities reference ""roles"" but others reference ""principals""? in a typical organization, what realworld entities would my roles vs. principals map to? the acl documentation could use more information about the motivation of acls and examples of configuring acls to meet realworld security policies.   we should give some examples of making reservations when the role and principal are different, and why you would want to do that   we should add an example to the acl page that includes setting acls for reservations and/or persistent volumes",2,val
MESOS-4454,Create common sha512 compute utility function.,add common utility function for computing digests. start with `sha512` since its immediately needed by appc image fetcher. ,2,val
MESOS-4457,Implement tests for the new Executor library,we need to add tests for the executor library src/executor/executor.cpp. one possible approach would be to use the existing tests in src/tests/scheduler_tests.cpp and make them use the new executor library.,3,val
MESOS-4459,Implement AuthN handling on the scheduler library,"currently, we do not have the ability of passing credentials via the scheduler library. once the master supports authn handling for the /scheduler endpoint, we would need to add this support to the library.",3,val
MESOS-4460,Enable Framework->Executor message optimization for HTTP API,"currently, we support sending framework >executor messages directly as an optimization. this is not currently possible with using the scheduler http api.  we should think about exploring possible alternatives for supporting this optimization.",13,val
MESOS-4461,Enable Executor->Framework message optimization for HTTP API,"currently, we support sending executor >framework messages directly as an optimization. this is not currently possible with using the scheduler http api. we should think about exploring possible alternatives for supporting this optimization.",13,val
MESOS-4466,Implement `waitpid` in Windows,nan,5,val
MESOS-4471,Implement process querying/counting in Windows,nan,2,val
MESOS-4478,ReviewBot seemed to be crashing ReviewBoard server when posting large reviews,the bot is currently tripping on this review  https:/reviews.apache.org/r/42506/ (see builds #10973 to #10978).     looked at the server logs and said he saw 'mysql going away' message when the mesos bot was making these requests. i think that error is a bit misleading because it happens only for this review (which has a huge error log due to bad patch). the bot has successfully posted reviews for other review requests which had no error log (good patch).    one way to fix this would be to just post a tail of the error log (and perhaps link to jenkins console or some other service for the longer error text).,2,val
MESOS-4479,Implement reservation labels,nan,5,val
MESOS-4487,Introduce status() interface in `Containerizer`,"in the containerizer, during container isolation, the isolators end up modifying the state of the containers. examples would be ip address allocation to a container by the 'network isolator, or netcls handle allocation by the cgroup/netcls isolator.     often times the state of the container, needs to be exposed to operators through the state.json endpoint. for e.g. operators or frameworks might want to know the ipaddress configured on a particular container, or the net_cls handle associated with a container to configure the right tc rules. however, at present, there is no clean interface for the slave to retrieve the state of a container from the containerizer for any of the launched containers. thus, we need to introduce a `status` interface in the `containerizer` base class, in order for the slave to expose container state information in its state.json.   ",2,val
MESOS-4488,Define a CgroupInfo protobuf to expose cgroup isolator configuration.,"within `mesoscontainerizer` we have an isolator associated with each linux cgroup subsystem. the isolators apply subsystem specific configuration on the containers before launching the containers. for e.g cgroup/netcls isolator applies netcls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu share isolator configures cpu shares.     currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. we therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. this protobuf will be filled in by the cgroup isolator and will be stored as part of `containerconfig` in the containerizer. ",1,val
MESOS-4489,The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus,the `cgroup/netcls` isolator is responsible for allocating network handles to containers launched within a netcls cgroup. the `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `containerstatus` when the containerizer queries the status() method of the isolator. the information itself will go as part of a `cgroupinfo` protobuf that will be defined as part of mesos 4488 .  ,1,val
MESOS-4490,Get container status information in slave. ,"as part of mesos 4487 an interface will be introduce into the `containerizer` to allow agents to retrieve container state information. the agent needs to use this interface to retrieve container state information during status updates from the executor. the container state information can be then use by the agent to expose various isolator specific configuration (for e.g., ip address allocated by network isolators, netcls handles allocated by `cgroups/netcls` isolator), that has been applied to the container, in the state.json endpoint.  ",3,val
MESOS-4493,Add ability to create symlink on Windows,nan,3,val
MESOS-4494,"Implement `size`, `usage`, and other disk metrics reporting on Windows.",nan,3,val
MESOS-4495,Delete `os::chown` on Windows,nan,1,val
MESOS-4498,"Refactor os.hpp to be less monolithic, and more cross-platform compatible",nan,1,val
MESOS-4499,Docker provisioner store should reuse existing layers in the cache.,"currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.    this is problematic because anytime a user deploys a new image, mesos will fetch all layers of that new image, even though most of the layers are already cached locally.    ",5,val
MESOS-4500,Expose ExecutorInfo and TaskInfo for isolators.,"currently we do not have these info for isolator. image once we have docker runtime isolator, commandinfo is necessary to support either custom executor or command executor. ",2,val
MESOS-4505,Hierarchical allocator performance is slow due to Quota,"since we do not strip the non scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.    one approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. this is valid as quota can currently only be set for scalar resource types.",3,val
MESOS-4506,Posix disk isolator should ignore disk quota enforcement for MOUNT type disk resources.,"we assume mount type disk is exclusive and the underlying filesystem will enforce the quota (i.e., the application won't be able to exceed the quota, and will get a write error it the disk is full).    therefore, there's no need to enforce it's quota in posix disk isolator.",2,val
MESOS-4512,Render quota status consistently with other endpoints.,"currently quota status endpoint returns a collection of quotainfo protos converted to json. an example response looks like this:              },                    }        ]      }    ]  }      presence of some fields, e.g. ""role"", is misleading. to address this issue and make the output more informative, we should probably introduce a  model() function for quotastatus.",3,val
MESOS-4513,Build failure when using gcc-4.9 - signed/unsigned mismatch.,"when building the current master, the following happens when using gcc4.9:      mv f examples/.deps/persistentvolumeframeworkpersistentvolumeframework.tpo examples/.deps/persistentvolumeframeworkpersistentvolumeframework.po  g4.9 dpackagename=\""mesos\"" dpackagetarname=\""mesos\"" dpackageversion=\""0.27.0\"" dpackagestring=\""mesos\ 0.27.0\"" dpackagebugreport=\""\"" dpackageurl=\""\"" dpackage=\""mesos\"" dversion=\""0.27.0\"" dstdcheaders=1 dhavesystypesh=1 dhavesysstath=1 dhavestdlibh=1 dhavestringh=1 dhavememoryh=1 dhavestringsh=1 dhaveinttypesh=1 dhavestdinth=1 dhaveunistdh=1 dhavedlfcnh=1 dltobjdir=\"".libs/\"" dhavepthreadprioinherit=1 dhavepthread=1 dhavelibz=1 dhavelibcurl=1 dhaveaprpoolsh=1 dhavelibapr1=1 dhavesvnversionh=1 dhavelibsvnsubr1=1 dhavesvndeltah=1 dhavelibsvndelta1=1 dhavelibsasl2=1 i. i../../src   wall werror dlibdir=\""/usr/local/lib\"" dpkglibexecdir=\""/usr/local/libexec/mesos\"" dpkgdatadir=\""/usr/local/share/mesos\"" i../../include i../../3rdparty/libprocess/include i../../3rdparty/libprocess/3rdparty/stout/include i../include i../include/mesos isystem ../3rdparty/libprocess/3rdparty/boost1.53.0 i../3rdparty/libprocess/3rdparty/picojson1.3.0 dpicojsonuseint64 dstdcformatmacros i../3rdparty/libprocess/3rdparty/protobuf2.5.0/src i../3rdparty/libprocess/3rdparty/glog0.3.3/src i../3rdparty/libprocess/3rdparty/glog0.3.3/src i../3rdparty/leveldb/include i../3rdparty/zookeeper3.4.5/src/c/include i../3rdparty/zookeeper3.4.5/src/c/generated i../3rdparty/libprocess/3rdparty/protobuf2.5.0/src dsourcedir=\""/users/till/development/mesosprivate/build/..\"" dbuilddir=\""/users/till/development/mesosprivate/build\"" i../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include i../3rdparty/libprocess/3rdparty/gmock1.7.0/include  i/usr/local/opt/openssl/include i/usr/local/opt/libevent/include i/usr/local/opt/subversion/include/subversion1 i/usr/include/apr1 i/usr/include/apr1.0  dthreadsafe pthread g1 o0 wnounusedlocaltypedefs std=c11 dgtestuseowntr1tuple=1 dgtestlangcxx11 mt tests/mesostestscontainerloggertests.o md mp mf tests/.deps/mesostestscontainerloggertests.tpo c o tests/mesostestscontainerloggertests.o `test f 'tests/containerloggertests.cpp'  echo '../../src/'`tests/containerizer.cpp  in file included from ../3rdparty/libprocess/3rdparty/gmock1.7.0/include/gmock/internal/gmockinternalutils.h:47:0,                   from ../3rdparty/libprocess/3rdparty/gmock1.7.0/include/gmock/gmockactions.h:46,                   from ../3rdparty/libprocess/3rdparty/gmock1.7.0/include/gmock/gmock.h:58,                   from ../../src/tests/containerloggertests.cpp:21:  ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h: in instantiation of 'testing::assertionresult testing::internal::cmphelperle(const char, const char, const t1&, const t2&) [with t1 = int; t2 = long long unsigned int]':  ../../src/tests/containerloggertests.cpp:467:3:   required from here  ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h:1579:28: error: comparison between signed and unsigned integer expressions [werror=signcompare]   gtestimplcmphelper(le, <=);                                ../3rdparty/libprocess/3rdparty/gmock1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'gtestimplcmphelper'     if (val1 op val2) ",1,val
MESOS-4515,ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.,  [17:24:58][step 7/7] logrotate: bad argument version: unknown error  [17:24:58][step 7/7] f0126 17:24:57.913729  4503 containerloggertests.cpp:380] checksome(containerizer): failed to create container logger: failed to create container logger module 'orgapachemesoslogrotatecontainerlogger': error creating module instance for 'orgapachemesoslogrotatecontainerlogger'   [17:24:58][step 7/7]  check failure stack trace:   [17:24:58][step 7/7]     @     0x7f11ae0d2d40  google::logmessage::fail()  [17:24:58][step 7/7]     @     0x7f11ae0d2c9c  google::logmessage::sendtolog()  [17:24:58][step 7/7]     @     0x7f11ae0d2692  google::logmessage::flush()  [17:24:58][step 7/7]     @     0x7f11ae0d544c  google::logmessagefatal::logmessagefatal()  [17:24:58][step 7/7]     @           0x983927  checkfatal::checkfatal()  [17:24:58][step 7/7]     @           0xa9a18b  mesos::internal::tests::containerloggertestlogrotaterotateinsandboxtest::testbody()  [17:24:58][step 7/7]     @          0x1623a4e  testing::internal::handlesehexceptionsinmethodifsupported/()  [17:24:58][step 7/7]     @          0x161eab2  testing::internal::handleexceptionsinmethodifsupported/()  [17:24:58][step 7/7]     @          0x15ffdfd  testing::test::run()  [17:24:58][step 7/7]     @          0x160058b  testing::testinfo::run()  [17:24:58][step 7/7]     @          0x1600bc6  testing::testcase::run()  [17:24:58][step 7/7]     @          0x1607515  testing::internal::unittestimpl::runalltests()  [17:24:58][step 7/7]     @          0x16246dd  testing::internal::handlesehexceptionsinmethodifsupported/()  [17:24:58][step 7/7]     @          0x161f608  testing::internal::handleexceptionsinmethodifsupported/()  [17:24:58][step 7/7]     @          0x1606245  testing::unittest::run()  [17:24:58][step 7/7]     @           0xde36b6  runalltests()  [17:24:58][step 7/7]     @           0xde32cc  main  [17:24:58][step 7/7]     @     0x7f11a8896d5d  libcstartmain  [17:24:58][step 7/7]     @           0x981fc9  (unknown)  ,1,val
MESOS-4517,Introduce docker runtime isolator.,"currently docker image default configuration are included in `provisioninfo`. we should grab necessary config from `provisioninfo` into `containerinfo`, and handle all these runtime informations inside of docker runtime isolator. return a `containerlaunchinfo` containing `working_dir`, `env` and merged `commandinfo`, etc.",3,val
MESOS-4520,Introduce a status() interface for isolators,"while launching a container mesos isolators end up configuring/modifying various properties of the container. for e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. similary network isolator (net modules, port mapping) configure the ip address and ports associated with a container.     currently, there are not interface in the isolator to extract the run time state of these properties for a given container. therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",1,val
MESOS-4523,Enable benchmark tests in ASF CI,it would be nice to enable benchmark tests in the asf ci so that we can catch performance regressions (esp. during releases).,3,val
MESOS-4526,Include the allocated portion of reserved resources in the role sorter for DRF.,"reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. that is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as nonrevocable.    in the shortterm, we should at least account for the allocated portion of the reservation.",1,val
MESOS-4527,Include allocated portion of the reserved resources in the quota role sorter for DRF.,"similar to mesos4526, reserved resources should be accounted for in the quota role sorter regardless of their allocation state. in the shortterm, we should at least account them if they are allocated.",1,val
MESOS-4528,Account for reserved resources in the quota guarantee check.,reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool.,2,val
MESOS-4529,Update the allocator to not offer unreserved resources beyond quota.,"eventually, we will want to offer unreserved resources as revocable beyond the role's quota. rather than offering non revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",2,val
MESOS-4530,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky,"while running the command    sudo ./bin/mesostests.sh gtestfilter=""cgroupsanyhierarchywithcpumemorytest.rootcgroupslisten:cgroupsanyhierarchymemorypressuretest.rootincreaserss"" gtestrepeat=10 gtestbreakonfailure    one eventually gets the following output:    [ run      ] netclsisolatortest.rootcgroupsnetclsisolate  ../../src/tests/containerizer/isolatortests.cpp:870: failure  containerizer: could not create isolator 'cgroups/netcls': unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/netcls,netprio  [  failed  ] netclsisolatortest.rootcgroupsnetclsisolate (75 ms)  ",1,val
MESOS-4531,Document multi-disk support.,nan,2,val
MESOS-4534,Resources object can be mutated through the public API,"the resources object current allows mutation of it's internal state through the public mutable iterator interface.  this can cause issues when the mutation involved stripping certain qualifiers on a resource, as they will not be summed together at the end of the mutation (even though they should be).    the contains() math will not work correctly if two addable resources are not summed together on the lhs of the contains check.",3,val
MESOS-4535,Logrotate ContainerLogger may not handle FD ownership correctly,"one of the patches for [mesos 4136] introduced the fdtype::owned enum for subprocess::io::fd.    the way the logrotate module uses this is slightly incorrect:  # the module starts a subprocess with an output subprocess::pipe().  # that pipe's fd is passed into another subprocess via subprocess::io::fd(pipe, io::owned).  # when the second subprocess starts, the pipe's fd is closed in the parent.  # when the first subprocess terminates, the existing code will try to close the pipe again.  this effectively closes a random fd.",1,val
MESOS-4536,"Add abstractions of ""owned"" and ""shared"" file descriptors to libprocess.",libprocess currently manages file descriptors as plain int s.  this leads to some easily missed bugs regarding duplicated or closed fds.    we should introduce an abstraction (like uniqueptr and sharedptr) so that fd ownership can be expressed alongside the affected code.,3,val
MESOS-4539,Exclude paths in Posix disk isolator should be absolute paths.,"since du exclude uses pattern matching. a relative path might accidentally matches an irrelevant directory/file. for instance,      /tmp/testpath $ tree  .   aaa      exc          file   exc       file    3 directories, 2 files  /tmp/testpath $ du exclude /tmp/testpath/exc /tmp/testpath/  8    /tmp/testpath/aaa/exc  12    /tmp/testpath/aaa  16    /tmp/testpath/  /tmp/testpath $ du exclude exc /tmp/testpath/  4    /tmp/testpath/aaa  8    /tmp/testpath/  /tmp/testpath $  ",2,val
MESOS-4540,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6,this test fails in my centos 6 vm due to a cgroups issue:      [ run      ] netclsisolatortest.rootcgroupsnetclsisolate  i0127 19:15:06.637328 25347 exec.cpp:134] version: 0.28.0  i0127 19:15:06.648378 25378 exec.cpp:208] executor registered on slave 6edafba09dbd4e6eb10ec6f935e58d41s0  registered executor on localhost  starting task b745d88e3fbe4af980b3e43484e37acf  sh c 'sleep 1000'  forked command at 25385  ../../src/tests/containerizer/isolatortests.cpp:926: failure  pids: failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/netcls' is not a valid hierarchy  i0127 19:15:06.662083 25376 exec.cpp:381] executor asked to shutdown  shutting down  sending sigterm to process tree at pid 25385  [  failed  ] netclsisolatortest.rootcgroupsnetclsisolate (335 ms)  ,1,val
MESOS-4542,MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.,"can be reproduced by running glogv=1 gtestfilter=""masterquotatest.availableresourcesafterrescinding"" ./bin/mesostests.sh gtestshuffle gtestbreakonfailure gtestrepeat=1000 verbose.    verbose log from a bad run:    [ run      ] masterquotatest.availableresourcesafterrescinding  i0128 12:20:27.568657 2080858880 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0128 12:20:27.570142 2080858880 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0128 12:20:27.583225 2080858880 leveldb.cpp:174] opened db in 6241us  i0128 12:20:27.584353 2080858880 leveldb.cpp:181] compacted db in 1026us  i0128 12:20:27.584429 2080858880 leveldb.cpp:196] created db iterator in 12us  i0128 12:20:27.584442 2080858880 leveldb.cpp:202] seeked to beginning of db in 7us  i0128 12:20:27.584453 2080858880 leveldb.cpp:271] iterated through 0 keys in the db in 6us  i0128 12:20:27.584475 2080858880 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0128 12:20:27.584918 300445696 recover.cpp:447] starting replica recovery  i0128 12:20:27.585113 300445696 recover.cpp:473] replica is in empty status  i0128 12:20:27.585916 297226240 replica.cpp:673] replica in empty status received a broadcasted recover request from (18274)@192.168.178.24:51278  i0128 12:20:27.586086 297762816 recover.cpp:193] received a recover response from a replica in empty status  i0128 12:20:27.586449 297226240 recover.cpp:564] updating replica status to starting  i0128 12:20:27.587204 300445696 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 624us  i0128 12:20:27.587242 300445696 replica.cpp:320] persisted replica status to starting  i0128 12:20:27.587376 299372544 recover.cpp:473] replica is in starting status  i0128 12:20:27.588050 300982272 replica.cpp:673] replica in starting status received a broadcasted recover request from (18275)@192.168.178.24:51278  i0128 12:20:27.588235 300445696 recover.cpp:193] received a recover response from a replica in starting status  i0128 12:20:27.588572 297762816 recover.cpp:564] updating replica status to voting  i0128 12:20:27.588850 297226240 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 140us  i0128 12:20:27.588879 297226240 replica.cpp:320] persisted replica status to voting  i0128 12:20:27.588975 299909120 recover.cpp:578] successfully joined the paxos group  i0128 12:20:27.589154 299909120 recover.cpp:462] recover process terminated  i0128 12:20:27.599486 298835968 master.cpp:374] master 531344bd56f44e4f8f6fa6a9d36058c7 (alexr.fritz.box) started on 192.168.178.24:51278  i0128 12:20:27.599520 298835968 master.cpp:376] flags at startup: acls="""" allocationinterval=""50ms"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/private/tmp/nlzpso/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" roles=""role1,role2"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/private/tmp/nlzpso/master"" zksessiontimeout=""10secs""  i0128 12:20:27.599753 298835968 master.cpp:421] master only allowing authenticated frameworks to register  i0128 12:20:27.599769 298835968 master.cpp:426] master only allowing authenticated slaves to register  i0128 12:20:27.599781 298835968 credentials.hpp:35] loading credentials for authentication from '/private/tmp/nlzpso/credentials'  i0128 12:20:27.600082 298835968 master.cpp:466] using default 'crammd5' authenticator  i0128 12:20:27.600163 298835968 master.cpp:535] using default 'basic' http authenticator  i0128 12:20:27.600327 298835968 master.cpp:569] authorization enabled  w0128 12:20:27.600345 298835968 master.cpp:629] the 'roles' flag is deprecated. this flag will be removed in the future. see the mesos 0.27 upgrade notes for more information  i0128 12:20:27.600497 297762816 whitelistwatcher.cpp:77] no whitelist given  i0128 12:20:27.600503 297226240 hierarchical.cpp:144] initialized hierarchical allocator process  i0128 12:20:27.601965 297226240 master.cpp:1710] the newly elected leader is master@192.168.178.24:51278 with id 531344bd56f44e4f8f6fa6a9d36058c7  i0128 12:20:27.601995 297226240 master.cpp:1723] elected as the leading master!  i0128 12:20:27.602007 297226240 master.cpp:1468] recovering from registrar  i0128 12:20:27.602083 300445696 registrar.cpp:307] recovering registrar  i0128 12:20:27.602460 297226240 log.cpp:659] attempting to start the writer  i0128 12:20:27.603514 299909120 replica.cpp:493] replica received implicit promise request from (18277)@192.168.178.24:51278 with proposal 1  i0128 12:20:27.603734 299909120 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 205us  i0128 12:20:27.603768 299909120 replica.cpp:342] persisted promised to 1  i0128 12:20:27.604194 299909120 coordinator.cpp:238] coordinator attempting to fill missing positions  i0128 12:20:27.605311 299372544 replica.cpp:388] replica received explicit promise request from (18278)@192.168.178.24:51278 for position 0 with proposal 2  i0128 12:20:27.605468 299372544 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 133us  i0128 12:20:27.605494 299372544 replica.cpp:712] persisted action at 0  i0128 12:20:27.606441 298835968 replica.cpp:537] replica received write request for position 0 from (18279)@192.168.178.24:51278  i0128 12:20:27.606492 298835968 leveldb.cpp:436] reading position from leveldb took 29us  i0128 12:20:27.606665 298835968 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 151us  i0128 12:20:27.606688 298835968 replica.cpp:712] persisted action at 0  i0128 12:20:27.607244 297226240 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0128 12:20:27.607409 297226240 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 152us  i0128 12:20:27.607441 297226240 replica.cpp:712] persisted action at 0  i0128 12:20:27.607457 297226240 replica.cpp:697] replica learned nop action at position 0  i0128 12:20:27.607853 297226240 log.cpp:675] writer started with ending position 0  i0128 12:20:27.608649 299372544 leveldb.cpp:436] reading position from leveldb took 158us  i0128 12:20:27.609539 298835968 registrar.cpp:340] successfully fetched the registry (0b) in 7.426816ms  i0128 12:20:27.609763 298835968 registrar.cpp:439] applied 1 operations in 54us; attempting to update the 'registry'  i0128 12:20:27.610216 300982272 log.cpp:683] attempting to append 186 bytes to the log  i0128 12:20:27.610297 298835968 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0128 12:20:27.611016 299909120 replica.cpp:537] replica received write request for position 1 from (18280)@192.168.178.24:51278  i0128 12:20:27.611188 299909120 leveldb.cpp:341] persisting action (205 bytes) to leveldb took 153us  i0128 12:20:27.611222 299909120 replica.cpp:712] persisted action at 1  i0128 12:20:27.611843 299909120 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0128 12:20:27.612004 299909120 leveldb.cpp:341] persisting action (207 bytes) to leveldb took 147us  i0128 12:20:27.612035 299909120 replica.cpp:712] persisted action at 1  i0128 12:20:27.612052 299909120 replica.cpp:697] replica learned append action at position 1  i0128 12:20:27.612742 300982272 registrar.cpp:484] successfully updated the 'registry' in 2.924032ms  i0128 12:20:27.612846 300982272 registrar.cpp:370] successfully recovered registrar  i0128 12:20:27.612936 298835968 log.cpp:702] attempting to truncate the log to 1  i0128 12:20:27.613005 297762816 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0128 12:20:27.613323 298299392 master.cpp:1520] recovered 0 slaves from the registry (147b) ; allowing 10mins for slaves to reregister  i0128 12:20:27.613364 298835968 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0128 12:20:27.613966 300445696 replica.cpp:537] replica received write request for position 2 from (18281)@192.168.178.24:51278  i0128 12:20:27.614131 300445696 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 151us  i0128 12:20:27.614166 300445696 replica.cpp:712] persisted action at 2  i0128 12:20:27.614660 299372544 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0128 12:20:27.614828 299372544 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 158us  i0128 12:20:27.614876 299372544 leveldb.cpp:399] deleting 1 keys from leveldb took 28us  i0128 12:20:27.614898 299372544 replica.cpp:712] persisted action at 2  i0128 12:20:27.614915 299372544 replica.cpp:697] replica learned truncate action at position 2  i0128 12:20:27.625591 2080858880 containerizer.cpp:143] using isolation: posix/cpu,posix/mem,filesystem/posix  i0128 12:20:27.629758 298299392 slave.cpp:192] slave started on 871)@192.168.178.24:51278  i0128 12:20:27.629791 298299392 slave.cpp:193] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/users/alex/projects/mesos/build/default/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf""  i0128 12:20:27.630067 298299392 credentials.hpp:83] loading credential for authentication from '/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf/credential'  i0128 12:20:27.630223 298299392 slave.cpp:323] slave using credential for: testprincipal  i0128 12:20:27.630360 298299392 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0128 12:20:27.630818 298299392 slave.cpp:463] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0128 12:20:27.630869 298299392 slave.cpp:471] slave attributes: [  ]  i0128 12:20:27.630882 298299392 slave.cpp:476] slave hostname: alexr.fritz.box  i0128 12:20:27.631352 300982272 state.cpp:58] recovering state from '/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf/meta'  i0128 12:20:27.631515 299909120 statusupdatemanager.cpp:200] recovering status update manager  i0128 12:20:27.631702 298835968 containerizer.cpp:390] recovering containerizer  i0128 12:20:27.632589 297226240 provisioner.cpp:245] provisioner recovery complete  i0128 12:20:27.632807 298835968 slave.cpp:4495] finished recovery  i0128 12:20:27.633539 298835968 slave.cpp:4667] querying resource estimator for oversubscribable resources  i0128 12:20:27.633752 300445696 statusupdatemanager.cpp:174] pausing sending status updates  i0128 12:20:27.633754 298835968 slave.cpp:795] new master detected at master@192.168.178.24:51278  i0128 12:20:27.633806 298835968 slave.cpp:858] authenticating with master master@192.168.178.24:51278  i0128 12:20:27.633824 298835968 slave.cpp:863] using default crammd5 authenticatee  i0128 12:20:27.633903 298835968 slave.cpp:831] detecting new master  i0128 12:20:27.633913 299372544 authenticatee.cpp:121] creating new client sasl connection  i0128 12:20:27.634016 298835968 slave.cpp:4681] received oversubscribable resources  from the resource estimator  i0128 12:20:27.634076 297226240 master.cpp:5521] authenticating slave(871)@192.168.178.24:51278  i0128 12:20:27.634130 299372544 authenticator.cpp:413] starting authentication session for crammd5authenticatee(1741)@192.168.178.24:51278  i0128 12:20:27.634255 297226240 authenticator.cpp:98] creating new server sasl connection  i0128 12:20:27.634348 300982272 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0128 12:20:27.634367 300982272 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0128 12:20:27.634454 298835968 authenticator.cpp:203] received sasl authentication start  i0128 12:20:27.634515 298835968 authenticator.cpp:325] authentication requires more steps  i0128 12:20:27.634572 298835968 authenticatee.cpp:258] received sasl authentication step  i0128 12:20:27.634706 297226240 authenticator.cpp:231] received sasl authentication step  i0128 12:20:27.634757 297226240 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'alexr.fritz.box' server fqdn: 'alexr.fritz.box' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0128 12:20:27.634771 297226240 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0128 12:20:27.634793 297226240 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0128 12:20:27.634809 297226240 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'alexr.fritz.box' server fqdn: 'alexr.fritz.box' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0128 12:20:27.634819 297226240 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0128 12:20:27.634827 297226240 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0128 12:20:27.634893 297226240 authenticator.cpp:317] authentication success  i0128 12:20:27.634958 298835968 authenticatee.cpp:298] authentication success  i0128 12:20:27.635030 298299392 master.cpp:5551] successfully authenticated principal 'testprincipal' at slave(871)@192.168.178.24:51278  i0128 12:20:27.635079 300445696 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(1741)@192.168.178.24:51278  i0128 12:20:27.635195 299372544 slave.cpp:926] successfully authenticated with master master@192.168.178.24:51278  i0128 12:20:27.635273 299372544 slave.cpp:1320] will retry registration in 5.823453ms if necessary  i0128 12:20:27.635365 299909120 master.cpp:4235] registering slave at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd56f44e4f8f6fa6a9d36058c7s0  i0128 12:20:27.635542 297762816 registrar.cpp:439] applied 1 operations in 41us; attempting to update the 'registry'  i0128 12:20:27.635889 299372544 log.cpp:683] attempting to append 358 bytes to the log  i0128 12:20:27.636011 298299392 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0128 12:20:27.636693 300982272 replica.cpp:537] replica received write request for position 3 from (18295)@192.168.178.24:51278  i0128 12:20:27.636860 300982272 leveldb.cpp:341] persisting action (377 bytes) to leveldb took 139us  i0128 12:20:27.636885 300982272 replica.cpp:712] persisted action at 3  i0128 12:20:27.637380 299909120 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0128 12:20:27.637547 299909120 leveldb.cpp:341] persisting action (379 bytes) to leveldb took 132us  i0128 12:20:27.637573 299909120 replica.cpp:712] persisted action at 3  i0128 12:20:27.637589 299909120 replica.cpp:697] replica learned append action at position 3  i0128 12:20:27.638362 298835968 registrar.cpp:484] successfully updated the 'registry' in 2.77504ms  i0128 12:20:27.638589 300445696 log.cpp:702] attempting to truncate the log to 3  i0128 12:20:27.638684 298299392 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0128 12:20:27.638825 300445696 slave.cpp:3435] received ping from slaveobserver(871)@192.168.178.24:51278  i0128 12:20:27.639081 300982272 hierarchical.cpp:473] added slave 531344bd56f44e4f8f6fa6a9d36058c7s0 (alexr.fritz.box) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0128 12:20:27.639117 299909120 master.cpp:4303] registered slave 531344bd56f44e4f8f6fa6a9d36058c7s0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0128 12:20:27.639165 300982272 hierarchical.cpp:1403] no resources available to allocate!  i0128 12:20:27.639168 297226240 slave.cpp:970] registered with master master@192.168.178.24:51278; given slave id 531344bd56f44e4f8f6fa6a9d36058c7s0  i0128 12:20:27.639189 297226240 fetcher.cpp:81] clearing fetcher cache  i0128 12:20:27.639183 300982272 hierarchical.cpp:1116] performed allocation for slave 531344bd56f44e4f8f6fa6a9d36058c7s0 in 77us  i0128 12:20:27.639348 297762816 statusupdatemanager.cpp:181] resuming sending status updates  i0128 12:20:27.639519 298835968 replica.cpp:537] replica received write request for position 4 from (18296)@192.168.178.24:51278  i0128 12:20:27.639678 298835968 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 142us  i0128 12:20:27.639708 298835968 replica.cpp:712] persisted action at 4  i0128 12:20:27.640115 300982272 replica.cpp:691] replica received learned notice for position 4 from @0.0.0.0:0  i0128 12:20:27.640276 300982272 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 137us  i0128 12:20:27.640312 300982272 leveldb.cpp:399] deleting 2 keys from leveldb took 21us  i0128 12:20:27.640326 300982272 replica.cpp:712] persisted action at 4  i0128 12:20:27.640336 300982272 replica.cpp:697] replica learned truncate action at position 4  i0128 12:20:27.642145 297226240 slave.cpp:993] checkpointing slaveinfo to '/tmp/masterquotatestavailableresourcesafterrescindinggs9qcf/meta/slaves/531344bd56f44e4f8f6fa6a9d36058c7s0/slave.info'  i0128 12:20:27.643354 297226240 slave.cpp:1029] forwarding total oversubscribed resources   i0128 12:20:27.643458 300445696 master.cpp:4644] received update of slave 531344bd56f44e4f8f6fa6a9d36058c7s0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources   i0128 12:20:27.643710 298299392 hierarchical.cpp:531] slave 531344bd56f44e4f8f6fa6a9d36058c7s0 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports( ):[31000 32000], allocated: )  i0128 12:20:27.643769 298299392 hierarchical.cpp:1403] no resources available to allocate!  i0128 12:20:27.643805 2...",3,val
MESOS-4544,Propose design doc for agent partitioning behavior,nan,8,val
MESOS-4545,Propose design doc for reliable floating point behavior,nan,3,val
MESOS-4546,Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect,"sample mesos agent log: https:/gist.github.com/brndnmtthws/fb846fa988487250a809    note, zookeeper has a function to change the list of servers at runtime: https:/github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#l1207l1232    this comes up when using an aws autoscalinggroup for managing the set of masters.     the agent when it comes up the first time, resolves the zk:/ string. once all the hosts that were in the original string fail (each fails, is replaced by a new machine, which has the same dns name), the agent just keeps spinning in an internal loop, never reresolving the dns names.    two solutions i see are   1. update the list of servers / reresolve  2. have the agent detect it hasn't connected recently, and kill itself (which will force a reresolution when the agent starts back up)",3,val
MESOS-4554,Investigate test suite crashes after ZK socket disconnections.,"showed up on asf ci:  https:/builds.apache.org/job/mesos/compiler=clang,configuration=verbose%20enablelibevent%20enablessl,os=ubuntu:14.04,labelexp=docker%7c%7chadoop/1579/console    the test crashed with the following logs:    [ run      ] contenttype/executorhttpapitest.defaultaccept/1  i0129 02:00:35.137161 31926 leveldb.cpp:174] opened db in 118.902333ms  i0129 02:00:35.187021 31926 leveldb.cpp:181] compacted db in 49.836241ms  i0129 02:00:35.187088 31926 leveldb.cpp:196] created db iterator in 33825ns  i0129 02:00:35.187109 31926 leveldb.cpp:202] seeked to beginning of db in 7965ns  i0129 02:00:35.187121 31926 leveldb.cpp:271] iterated through 0 keys in the db in 6350ns  i0129 02:00:35.187165 31926 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0129 02:00:35.188433 31950 recover.cpp:447] starting replica recovery  i0129 02:00:35.188796 31950 recover.cpp:473] replica is in empty status  i0129 02:00:35.190021 31949 replica.cpp:673] replica in empty status received a broadcasted recover request from (11817)@172.17.0.3:60904  i0129 02:00:35.190569 31958 recover.cpp:193] received a recover response from a replica in empty status  i0129 02:00:35.190994 31959 recover.cpp:564] updating replica status to starting  i0129 02:00:35.191522 31953 master.cpp:374] master 823f2212bf284dd6959d796029d32afb (90665f991b70) started on 172.17.0.3:60904  i0129 02:00:35.191640 31953 master.cpp:376] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/b9o6zq/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""25secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.28.0/inst/share/mesos/webui"" workdir=""/tmp/b9o6zq/master"" zksessiontimeout=""10secs""  i0129 02:00:35.191926 31953 master.cpp:421] master only allowing authenticated frameworks to register  i0129 02:00:35.191936 31953 master.cpp:426] master only allowing authenticated slaves to register  i0129 02:00:35.191943 31953 credentials.hpp:35] loading credentials for authentication from '/tmp/b9o6zq/credentials'  i0129 02:00:35.192229 31953 master.cpp:466] using default 'crammd5' authenticator  i0129 02:00:35.192366 31953 master.cpp:535] using default 'basic' http authenticator  i0129 02:00:35.192530 31953 master.cpp:569] authorization enabled  i0129 02:00:35.192719 31950 whitelistwatcher.cpp:77] no whitelist given  i0129 02:00:35.192756 31957 hierarchical.cpp:144] initialized hierarchical allocator process  i0129 02:00:35.194291 31955 master.cpp:1710] the newly elected leader is master@172.17.0.3:60904 with id 823f2212bf284dd6959d796029d32afb  i0129 02:00:35.194335 31955 master.cpp:1723] elected as the leading master!  i0129 02:00:35.194350 31955 master.cpp:1468] recovering from registrar  i0129 02:00:35.194545 31958 registrar.cpp:307] recovering registrar  i0129 02:00:35.220226 31948 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 29.150097ms  i0129 02:00:35.220262 31948 replica.cpp:320] persisted replica status to starting  i0129 02:00:35.220484 31959 recover.cpp:473] replica is in starting status  i0129 02:00:35.221220 31954 replica.cpp:673] replica in starting status received a broadcasted recover request from (11819)@172.17.0.3:60904  i0129 02:00:35.221539 31959 recover.cpp:193] received a recover response from a replica in starting status  i0129 02:00:35.221871 31954 recover.cpp:564] updating replica status to voting  i0129 02:00:35.245329 31949 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 23.326002ms  i0129 02:00:35.245367 31949 replica.cpp:320] persisted replica status to voting  i0129 02:00:35.245522 31955 recover.cpp:578] successfully joined the paxos group  i0129 02:00:35.245800 31955 recover.cpp:462] recover process terminated  i0129 02:00:35.246181 31951 log.cpp:659] attempting to start the writer  i0129 02:00:35.247228 31953 replica.cpp:493] replica received implicit promise request from (11820)@172.17.0.3:60904 with proposal 1  i0129 02:00:35.270472 31953 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 23.225846ms  i0129 02:00:35.270510 31953 replica.cpp:342] persisted promised to 1  i0129 02:00:35.271306 31957 coordinator.cpp:238] coordinator attempting to fill missing positions  i0129 02:00:35.272373 31949 replica.cpp:388] replica received explicit promise request from (11821)@172.17.0.3:60904 for position 0 with proposal 2  i0129 02:00:35.295600 31949 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 23.181008ms  i0129 02:00:35.295639 31949 replica.cpp:712] persisted action at 0  i0129 02:00:35.296815 31950 replica.cpp:537] replica received write request for position 0 from (11822)@172.17.0.3:60904  i0129 02:00:35.296879 31950 leveldb.cpp:436] reading position from leveldb took 43203ns  i0129 02:00:35.320659 31950 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 23.753935ms  i0129 02:00:35.320699 31950 replica.cpp:712] persisted action at 0  i0129 02:00:35.321394 31950 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0129 02:00:35.345837 31950 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 24.358655ms  i0129 02:00:35.345877 31950 replica.cpp:712] persisted action at 0  i0129 02:00:35.345898 31950 replica.cpp:697] replica learned nop action at position 0  i0129 02:00:35.346683 31950 log.cpp:675] writer started with ending position 0  i0129 02:00:35.347913 31957 leveldb.cpp:436] reading position from leveldb took 55621ns  i0129 02:00:35.349047 31947 registrar.cpp:340] successfully fetched the registry (0b) in 154.395904ms  i0129 02:00:35.349185 31947 registrar.cpp:439] applied 1 operations in 46347ns; attempting to update the 'registry'  i0129 02:00:35.350008 31952 log.cpp:683] attempting to append 170 bytes to the log  i0129 02:00:35.350132 31957 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0129 02:00:35.351042 31953 replica.cpp:537] replica received write request for position 1 from (11823)@172.17.0.3:60904  i0129 02:00:35.370906 31953 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 19.829257ms  i0129 02:00:35.370946 31953 replica.cpp:712] persisted action at 1  i0129 02:00:35.371840 31952 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0129 02:00:35.396082 31952 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 24.218894ms  i0129 02:00:35.396122 31952 replica.cpp:712] persisted action at 1  i0129 02:00:35.396144 31952 replica.cpp:697] replica learned append action at position 1  i0129 02:00:35.397250 31954 registrar.cpp:484] successfully updated the 'registry' in 47.99104ms  i0129 02:00:35.397452 31954 registrar.cpp:370] successfully recovered registrar  i0129 02:00:35.397678 31946 log.cpp:702] attempting to truncate the log to 1  i0129 02:00:35.397881 31956 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0129 02:00:35.398066 31951 master.cpp:1520] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0129 02:00:35.398111 31957 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0129 02:00:35.398982 31955 replica.cpp:537] replica received write request for position 2 from (11824)@172.17.0.3:60904  i0129 02:00:35.421293 31955 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 22.286476ms  i0129 02:00:35.421339 31955 replica.cpp:712] persisted action at 2  i0129 02:00:35.422046 31944 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0129 02:00:35.446316 31944 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 24.246177ms  i0129 02:00:35.446406 31944 leveldb.cpp:399] deleting ~1 keys from leveldb took 84415ns  i0129 02:00:35.446466 31944 replica.cpp:712] persisted action at 2  i0129 02:00:35.446491 31944 replica.cpp:697] replica learned truncate action at position 2  i0129 02:00:35.452579 31957 slave.cpp:192] slave started on 372)@172.17.0.3:60904  i0129 02:00:35.452620 31957 slave.cpp:193] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/contenttypeexecutorhttpapitestdefaultaccept1r4guhm/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/contenttypeexecutorhttpapitestdefaultaccept1r4guhm/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.28.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/contenttypeexecutorhttpapitestdefaultaccept1r4guhm""  i0129 02:00:35.453012 31957 credentials.hpp:83] loading credential for authentication from '/tmp/contenttypeexecutorhttpapitestdefaultaccept1r4guhm/credential'  i0129 02:00:35.453191 31957 slave.cpp:323] slave using credential for: testprincipal  i0129 02:00:35.453368 31957 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0129 02:00:35.453853 31957 slave.cpp:463] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0129 02:00:35.453938 31957 slave.cpp:471] slave attributes: [  ]  i0129 02:00:35.453953 31957 slave.cpp:476] slave hostname: 90665f991b70  i0129 02:00:35.454794 31950 state.cpp:58] recovering state from '/tmp/contenttypeexecutorhttpapitestdefaultaccept1r4guhm/meta'  i0129 02:00:35.455080 31948 statusupdatemanager.cpp:200] recovering status update manager  i0129 02:00:35.455225 31926 sched.cpp:222] version: 0.28.0  i0129 02:00:35.455535 31956 slave.cpp:4495] finished recovery  i0129 02:00:35.455798 31945 sched.cpp:326] new master detected at master@172.17.0.3:60904  i0129 02:00:35.455879 31945 sched.cpp:382] authenticating with master master@172.17.0.3:60904  i0129 02:00:35.455904 31945 sched.cpp:389] using default crammd5 authenticatee  i0129 02:00:35.455943 31956 slave.cpp:4667] querying resource estimator for oversubscribable resources  i0129 02:00:35.456167 31950 authenticatee.cpp:121] creating new client sasl connection  i0129 02:00:35.456218 31953 statusupdatemanager.cpp:174] pausing sending status updates  i0129 02:00:35.456219 31956 slave.cpp:795] new master detected at master@172.17.0.3:60904  i0129 02:00:35.456298 31956 slave.cpp:858] authenticating with master master@172.17.0.3:60904  i0129 02:00:35.456323 31956 slave.cpp:863] using default crammd5 authenticatee  i0129 02:00:35.456490 31948 authenticatee.cpp:121] creating new client sasl connection  i0129 02:00:35.456492 31956 slave.cpp:831] detecting new master  i0129 02:00:35.456588 31946 master.cpp:5521] authenticating scheduler93e745f00e484a8fb22793569976c5e8@172.17.0.3:60904  i0129 02:00:35.456686 31956 slave.cpp:4681] received oversubscribable resources  from the resource estimator  i0129 02:00:35.456805 31953 authenticator.cpp:413] starting authentication session for crammd5authenticatee(804)@172.17.0.3:60904  i0129 02:00:35.456878 31946 master.cpp:5521] authenticating slave(372)@172.17.0.3:60904  i0129 02:00:35.457124 31953 authenticator.cpp:413] starting authentication session for crammd5authenticatee(805)@172.17.0.3:60904  i0129 02:00:35.457157 31948 authenticator.cpp:98] creating new server sasl connection  i0129 02:00:35.457373 31946 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0129 02:00:35.457381 31951 authenticator.cpp:98] creating new server sasl connection  i0129 02:00:35.457491 31946 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0129 02:00:35.457598 31946 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0129 02:00:35.457612 31951 authenticator.cpp:203] received sasl authentication start  i0129 02:00:35.457635 31946 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0129 02:00:35.457680 31951 authenticator.cpp:325] authentication requires more steps  i0129 02:00:35.457767 31954 authenticator.cpp:203] received sasl authentication start  i0129 02:00:35.457768 31948 authenticatee.cpp:258] received sasl authentication step  i0129 02:00:35.457830 31954 authenticator.cpp:325] authentication requires more steps  i0129 02:00:35.457885 31948 authenticator.cpp:231] received sasl authentication step  i0129 02:00:35.457918 31948 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '90665f991b70' server fqdn: '90665f991b70' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0129 02:00:35.457933 31948 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0129 02:00:35.457954 31959 authenticatee.cpp:258] received sasl authentication step  i0129 02:00:35.457993 31948 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0129 02:00:35.458031 31948 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '90665f991b70' server fqdn: '90665f991b70' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0129 02:00:35.458050 31948 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0129 02:00:35.458065 31948 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0129 02:00:35.458096 31948 authenticator.cpp:317] authentication success  i0129 02:00:35.458112 31944 authenticator.cpp:231] received sasl authentication step  i0129 02:00:35.458142 31944 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '90665f991b70' server fqdn: '90665f991b70' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0129 02:00:35.458173 31944 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0129 02:00:35.458206 31954 authenticatee.cpp:298] authentication success  i0129 02:00:35.458256 31957 master.cpp:5551] successfully authenticated principal 'testprincipal' at scheduler93e745f00e484a8fb22793569976c5e8@172.17.0.3:60904  i0129 02:00:35.458206 31944 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0129 02:00:35.458360 31944 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '90665f991b70' server fqdn: '90665f991b70' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0129 02:00:35.458382 31944 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0129 02:00:35.458397 31944 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0129 02:00:35.458489 31944 authenticator.cpp:317] authentication success  i0129 02:00:35.458623 31953 sched.cpp:471] successfully authenticated with master master@172.17.0.3:60904  i0129 02:00:35.458649 31953 sched.cpp:780] sending subscribe call to master@172.17.0.3:60904  i0129 02:00:35.458653 31956 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(804)@172.17.0.3:60904  i0129 02:00:35.458673 31951 authenticatee.cpp:298] authentication success  i0129 02:00:35.458709 31952 master.cpp:5551] successfully authenticated principal 'testprincipal' at slave(372)@172.17.0.3:60904  i0129 02:00:35.458906 31955 slave.cpp:926] successfully authenticated with master master@172.17.0.3:60904  i0129 02:00:35.458983 31956 authenticator.cpp:431] authentication session cleanup for crammd5_authenticatee(805)@172.17.0.3:60904  i0129 02:00:35.459033 31955 slave.cpp:1320] will retry registration in 7.075135ms if necessary  i0129 02:00:35.459128 31953 sched.cpp:813] will retry registration in 86.579738ms if necessary  i0129 02:00:35.459193 31950 master.cpp:4235] registering slave at slave(372)@172.17.0.3:60904 (90665f991b70) with id 823f2212bf284dd6959d796029d32afbs0  i0129 02:00:35.459489 31950 master.cpp:2278] received subscribe call for framework 'default' at scheduler93e745f00e484a8fb22793569976c5e8@172.17.0.3:60904  i0129 02:00:35.459513 31950 master.cpp:1749] authorizing framework principal 'testprincipal' to receive offers for role ''  i0129 02:00:35.459516 31959 registrar.cpp:439] applied 1 operations in 62499ns; attempting to update the 'registry'  i0129 02:00:35.459766 31956 master.cpp:2349] subscribing framework default with checkpointing disabled and capabilities [  ]  i0129 02:00:35.460095 31955 log.cpp:683] attempting to append 339 bytes to the log  i0129 02:00:35.460192 31948 hierarchical.cpp:265] added framework 823f2212bf284dd6959d796029d32afb0000  i0129 02:00:35.460247 31956 sched.cpp:707] framework registered with 823f2212bf284dd6959d796029d32afb 0000  i0129 02:00:35.460314 31958 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0129 02:00:35.460388 31948 hierarchical.cpp:1403] no resources available to allocate!  i0129 02:00:35.460449 31948 hierarchical.cpp:1498] no inverse offers to send out!  i0129 02:00:35.460402 31956 sched.cpp:721] scheduler::registered took 136519ns  i0129 02:00:35.460482 31948 hierarchical.cpp:1096] performed allocation for 0 slaves in 158218ns  i0129 02:00:35.461187 31944 replica.cpp:537] replica received write request for position 3 from (11829)@172.17.0.3:60904  i0129 02:00:35.467929 31954 slave.cpp:1320] will retry registration in 14.701381ms if necessary  i0129 02:00:35.468183 31952 master.cpp:4223] ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress  i0129 02:00:35.483300 31959 slave.cpp:1320] will retry registration in 8.003223ms if necessary  i0129 02:00:35.483500 31946 master.cpp:4223] ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress  i0129 02:00:35.491843 31945 slave.cpp:1320] will retry registration in 52.952447ms if necessary  i0129 02:00:35.491962 31948 master.cpp:4223]...",3,val
MESOS-4557,Automatically generate command-line flag documentation,"to ensure that the command line flag documentation in configuration.md stays in sync with the help strings in the various flags.cpp files, it could be beneficial to automate the generation of those docs. such a script could be run as part of the build process, ensuring that changes to the help strings would show up in the documentation as well.    in addition to parsing and formatting the help strings for display as html, this could also involve specifying collections of flags to be grouped together in order to provide logical structure to the configuration.md documentation.",3,val
MESOS-4558,Reduce the running time of benchmark tests.,"currently benchmark tests take a long time (>5 hours). it would be nice to reduce the total time taken by the benchmark tests to enable us to run them on asf ci.    command to run only benchmark tests    mesosbenchmark=1 gtestfilter=""benchmark"" make check  ",2,val
MESOS-4559,Run benchmark tests in ASF CI,the build job is already created on asf ci (https:/builds.apache.org/job/mesosbenchmarks/) but is currently disabled due to mesos4558.,2,val
MESOS-4562,"Mesos UI shows wrong count for ""started"" tasks","the task started field shows the number of tasks in state ""tasksstarting"" as opposed to those in ""taskrunning"" state.",2,val
MESOS-4564,Separate Appc protobuf messages to its own file.,it would be cleaner to keep the appc protobuf messages separate from other mesos messages.,2,val
MESOS-4566,Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.,"a few of the critical code paths in jsonify involve unnecessary temporary string construction and copies (inherited from the json:: ). for example, strings::trim is used to remove trailing 0s from printing doubles. we print doubles a lot, and therefore constructing a temporary std::string on printing of every double is extremely costly. this ticket captures the work involved in avoiding them.",1,val
MESOS-4567,Deprecate TASK_STARTING state,"we currently have the following task stages:     taskstaging > set by slave   taskstarting > set by the executor (?)   taskrunning > set by the executor when the task is running    taskxxx > task termination statuses    the confusion here is about taskstarting. this is the state between taskstaging and taskrunning and is somewhat non intuitive for the reader. further, looks like no where in the source code, we are setting the taskstarting state.    why shouldn't we just deprecate/remove it?",2,val
MESOS-4570,DockerFetcherPluginTest.INTERNET_CURL_FetchImage seems flaky.,"  ../configure enablessl enablelibevent && make check        gtestrepeat=1 gtestbreakonfailure  gtestfilter=dockerfetcherplugintest.internetcurlfetchimage       failed at the 22nd run.       [ run      ] dockerfetcherplugintest.internetcurlfetchimage  ../../src/tests/urifetchertests.cpp:276: failure  failed to wait 15secs for fetcher.get()>fetch(uri, dir)   aborted at 1454207653 (unix time) try ""date  d @1454207653"" if you are using gnu date   pc: @          0x167023a testing::unittest::addtestpartresult()   sigsegv (@0x0) received by pid 19868 (tid 0x7f500fc877c0) from pid 0; stack trace:       @     0x7f5008f368d0 (unknown)      @          0x167023a testing::unittest::addtestpartresult()      @          0x1664c73 testing::internal::asserthelper::operator=()      @          0x146ac6f mesos::internal::tests::dockerfetcherplugintestinternetcurlfetchimagetest::testbody()      @          0x168dc70 testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x1688cc8 testing::internal::handleexceptionsinmethodifsupported/()      @          0x166a013 testing::test::run()      @          0x166a7a1 testing::testinfo::run()      @          0x166addc testing::testcase::run()      @          0x167172b testing::internal::unittestimpl::runalltests()      @          0x168e8ff testing::internal::handlesehexceptionsinmethodifsupported/()      @          0x168981e testing::internal::handleexceptionsinmethodifsupported/()      @          0x167045b testing::unittest::run()      @           0xe2d476 runall_tests()      @           0xe2d08c main      @     0x7f5008b9fb45 (unknown)      @           0x9c6bf9 (unknown)  ",1,val
MESOS-4573,Design doc for scheduler HTTP Stream IDs,"this ticket is for the design of http stream ids, for use with http schedulers. these ids allow mesos to distinguish between different instances of http framework schedulers.",5,val
MESOS-4575,Fix Appc image caching to share with image fetcher,"as appc image fetcher is being developed, image cache needs to be shared between store and the image fetcher.",3,val
MESOS-4576,"Introduce a stout helper for ""which""",we may want to add a helper to stout/os.hpp that will natively emulate the functionality of the linux utility which.  i.e.    option/ which(const string& command)        this helper may be useful:   for test filters in src/tests/environment.cpp   a few tests in src/tests/containerizer/portmappingtests.cpp   the sha512 utility in src/common/command_utils.cpp   as runtime checks in the logrotatecontainerlogger    etc.,2,val
MESOS-4582,"state.json serving duplicate ""active"" fields","state.json is serving duplicate ""active"" fields in frameworks.  see the framework ""47df96c23f854bc5b781709b2c30c752 0000"" in the attached file",1,val
MESOS-4583,Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`,we already have examples/testframework.cpp for testing pid based frameworks. we would ideally want to rename eventcall_framework to correctly reflect that it's an example for http based framework.,1,val
MESOS-4584,Update Rakefile for mesos site generation,the stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.,2,val
MESOS-4590,"Add test case for reservations with same role, different principals",we don't have a test case that covers $subject; we probably should.,2,val
MESOS-4591,`/reserve` and `/create-volumes` endpoints allow operations for any role,"when frameworks reserve resources, the validation of the operation ensures that the role of the reservation matches the role of the framework. for the case of the /reserve operator endpoint, however, the operator has no role to validate, so this check isn't performed.    this means that if an acl exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for any role through the operator endpoint.    we should restrict reservations made through the operator endpoint to specified roles. a few possibilities:   the object of the reserveresources acl could be changed from resources to roles   a second acl could be added for authorization of reserve operations, with an object of role    our conception of the resources object in the reserveresources acl could be expanded to include role information, i.e., disk(role1);mem(role1)",3,val
MESOS-4596,Add common Appc spec utilities., add common utility functions such as :         validating image information against actual data in the image directory.         getting list of dependencies at depth 1 for an image.          getting image path simple image discovery.  ,2,val
MESOS-4598,Logrotate ContainerLogger should not remove IP from environment.,"the logrotatecontainerlogger starts libprocess using subprocesses.  libprocess initialization will attempt to resolve the ip from the hostname.  if a dns service is not available, this step will fail, which terminates the logger subprocess prematurely.    since the logger subprocesses live on the agent, they should use the same libprocess_ip supplied to the agent.",1,val
MESOS-4600,Use `std::quoted` for strings in error messages,we'd like to have a consistent format for error strings through the code base.   as per this comment:  https:/issues.apache.org/jira/browse/mesos3772?focusedcommentid=14965652&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment14965652    we can then overload the stream operator to make sur strings are quoted as needed.    note: we need to first require compilers that support c14. for now we have to wait for msvc to be part of that list.,3,val
MESOS-4604,ROOT_DOCKER_DockerHealthyTask is flaky.,"log from teamcity that is running sudo ./bin/mesostests.sh on aws ec2 instances:    [18:27:14][step 8/8] [ ] 8 tests from healthchecktest  [18:27:14][step 8/8] [ run      ] healthchecktest.healthytask  [18:27:17][step 8/8] [       ok ] healthchecktest.healthytask (2222 ms)  [18:27:17][step 8/8] [ run      ] healthchecktest.rootdockerdockerhealthytask  [18:27:36][step 8/8] ../../src/tests/healthchecktests.cpp:388: failure  [18:27:36][step 8/8] failed to wait 15secs for termination  [18:27:36][step 8/8] f0204 18:27:35.981302 23085 logging.cpp:64] raw: pure virtual method called  [18:27:36][step 8/8]     @     0x7f7077055e1c  google::logmessage::fail()  [18:27:36][step 8/8]     @     0x7f707705ba6f  google::rawlog()  [18:27:36][step 8/8]     @     0x7f70760f76c9  cxapurevirtual  [18:27:36][step 8/8]     @           0xa9423c  mesos::internal::tests::cluster::slaves::shutdown()  [18:27:36][step 8/8]     @          0x1074e45  mesos::internal::tests::mesostest::shutdownslaves()  [18:27:36][step 8/8]     @          0x1074de4  mesos::internal::tests::mesostest::shutdown()  [18:27:36][step 8/8]     @          0x1070ec7  mesos::internal::tests::mesostest::teardown()  [18:27:36][step 8/8]     @          0x16eb7b2  testing::internal::handlesehexceptionsinmethodifsupported/()  [18:27:36][step 8/8]     @          0x16e61a9  testing::internal::handleexceptionsinmethodifsupported/()  [18:27:36][step 8/8]     @          0x16c56aa  testing::test::run()  [18:27:36][step 8/8]     @          0x16c5e89  testing::testinfo::run()  [18:27:36][step 8/8]     @          0x16c650a  testing::testcase::run()  [18:27:36][step 8/8]     @          0x16cd1f6  testing::internal::unittestimpl::runalltests()  [18:27:36][step 8/8]     @          0x16ec513  testing::internal::handlesehexceptionsinmethodifsupported/()  [18:27:36][step 8/8]     @          0x16e6df1  testing::internal::handleexceptionsinmethodifsupported/()  [18:27:36][step 8/8]     @          0x16cbe26  testing::unittest::run()  [18:27:36][step 8/8]     @           0xe54c84  runalltests()  [18:27:36][step 8/8]     @           0xe54867  main  [18:27:36][step 8/8]     @     0x7f7071560a40  (unknown)  [18:27:36][step 8/8]     @           0x9b52d9  start  [18:27:36][step 8/8] aborted (core dumped)  [18:27:36][step 8/8] process exited with code 134    happens with ubuntu 15.04, centos 6, centos 7 quite_ often. ",2,val
MESOS-4609,Subprocess should be more intelligent about setting/inheriting libprocess environment variables ,"mostly copied from https:/issues.apache.org/jira/browse/mesos4598?focusedcommentid=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:commenttabpanel#comment15133497    a subprocess inheriting the environment variables libprocess may run into some accidental fatalities:     bind failure > exit    subprocess sets a different port on purpose  nothing happens (?) |    (?) = means this is usually the case, but not 100%.    a complete fix would look something like:   if the subprocess call gets environment = none(), we should automatically remove libprocessport from the inherited environment.     the parts of https:/github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#l265 dealing with libprocess & libmesos should be refactored into libprocess as a helper.  we would use this helper for the containerizer, fetcher, and containerlogger module.   if the subprocess call is given libprocessport == os::getenv(""libprocessport""), we can log(warn) and unset the env var locally.",2,val
MESOS-4611,Passing a lambda to dispatch() always matches the template returning void,"the following idiom does not currently compile:        future/ initialized = dispatch(pid, [] () > nothing );      this seems nonintuitive because the following template exists for dispatch:      template /  future/ dispatch(const upid& pid, const std::function/& f)  ));      internal::dispatch(pid, f);        return promise>future();  }           however, lambdas cannot be implicitly cast to a corresponding std::function/ type.  to make this work, you have to explicitly type the lambda before passing it to dispatch.        std::function/ f = []() ;    future/ initialized = dispatch(pid, f);      we should add template support to allow lambdas to be passed to dispatch() without explicit typing.   ",5,val
MESOS-4612,Update vendored ZooKeeper to 3.4.8,see: http:/zookeeper.apache.org/doc/r3.4.8/releasenotes.html for improvements / bug fixes    added a new patch that solved [zookeeper1643](https:/issues.apache.org/jira/browse/zookeeper1643)    the original patch: /,3,val
MESOS-4614,SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky,"just saw this failure on the asf ci:      [ run      ] slaverecoverytest/0.cleanuphttpexecutor  i0206 00:22:44.791671  2824 leveldb.cpp:174] opened db in 2.539372ms  i0206 00:22:44.792459  2824 leveldb.cpp:181] compacted db in 740473ns  i0206 00:22:44.792510  2824 leveldb.cpp:196] created db iterator in 24164ns  i0206 00:22:44.792532  2824 leveldb.cpp:202] seeked to beginning of db in 1831ns  i0206 00:22:44.792548  2824 leveldb.cpp:271] iterated through 0 keys in the db in 342ns  i0206 00:22:44.792605  2824 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0206 00:22:44.793256  2847 recover.cpp:447] starting replica recovery  i0206 00:22:44.793480  2847 recover.cpp:473] replica is in empty status  i0206 00:22:44.794538  2847 replica.cpp:673] replica in empty status received a broadcasted recover request from (9472)@172.17.0.2:43484  i0206 00:22:44.795040  2848 recover.cpp:193] received a recover response from a replica in empty status  i0206 00:22:44.795644  2848 recover.cpp:564] updating replica status to starting  i0206 00:22:44.796519  2850 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 752810ns  i0206 00:22:44.796545  2850 replica.cpp:320] persisted replica status to starting  i0206 00:22:44.796725  2848 recover.cpp:473] replica is in starting status  i0206 00:22:44.797828  2857 replica.cpp:673] replica in starting status received a broadcasted recover request from (9473)@172.17.0.2:43484  i0206 00:22:44.798355  2850 recover.cpp:193] received a recover response from a replica in starting status  i0206 00:22:44.799193  2850 recover.cpp:564] updating replica status to voting  i0206 00:22:44.799583  2855 master.cpp:376] master 0b206a40a9c34d44a5bd8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484  i0206 00:22:44.799609  2855 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/n2fxqv/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.28.0/inst/share/mesos/webui"" workdir=""/tmp/n2fxqv/master"" zksessiontimeout=""10secs""  i0206 00:22:44.799991  2855 master.cpp:423] master only allowing authenticated frameworks to register  i0206 00:22:44.800009  2855 master.cpp:428] master only allowing authenticated slaves to register  i0206 00:22:44.800020  2855 credentials.hpp:35] loading credentials for authentication from '/tmp/n2fxqv/credentials'  i0206 00:22:44.800245  2850 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 679345ns  i0206 00:22:44.800370  2850 replica.cpp:320] persisted replica status to voting  i0206 00:22:44.800397  2855 master.cpp:468] using default 'crammd5' authenticator  i0206 00:22:44.800693  2855 master.cpp:537] using default 'basic' http authenticator  i0206 00:22:44.800815  2855 master.cpp:571] authorization enabled  i0206 00:22:44.801216  2850 recover.cpp:578] successfully joined the paxos group  i0206 00:22:44.801604  2850 recover.cpp:462] recover process terminated  i0206 00:22:44.801759  2856 whitelistwatcher.cpp:77] no whitelist given  i0206 00:22:44.801725  2847 hierarchical.cpp:144] initialized hierarchical allocator process  i0206 00:22:44.803982  2855 master.cpp:1712] the newly elected leader is master@172.17.0.2:43484 with id 0b206a40a9c34d44a5bd8032d60a32ca  i0206 00:22:44.804026  2855 master.cpp:1725] elected as the leading master!  i0206 00:22:44.804059  2855 master.cpp:1470] recovering from registrar  i0206 00:22:44.804424  2855 registrar.cpp:307] recovering registrar  i0206 00:22:44.805202  2855 log.cpp:659] attempting to start the writer  i0206 00:22:44.806782  2856 replica.cpp:493] replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1  i0206 00:22:44.807368  2856 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 547939ns  i0206 00:22:44.807395  2856 replica.cpp:342] persisted promised to 1  i0206 00:22:44.808375  2856 coordinator.cpp:238] coordinator attempting to fill missing positions  i0206 00:22:44.809460  2848 replica.cpp:388] replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2  i0206 00:22:44.809929  2848 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 427561ns  i0206 00:22:44.809967  2848 replica.cpp:712] persisted action at 0  i0206 00:22:44.811035  2850 replica.cpp:537] replica received write request for position 0 from (9477)@172.17.0.2:43484  i0206 00:22:44.811149  2850 leveldb.cpp:436] reading position from leveldb took 36452ns  i0206 00:22:44.811532  2850 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 318924ns  i0206 00:22:44.811615  2850 replica.cpp:712] persisted action at 0  i0206 00:22:44.812532  2850 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0206 00:22:44.813117  2850 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 476530ns  i0206 00:22:44.813143  2850 replica.cpp:712] persisted action at 0  i0206 00:22:44.813166  2850 replica.cpp:697] replica learned nop action at position 0  i0206 00:22:44.813984  2848 log.cpp:675] writer started with ending position 0  i0206 00:22:44.815549  2848 leveldb.cpp:436] reading position from leveldb took 31800ns  i0206 00:22:44.817061  2848 registrar.cpp:340] successfully fetched the registry (0b) in 12.591104ms  i0206 00:22:44.817319  2848 registrar.cpp:439] applied 1 operations in 63480ns; attempting to update the 'registry'  i0206 00:22:44.818780  2845 log.cpp:683] attempting to append 170 bytes to the log  i0206 00:22:44.818981  2845 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0206 00:22:44.819941  2845 replica.cpp:537] replica received write request for position 1 from (9478)@172.17.0.2:43484  i0206 00:22:44.820582  2845 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 600949ns  i0206 00:22:44.820608  2845 replica.cpp:712] persisted action at 1  i0206 00:22:44.821552  2845 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0206 00:22:44.821934  2845 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 352813ns  i0206 00:22:44.821960  2845 replica.cpp:712] persisted action at 1  i0206 00:22:44.821979  2845 replica.cpp:697] replica learned append action at position 1  i0206 00:22:44.823447  2845 registrar.cpp:484] successfully updated the 'registry' in 5.987072ms  i0206 00:22:44.823580  2845 registrar.cpp:370] successfully recovered registrar  i0206 00:22:44.823833  2845 log.cpp:702] attempting to truncate the log to 1  i0206 00:22:44.824203  2845 master.cpp:1522] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0206 00:22:44.824291  2845 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0206 00:22:44.824645  2845 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0206 00:22:44.825222  2850 replica.cpp:537] replica received write request for position 2 from (9479)@172.17.0.2:43484  i0206 00:22:44.825742  2850 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 481617ns  i0206 00:22:44.825772  2850 replica.cpp:712] persisted action at 2  i0206 00:22:44.826748  2852 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0206 00:22:44.827368  2852 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 588591ns  i0206 00:22:44.827432  2852 leveldb.cpp:399] deleting ~1 keys from leveldb took 33059ns  i0206 00:22:44.827450  2852 replica.cpp:712] persisted action at 2  i0206 00:22:44.827468  2852 replica.cpp:697] replica learned truncate action at position 2  i0206 00:22:44.838011  2824 containerizer.cpp:149] using isolation: posix/cpu,posix/mem,filesystem/posix  w0206 00:22:44.838873  2824 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i0206 00:22:44.843785  2857 slave.cpp:193] slave started on 172.17.0.2:43484  i0206 00:22:44.843819  2857 slave.cpp:194] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/slaverecoverytest0cleanuphttpexecutorkaxwvw/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/slaverecoverytest0cleanuphttpexecutorkaxwvw/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.28.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/slaverecoverytest0cleanuphttpexecutorkaxwvw""  i0206 00:22:44.844292  2857 credentials.hpp:83] loading credential for authentication from '/tmp/slaverecoverytest0cleanuphttpexecutorkaxwvw/credential'  i0206 00:22:44.844518  2857 slave.cpp:324] slave using credential for: testprincipal  i0206 00:22:44.844696  2857 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0206 00:22:44.845243  2857 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 00:22:44.845326  2857 slave.cpp:472] slave attributes: [  ]  i0206 00:22:44.845342  2857 slave.cpp:477] slave hostname: 6632562f1ade  i0206 00:22:44.845953  2824 sched.cpp:222] version: 0.28.0  i0206 00:22:44.846853  2848 sched.cpp:326] new master detected at master@172.17.0.2:43484  i0206 00:22:44.846936  2848 sched.cpp:382] authenticating with master master@172.17.0.2:43484  i0206 00:22:44.846958  2848 sched.cpp:389] using default crammd5 authenticatee  i0206 00:22:44.847692  2858 state.cpp:58] recovering state from '/tmp/slaverecoverytest0cleanuphttpexecutorkaxwvw/meta'  i0206 00:22:44.848108  2850 statusupdatemanager.cpp:200] recovering status update manager  i0206 00:22:44.848325  2852 containerizer.cpp:397] recovering containerizer  i0206 00:22:44.848603  2845 authenticatee.cpp:121] creating new client sasl connection  i0206 00:22:44.849719  2845 master.cpp:5523] authenticating scheduler63899759d7fc42b2837157484f352895@172.17.0.2:43484  i0206 00:22:44.850052  2852 authenticator.cpp:413] starting authentication session for crammd5authenticatee(662)@172.17.0.2:43484  i0206 00:22:44.850227  2854 provisioner.cpp:245] provisioner recovery complete  i0206 00:22:44.850410  2852 authenticator.cpp:98] creating new server sasl connection  i0206 00:22:44.850692  2852 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0206 00:22:44.850720  2852 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0206 00:22:44.850805  2852 authenticator.cpp:203] received sasl authentication start  i0206 00:22:44.850862  2852 authenticator.cpp:325] authentication requires more steps  i0206 00:22:44.850939  2852 authenticatee.cpp:258] received sasl authentication step  i0206 00:22:44.851027  2852 authenticator.cpp:231] received sasl authentication step  i0206 00:22:44.851052  2852 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '6632562f1ade' server fqdn: '6632562f1ade' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0206 00:22:44.851063  2852 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0206 00:22:44.851102  2852 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0206 00:22:44.851121  2852 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '6632562f1ade' server fqdn: '6632562f1ade' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0206 00:22:44.851130  2852 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0206 00:22:44.851136  2852 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0206 00:22:44.851150  2852 authenticator.cpp:317] authentication success  i0206 00:22:44.851219  2850 authenticatee.cpp:298] authentication success  i0206 00:22:44.851310  2850 master.cpp:5553] successfully authenticated principal 'testprincipal' at scheduler63899759d7fc42b2837157484f352895@172.17.0.2:43484  i0206 00:22:44.851485  2849 slave.cpp:4496] finished recovery  i0206 00:22:44.852154  2843 sched.cpp:471] successfully authenticated with master master@172.17.0.2:43484  i0206 00:22:44.852175  2843 sched.cpp:776] sending subscribe call to master@172.17.0.2:43484  i0206 00:22:44.852262  2843 sched.cpp:809] will retry registration in 939.183679ms if necessary  i0206 00:22:44.852375  2844 master.cpp:2280] received subscribe call for framework 'default' at scheduler63899759d7fc42b2837157484f352895@172.17.0.2:43484  i0206 00:22:44.852448  2844 master.cpp:1751] authorizing framework principal 'testprincipal' to receive offers for role ''  i0206 00:22:44.852699  2852 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(662)@172.17.0.2:43484  i0206 00:22:44.852782  2844 master.cpp:2351] subscribing framework default with checkpointing enabled and capabilities [  ]  i0206 00:22:44.853056  2849 slave.cpp:4668] querying resource estimator for oversubscribable resources  i0206 00:22:44.853421  2856 hierarchical.cpp:265] added framework 0b206a40a9c34d44a5bd8032d60a32ca0000  i0206 00:22:44.853513  2856 hierarchical.cpp:1403] no resources available to allocate!  i0206 00:22:44.853582  2844 sched.cpp:703] framework registered with 0b206a40a9c34d44a5bd8032d60a32ca0000  i0206 00:22:44.853613  2852 slave.cpp:4682] received oversubscribable resources  from the resource estimator  i0206 00:22:44.853663  2844 sched.cpp:717] scheduler::registered took 53762ns  i0206 00:22:44.853899  2843 slave.cpp:796] new master detected at master@172.17.0.2:43484  i0206 00:22:44.853955  2854 statusupdatemanager.cpp:174] pausing sending status updates  i0206 00:22:44.853997  2856 hierarchical.cpp:1498] no inverse offers to send out!  i0206 00:22:44.853960  2843 slave.cpp:859] authenticating with master master@172.17.0.2:43484  i0206 00:22:44.854035  2843 slave.cpp:864] using default crammd5 authenticatee  i0206 00:22:44.854030  2856 hierarchical.cpp:1096] performed allocation for 0 slaves in 581355ns  i0206 00:22:44.854182  2843 slave.cpp:832] detecting new master  i0206 00:22:44.854277  2854 authenticatee.cpp:121] creating new client sasl connection  i0206 00:22:44.854517  2843 master.cpp:5523] authenticating slave@172.17.0.2:43484  i0206 00:22:44.854603  2854 authenticator.cpp:413] starting authentication session for crammd5authenticatee(663)@172.17.0.2:43484  i0206 00:22:44.854836  2855 authenticator.cpp:98] creating new server sasl connection  i0206 00:22:44.855013  2852 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0206 00:22:44.855044  2852 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0206 00:22:44.855139  2855 authenticator.cpp:203] received sasl authentication start  i0206 00:22:44.855186  2855 authenticator.cpp:325] authentication requires more steps  i0206 00:22:44.855263  2855 authenticatee.cpp:258] received sasl authentication step  i0206 00:22:44.855352  2855 authenticator.cpp:231] received sasl authentication step  i0206 00:22:44.855381  2855 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '6632562f1ade' server fqdn: '6632562f1ade' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0206 00:22:44.855389  2855 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0206 00:22:44.855419  2855 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0206 00:22:44.855438  2855 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '6632562f1ade' server fqdn: '6632562f1ade' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0206 00:22:44.855448  2855 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0206 00:22:44.855453  2855 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0206 00:22:44.855464  2855 authenticator.cpp:317] authentication success  i0206 00:22:44.855540  2851 authenticatee.cpp:298] authentication success  i0206 00:22:44.855721  2851 authenticator.cpp:431] authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484  i0206 00:22:44.855832  2852 slave.cpp:927] successfully authenticated with master master@172.17.0.2:43484  i0206 00:22:44.855615  2855 master.cpp:5553] successfully authenticated principal 'testprincipal' at slave@172.17.0.2:43484  i0206 00:22:44.855973  2852 slave.cpp:1321] will retry registration in 9.327708ms if necessary  i0206 00:22:44.856145  2854 master.cpp:4237] registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40a9c34d44a5bd8032d60a32ca s0  i0206 00:22:44.856598  2851 registrar.cpp:439] applied 1 operations in 59112ns; attempting to update the 'registry'  i0206 00:22:44.857403  2851 log.cpp:683] attempting to append 339 bytes to the log  i0206 00:22:44.857525  2855 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0206 00:22:44.858482  2844 replica.cpp:537] replica received write request for position 3 from (9493)@172.17.0.2:43484  i0206 00:22:44.858755  2844 leveldb.cpp:341] persisting action (358 bytes) to leveldb took 228484ns  i0206 00:22:44.858855  2844 replica.cpp:712] persisted action at 3  i0206 00:22:44.859751  2852 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0206 00:22:44.860332  2852 leveldb.cpp:341] persisting action (360 bytes) to leveldb took 549638ns  i0206 00:22:44.860358  2852 replica.cpp:712] persisted action at 3  i0206 00:22:44.860411  2852 replica.cpp:697] replica learned append action at position 3  i0206 00:22:44.862709  2856 registrar.cpp:484] succe...",3,val
MESOS-4615,ContainerLoggerTest.DefaultToSandbox is flaky,"just saw this failure on the asf ci:      [ run      ] containerloggertest.defaulttosandbox  i0206 01:25:03.766458  2824 leveldb.cpp:174] opened db in 72.979786ms  i0206 01:25:03.811712  2824 leveldb.cpp:181] compacted db in 45.162067ms  i0206 01:25:03.811810  2824 leveldb.cpp:196] created db iterator in 26090ns  i0206 01:25:03.811828  2824 leveldb.cpp:202] seeked to beginning of db in 3173ns  i0206 01:25:03.811839  2824 leveldb.cpp:271] iterated through 0 keys in the db in 497ns  i0206 01:25:03.811900  2824 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0206 01:25:03.812785  2849 recover.cpp:447] starting replica recovery  i0206 01:25:03.813043  2849 recover.cpp:473] replica is in empty status  i0206 01:25:03.814668  2854 replica.cpp:673] replica in empty status received a broadcasted recover request from (371)@172.17.0.8:37843  i0206 01:25:03.815210  2849 recover.cpp:193] received a recover response from a replica in empty status  i0206 01:25:03.815732  2854 recover.cpp:564] updating replica status to starting  i0206 01:25:03.819664  2857 master.cpp:376] master 914b62f995f64c57a7e39b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843  i0206 01:25:03.819703  2857 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/h5vu5i/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.28.0/inst/share/mesos/webui"" workdir=""/tmp/h5vu5i/master"" zksessiontimeout=""10secs""  i0206 01:25:03.820241  2857 master.cpp:423] master only allowing authenticated frameworks to register  i0206 01:25:03.820257  2857 master.cpp:428] master only allowing authenticated slaves to register  i0206 01:25:03.820269  2857 credentials.hpp:35] loading credentials for authentication from '/tmp/h5vu5i/credentials'  i0206 01:25:03.821110  2857 master.cpp:468] using default 'crammd5' authenticator  i0206 01:25:03.821311  2857 master.cpp:537] using default 'basic' http authenticator  i0206 01:25:03.821636  2857 master.cpp:571] authorization enabled  i0206 01:25:03.821979  2846 hierarchical.cpp:144] initialized hierarchical allocator process  i0206 01:25:03.822057  2846 whitelistwatcher.cpp:77] no whitelist given  i0206 01:25:03.825460  2847 master.cpp:1712] the newly elected leader is master@172.17.0.8:37843 with id 914b62f995f64c57a7e39b06e2c1c8de  i0206 01:25:03.825512  2847 master.cpp:1725] elected as the leading master!  i0206 01:25:03.825533  2847 master.cpp:1470] recovering from registrar  i0206 01:25:03.825835  2847 registrar.cpp:307] recovering registrar  i0206 01:25:03.848212  2854 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 32.226093ms  i0206 01:25:03.848299  2854 replica.cpp:320] persisted replica status to starting  i0206 01:25:03.848702  2854 recover.cpp:473] replica is in starting status  i0206 01:25:03.850728  2858 replica.cpp:673] replica in starting status received a broadcasted recover request from (373)@172.17.0.8:37843  i0206 01:25:03.851230  2854 recover.cpp:193] received a recover response from a replica in starting status  i0206 01:25:03.852018  2854 recover.cpp:564] updating replica status to voting  i0206 01:25:03.881681  2854 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 29.184163ms  i0206 01:25:03.881772  2854 replica.cpp:320] persisted replica status to voting  i0206 01:25:03.882058  2854 recover.cpp:578] successfully joined the paxos group  i0206 01:25:03.882258  2854 recover.cpp:462] recover process terminated  i0206 01:25:03.883076  2854 log.cpp:659] attempting to start the writer  i0206 01:25:03.885040  2854 replica.cpp:493] replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1  i0206 01:25:03.915132  2854 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 29.980589ms  i0206 01:25:03.915215  2854 replica.cpp:342] persisted promised to 1  i0206 01:25:03.916038  2856 coordinator.cpp:238] coordinator attempting to fill missing positions  i0206 01:25:03.917659  2856 replica.cpp:388] replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2  i0206 01:25:03.948698  2856 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 30.974607ms  i0206 01:25:03.948786  2856 replica.cpp:712] persisted action at 0  i0206 01:25:03.950920  2849 replica.cpp:537] replica received write request for position 0 from (376)@172.17.0.8:37843  i0206 01:25:03.951011  2849 leveldb.cpp:436] reading position from leveldb took 44263ns  i0206 01:25:03.982026  2849 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 30.947321ms  i0206 01:25:03.982225  2849 replica.cpp:712] persisted action at 0  i0206 01:25:03.983867  2849 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0206 01:25:04.015499  2849 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 30.957888ms  i0206 01:25:04.015591  2849 replica.cpp:712] persisted action at 0  i0206 01:25:04.015682  2849 replica.cpp:697] replica learned nop action at position 0  i0206 01:25:04.016666  2849 log.cpp:675] writer started with ending position 0  i0206 01:25:04.017881  2855 leveldb.cpp:436] reading position from leveldb took 56779ns  i0206 01:25:04.018934  2852 registrar.cpp:340] successfully fetched the registry (0b) in 193.048064ms  i0206 01:25:04.019076  2852 registrar.cpp:439] applied 1 operations in 38180ns; attempting to update the 'registry'  i0206 01:25:04.020100  2844 log.cpp:683] attempting to append 170 bytes to the log  i0206 01:25:04.020288  2855 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0206 01:25:04.021323  2844 replica.cpp:537] replica received write request for position 1 from (377)@172.17.0.8:37843  i0206 01:25:04.054726  2844 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 33.309419ms  i0206 01:25:04.054818  2844 replica.cpp:712] persisted action at 1  i0206 01:25:04.055933  2844 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0206 01:25:04.088142  2844 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 32.116643ms  i0206 01:25:04.088230  2844 replica.cpp:712] persisted action at 1  i0206 01:25:04.088265  2844 replica.cpp:697] replica learned append action at position 1  i0206 01:25:04.090070  2856 registrar.cpp:484] successfully updated the 'registry' in 70.90816ms  i0206 01:25:04.090338  2851 log.cpp:702] attempting to truncate the log to 1  i0206 01:25:04.090358  2856 registrar.cpp:370] successfully recovered registrar  i0206 01:25:04.090507  2847 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0206 01:25:04.090867  2858 master.cpp:1522] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0206 01:25:04.091449  2858 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0206 01:25:04.092280  2857 replica.cpp:537] replica received write request for position 2 from (378)@172.17.0.8:37843  i0206 01:25:04.125702  2857 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 33.192265ms  i0206 01:25:04.125804  2857 replica.cpp:712] persisted action at 2  i0206 01:25:04.127400  2857 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0206 01:25:04.157727  2857 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 30.268594ms  i0206 01:25:04.157905  2857 leveldb.cpp:399] deleting ~1 keys from leveldb took 88436ns  i0206 01:25:04.157941  2857 replica.cpp:712] persisted action at 2  i0206 01:25:04.157984  2857 replica.cpp:697] replica learned truncate action at position 2  i0206 01:25:04.166174  2824 containerizer.cpp:149] using isolation: posix/cpu,posix/mem,filesystem/posix  w0206 01:25:04.166954  2824 backend.cpp:48] failed to create 'bind' backend: bindbackend requires root privileges  i0206 01:25:04.172008  2844 slave.cpp:193] slave started on 9)@172.17.0.8:37843  i0206 01:25:04.172046  2844 slave.cpp:194] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/containerloggertestdefaulttosandboxfmaksw/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/containerloggertestdefaulttosandboxfmaksw/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.28.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/containerloggertestdefaulttosandboxfmaksw""  i0206 01:25:04.172569  2844 credentials.hpp:83] loading credential for authentication from '/tmp/containerloggertestdefaulttosandboxfmaksw/credential'  i0206 01:25:04.172886  2844 slave.cpp:324] slave using credential for: testprincipal  i0206 01:25:04.173141  2844 resources.cpp:564] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0206 01:25:04.173620  2844 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 01:25:04.173686  2844 slave.cpp:472] slave attributes: [  ]  i0206 01:25:04.173702  2844 slave.cpp:477] slave hostname: 74ef606c4063  i0206 01:25:04.174816  2847 state.cpp:58] recovering state from '/tmp/containerloggertestdefaulttosandboxfmaksw/meta'  i0206 01:25:04.175441  2847 statusupdatemanager.cpp:200] recovering status update manager  i0206 01:25:04.175678  2858 containerizer.cpp:397] recovering containerizer  i0206 01:25:04.177573  2858 provisioner.cpp:245] provisioner recovery complete  i0206 01:25:04.178231  2847 slave.cpp:4496] finished recovery  i0206 01:25:04.178834  2847 slave.cpp:4668] querying resource estimator for oversubscribable resources  i0206 01:25:04.179405  2847 slave.cpp:796] new master detected at master@172.17.0.8:37843  i0206 01:25:04.179500  2847 slave.cpp:859] authenticating with master master@172.17.0.8:37843  i0206 01:25:04.179525  2847 slave.cpp:864] using default crammd5 authenticatee  i0206 01:25:04.179656  2858 statusupdatemanager.cpp:174] pausing sending status updates  i0206 01:25:04.179798  2847 slave.cpp:832] detecting new master  i0206 01:25:04.179891  2852 authenticatee.cpp:121] creating new client sasl connection  i0206 01:25:04.179916  2847 slave.cpp:4682] received oversubscribable resources  from the resource estimator  i0206 01:25:04.180286  2847 master.cpp:5523] authenticating slave(9)@172.17.0.8:37843  i0206 01:25:04.180569  2847 authenticator.cpp:413] starting authentication session for crammd5authenticatee(32)@172.17.0.8:37843  i0206 01:25:04.181000  2847 authenticator.cpp:98] creating new server sasl connection  i0206 01:25:04.181315  2847 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0206 01:25:04.181387  2847 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0206 01:25:04.181562  2847 authenticator.cpp:203] received sasl authentication start  i0206 01:25:04.181648  2847 authenticator.cpp:325] authentication requires more steps  i0206 01:25:04.181843  2847 authenticatee.cpp:258] received sasl authentication step  i0206 01:25:04.182034  2853 authenticator.cpp:231] received sasl authentication step  i0206 01:25:04.182071  2853 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '74ef606c4063' server fqdn: '74ef606c4063' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0206 01:25:04.182093  2853 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0206 01:25:04.182145  2853 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0206 01:25:04.182173  2853 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '74ef606c4063' server fqdn: '74ef606c4063' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0206 01:25:04.182185  2853 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0206 01:25:04.182193  2853 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0206 01:25:04.182211  2853 authenticator.cpp:317] authentication success  i0206 01:25:04.182333  2849 authenticatee.cpp:298] authentication success  i0206 01:25:04.182422  2853 master.cpp:5553] successfully authenticated principal 'testprincipal' at slave(9)@172.17.0.8:37843  i0206 01:25:04.182510  2853 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(32)@172.17.0.8:37843  i0206 01:25:04.182945  2849 slave.cpp:927] successfully authenticated with master master@172.17.0.8:37843  i0206 01:25:04.183178  2849 slave.cpp:1321] will retry registration in 9.87937ms if necessary  i0206 01:25:04.183466  2852 master.cpp:4237] registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f995f64c57a7e39b06e2c1c8des0  i0206 01:25:04.184039  2845 registrar.cpp:439] applied 1 operations in 89453ns; attempting to update the 'registry'  i0206 01:25:04.185288  2856 log.cpp:683] attempting to append 339 bytes to the log  i0206 01:25:04.185672  2850 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0206 01:25:04.186674  2846 replica.cpp:537] replica received write request for position 3 from (392)@172.17.0.8:37843  i0206 01:25:04.195863  2856 slave.cpp:1321] will retry registration in 11.038094ms if necessary  i0206 01:25:04.196233  2856 master.cpp:4225] ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  i0206 01:25:04.208094  2856 slave.cpp:1321] will retry registration in 27.881223ms if necessary  i0206 01:25:04.208472  2856 master.cpp:4225] ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  i0206 01:25:04.216698  2846 leveldb.cpp:341] persisting action (358 bytes) to leveldb took 29.961291ms  i0206 01:25:04.216789  2846 replica.cpp:712] persisted action at 3  i0206 01:25:04.218246  2845 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0206 01:25:04.237861  2846 slave.cpp:1321] will retry registration in 1.006941ms if necessary  i0206 01:25:04.238221  2846 master.cpp:4225] ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  i0206 01:25:04.239858  2856 slave.cpp:1321] will retry registration in 167.305686ms if necessary  i0206 01:25:04.240044  2856 master.cpp:4225] ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress  i0206 01:25:04.241482  2845 leveldb.cpp:341] persisting action (360 bytes) to leveldb took 23.193162ms  i0206 01:25:04.241524  2845 replica.cpp:712] persisted action at 3  i0206 01:25:04.241557  2845 replica.cpp:697] replica learned append action at position 3  i0206 01:25:04.243746  2844 registrar.cpp:484] successfully updated the 'registry' in 59.587072ms  i0206 01:25:04.244210  2857 log.cpp:702] attempting to truncate the log to 3  i0206 01:25:04.244344  2845 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0206 01:25:04.244597  2856 master.cpp:4305] registered slave 914b62f995f64c57a7e39b06e2c1c8des0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0206 01:25:04.244746  2843 slave.cpp:3436] received ping from slaveobserver(8)@172.17.0.8:37843  i0206 01:25:04.244976  2845 hierarchical.cpp:473] added slave 914b62f995f64c57a7e39b06e2c1c8des0 (74ef606c4063) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0206 01:25:04.245072  2843 slave.cpp:971] registered with master master@172.17.0.8:37843; given slave id 914b62f995f64c57a7e39b06e2c1c8des0  i0206 01:25:04.245121  2843 fetcher.cpp:81] clearing fetcher cache  i0206 01:25:04.245146  2845 hierarchical.cpp:1403] no resources available to allocate!  i0206 01:25:04.245178  2845 hierarchical.cpp:1116] performed allocation for slave 914b62f995f64c57a7e39b06e2c1c8des0 in 159744ns  i0206 01:25:04.245465  2846 statusupdatemanager.cpp:181] resuming sending status updates  i0206 01:25:04.245776  2843 slave.cpp:994] checkpointing slaveinfo to '/tmp/containerloggertestdefaulttosandboxfmaksw/meta/slaves/914b62f995f64c57a7e39b06e2c1c8des0/slave.info'  i0206 01:25:04.245745  2846 replica.cpp:537] replica received write request for position 4 from (393)@172.17.0.8:37843  i0206 01:25:04.246273  2843 slave.cpp:1030] forwarding total oversubscribed resources   i0206 01:25:04.246507  2850 master.cpp:4646] received update of slave 914b62f995f64c57a7e39b06e2c1c8des0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources   i0206 01:25:04.247180  2824 sched.cpp:222] version: 0.28.0  i0206 01:25:04.247155  2850 hierarchical.cpp:531] slave 914b62f995f64c57a7e39b06e2c1c8des0 (74ef606c4063) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports( ):[3100032000], allocated: )  i0206 01:25:04.247357  2850 hierarchical.cpp:1403] no resources available to allocate!  i0206 01:25:04.247406  2850 hierarchical.cpp:1116] performed allocation for slave 914b62f995f64c57a7e39b06e2c1c8des0 in 183250ns  i0206 01:25:04.247938  2854 sched.cpp:326] new master detected at master@172.17.0.8:37843  i0206 01:25:04.248157  2854 sched.cpp:382] authenticating with master master@172.17.0.8:37843  i0206 01:25:04.248265  2854 sched.cpp:389] using default crammd5 authenticatee  i0206 01:25:04.248769  2854 authenticatee.cpp:121] creating new client sasl connection  i0206 01:25:04.249311  2854 master.cpp:5523] authenticating schedulerf50aad7578d04d9fb1a4 488d5ab932d6@172.17.0.8:37843  i0206 01:25:04.249646  2854 authenticator.cpp:413] starting authentication sess...",1,val
MESOS-4619,Remove markdown files from doxygen pages,the doxygen html pages corresponding to doc/  markdown files are redundant and have broken links. they don't serve any reasonable purpose in doxygen site.,1,val
MESOS-4622,Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.,"as part of the netcls epic, we introduce an agent flag called `cgroupnetclsprimary_handle` . we need to update configuration.md with the corresponding help string. ",1,val
MESOS-4623,Add a stub Nvidia GPU isolator.,"we'll first wire up a skeleton nvidia gpu isolator, which needs to be guarded by a configure flag due to the dependency on nvml.",3,val
MESOS-4624,"Add allocation metrics for ""gpus"" resources.","allocation metrics are currently hardcoded to include only \[""cpus"", ""mem"", ""disk""\] resources. we'll need to add ""gpus"" to the list to start, possibly following up on the todo to remove the hardcoding.    see:  https:/github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#l266l269  https:/github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#l123l126  ",1,val
MESOS-4625,Implement Nvidia GPU isolation w/o filesystem isolation enabled.,"the nvidia gpu isolator will need to use the device cgroup to restrict access to gpu resources, and will need to recover this information after agent failover. for now this will require that the operator specifies the gpu devices via a flag.    to handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using gpu resources, we'll tackle this in a separate ticket.",5,val
MESOS-4626,Support Nvidia GPUs with filesystem isolation enabled in mesos containerizer.,"when filesystem isolation is enabled in the mesos containerizer, containers that use nvidia gpu resources need access to gpu libraries residing on the host.    we'll need to provide a means for operators to inject the necessary volumes into all containers that use ""gpus"" resources.    see the nvidiadocker project for more details:  https:/github.com/nvidia/nvidiadocker/blob/fda10b2d27bf5578cc5337c23877f827e4d1ed77/tools/src/nvidia/volumes.go#l50l103",13,val
MESOS-4629,Implement fault tolerance tests for the HTTP Scheduler API.,"currently, the http v1 api does not have fault tolerance tests similar to the one in src/tests/faulttolerancetests.cpp.     for more information see mesos 3355.",5,val
MESOS-4630,Implement partition tests for the HTTP Scheduler API.,"currently, the http v1 api does not have partition tests similar to the one in src/tests/partition_tests.cpp.    for more information see mesos 3355.",5,val
MESOS-4633,Tests will dereference stack allocated agent objects upon assertion/expectation failure.,"tests that use the startslave test helper are generally fragile when the test fails an assert/expect in the middle of the test.  this is because the startslave helper takes raw pointer arguments, which may be stack allocated.    in case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  the test cleanup may dereference some of these destroyed objects, leading to a test crash like:    [18:27:36][step 8/8] f0204 18:27:35.981302 23085 logging.cpp:64] raw: pure virtual method called  [18:27:36][step 8/8]     @     0x7f7077055e1c  google::logmessage::fail()  [18:27:36][step 8/8]     @     0x7f707705ba6f  google::rawlog()  [18:27:36][step 8/8]     @     0x7f70760f76c9  cxapurevirtual  [18:27:36][step 8/8]     @           0xa9423c  mesos::internal::tests::cluster::slaves::shutdown()  [18:27:36][step 8/8]     @          0x1074e45  mesos::internal::tests::mesostest::shutdownslaves()  [18:27:36][step 8/8]     @          0x1074de4  mesos::internal::tests::mesostest::shutdown()  [18:27:36][step 8/8]     @          0x1070ec7  mesos::internal::tests::mesostest::teardown()      the startslave helper should take shared_ptr arguments instead.  this also means that we can remove the shutdown helper from most of these tests.",5,val
MESOS-4634,Tests will dereference stack allocated master objects upon assertion/expectation failure.,"tests that use the startmaster test helper are generally fragile when the test fails an assert/expect in the middle of the test.  this is because the startmaster helper takes raw pointer arguments, which may be stack allocated.    in case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  the test cleanup may dereference some of these destroyed objects, leading to a test crash like:    [18:27:36][step 8/8] f0204 18:27:35.981302 23085 logging.cpp:64] raw: pure virtual method called  [18:27:36][step 8/8]     @     0x7f7077055e1c  google::logmessage::fail()  [18:27:36][step 8/8]     @     0x7f707705ba6f  google::rawlog()  [18:27:36][step 8/8]     @     0x7f70760f76c9  cxapurevirtual  [18:27:36][step 8/8]     @           0xa9423c  mesos::internal::tests::cluster::slaves::shutdown()  [18:27:36][step 8/8]     @          0x1074e45  mesos::internal::tests::mesostest::shutdownslaves()  [18:27:36][step 8/8]     @          0x1074de4  mesos::internal::tests::mesostest::shutdown()  [18:27:36][step 8/8]     @          0x1070ec7  mesos::internal::tests::mesostest::teardown()      the startmaster helper should take shared_ptr arguments instead.  this also means that we can remove the shutdown helper from most of these tests.",5,val
MESOS-4636,Add parent hook to subprocess.,nan,3,val
MESOS-4637,Docker process executor can die with agent unit on systemd.,nan,1,val
MESOS-4639,Posix process executor can die with agent unit on systemd.,nan,1,val
MESOS-4640,Logrotate container logger can die with agent unit on systemd.,nan,1,val
MESOS-4657,Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.,we need to add log(info) during the prepare phase of `cgroups/netcls` for debugging management of `netcls` handles within the isolator. ,1,val
MESOS-4660,Document net_cls isolator in docs/mesos-containerizer.md.,we need to add a section in the doc to describe how to use cgroups/net_cls isolator.,1,val
MESOS-4667,Expose persistent volume information in HTTP endpoints,the perslave reserved_resources information returned by /state does not seem to include information about persistent volumes. this makes it hard for operators to use the /destroyvolumes endpoint.,3,val
MESOS-4669,Add common compression utility,we need gzip uncompress utility for appc image fetching functionality. the images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,2,val
MESOS-4670,`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.,"the composingcontainerizer currently does not have a `status` method. this results in no `containerstatus` being updated in the agent, when uses `composingcontainerizer` to launch containers. this would specifically happen when the agent is launched with `containerizer=docker,mesos`",1,val
MESOS-4671,Status updates from executor can be forwarded out of order by the Agent.,"previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received.     however, that seems to be no longer valid due to a recently introduced change in the agent:      / before sending update, we need to retrieve the container status.    containerizer>status(executor>containerid)      .onany(defer(self(),                   &slave::statusupdate,                   update,                   pid,                   executor >id,                   lambda::1));      this can sometimes lead to status updates being sent out of order depending on the order the future is fulfilled from the call to status(...).",1,val
MESOS-4674,Linux filesystem isolator tests are flaky.,"linuxfilesystemisolatortest.rootimageinvolumewithrootfilesystem sometimes fails on centos 7 with this kind of output:    ../../src/tests/containerizer/filesystemisolatortests.cpp:1054: failure  failed to wait 2mins for launch      linuxfilesystemisolatortest.rootmultiplecontainers often has this output:    ../../src/tests/containerizer/filesystemisolatortests.cpp:1138: failure  failed to wait 1mins for launch1      whether ssl is configured makes no difference.    this test may also fail on other platforms, but more rarely.    ",3,val
MESOS-4675,Cannot disable systemd support,"on certain platforms the systemd init system is available, but not used.  not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",1,val
MESOS-4676,ROOT_DOCKER_Logs is flaky.,"  [18:06:25][step 8/8] [ run      ] dockercontainerizertest.rootdockerlogs  [18:06:25][step 8/8] i0215 17:06:25.256103  1740 leveldb.cpp:174] opened db in 6.548327ms  [18:06:25][step 8/8] i0215 17:06:25.258002  1740 leveldb.cpp:181] compacted db in 1.837816ms  [18:06:25][step 8/8] i0215 17:06:25.258059  1740 leveldb.cpp:196] created db iterator in 22044ns  [18:06:25][step 8/8] i0215 17:06:25.258076  1740 leveldb.cpp:202] seeked to beginning of db in 2347ns  [18:06:25][step 8/8] i0215 17:06:25.258091  1740 leveldb.cpp:271] iterated through 0 keys in the db in 571ns  [18:06:25][step 8/8] i0215 17:06:25.258152  1740 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [18:06:25][step 8/8] i0215 17:06:25.258936  1758 recover.cpp:447] starting replica recovery  [18:06:25][step 8/8] i0215 17:06:25.259177  1758 recover.cpp:473] replica is in empty status  [18:06:25][step 8/8] i0215 17:06:25.260327  1757 replica.cpp:673] replica in empty status received a broadcasted recover request from (13608)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.260545  1758 recover.cpp:193] received a recover response from a replica in empty status  [18:06:25][step 8/8] i0215 17:06:25.261065  1757 master.cpp:376] master 112363e2c68049468feed0626ed8b21e (ip172302239.mesosphere.io) started on 172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.261209  1761 recover.cpp:564] updating replica status to starting  [18:06:25][step 8/8] i0215 17:06:25.261086  1757 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/hncllj/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/hncllj/master"" zksessiontimeout=""10secs""  [18:06:25][step 8/8] i0215 17:06:25.261446  1757 master.cpp:423] master only allowing authenticated frameworks to register  [18:06:25][step 8/8] i0215 17:06:25.261456  1757 master.cpp:428] master only allowing authenticated slaves to register  [18:06:25][step 8/8] i0215 17:06:25.261462  1757 credentials.hpp:35] loading credentials for authentication from '/tmp/hncllj/credentials'  [18:06:25][step 8/8] i0215 17:06:25.261723  1757 master.cpp:468] using default 'crammd5' authenticator  [18:06:25][step 8/8] i0215 17:06:25.261855  1757 master.cpp:537] using default 'basic' http authenticator  [18:06:25][step 8/8] i0215 17:06:25.262022  1757 master.cpp:571] authorization enabled  [18:06:25][step 8/8] i0215 17:06:25.262177  1755 hierarchical.cpp:144] initialized hierarchical allocator process  [18:06:25][step 8/8] i0215 17:06:25.262177  1758 whitelistwatcher.cpp:77] no whitelist given  [18:06:25][step 8/8] i0215 17:06:25.262899  1760 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.517992ms  [18:06:25][step 8/8] i0215 17:06:25.262924  1760 replica.cpp:320] persisted replica status to starting  [18:06:25][step 8/8] i0215 17:06:25.263144  1754 recover.cpp:473] replica is in starting status  [18:06:25][step 8/8] i0215 17:06:25.264010  1757 master.cpp:1712] the newly elected leader is master@172.30.2.239:39785 with id 112363e2c68049468feed0626ed8b21e  [18:06:25][step 8/8] i0215 17:06:25.264044  1757 master.cpp:1725] elected as the leading master!  [18:06:25][step 8/8] i0215 17:06:25.264061  1757 master.cpp:1470] recovering from registrar  [18:06:25][step 8/8] i0215 17:06:25.264117  1760 replica.cpp:673] replica in starting status received a broadcasted recover request from (13610)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.264197  1758 registrar.cpp:307] recovering registrar  [18:06:25][step 8/8] i0215 17:06:25.264827  1756 recover.cpp:193] received a recover response from a replica in starting status  [18:06:25][step 8/8] i0215 17:06:25.265219  1757 recover.cpp:564] updating replica status to voting  [18:06:25][step 8/8] i0215 17:06:25.267302  1754 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.887739ms  [18:06:25][step 8/8] i0215 17:06:25.267326  1754 replica.cpp:320] persisted replica status to voting  [18:06:25][step 8/8] i0215 17:06:25.267453  1759 recover.cpp:578] successfully joined the paxos group  [18:06:25][step 8/8] i0215 17:06:25.267632  1759 recover.cpp:462] recover process terminated  [18:06:25][step 8/8] i0215 17:06:25.268007  1757 log.cpp:659] attempting to start the writer  [18:06:25][step 8/8] i0215 17:06:25.269055  1759 replica.cpp:493] replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1  [18:06:25][step 8/8] i0215 17:06:25.270488  1759 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.406068ms  [18:06:25][step 8/8] i0215 17:06:25.270511  1759 replica.cpp:342] persisted promised to 1  [18:06:25][step 8/8] i0215 17:06:25.271078  1761 coordinator.cpp:238] coordinator attempting to fill missing positions  [18:06:25][step 8/8] i0215 17:06:25.272146  1756 replica.cpp:388] replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2  [18:06:25][step 8/8] i0215 17:06:25.273478  1756 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 1.297217ms  [18:06:25][step 8/8] i0215 17:06:25.273500  1756 replica.cpp:712] persisted action at 0  [18:06:25][step 8/8] i0215 17:06:25.274355  1757 replica.cpp:537] replica received write request for position 0 from (13613)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.274405  1757 leveldb.cpp:436] reading position from leveldb took 25294ns  [18:06:25][step 8/8] i0215 17:06:25.275800  1757 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.362978ms  [18:06:25][step 8/8] i0215 17:06:25.275823  1757 replica.cpp:712] persisted action at 0  [18:06:25][step 8/8] i0215 17:06:25.276348  1755 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [18:06:25][step 8/8] i0215 17:06:25.277765  1755 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.391531ms  [18:06:25][step 8/8] i0215 17:06:25.277788  1755 replica.cpp:712] persisted action at 0  [18:06:25][step 8/8] i0215 17:06:25.277802  1755 replica.cpp:697] replica learned nop action at position 0  [18:06:25][step 8/8] i0215 17:06:25.278336  1754 log.cpp:675] writer started with ending position 0  [18:06:25][step 8/8] i0215 17:06:25.279371  1755 leveldb.cpp:436] reading position from leveldb took 29214ns  [18:06:25][step 8/8] i0215 17:06:25.280272  1758 registrar.cpp:340] successfully fetched the registry (0b) in 16.02688ms  [18:06:25][step 8/8] i0215 17:06:25.280385  1758 registrar.cpp:439] applied 1 operations in 31040ns; attempting to update the 'registry'  [18:06:25][step 8/8] i0215 17:06:25.281054  1755 log.cpp:683] attempting to append 210 bytes to the log  [18:06:25][step 8/8] i0215 17:06:25.281165  1757 coordinator.cpp:348] coordinator attempting to write append action at position 1  [18:06:25][step 8/8] i0215 17:06:25.281780  1757 replica.cpp:537] replica received write request for position 1 from (13614)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.283159  1757 leveldb.cpp:341] persisting action (229 bytes) to leveldb took 1.348041ms  [18:06:25][step 8/8] i0215 17:06:25.283184  1757 replica.cpp:712] persisted action at 1  [18:06:25][step 8/8] i0215 17:06:25.283695  1759 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [18:06:25][step 8/8] i0215 17:06:25.285059  1759 leveldb.cpp:341] persisting action (231 bytes) to leveldb took 1.334577ms  [18:06:25][step 8/8] i0215 17:06:25.285084  1759 replica.cpp:712] persisted action at 1  [18:06:25][step 8/8] i0215 17:06:25.285099  1759 replica.cpp:697] replica learned append action at position 1  [18:06:25][step 8/8] i0215 17:06:25.285910  1758 registrar.cpp:484] successfully updated the 'registry' in 5.46816ms  [18:06:25][step 8/8] i0215 17:06:25.286043  1758 registrar.cpp:370] successfully recovered registrar  [18:06:25][step 8/8] i0215 17:06:25.286121  1755 log.cpp:702] attempting to truncate the log to 1  [18:06:25][step 8/8] i0215 17:06:25.286301  1756 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [18:06:25][step 8/8] i0215 17:06:25.286478  1759 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  [18:06:25][step 8/8] i0215 17:06:25.286476  1754 master.cpp:1522] recovered 0 slaves from the registry (171b) ; allowing 10mins for slaves to reregister  [18:06:25][step 8/8] i0215 17:06:25.287137  1755 replica.cpp:537] replica received write request for position 2 from (13615)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.289104  1755 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.938609ms  [18:06:25][step 8/8] i0215 17:06:25.289127  1755 replica.cpp:712] persisted action at 2  [18:06:25][step 8/8] i0215 17:06:25.289667  1759 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [18:06:25][step 8/8] i0215 17:06:25.290956  1759 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 1.256421ms  [18:06:25][step 8/8] i0215 17:06:25.291007  1759 leveldb.cpp:399] deleting ~1 keys from leveldb took 28064ns  [18:06:25][step 8/8] i0215 17:06:25.291021  1759 replica.cpp:712] persisted action at 2  [18:06:25][step 8/8] i0215 17:06:25.291038  1759 replica.cpp:697] replica learned truncate action at position 2  [18:06:25][step 8/8] i0215 17:06:25.300550  1760 slave.cpp:193] slave started on 393)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.300573  1760 slave.cpp:194] flags at startup: appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/dockercontainerizertestrootdockerlogsa4ns2n/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/dockercontainerizertestrootdockerlogsa4ns2n/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/dockercontainerizertestrootdockerlogsa4ns2n""  [18:06:25][step 8/8] i0215 17:06:25.300868  1760 credentials.hpp:83] loading credential for authentication from '/tmp/dockercontainerizertestrootdockerlogsa4ns2n/credential'  [18:06:25][step 8/8] i0215 17:06:25.301030  1760 slave.cpp:324] slave using credential for: testprincipal  [18:06:25][step 8/8] i0215 17:06:25.301180  1760 resources.cpp:576] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  [18:06:25][step 8/8] trying semicolondelimited string format instead  [18:06:25][step 8/8] i0215 17:06:25.301553  1760 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  [18:06:25][step 8/8] i0215 17:06:25.301609  1760 slave.cpp:472] slave attributes: [  ]  [18:06:25][step 8/8] i0215 17:06:25.301620  1760 slave.cpp:477] slave hostname: ip172302239.mesosphere.io  [18:06:25][step 8/8] i0215 17:06:25.302417  1757 state.cpp:58] recovering state from '/tmp/dockercontainerizertestrootdockerlogsa4ns2n/meta'  [18:06:25][step 8/8] i0215 17:06:25.302515  1740 sched.cpp:222] version: 0.28.0  [18:06:25][step 8/8] i0215 17:06:25.302772  1755 statusupdatemanager.cpp:200] recovering status update manager  [18:06:25][step 8/8] i0215 17:06:25.302956  1758 docker.cpp:559] recovering docker containers  [18:06:25][step 8/8] i0215 17:06:25.303050  1761 sched.cpp:326] new master detected at master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303133  1754 slave.cpp:4565] finished recovery  [18:06:25][step 8/8] i0215 17:06:25.303154  1761 sched.cpp:382] authenticating with master master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303169  1761 sched.cpp:389] using default crammd5 authenticatee  [18:06:25][step 8/8] i0215 17:06:25.303364  1759 authenticatee.cpp:121] creating new client sasl connection  [18:06:25][step 8/8] i0215 17:06:25.303467  1754 slave.cpp:4737] querying resource estimator for oversubscribable resources  [18:06:25][step 8/8] i0215 17:06:25.303668  1756 master.cpp:5523] authenticating scheduler806c70e31cf6418faa306bb26db42d18@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303707  1760 statusupdatemanager.cpp:174] pausing sending status updates  [18:06:25][step 8/8] i0215 17:06:25.303707  1754 slave.cpp:796] new master detected at master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303767  1755 authenticator.cpp:413] starting authentication session for crammd5authenticatee(829)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303791  1754 slave.cpp:859] authenticating with master master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.303805  1754 slave.cpp:864] using default crammd5 authenticatee  [18:06:25][step 8/8] i0215 17:06:25.303956  1754 slave.cpp:832] detecting new master  [18:06:25][step 8/8] i0215 17:06:25.303971  1761 authenticatee.cpp:121] creating new client sasl connection  [18:06:25][step 8/8] i0215 17:06:25.303984  1760 authenticator.cpp:98] creating new server sasl connection  [18:06:25][step 8/8] i0215 17:06:25.304131  1754 slave.cpp:4751] received oversubscribable resources  from the resource estimator  [18:06:25][step 8/8] i0215 17:06:25.304275  1757 master.cpp:5523] authenticating slave(393)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.304344  1754 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  [18:06:25][step 8/8] i0215 17:06:25.304369  1754 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  [18:06:25][step 8/8] i0215 17:06:25.304373  1761 authenticator.cpp:413] starting authentication session for crammd5authenticatee(830)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.304440  1757 authenticator.cpp:203] received sasl authentication start  [18:06:25][step 8/8] i0215 17:06:25.304491  1757 authenticator.cpp:325] authentication requires more steps  [18:06:25][step 8/8] i0215 17:06:25.304548  1754 authenticator.cpp:98] creating new server sasl connection  [18:06:25][step 8/8] i0215 17:06:25.304582  1761 authenticatee.cpp:258] received sasl authentication step  [18:06:25][step 8/8] i0215 17:06:25.304688  1761 authenticator.cpp:231] received sasl authentication step  [18:06:25][step 8/8] i0215 17:06:25.304714  1761 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302239.mesosphere.io' server fqdn: 'ip172302239.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   [18:06:25][step 8/8] i0215 17:06:25.304723  1761 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [18:06:25][step 8/8] i0215 17:06:25.304767  1761 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [18:06:25][step 8/8] i0215 17:06:25.304805  1761 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302239.mesosphere.io' server fqdn: 'ip172302239.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   [18:06:25][step 8/8] i0215 17:06:25.304817  1761 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  [18:06:25][step 8/8] i0215 17:06:25.304824  1761 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  [18:06:25][step 8/8] i0215 17:06:25.304836  1761 authenticator.cpp:317] authentication success  [18:06:25][step 8/8] i0215 17:06:25.304841  1758 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  [18:06:25][step 8/8] i0215 17:06:25.304870  1758 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  [18:06:25][step 8/8] i0215 17:06:25.304909  1757 authenticatee.cpp:298] authentication success  [18:06:25][step 8/8] i0215 17:06:25.304983  1756 authenticator.cpp:203] received sasl authentication start  [18:06:25][step 8/8] i0215 17:06:25.305033  1756 authenticator.cpp:325] authentication requires more steps  [18:06:25][step 8/8] i0215 17:06:25.305042  1759 master.cpp:5553] successfully authenticated principal 'testprincipal' at scheduler806c70e31cf6418faa306bb26db42d18@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.305071  1755 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(829)@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.305124  1756 authenticatee.cpp:258] received sasl authentication step  [18:06:25][step 8/8] i0215 17:06:25.305222  1758 sched.cpp:471] successfully authenticated with master master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.305246  1758 sched.cpp:776] sending subscribe call to master@172.30.2.239:39785  [18:06:25][step 8/8] i0215 17:06:25.305286  1760 authenticator.cpp:231] received sasl authentication step  [18:06:25][step 8/8] i0215 17:06:25.305310  1760 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302239.mesosphere.io' server fqdn: 'ip172302239.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   [18:06:25][step 8/8] i0215 17:06:25.305318  1760 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [18:06:25][step 8/8] i0215 17:06:25.305344  1760 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [18:06:25][step 8/8] i0215 17:06:25.305363  1758 sched.cpp:809] will retry registration in 1.888777185secs if necessary  [18:06:25][step 8/8] i0215 17:06:25.305379  1760 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302239.mesosphere.io' server fqdn: 'ip172302239.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   [18:06:25][step 8/8] i0215 17:06:25.305397  1760 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxprop_authzid == true  [18:06:25][step 8/...",2,val
MESOS-4678,Upgrade vendored Protobuf to 2.6.1,"we currently vendor protobuf 2.5.0. we should upgrade to protobuf 2.6.1. this introduces various bugfixes, performance improvements, and at least one new feature we might want to eventually take advantage of (map data type). afaik there should be no backward compatibility concerns.",3,val
MESOS-4683,Document docker runtime isolator.,"should include the following information:    what features are currently supported in docker runtime isolator.  how to use the docker runtime isolator (user manual).   compare the different semantics v.s. docker containerizer, and explain why.",2,val
MESOS-4684,Create base docker image for test suite.,this should be widely used for unified containerizer testing. should basically include:    at least one layer.  repositories.    for each layer:  root file system as a layer tar ball.  docker image json (manifest).   docker version.,3,val
MESOS-4686,Implement master failover tests for the scheduler library.,"currently, the scheduler library creates its own masterdetector object internally. we would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover.",3,val
MESOS-4687,Implement reliable floating point for scalar resources,design doc: https:/docs.google.com/document/d/14qlxjzsfipfynbx0usljr0gelsq8hdzjuww6kay_dxc/edit?usp=sharing,5,val
MESOS-4689,Design doc for v1 Operator API,we need to design how the v1 operator api (all the http endpoints exposed by master/agent that are not for scheduler/executor interactions) looks and works.,8,val
MESOS-4690,Reorganize 3rdparty directory,this issues is currently being discussed in the dev mailing list:  http:/ archive.com/dev@mesos.apache.org/msg34349.html,5,val
MESOS-4691,Add a HierarchicalAllocator benchmark with reservation labels.,"with labels being part of the reservationinfo, we should ensure that we don't observe a significant performance degradation in the allocator.",3,val
MESOS-4695,SlaveTest.StateEndpoint is flaky,"  [ run      ] slavetest.stateendpoint  ../../src/tests/slavetests.cpp:1220: failure  value of: state.values[""starttime""].as/().as/()    actual: 1458159086  expected: static_cast/(clock::now().secs())    which is: 1458159085  [  failed  ] slavetest.stateendpoint (193 ms)      even though this test does clock::pause() before starting the agent, there's a possibility that a numified stringified double to not equal itself, even after rounding to the nearest int.",1,val
MESOS-4696,Allow Reserve operations by a principal without `ReservationInfo.principal`,"currently, we require a framework or operator to specify `reservationinfo.principal` when they reserve resources. this isn't necessary, however; we already know the principal and can fill in the field if it isn't set already.",2,val
MESOS-4702,"Document default value of ""offer_timeout""","there isn't a default value (i.e., offers do not timeout by default), but we should clarify this in flags.cpp and configuration.md.",1,val
MESOS-4703,"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","stout configuration is replicated in at least 3 configuration files  stout itself, libprocess, and agent. more will follow in the future.    we should make a stoutconfigure.cmake that can be included by any package downstream.",1,val
MESOS-4704,Enable zlib on Windows.,nan,1,val
MESOS-4712,Remove 'force' field from the Subscribe Call in v1 Scheduler API,"we/i introduced the `force` field in subscribe call to deal with scheduler partition cases. having thought a bit more and discussing with few other folks ([anandmazumdar], [greggomann]), i think we can get away from not having that field in the v1 api. the obvious advantage of removing the field is that framework devs don't have to think about how/when to set the field (the current semantics are a bit confusing).    the new workflow when a master receives a subscribe call is that master always accepts this call and closes any existing connection (after sending error event) from the same scheduler (identified by framework id).      the expectation from schedulers is that they must close the old subscribe connection before resending a new subscribe call.    lets look at some tricky scenarios and see how this works and why it is safe.    1) connection disconnection @ the scheduler but not @ the master       scheduler sees the disconnection and sends a new subscribe call. master sends error on the old connection (won't be received by the scheduler because the connection is already closed) and closes it.    2) connection disconnection @ master but not @ scheduler    scheduler realizes this from lack of heartbeat events. it then closes its existing connection and sends a new subscribe call. master accepts the new subscribe call. there is no old connection to close on the master as it is already closed.    3) scheduler failover but no disconnection @ master    newly elected scheduler sends a subscribe call. master sends error event and closes the old connection (won't be received because the old scheduler failed over).    4) if scheduler a got partitioned (but is alive and connected with master) and scheduler b got elected as new leader.    when scheduler b sends subscribe, master sends error and closes the connection from scheduler a. master accepts scheduler b's connection. typically scheduler a aborts after receiving error and gets restarted. after restart it won't become the leader because scheduler b is already elected.    5) scheduler sends subscribe, times out, closes the subscribe connection (a) and sends a new subscribe (b). master receives subscribe (b) and then receives subscribe (a) but doesn't see a's disconnection yet.    master first accepts subscribe (b). after it receives subscribe (a), it sends error to subscribe (b) and closes that connection. when it accepts subscribe (a) and tries to send subscribed event the connection closure is detected. scheduler retries the subscribe connection after a backoff. i think this is a rare enough race for it to happen continuously in a loop.    ",5,val
MESOS-4713,ReviewBot should not fail hard if there are circular dependencies in a review chain,"instead of failing hard, reviewbot should post an error to the review that a circular dependency is detected.",2,val
MESOS-4714,"""make DESTDIR=<path> install"" broken",there is a missing '$(destdir)' prefix in the installdatahook that causes destdir builds to be broken.,2,val
MESOS-4718,Add allocator metric for number of completed allocation runs,nan,1,val
MESOS-4719,Add allocator metric for number of offers each framework received,"a counter for the number of allocations to a framework can be used to monitor allocation progress, e.g., when agents are added to a cluster, and as other frameworks are added or removed.    currently, an offer by the hierarchical allocator to a framework consists of a list of resources on possibly many agents. resources might be offered in order to satisfy outstanding quota or for fairness. to capture allocations on fine granularity we should not count the number of offers, but instead the pieces making up that offer, as such a metric would better resolve the effect of changes (e.g., adding/removing a framework).  ",2,val
MESOS-4720,Add allocator metrics for total vs offered/allocated resources.,"exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees. we should expose at least allocated or available, and total.",2,val
MESOS-4721,Expose allocation algorithm latency via a metric.,"the allocation algorithm has grown to become fairly expensive, gaining visibility into its latency enables monitoring and alerting.    similar allocator timing related information is already exposed in the log, but should also be exposed via an endpoint.",1,val
MESOS-4722,Add allocator metric for number of active offer filters,to diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed.,1,val
MESOS-4723,Add allocator metric for currently satisfied quotas,we currently expose information on set quotas via dedicated quota endpoints. to diagnose allocator problems one additionally needs information about used quotas.,2,val
MESOS-4724,Add allocator metric for currrent dominant shares of frameworks and roles,nan,5,val
MESOS-4726,Document scheduler driver calls in framework development guide.,"the interface examples are slightly out of sync with scheduler.hpp, most notably missing the new acceptoffers call.",2,val
MESOS-4731,Update /frameworks to use jsonify,this should let us remove the duplicated code in http.cpp between model(framework) and json(full/).,3,val
MESOS-4736,DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6,"this test passes consistently on other os's, but fails consistently on centos 6.    verbose logs from test failure:    [ run      ] dockercontainerizertest.rootdockerlaunchwithpersistentvolumes  i0222 18:16:12.327957 26681 leveldb.cpp:174] opened db in 7.466102ms  i0222 18:16:12.330528 26681 leveldb.cpp:181] compacted db in 2.540139ms  i0222 18:16:12.330580 26681 leveldb.cpp:196] created db iterator in 16908ns  i0222 18:16:12.330592 26681 leveldb.cpp:202] seeked to beginning of db in 1403ns  i0222 18:16:12.330600 26681 leveldb.cpp:271] iterated through 0 keys in the db in 315ns  i0222 18:16:12.330634 26681 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0222 18:16:12.331082 26698 recover.cpp:447] starting replica recovery  i0222 18:16:12.331289 26698 recover.cpp:473] replica is in empty status  i0222 18:16:12.332162 26703 replica.cpp:673] replica in empty status received a broadcasted recover request from (13761)@172.30.2.148:35274  i0222 18:16:12.332701 26701 recover.cpp:193] received a recover response from a replica in empty status  i0222 18:16:12.333230 26699 recover.cpp:564] updating replica status to starting  i0222 18:16:12.334102 26698 master.cpp:376] master 652149b439324d8bba6f8c9d9045be70 (ip172302148.mesosphere.io) started on 172.30.2.148:35274  i0222 18:16:12.334116 26698 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/qehlbs/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/qehlbs/master"" zksessiontimeout=""10secs""  i0222 18:16:12.334354 26698 master.cpp:423] master only allowing authenticated frameworks to register  i0222 18:16:12.334363 26698 master.cpp:428] master only allowing authenticated slaves to register  i0222 18:16:12.334369 26698 credentials.hpp:35] loading credentials for authentication from '/tmp/qehlbs/credentials'  i0222 18:16:12.335366 26698 master.cpp:468] using default 'crammd5' authenticator  i0222 18:16:12.335492 26698 master.cpp:537] using default 'basic' http authenticator  i0222 18:16:12.335623 26698 master.cpp:571] authorization enabled  i0222 18:16:12.335752 26703 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 2.314693ms  i0222 18:16:12.335769 26700 whitelistwatcher.cpp:77] no whitelist given  i0222 18:16:12.335778 26703 replica.cpp:320] persisted replica status to starting  i0222 18:16:12.335821 26697 hierarchical.cpp:144] initialized hierarchical allocator process  i0222 18:16:12.335965 26701 recover.cpp:473] replica is in starting status  i0222 18:16:12.336771 26703 replica.cpp:673] replica in starting status received a broadcasted recover request from (13763)@172.30.2.148:35274  i0222 18:16:12.337191 26696 recover.cpp:193] received a recover response from a replica in starting status  i0222 18:16:12.337635 26700 recover.cpp:564] updating replica status to voting  i0222 18:16:12.337671 26703 master.cpp:1712] the newly elected leader is master@172.30.2.148:35274 with id 652149b439324d8bba6f8c9d9045be70  i0222 18:16:12.337698 26703 master.cpp:1725] elected as the leading master!  i0222 18:16:12.337713 26703 master.cpp:1470] recovering from registrar  i0222 18:16:12.337828 26696 registrar.cpp:307] recovering registrar  i0222 18:16:12.339972 26702 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 2.06039ms  i0222 18:16:12.339994 26702 replica.cpp:320] persisted replica status to voting  i0222 18:16:12.340082 26700 recover.cpp:578] successfully joined the paxos group  i0222 18:16:12.340267 26700 recover.cpp:462] recover process terminated  i0222 18:16:12.340591 26699 log.cpp:659] attempting to start the writer  i0222 18:16:12.341594 26698 replica.cpp:493] replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1  i0222 18:16:12.343598 26698 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.97941ms  i0222 18:16:12.343619 26698 replica.cpp:342] persisted promised to 1  i0222 18:16:12.344182 26698 coordinator.cpp:238] coordinator attempting to fill missing positions  i0222 18:16:12.345285 26702 replica.cpp:388] replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2  i0222 18:16:12.347275 26702 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 1.960198ms  i0222 18:16:12.347296 26702 replica.cpp:712] persisted action at 0  i0222 18:16:12.348201 26703 replica.cpp:537] replica received write request for position 0 from (13766)@172.30.2.148:35274  i0222 18:16:12.348247 26703 leveldb.cpp:436] reading position from leveldb took 21399ns  i0222 18:16:12.350667 26703 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 2.39166ms  i0222 18:16:12.350690 26703 replica.cpp:712] persisted action at 0  i0222 18:16:12.351191 26696 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0222 18:16:12.353152 26696 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.935798ms  i0222 18:16:12.353173 26696 replica.cpp:712] persisted action at 0  i0222 18:16:12.353188 26696 replica.cpp:697] replica learned nop action at position 0  i0222 18:16:12.353639 26696 log.cpp:675] writer started with ending position 0  i0222 18:16:12.354508 26697 leveldb.cpp:436] reading position from leveldb took 25625ns  i0222 18:16:12.355274 26696 registrar.cpp:340] successfully fetched the registry (0b) in 17.406976ms  i0222 18:16:12.355357 26696 registrar.cpp:439] applied 1 operations in 20977ns; attempting to update the 'registry'  i0222 18:16:12.355929 26697 log.cpp:683] attempting to append 210 bytes to the log  i0222 18:16:12.356032 26703 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0222 18:16:12.356657 26698 replica.cpp:537] replica received write request for position 1 from (13767)@172.30.2.148:35274  i0222 18:16:12.358566 26698 leveldb.cpp:341] persisting action (229 bytes) to leveldb took 1.881945ms  i0222 18:16:12.358588 26698 replica.cpp:712] persisted action at 1  i0222 18:16:12.359081 26697 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0222 18:16:12.361002 26697 leveldb.cpp:341] persisting action (231 bytes) to leveldb took 1.894331ms  i0222 18:16:12.361023 26697 replica.cpp:712] persisted action at 1  i0222 18:16:12.361038 26697 replica.cpp:697] replica learned append action at position 1  i0222 18:16:12.361883 26697 registrar.cpp:484] successfully updated the 'registry' in 6.482944ms  i0222 18:16:12.361981 26697 registrar.cpp:370] successfully recovered registrar  i0222 18:16:12.362052 26701 log.cpp:702] attempting to truncate the log to 1  i0222 18:16:12.362167 26703 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0222 18:16:12.362421 26696 master.cpp:1522] recovered 0 slaves from the registry (171b) ; allowing 10mins for slaves to reregister  i0222 18:16:12.362447 26698 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0222 18:16:12.362911 26701 replica.cpp:537] replica received write request for position 2 from (13768)@172.30.2.148:35274  i0222 18:16:12.364760 26701 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.819954ms  i0222 18:16:12.364783 26701 replica.cpp:712] persisted action at 2  i0222 18:16:12.365384 26697 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0222 18:16:12.367961 26697 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 2.55143ms  i0222 18:16:12.368015 26697 leveldb.cpp:399] deleting ~1 keys from leveldb took 28196ns  i0222 18:16:12.368028 26697 replica.cpp:712] persisted action at 2  i0222 18:16:12.368044 26697 replica.cpp:697] replica learned truncate action at position 2  i0222 18:16:12.376824 26703 slave.cpp:193] slave started on 396)@172.30.2.148:35274  i0222 18:16:12.376838 26703 slave.cpp:194] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/dockercontainerizertestrootdockerlaunchwithpersistentvolumesu5vzx1/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerauthserver=""https:/auth.docker.io"" dockerkillorphans=""true"" dockerpullertimeout=""60"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/dockercontainerizertestrootdockerlaunchwithpersistentvolumesu5vzx1/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpu:2;mem:2048;disk(role1):2048"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/dockercontainerizertestrootdockerlaunchwithpersistentvolumesu5vzx1""  i0222 18:16:12.377109 26703 credentials.hpp:83] loading credential for authentication from '/tmp/dockercontainerizertestrootdockerlaunchwithpersistentvolumesu5vzx1/credential'  i0222 18:16:12.377300 26703 slave.cpp:324] slave using credential for: testprincipal  i0222 18:16:12.377439 26703 resources.cpp:576] parsing resources as json failed: cpu:2;mem:2048;disk(role1):2048  trying semicolondelimited string format instead  i0222 18:16:12.377804 26703 slave.cpp:464] slave resources: cpu():2; mem():2048; disk(role1):2048; cpus():8; ports():[3100032000]  i0222 18:16:12.377881 26703 slave.cpp:472] slave attributes: [  ]  i0222 18:16:12.377889 26703 slave.cpp:477] slave hostname: ip172302148.mesosphere.io  i0222 18:16:12.378779 26701 state.cpp:58] recovering state from '/tmp/dockercontainerizertestrootdockerlaunchwithpersistentvolumesu5vzx1/meta'  i0222 18:16:12.379092 26697 statusupdatemanager.cpp:200] recovering status update manager  i0222 18:16:12.379156 26681 sched.cpp:222] version: 0.28.0  i0222 18:16:12.379250 26697 docker.cpp:722] recovering docker containers  i0222 18:16:12.379421 26703 slave.cpp:4565] finished recovery  i0222 18:16:12.379627 26700 sched.cpp:326] new master detected at master@172.30.2.148:35274  i0222 18:16:12.379735 26703 slave.cpp:4737] querying resource estimator for oversubscribable resources  i0222 18:16:12.379765 26700 sched.cpp:382] authenticating with master master@172.30.2.148:35274  i0222 18:16:12.379781 26700 sched.cpp:389] using default crammd5 authenticatee  i0222 18:16:12.379964 26696 statusupdatemanager.cpp:174] pausing sending status updates  i0222 18:16:12.379992 26702 authenticatee.cpp:121] creating new client sasl connection  i0222 18:16:12.380030 26697 slave.cpp:796] new master detected at master@172.30.2.148:35274  i0222 18:16:12.380106 26697 slave.cpp:859] authenticating with master master@172.30.2.148:35274  i0222 18:16:12.380127 26697 slave.cpp:864] using default crammd5 authenticatee  i0222 18:16:12.380188 26699 master.cpp:5526] authenticating scheduler1850b1cd33964479b2f347ee6c3fa270@172.30.2.148:35274  i0222 18:16:12.380269 26700 authenticator.cpp:413] starting authentication session for crammd5authenticatee(832)@172.30.2.148:35274  i0222 18:16:12.380280 26698 authenticatee.cpp:121] creating new client sasl connection  i0222 18:16:12.380307 26697 slave.cpp:832] detecting new master  i0222 18:16:12.380450 26697 slave.cpp:4751] received oversubscribable resources  from the resource estimator  i0222 18:16:12.380452 26699 master.cpp:5526] authenticating slave(396)@172.30.2.148:35274  i0222 18:16:12.380506 26698 authenticator.cpp:98] creating new server sasl connection  i0222 18:16:12.380540 26697 authenticator.cpp:413] starting authentication session for crammd5authenticatee(833)@172.30.2.148:35274  i0222 18:16:12.380635 26700 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0222 18:16:12.380659 26700 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0222 18:16:12.380762 26700 authenticator.cpp:203] received sasl authentication start  i0222 18:16:12.380765 26701 authenticator.cpp:98] creating new server sasl connection  i0222 18:16:12.380843 26700 authenticator.cpp:325] authentication requires more steps  i0222 18:16:12.380911 26698 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0222 18:16:12.380931 26702 authenticatee.cpp:258] received sasl authentication step  i0222 18:16:12.380936 26698 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0222 18:16:12.381036 26702 authenticator.cpp:231] received sasl authentication step  i0222 18:16:12.381052 26698 authenticator.cpp:203] received sasl authentication start  i0222 18:16:12.381062 26702 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302148' server fqdn: 'ip172302148' saslauxpropoverride: false saslauxpropauthzid: false   i0222 18:16:12.381072 26702 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0222 18:16:12.381104 26702 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0222 18:16:12.381104 26698 authenticator.cpp:325] authentication requires more steps  i0222 18:16:12.381134 26702 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302148' server fqdn: 'ip172302148' saslauxpropoverride: false saslauxpropauthzid: true   i0222 18:16:12.381142 26702 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0222 18:16:12.381147 26702 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0222 18:16:12.381162 26702 authenticator.cpp:317] authentication success  i0222 18:16:12.381184 26698 authenticatee.cpp:258] received sasl authentication step  i0222 18:16:12.381247 26699 authenticatee.cpp:298] authentication success  i0222 18:16:12.381283 26696 authenticator.cpp:231] received sasl authentication step  i0222 18:16:12.381311 26696 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302148' server fqdn: 'ip172302148' saslauxpropoverride: false saslauxpropauthzid: false   i0222 18:16:12.381325 26696 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0222 18:16:12.381319 26701 master.cpp:5556] successfully authenticated principal 'testprincipal' at scheduler1850b1cd33964479b2f347ee6c3fa270@172.30.2.148:35274  i0222 18:16:12.381345 26700 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(832)@172.30.2.148:35274  i0222 18:16:12.381361 26696 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0222 18:16:12.381397 26696 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302148' server fqdn: 'ip172302148' saslauxpropoverride: false saslauxpropauthzid: true   i0222 18:16:12.381413 26696 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0222 18:16:12.381422 26696 auxprop.cpp:129] skipping auxiliary property ' cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0222 18:16:12.381441 26696 authenticator.cpp:317] authentication success  i0222 18:16:12.381548 26698 sched.cpp:471] successfully authenticated with master master@172.30.2.148:35274  i0222 18:16:12.381563 26698 sched.cpp:776] sending subscribe call to master@172.30.2.148:35274  i0222 18:16:12.381634 26700 authenticatee.cpp:298] authentication success  i0222 18:16:12.381660 26698 sched.cpp:809] will retry registration in 770.60771ms if necessary  i0222 18:16:12.381675 26697 master.cpp:5556] successfully authenticated principal 'testprincipal' at slave(396)@172.30.2.148:35274  i0222 18:16:12.381734 26702 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(833)@172.30.2.148:35274  i0222 18:16:12.381811 26697 master.cpp:2280] received subscribe call for framework 'default' at scheduler1850b1cd33964479b2f347ee6c3fa270@172.30.2.148:35274  i0222 18:16:12.381882 26697 master.cpp:1751] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i0222 18:16:12.382004 26698 slave.cpp:927] successfully authenticated with master master@172.30.2.148:35274  i0222 18:16:12.382123 26698 slave.cpp:1321] will retry registration in 8.1941ms if necessary  i0222 18:16:12.382282 26701 master.cpp:4240] registering slave at slave(396)@172.30.2.148:35274 (ip172302148.mesosphere.io) with id 652149b439324d8bba6f8c9d9045be70s0  i0222 18:16:12.382482 26701 master.cpp:2351] subscribing framework default with checkpointing disabled and capabilities [  ]  i0222 18:16:12.382612 26703 registrar.cpp:439] applied 1 operations in 46327ns; attempting to update the 'registry'  i0222 18:16:12.382829 26699 hierarchical.cpp:265] added framework 652149b439324d8bba6f8c9d9045be700000  i0222 18:16:12.382910 26699 hierarchical.cpp:1434] no resources available to allocate!  i0222 18:16:12.382915 26701 sched.cpp:703] framework registered with 652149b439324d8bba6f8c9d9045be70 0000  i0222 18:16:12.382936 26699 hierarchical.cpp:1529] no inverse offers to send out!  i0222 18:16:12.382953 26699 hierarchical.cpp:1127] performed allocation for 0 slaves in 89949ns  i0222 18:16:12.382982 26701 sched.cpp:717] scheduler::registered took 46498ns  i0222 18:16:12.383536 26698 log.cpp:683] attempting to append 423 bytes to the log  i0222 18:16:12.383628 26699 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0222 18:16:12.384196 26700 replica.cpp:537] replica received write request for position 3 from (13775)@172.30.2.148:35274  i0222 18:16:12.386602 26700 leveldb.cpp:341] persisting action (442 bytes) to leveldb took 2.377119ms  i0222 18:16:12.386625 26700 replica.cpp:712] persisted action at 3  i0222 18:16:12.387104 26698 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0222 18:16:12.389159 26698 leveldb.cpp:341] persisting action (444 bytes) to leveldb took 2.032301ms  i0222 18:16:12.389181 26698 replica.cpp:712] persisted action at 3  i0222 18:16:12.389196 26698 replica.cpp:697] replica learned append action at position 3  i0222 18:16:12.390281 26698 registrar.cpp:484] suc...",2,val
MESOS-4746,CMake: Add leveldb library to 3rdparty external builds.,nan,3,val
MESOS-4747,ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation,"some cleanup of spawned processes is missing in containerloggertest.mesoscontainerizerrecover so that when the test is run in isolation the global teardown might find lingering processes.    [==========] running 1 test from 1 test case.  [] global test environment setup.  [] 1 test from containerloggertest  [ run      ] containerloggertest.mesoscontainerizerrecover  [       ok ] containerloggertest.mesoscontainerizerrecover (13 ms)  [] 1 test from containerloggertest (13 ms total)    [] global test environment teardown  ../../src/tests/environment.cpp:728: failure  failed  tests completed with child processes remaining:  + 7112 /some/path/src/mesos/build/src/.libs/mesostests gtest_filter=containerloggertest.mesoscontainerizerrecover   \ 7130 (sh)  [==========] 1 test from 1 test case ran. (23 ms total)  [  passed  ] 1 test.  [  failed  ] 0 tests, listed below:     0 failed tests      observered on os x with clang trunk and an unoptimized build.  ",1,val
MESOS-4748,Add Appc image fetcher tests.,mesos now has support for fetching appc images. add tests that verifies the new component.,3,val
MESOS-4749,Move HTB out of containers,"currently we set a fixed htb bandwidth in each of the container, which makes it impossible to share the link if idle. as the first step, we should move it out of the containers, into the qdisc hierarchy of the physical interface.",3,val
MESOS-4750,Document: Mesos Executor expects all SSL_* environment variables to be set,"i was trying to run docker containers in a fully ssl ized mesos cluster but ran into problems because the executor was failing with a ""failed to shutdown socket with fd 10: transport endpoint is not connected"".    my understanding of why this is happening is because the executor was trying to report its status to mesos slave over https, but doesnt have the appropriate certs/env setup inside the executor.    (thanks to mslackbot/joseph for helping me figure this out on #mesos)    it turns out, the executor expects all ssl variables to be set inside `commandinfo.environment` which gets picked up by the executor to successfully reports its status to the slave.    this part of executor needing all the ssl variables to be set in its environment is missing in the mesos ssl transitioning guide. i request you to please add this vital information to the doc.",2,val
MESOS-4754,"The ""executors"" field is exposed under a backwards incompatible schema.","in 0.26.0, the master's /state endpoint generated the following:      ,            ""executorid"": ""default"",            ""frameworkid"": ""0ea528a964ba417f98ea9c4b8d418db60000"",            ""name"": ""long lived executor (c)"",            ""resources"": ,            ""slaveid"": ""8a51367803a14cb59279c3c0c591f1d8s0""          }        ],        / ... /      }    ]    / ... /  }      in 0.27.1, the executorinfo is mistakenly exposed in the raw protobuf schema:      ,            ""executorid"": ,            ""frameworkid"": ,            ""name"": ""long lived executor (c)"",            ""slaveid"": ""8a51367803a14cb59279c3c0c591f1d8s0"",            ""source"": ""cpplonglived_framework""          }        ],        / ... /      }    ]    / ... /  }      this is a backwards incompatible api change.",2,val
MESOS-4757,Mesos containerizer should get uid/gids before pivot_root.,"currently, we call os::su(user) after pivotroot. this is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. we should instead, get the uid/gids before pivotroot, and call setuid/setgroups after pivot_root.",3,val
MESOS-4758,Add a 'name' field into NetworkInfo.,"this allows the framework writer to specify the name of the network they want their container to join.    why not using 'groups'? that's because there might be multiple groups under a single network (e.g., admin vs. user, public vs. private, etc.).",1,val
MESOS-4759,Add network/cni isolator for Mesos containerizer.,see the design doc for more context (mesos 4742).    the isolator will interact with cni plugins to create the network for the container to join.,8,val
MESOS-4760,Expose metrics and gauges for fetcher cache usage and hit rate,"to evaluate the fetcher cache and calibrate the value of the fetchercachesize flag, it would be useful to have metrics and gauges on agents that expose operational statistics like cache hit rate, occupied cache size, and time spent downloading resources that were not present.",2,val
MESOS-4761,Add agent flags to allow operators to specify CNI plugin and config directories.,"according to design doc, we plan to add the following flags:    networkcnipluginsdir  location of the cni plugin binaries. the network/cni isolator will find cni plugins under this directory so that it can execute the plugins to add/delete container from the cni networks. it is the operators responsibility to install the cni plugin binaries in the specified directory.    networkcniconfigdir  location of the cni network configuration files. for each network that containers launched in mesos agent can connect to, the operator should install a network configuration file in json format in the specified directory.",2,val
MESOS-4762,Setup proper DNS resolver for containers in network/cni isolator.,please get more context from the design doc (mesos 4742).    the cni plugin will return the dns information about the network. the network/cni isolator needs to properly setup /etc/resolv.conf for the container. we should consider the following cases:  1) container is using host filesystem  2) container is using a different filesystem  3) custom executor and command executor,5,val
MESOS-4763,Add test mock for CNI plugins.,"in order to test the network/cni isolator, we need to mock the behavior of an cni plugin. one option is to write a mock script which acts as a cni plugin. the isolator will talk to the mock script the same way it talks to an actual cni plugin.    the mock script can just join the host network?",5,val
MESOS-4764,The network/cni isolator should report assigned IP address. ,"in order for service discovery to work in some cases, the network/cni isolator needs to report the assigned ip address through the isolator >status() interface.",3,val
MESOS-4768,MasterMaintenanceTest.InverseOffers is flaky,"[mesos4169] significantly sped up this test, but also surfaced some more flakiness.  this can be fixed in the same way as [mesos4059].    verbose logs from asf centos7 build:    [ run      ] mastermaintenancetest.inverseoffers  i0224 22:35:53.714018  1948 leveldb.cpp:174] opened db in 2.034387ms  i0224 22:35:53.714663  1948 leveldb.cpp:181] compacted db in 608839ns  i0224 22:35:53.714709  1948 leveldb.cpp:196] created db iterator in 19043ns  i0224 22:35:53.714844  1948 leveldb.cpp:202] seeked to beginning of db in 2330ns  i0224 22:35:53.714956  1948 leveldb.cpp:271] iterated through 0 keys in the db in 518ns  i0224 22:35:53.715092  1948 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0224 22:35:53.715646  1968 recover.cpp:447] starting replica recovery  i0224 22:35:53.715915  1981 recover.cpp:473] replica is in empty status  i0224 22:35:53.717067  1972 replica.cpp:673] replica in empty status received a broadcasted recover request from (4533)@172.17.0.1:36678  i0224 22:35:53.717445  1981 recover.cpp:193] received a recover response from a replica in empty status  i0224 22:35:53.717888  1978 recover.cpp:564] updating replica status to starting  i0224 22:35:53.718585  1979 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 525061ns  i0224 22:35:53.718618  1979 replica.cpp:320] persisted replica status to starting  i0224 22:35:53.718827  1982 recover.cpp:473] replica is in starting status  i0224 22:35:53.719728  1969 replica.cpp:673] replica in starting status received a broadcasted recover request from (4534)@172.17.0.1:36678  i0224 22:35:53.719974  1971 recover.cpp:193] received a recover response from a replica in starting status  i0224 22:35:53.720369  1970 recover.cpp:564] updating replica status to voting  i0224 22:35:53.720789  1982 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 322308ns  i0224 22:35:53.720823  1982 replica.cpp:320] persisted replica status to voting  i0224 22:35:53.720968  1982 recover.cpp:578] successfully joined the paxos group  i0224 22:35:53.721101  1982 recover.cpp:462] recover process terminated  i0224 22:35:53.721698  1982 master.cpp:376] master aab18b6178114c43a672d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678  i0224 22:35:53.721719  1982 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""false"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/mjbcwp/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.28.0/inst/share/mesos/webui"" workdir=""/tmp/mjbcwp/master"" zksessiontimeout=""10secs""  i0224 22:35:53.722039  1982 master.cpp:425] master allowing unauthenticated frameworks to register  i0224 22:35:53.722053  1982 master.cpp:428] master only allowing authenticated slaves to register  i0224 22:35:53.722061  1982 credentials.hpp:35] loading credentials for authentication from '/tmp/mjbcwp/credentials'  i0224 22:35:53.722394  1982 master.cpp:468] using default 'crammd5' authenticator  i0224 22:35:53.722525  1982 master.cpp:537] using default 'basic' http authenticator  i0224 22:35:53.722661  1982 master.cpp:571] authorization enabled  i0224 22:35:53.722813  1968 hierarchical.cpp:144] initialized hierarchical allocator process  i0224 22:35:53.722846  1980 whitelistwatcher.cpp:77] no whitelist given  i0224 22:35:53.724957  1977 master.cpp:1712] the newly elected leader is master@172.17.0.1:36678 with id aab18b6178114c43a672d1a63818c880  i0224 22:35:53.725000  1977 master.cpp:1725] elected as the leading master!  i0224 22:35:53.725023  1977 master.cpp:1470] recovering from registrar  i0224 22:35:53.725306  1967 registrar.cpp:307] recovering registrar  i0224 22:35:53.725808  1977 log.cpp:659] attempting to start the writer  i0224 22:35:53.727145  1973 replica.cpp:493] replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1  i0224 22:35:53.727728  1973 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 424560ns  i0224 22:35:53.727828  1973 replica.cpp:342] persisted promised to 1  i0224 22:35:53.729080  1973 coordinator.cpp:238] coordinator attempting to fill missing positions  i0224 22:35:53.731009  1979 replica.cpp:388] replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2  i0224 22:35:53.731580  1979 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 478479ns  i0224 22:35:53.731613  1979 replica.cpp:712] persisted action at 0  i0224 22:35:53.734354  1979 replica.cpp:537] replica received write request for position 0 from (4538)@172.17.0.1:36678  i0224 22:35:53.734485  1979 leveldb.cpp:436] reading position from leveldb took 60879ns  i0224 22:35:53.735877  1979 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.324061ms  i0224 22:35:53.735930  1979 replica.cpp:712] persisted action at 0  i0224 22:35:53.737061  1970 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0224 22:35:53.738881  1970 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.772814ms  i0224 22:35:53.738939  1970 replica.cpp:712] persisted action at 0  i0224 22:35:53.738975  1970 replica.cpp:697] replica learned nop action at position 0  i0224 22:35:53.740136  1976 log.cpp:675] writer started with ending position 0  i0224 22:35:53.741750  1976 leveldb.cpp:436] reading position from leveldb took 74863ns  i0224 22:35:53.743479  1976 registrar.cpp:340] successfully fetched the registry (0b) in 18.11968ms  i0224 22:35:53.743755  1976 registrar.cpp:439] applied 1 operations in 56670ns; attempting to update the 'registry'  i0224 22:35:53.745604  1978 log.cpp:683] attempting to append 170 bytes to the log  i0224 22:35:53.745905  1977 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0224 22:35:53.746968  1981 replica.cpp:537] replica received write request for position 1 from (4539)@172.17.0.1:36678  i0224 22:35:53.747480  1981 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 456947ns  i0224 22:35:53.747609  1981 replica.cpp:712] persisted action at 1  i0224 22:35:53.750448  1981 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0224 22:35:53.751158  1981 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 535163ns  i0224 22:35:53.751258  1981 replica.cpp:712] persisted action at 1  i0224 22:35:53.751389  1981 replica.cpp:697] replica learned append action at position 1  i0224 22:35:53.753149  1979 registrar.cpp:484] successfully updated the 'registry' in 9.228032ms  i0224 22:35:53.753324  1979 registrar.cpp:370] successfully recovered registrar  i0224 22:35:53.753593  1979 log.cpp:702] attempting to truncate the log to 1  i0224 22:35:53.753805  1979 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0224 22:35:53.754055  1981 master.cpp:1522] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0224 22:35:53.754349  1979 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0224 22:35:53.755764  1977 replica.cpp:537] replica received write request for position 2 from (4540)@172.17.0.1:36678  i0224 22:35:53.756459  1977 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 488559ns  i0224 22:35:53.756561  1977 replica.cpp:712] persisted action at 2  i0224 22:35:53.757932  1972 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0224 22:35:53.758400  1972 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 343827ns  i0224 22:35:53.758539  1972 leveldb.cpp:399] deleting 1 keys from leveldb took 34231ns  i0224 22:35:53.758658  1972 replica.cpp:712] persisted action at 2  i0224 22:35:53.758782  1972 replica.cpp:697] replica learned truncate action at position 2  i0224 22:35:53.778059  1978 slave.cpp:193] slave started on 115)@172.17.0.1:36678  i0224 22:35:53.778105  1978 slave.cpp:194] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/mastermaintenancetestinverseoffersywqvff/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/mastermaintenancetestinverseoffersywqvff/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostname=""maintenancehost"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.28.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/mastermaintenancetestinverseoffersywqvff""  i0224 22:35:53.778609  1978 credentials.hpp:83] loading credential for authentication from '/tmp/mastermaintenancetestinverseoffersywqvff/credential'  i0224 22:35:53.779175  1978 slave.cpp:324] slave using credential for: testprincipal  i0224 22:35:53.779520  1978 resources.cpp:576] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0224 22:35:53.780192  1978 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0224 22:35:53.780362  1978 slave.cpp:472] slave attributes: [  ]  i0224 22:35:53.780483  1978 slave.cpp:477] slave hostname: maintenancehost  i0224 22:35:53.782126  1967 state.cpp:58] recovering state from '/tmp/mastermaintenancetestinverseoffersywqvff/meta'  i0224 22:35:53.782892  1969 statusupdatemanager.cpp:200] recovering status update manager  i0224 22:35:53.783242  1969 slave.cpp:4565] finished recovery  i0224 22:35:53.784001  1969 slave.cpp:4737] querying resource estimator for oversubscribable resources  i0224 22:35:53.784678  1969 slave.cpp:796] new master detected at master@172.17.0.1:36678  i0224 22:35:53.784874  1967 statusupdatemanager.cpp:174] pausing sending status updates  i0224 22:35:53.784808  1969 slave.cpp:859] authenticating with master master@172.17.0.1:36678  i0224 22:35:53.784945  1969 slave.cpp:864] using default crammd5 authenticatee  i0224 22:35:53.785181  1969 slave.cpp:832] detecting new master  i0224 22:35:53.785326  1969 slave.cpp:4751] received oversubscribable resources  from the resource estimator  i0224 22:35:53.785557  1969 authenticatee.cpp:121] creating new client sasl connection  i0224 22:35:53.786227  1969 master.cpp:5526] authenticating slave(115)@172.17.0.1:36678  i0224 22:35:53.786492  1969 authenticator.cpp:413] starting authentication session for crammd5authenticatee(298)@172.17.0.1:36678  i0224 22:35:53.786962  1969 authenticator.cpp:98] creating new server sasl connection  i0224 22:35:53.787274  1969 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0224 22:35:53.787308  1969 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0224 22:35:53.787400  1969 authenticator.cpp:203] received sasl authentication start  i0224 22:35:53.787470  1969 authenticator.cpp:325] authentication requires more steps  i0224 22:35:53.787884  1972 authenticatee.cpp:258] received sasl authentication step  i0224 22:35:53.787992  1972 authenticator.cpp:231] received sasl authentication step  i0224 22:35:53.788027  1972 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4db5fa128d2d' server fqdn: '4db5fa128d2d' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0224 22:35:53.788040  1972 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0224 22:35:53.788090  1972 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0224 22:35:53.788122  1972 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4db5fa128d2d' server fqdn: '4db5fa128d2d' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0224 22:35:53.788136  1972 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0224 22:35:53.788146  1972 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0224 22:35:53.788164  1972 authenticator.cpp:317] authentication success  i0224 22:35:53.788331  1972 authenticatee.cpp:298] authentication success  i0224 22:35:53.788439  1972 master.cpp:5556] successfully authenticated principal 'testprincipal' at slave(115)@172.17.0.1:36678  i0224 22:35:53.788529  1972 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(298)@172.17.0.1:36678  i0224 22:35:53.788988  1972 slave.cpp:927] successfully authenticated with master master@172.17.0.1:36678  i0224 22:35:53.789139  1972 slave.cpp:1321] will retry registration in 1.535786ms if necessary  i0224 22:35:53.789515  1972 master.cpp:4240] registering slave at slave(115)@172.17.0.1:36678 (maintenancehost) with id aab18b6178114c43a672d1a63818c880s0  i0224 22:35:53.790577  1972 registrar.cpp:439] applied 1 operations in 78745ns; attempting to update the 'registry'  i0224 22:35:53.791128  1971 process.cpp:3141] handling http event for process 'master' with path: '/master/maintenance/schedule'  i0224 22:35:53.791877  1971 http.cpp:501] http post for /master/maintenance/schedule from 172.17.0.1:45095  i0224 22:35:53.793313  1972 log.cpp:683] attempting to append 343 bytes to the log  i0224 22:35:53.793586  1972 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0224 22:35:53.794533  1971 replica.cpp:537] replica received write request for position 3 from (4547)@172.17.0.1:36678  i0224 22:35:53.794862  1971 leveldb.cpp:341] persisting action (362 bytes) to leveldb took 283614ns  i0224 22:35:53.794893  1971 replica.cpp:712] persisted action at 3  i0224 22:35:53.796646  1979 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0224 22:35:53.797102  1972 slave.cpp:1321] will retry registration in 17.198963ms if necessary  i0224 22:35:53.797186  1979 leveldb.cpp:341] persisting action (364 bytes) to leveldb took 498502ns  i0224 22:35:53.797230  1979 replica.cpp:712] persisted action at 3  i0224 22:35:53.797260  1979 replica.cpp:697] replica learned append action at position 3  i0224 22:35:53.797417  1972 master.cpp:4228] ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenancehost) as admission is already in progress  i0224 22:35:53.799119  1978 registrar.cpp:484] successfully updated the 'registry' in 8.45824ms  i0224 22:35:53.799613  1978 registrar.cpp:439] applied 1 operations in 176193ns; attempting to update the 'registry'  i0224 22:35:53.800472  1972 master.cpp:4308] registered slave aab18b6178114c43a672d1a63818c880s0 at slave(115)@172.17.0.1:36678 (maintenancehost) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0224 22:35:53.800623  1978 log.cpp:702] attempting to truncate the log to 3  i0224 22:35:53.801255  1969 hierarchical.cpp:473] added slave aab18b6178114c43a672d1a63818c880s0 (maintenancehost) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0224 22:35:53.801301  1978 slave.cpp:971] registered with master master@172.17.0.1:36678; given slave id aab18b6178114c43a672d1a63818c880s0  i0224 22:35:53.801331  1978 fetcher.cpp:81] clearing fetcher cache  i0224 22:35:53.801431  1969 hierarchical.cpp:1434] no resources available to allocate!  i0224 22:35:53.801466  1969 hierarchical.cpp:1147] performed allocation for slave aab18b6178114c43a672d1a63818c880s0 in 162751ns  i0224 22:35:53.801532  1969 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0224 22:35:53.801867  1978 slave.cpp:994] checkpointing slaveinfo to '/tmp/mastermaintenancetestinverseoffersywqvff/meta/slaves/aab18b6178114c43a672d1a63818c880s0/slave.info'  i0224 22:35:53.801877  1969 statusupdatemanager.cpp:181] resuming sending status updates  i0224 22:35:53.802898  1977 replica.cpp:537] replica received write request for position 4 from (4548)@172.17.0.1:36678  i0224 22:35:53.803252  1978 slave.cpp:1030] forwarding total oversubscribed resources   i0224 22:35:53.803640  1970 master.cpp:4649] received update of slave aab18b6178114c43a672d1a63818c880s0 at slave(115)@172.17.0.1:36678 (maintenancehost) with total oversubscribed resources   i0224 22:35:53.803858  1977 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 912626ns  i0224 22:35:53.803889  1977 replica.cpp:712] persisted action at 4  i0224 22:35:53.804144  1978 slave.cpp:3482] received ping from slaveobserver(117)@172.17.0.1:36678  i0224 22:35:53.804535  1971 hierarchical.cpp:531] slave aab18b6178114c43a672d1a63818c880s0 (maintenancehost) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports( ):[3100032000], allocated: )  i0224 22:35:53.804684  1971 hierarchical.cpp:1434] no resources available to allocate!  i0224 22:35:53.804714  1971 hierarchical.cpp:1147] performed allocation for slave aab18b6178114c43a672d1a63818c880 s0 in 131453ns  i0224 22:35:53.805541  1967 replica.cpp:691] replica received learned notice for position 4 from @0.0.0.0:0  i0224 22:35:53.805941  1967 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 366444ns  i0224 22:35:53.806015  1967 leveldb.cpp:399] deleting 2 keys from leveldb took 42808ns  i0224 22:35:53.806041  1967 replica.cpp:712] persisted action at 4  i0224 22:35:53.806066  1967 replica.cpp:697] replica learned truncate action at position 4  i0224 22:35:53.807355  1978 log.cpp:683] attempting to append 465 bytes to the log  i0224 22:35:53.807551  1978 coordinator.cpp:348] coordinator attempting to write append action at position 5  i0224 22:35:53.809638  1979 replica.cpp:537] replica received write request for position 5 from (4549)@172.17.0.1:36678  i0224 22:35:53.810858  1979 leveldb.cpp:341] persisting action (484 bytes) to leveldb took 1.167663ms  i0224 22:35:53.810904  1979 replica.cpp:712] persisted action at 5  i0224 22:35:53.811997  1979 replica.cpp:691] replica received learned notice for position 5 from @0.0.0.0:0  i0224 22:35:53...",1,val
MESOS-4771,Document the network/cni isolator.,"we need to document this isolator in mesoscontainerizer.md (e.g., how to configure it, what's the prerequisite, etc.)",3,val
MESOS-4772,TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing,"we need a way to assign finegrained ownership to tasks/executors so that multiuser frameworks can tell mesos to associate the task with a user identity (rather than just the framework principal+role). then, when an http user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finergrained, userlevel ownership.  some systems may want taskinfo.owner to represent a group rather than an individual user. that's fine as long as the framework sets the field to the group id in such a way that a group aware authorizer can interpret it.",2,val
MESOS-4776,Libprocess metrics/snapshot endpoint rate limiting should be configurable.,"currently the /metrics/snapshot endpoint in libprocess has a https:/github.com/apache/mesos/blob/0.27.1/3rdparty/libprocess/include/process/metrics/metrics.hpp#l52 rate limit of 2 requests per second:        metricsprocess()      : processbase(""metrics""),        limiter(2, seconds(1)) {}      this should be configurable via a libprocess environment variable so that users can control this when initializing libprocess.",2,val
MESOS-4778,Add appc/runtime isolator for runtime isolation for appc images.,"appc image also contains runtime information like 'exec', 'env', 'workingdirectory' etc.  https:/github.com/appc/spec/blob/master/spec/aci.md    similar to docker images, we need to support a subset of them (mainly 'exec', 'env' and 'workingdirectory').",13,val
MESOS-4780,Remove `user` and `rootfs` flags in Windows launcher.,nan,2,val
MESOS-4781,Executor env variables should not be leaked to the command task.,"currently, command task inherits the env variables of the command executor. this is less ideal because the command executor environment variables include some mesos internal env variables like mesosxxx and libprocessxxx. also, this behavior does not match what docker containerizer does. we should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",3,val
MESOS-4783,Disable rate limiting of the global metrics endpoint for mesos-tests execution,once we can optionally disable rate limiting in the global metrics endpoint with mesos4776 we should disable the rate limiting during the execution of mesostests.     rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting   rate limiting might incur additional wait time which might slown down tests,3,val
MESOS-4784,SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint,"the test attempts to observe a change in the slave/containerlauncherrors metric, but does not wait for the triggering action to take place. currently the test passes since hitting the endpoint blocks for some rate limit related time which provides under many circumstances enough wait time for the action to take place. ",1,val
MESOS-4785,Reorganize ACL subject/object descriptions.,"the authorization documentation would benefit from a reorganization of the acl subject/object descriptions. instead of simple lists of the available subjects and objects, it would be nice to see a table showing which subject and object is used with each action.",5,val
MESOS-4787,HTTP endpoint docs should use shorter paths,"my understanding is that the recommended path for the v1 scheduler api is /api/v1/scheduler, but the http endpoint http:/mesos.apache.org/documentation/latest/endpoints/ for this endpoint list the path as /master/api/v1/scheduler; the filename of the doc page is also in the master subdirectory.    similarly, we document the master state endpoint as /master/state, whereas the preferred name is now just /state, and so on for most of the other endpoints. unlike we the v1 api, we might want to consider backward compatibility and document both forms  not sure. but certainly it seems like we should encourage people to use the shorter paths, not the longer ones.",2,val
MESOS-4790,Revert external linkage of symbols in master/constants.hpp,"src/master/constants.hpp contains:      / todo(bmahler): it appears there may be a bug with gcc 4.1.2 in which the  / duration constants were not being initialized when having static linkage.  / this issue did not manifest in newer gcc's. specifically, 4.2.1 was ok.  / so we've moved these to have external linkage but perhaps in the future  / we can revert this.      from commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. we should investigate whether this is still necessary on supported compilers; it likely is not.",1,val
MESOS-4794,Add documentation around using the docker containerizer on CentOS 6.,"support for persistent volumes was added to the docker containerizer in https:/github.com/docker/docker/issues/14365, so we should add precautionary documentation in case anyone tries to use the docker containerizer on centos 6.",1,val
MESOS-4797,Add a couple of registrar tests for /weights endpoint,nan,2,val
MESOS-4798,Make existing scheduler library tests use the callback interface.,we need to migrate the existing tests in src/tests/schedulertests.cpp to use the new callback interface introduced in mesos3339. the changes to src/tests/mastermaintenance_tests.cpp would be done when mesos4831 is resolved.    for an example see schedulertest.schedulerfailover which already uses this new interface.,5,val
MESOS-4801,Updated `createFrameworkInfo` for hierarchical_allocator_tests.cpp.,the function of createframeworkinfo in hierarchicalallocatortests.cpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources.,1,val
MESOS-4802,Update leveldb patch file to suport PowerPC LE,"see: https:/github.com/google/leveldb/releases/tag/v1.18 for improvements / bug fixes.  the motivation is that leveldb 1.18 has officially supported ibm power (ppc64le), so this is needed by https:/issues.apache.org/jira/browse/mesos4312.    update: since someone updated leveldb to 1.4, so i only update the patch file to support powerpc le. because i don't think upgrade 3rdparty library frequently is a good thing.",3,val
MESOS-4803,Update vendored libev to 4.22,"the motivation is that libev 4.22 has officially supported ibm power (ppc64le), so this is needed by https:/issues.apache.org/jira/browse/mesos4312.",3,val
MESOS-4805,Update ry-http-parser-1c3624a to nodejs/http-parser 2.6.1,"see https:/github.com/nodejs/httpparser/releases/tag/v2.6.1.  the motivation is that nodejs/httpparser 2.6.1 has officially supported ibm power (ppc64le), so this is needed by https:/issues.apache.org/jira/browse/mesos4312.",3,val
MESOS-4807,IOTest.BufferedRead writes to the current directory,"libprocess's iotest.bufferedread writes to the current directory. this is bad for a number of reasons, e.g.,     should the test fail data might be leaked to random locations,   the test cannot be executed from a write only directory, or    executing the same test in parallel would race on the existence of the created file, and show bogus behavior.    the test should probably be executed from a temporary directory, e.g., via stout's temporarydirectorytest fixture.",1,val
MESOS-4810,ProvisionerDockerPullerTest.ROOT_INTERNET_CURL_ShellCommand fails.,"  [09:46:46] :  [step 11/11] [ run      ] provisionerdockerregistrypullertest.rootinternetcurlshellcommand  [09:46:46]w:  [step 11/11] i0229 09:46:46.628413  1166 leveldb.cpp:174] opened db in 4.242882ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.629926  1166 leveldb.cpp:181] compacted db in 1.483621ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.629966  1166 leveldb.cpp:196] created db iterator in 15498ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.629977  1166 leveldb.cpp:202] seeked to beginning of db in 1405ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.629984  1166 leveldb.cpp:271] iterated through 0 keys in the db in 239ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.630015  1166 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [09:46:46]w:  [step 11/11] i0229 09:46:46.630470  1183 recover.cpp:447] starting replica recovery  [09:46:46]w:  [step 11/11] i0229 09:46:46.630702  1180 recover.cpp:473] replica is in empty status  [09:46:46]w:  [step 11/11] i0229 09:46:46.631767  1182 replica.cpp:673] replica in empty status received a broadcasted recover request from (14567)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.632115  1183 recover.cpp:193] received a recover response from a replica in empty status  [09:46:46]w:  [step 11/11] i0229 09:46:46.632450  1186 recover.cpp:564] updating replica status to starting  [09:46:46]w:  [step 11/11] i0229 09:46:46.633476  1186 master.cpp:375] master 3fbb2fb04f18498ba4409acbf6923a13 (ip172302124.mesosphere.io) started on 172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.633491  1186 master.cpp:377] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/4uxxow/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/4uxxow/master"" zksessiontimeout=""10secs""  [09:46:46]w:  [step 11/11] i0229 09:46:46.633677  1186 master.cpp:422] master only allowing authenticated frameworks to register  [09:46:46]w:  [step 11/11] i0229 09:46:46.633685  1186 master.cpp:427] master only allowing authenticated slaves to register  [09:46:46]w:  [step 11/11] i0229 09:46:46.633692  1186 credentials.hpp:35] loading credentials for authentication from '/tmp/4uxxow/credentials'  [09:46:46]w:  [step 11/11] i0229 09:46:46.633851  1183 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.191043ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.633873  1183 replica.cpp:320] persisted replica status to starting  [09:46:46]w:  [step 11/11] i0229 09:46:46.633894  1186 master.cpp:467] using default 'crammd5' authenticator  [09:46:46]w:  [step 11/11] i0229 09:46:46.634003  1186 master.cpp:536] using default 'basic' http authenticator  [09:46:46]w:  [step 11/11] i0229 09:46:46.634062  1184 recover.cpp:473] replica is in starting status  [09:46:46]w:  [step 11/11] i0229 09:46:46.634109  1186 master.cpp:570] authorization enabled  [09:46:46]w:  [step 11/11] i0229 09:46:46.634249  1187 whitelistwatcher.cpp:77] no whitelist given  [09:46:46]w:  [step 11/11] i0229 09:46:46.634255  1184 hierarchical.cpp:144] initialized hierarchical allocator process  [09:46:46]w:  [step 11/11] i0229 09:46:46.634884  1187 replica.cpp:673] replica in starting status received a broadcasted recover request from (14569)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.635278  1181 recover.cpp:193] received a recover response from a replica in starting status  [09:46:46]w:  [step 11/11] i0229 09:46:46.635742  1187 recover.cpp:564] updating replica status to voting  [09:46:46]w:  [step 11/11] i0229 09:46:46.636391  1180 master.cpp:1711] the newly elected leader is master@172.30.2.124:37431 with id 3fbb2fb04f18498ba4409acbf6923a13  [09:46:46]w:  [step 11/11] i0229 09:46:46.636415  1180 master.cpp:1724] elected as the leading master!  [09:46:46]w:  [step 11/11] i0229 09:46:46.636430  1180 master.cpp:1469] recovering from registrar  [09:46:46]w:  [step 11/11] i0229 09:46:46.636554  1187 registrar.cpp:307] recovering registrar  [09:46:46]w:  [step 11/11] i0229 09:46:46.637111  1181 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.120322ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.637133  1181 replica.cpp:320] persisted replica status to voting  [09:46:46]w:  [step 11/11] i0229 09:46:46.637218  1186 recover.cpp:578] successfully joined the paxos group  [09:46:46]w:  [step 11/11] i0229 09:46:46.637354  1186 recover.cpp:462] recover process terminated  [09:46:46]w:  [step 11/11] i0229 09:46:46.637715  1182 log.cpp:659] attempting to start the writer  [09:46:46]w:  [step 11/11] i0229 09:46:46.638617  1184 replica.cpp:493] replica received implicit promise request from (14570)@172.30.2.124:37431 with proposal 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.639700  1184 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.057386ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.639722  1184 replica.cpp:342] persisted promised to 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.640251  1184 coordinator.cpp:238] coordinator attempting to fill missing positions  [09:46:46]w:  [step 11/11] i0229 09:46:46.641274  1185 replica.cpp:388] replica received explicit promise request from (14571)@172.30.2.124:37431 for position 0 with proposal 2  [09:46:46]w:  [step 11/11] i0229 09:46:46.642371  1185 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 1.061574ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.642396  1185 replica.cpp:712] persisted action at 0  [09:46:46]w:  [step 11/11] i0229 09:46:46.643299  1186 replica.cpp:537] replica received write request for position 0 from (14572)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.643349  1186 leveldb.cpp:436] reading position from leveldb took 21735ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.644448  1186 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.06671ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.644469  1186 replica.cpp:712] persisted action at 0  [09:46:46]w:  [step 11/11] i0229 09:46:46.645077  1181 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [09:46:46]w:  [step 11/11] i0229 09:46:46.646174  1181 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.069097ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.646198  1181 replica.cpp:712] persisted action at 0  [09:46:46]w:  [step 11/11] i0229 09:46:46.646211  1181 replica.cpp:697] replica learned nop action at position 0  [09:46:46]w:  [step 11/11] i0229 09:46:46.646716  1182 log.cpp:675] writer started with ending position 0  [09:46:46]w:  [step 11/11] i0229 09:46:46.647538  1183 leveldb.cpp:436] reading position from leveldb took 21456ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.648298  1186 registrar.cpp:340] successfully fetched the registry (0b) in 11.71072ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.648388  1186 registrar.cpp:439] applied 1 operations in 21138ns; attempting to update the 'registry'  [09:46:46]w:  [step 11/11] i0229 09:46:46.648947  1187 log.cpp:683] attempting to append 210 bytes to the log  [09:46:46]w:  [step 11/11] i0229 09:46:46.649050  1183 coordinator.cpp:348] coordinator attempting to write append action at position 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.649655  1187 replica.cpp:537] replica received write request for position 1 from (14573)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.650725  1187 leveldb.cpp:341] persisting action (229 bytes) to leveldb took 1.041938ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.650748  1187 replica.cpp:712] persisted action at 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.651198  1181 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [09:46:46]w:  [step 11/11] i0229 09:46:46.652312  1181 leveldb.cpp:341] persisting action (231 bytes) to leveldb took 1.092268ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.652335  1181 replica.cpp:712] persisted action at 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.652349  1181 replica.cpp:697] replica learned append action at position 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.653095  1187 registrar.cpp:484] successfully updated the 'registry' in 4.664064ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.653236  1187 registrar.cpp:370] successfully recovered registrar  [09:46:46]w:  [step 11/11] i0229 09:46:46.653306  1181 log.cpp:702] attempting to truncate the log to 1  [09:46:46]w:  [step 11/11] i0229 09:46:46.653476  1184 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [09:46:46]w:  [step 11/11] i0229 09:46:46.653642  1183 master.cpp:1521] recovered 0 slaves from the registry (171b) ; allowing 10mins for slaves to reregister  [09:46:46]w:  [step 11/11] i0229 09:46:46.653659  1181 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  [09:46:46]w:  [step 11/11] i0229 09:46:46.654270  1181 replica.cpp:537] replica received write request for position 2 from (14574)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.655357  1181 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.055267ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.655378  1181 replica.cpp:712] persisted action at 2  [09:46:46]w:  [step 11/11] i0229 09:46:46.655850  1184 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [09:46:46]w:  [step 11/11] i0229 09:46:46.657009  1184 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 1.137223ms  [09:46:46]w:  [step 11/11] i0229 09:46:46.657059  1184 leveldb.cpp:399] deleting ~1 keys from leveldb took 26459ns  [09:46:46]w:  [step 11/11] i0229 09:46:46.657074  1184 replica.cpp:712] persisted action at 2  [09:46:46]w:  [step 11/11] i0229 09:46:46.657089  1184 replica.cpp:697] replica learned truncate action at position 2  [09:46:46]w:  [step 11/11] i0229 09:46:46.665710  1166 containerizer.cpp:149] using isolation: docker/runtime,filesystem/linux  [09:46:46]w:  [step 11/11] i0229 09:46:46.672399  1166 linuxlauncher.cpp:101] using /sys/fs/cgroup/freezer as the freezer hierarchy for the linux launcher  [09:46:46]w:  [step 11/11] e0229 09:46:46.676822  1166 shell.hpp:93] command 'hadoop version 2>&1' failed; this is the output:  [09:46:46]w:  [step 11/11] sh: hadoop: command not found  [09:46:46]w:  [step 11/11] e0229 09:46:46.676851  1166 fetcher.cpp:58] failed to create uri fetcher plugin 'hadoop': failed to create hdfs client: failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a nonzero exit status: 127  [09:46:46]w:  [step 11/11] i0229 09:46:46.678383  1166 linux.cpp:81] making '/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv' a shared mount  [09:46:46]w:  [step 11/11] i0229 09:46:46.687223  1180 slave.cpp:193] slave started on 422)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.687248  1180 slave.cpp:194] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageproviders=""docker"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""docker/runtime,filesystem/linux"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv""  [09:46:46]w:  [step 11/11] i0229 09:46:46.687531  1180 credentials.hpp:83] loading credential for authentication from '/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv/credential'  [09:46:46]w:  [step 11/11] i0229 09:46:46.687666  1180 slave.cpp:324] slave using credential for: testprincipal  [09:46:46]w:  [step 11/11] i0229 09:46:46.687798  1180 resources.cpp:572] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  [09:46:46]w:  [step 11/11] trying semicolondelimited string format instead  [09:46:46]w:  [step 11/11] i0229 09:46:46.688151  1180 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  [09:46:46]w:  [step 11/11] i0229 09:46:46.688207  1180 slave.cpp:472] slave attributes: [  ]  [09:46:46]w:  [step 11/11] i0229 09:46:46.688217  1180 slave.cpp:477] slave hostname: ip172302124.mesosphere.io  [09:46:46]w:  [step 11/11] i0229 09:46:46.689259  1187 state.cpp:58] recovering state from '/tmp/provisionerdockerregistrypullertestrootinternetcurlshellcommand5bwcfv/meta'  [09:46:46]w:  [step 11/11] i0229 09:46:46.689394  1166 sched.cpp:222] version: 0.28.0  [09:46:46]w:  [step 11/11] i0229 09:46:46.689497  1180 statusupdatemanager.cpp:200] recovering status update manager  [09:46:46]w:  [step 11/11] i0229 09:46:46.689798  1182 containerizer.cpp:407] recovering containerizer  [09:46:46]w:  [step 11/11] i0229 09:46:46.690021  1186 sched.cpp:326] new master detected at master@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.690146  1186 sched.cpp:382] authenticating with master master@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.690162  1186 sched.cpp:389] using default crammd5 authenticatee  [09:46:46]w:  [step 11/11] i0229 09:46:46.690378  1181 authenticatee.cpp:121] creating new client sasl connection  [09:46:46]w:  [step 11/11] i0229 09:46:46.690688  1186 master.cpp:5540] authenticating scheduler52603476875a49a885d4c98d102cdfab@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.690801  1184 authenticator.cpp:413] starting authentication session for crammd5authenticatee(877)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.691025  1181 authenticator.cpp:98] creating new server sasl connection  [09:46:46]w:  [step 11/11] i0229 09:46:46.691314  1180 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  [09:46:46]w:  [step 11/11] i0229 09:46:46.691339  1180 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  [09:46:46]w:  [step 11/11] i0229 09:46:46.691437  1180 authenticator.cpp:203] received sasl authentication start  [09:46:46]w:  [step 11/11] i0229 09:46:46.691490  1180 authenticator.cpp:325] authentication requires more steps  [09:46:46]w:  [step 11/11] i0229 09:46:46.691581  1180 authenticatee.cpp:258] received sasl authentication step  [09:46:46]w:  [step 11/11] i0229 09:46:46.691684  1180 authenticator.cpp:231] received sasl authentication step  [09:46:46]w:  [step 11/11] i0229 09:46:46.691712  1180 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302124.mesosphere.io' server fqdn: 'ip172302124.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   [09:46:46]w:  [step 11/11] i0229 09:46:46.691726  1180 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [09:46:46]w:  [step 11/11] i0229 09:46:46.691768  1180 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [09:46:46]w:  [step 11/11] i0229 09:46:46.691802  1180 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302124.mesosphere.io' server fqdn: 'ip172302124.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   [09:46:46]w:  [step 11/11] i0229 09:46:46.691817  1180 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  [09:46:46]w:  [step 11/11] i0229 09:46:46.691829  1180 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  [09:46:46]w:  [step 11/11] i0229 09:46:46.691848  1180 authenticator.cpp:317] authentication success  [09:46:46]w:  [step 11/11] i0229 09:46:46.691944  1186 authenticatee.cpp:298] authentication success  [09:46:46]w:  [step 11/11] i0229 09:46:46.692011  1185 master.cpp:5570] successfully authenticated principal 'testprincipal' at scheduler52603476875a49a885d4c98d102cdfab@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.692056  1187 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(877)@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.692308  1184 sched.cpp:471] successfully authenticated with master master@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.692325  1184 sched.cpp:776] sending subscribe call to master@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.692399  1184 sched.cpp:809] will retry registration in 954.231367ms if necessary  [09:46:46]w:  [step 11/11] i0229 09:46:46.692505  1183 master.cpp:2279] received subscribe call for framework 'default' at scheduler52603476875a49a885d4c98d102cdfab@172.30.2.124:37431  [09:46:46]w:  [step 11/11] i0229 09:46:46.692553  1183 master.cpp:1750] authorizing framework principal 'testprincipal' to receive offers for role ''  [09:46:46]w:  [step 11/11] i0229 09:46:46.692836  1184 master.cpp:2350] subscribing framework default with checkpointing disabled and capabilities [  ]  [09:46:46]w:  [step 11/11] i0229 09:46:46.692942  1183 metadatamanager.cpp:188] no images to load from disk. docker provisioner image storage path '/tmp/mesos/store/docker/storedimages' does not exist  [09:46:46]w:  [step 11/11] i0229 09:46:46.693208  1180 provisioner.cpp:245] provisioner recovery complete  [09:46:46]w:  [step 11/11] i0229 09:46:46.693295  1186 hierarchical.cpp:265] added framework 3fbb2fb04f18498ba4409acbf6923a130000  [09:46:46]w:  [step 11/11] i0229 09:46:46.693357  1186 hierarchical.cpp:1437] no resources available to allocate!  [09:46:46]w:  [step 11/11] i0229 09:46:46.693397  1186 hierarchical.cpp:1532] no inverse offers to send out!  [09:46:46]w:  [step 11/11] i0229 09:46:46.693424  1186 ...",3,val
MESOS-4813,Implement base tests for unified container using local puller.,using command line executor to test shell commands with local docker images.,2,val
MESOS-4817,Remove internal usage of deprecated *.json endpoints.,"we still use the deprecated  .json internally (ui, tests, documentation). ",3,val
MESOS-4818,Add end to end testing for Appc images.,add tests that covers integration test of the appc provisioner feature with mesos containerizer.   ,3,val
MESOS-4819,Add documentation for Appc image discovery.,add documentation for the appc image discovery feature that covers:     use case   implementation detail (simple discovery).,3,val
MESOS-4820,Need to set `EXPOSED` ports from docker images into `ContainerConfig`,"most docker images have an `expose` command associated with them. this tells the container runtime the tcp ports that the microservice ""wishes"" to expose to the outside world.     with the `unified containerizer` project since `mesoscontainerizer` is going to natively support docker images it is imperative that the mesos container run time have a mechanism to expose ports listed in a docker image. the first step to achieve this is to extract this information from the `docker` image and set in the `containerconfig` . the `containerconfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",1,val
MESOS-4821,Introduce a port field in `ImageManifest` in order to set exposed ports for a container.,networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. this can be achieved by adding a field to the `imagemanifest` protobuf and allowing the `imageprovisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. ,1,val
MESOS-4822,Add support for local image fetching in Appc provisioner.,currently appc image provisioner supports http(s) fetching. it would be valuable to add support for local file path(uri) based  fetching.,2,val
MESOS-4823,Implement port forwarding in `network/cni` isolator,"most docker and appc images wish to expose ports that micro services are listening on, to the outside world. when containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). this can be done in the `network/cni` isolator.     the reason we would like this functionality to be implemented in the `network/cni` isolator, and not a cni plugin, is that the specifications currently do not support specifying port forwarding rules. further, to install these rules the isolator needs two pieces of information, the exposed ports and the ip address associated with the container. bother are available to the isolator.",2,val
MESOS-4824,"""filesystem/linux"" isolator does not unmount orphaned persistent volumes","a persistent volume can be orphaned when:  # a framework registers with checkpointing enabled.  # the framework starts a task + a persistent volume.  # the agent exits.  the task continues to run.  # something wipes the agent's meta directory.  this removes the checkpointed framework info from the agent.  # the agent comes back and recovers.  the framework for the task is not found, so the task is considered orphaned now.    the agent currently does not unmount the persistent volume, saying (with glog_v=1)     i0229 23:55:42.078940  5635 linux.cpp:711] ignoring cleanup request for unknown container: a35189d385d54d02b56867f675b6dc97      test implemented here: https:/reviews.apache.org/r/44122/",2,val
MESOS-4825,Master's slave reregister logic does not update version field,the master's logic for reregistering a slave does not update the version field if the slave re registers with a new version.,1,val
MESOS-4829,Remove `grace_period_seconds` field from Shutdown event v1 protobuf.,"there are two ways in which a shutdown of executor can be triggered:  1. if it receives an explicit `shutdown` message from the agent.  2. if the recovery timeout period has elapsed, and the executor still hasnt been able to (re )connect with the agent.    currently, the executor library relies on the field `graceperiodseconds` having a default value of 5 seconds to handle the second scenario. https:/github.com/apache/mesos/blob/master/src/executor/executor.cpp#l608    the driver used to trigger the grace period via a constant defined in src/slave/constants.cpp. https:/github.com/apache/mesos/blob/master/src/exec/exec.cpp#l92    the agent may want to force a shorter shutdown grace period (e.g. oversubscription eviction may have shorter deadline) in the future. for now, we can just read the value via an environment variable.",3,val
MESOS-4830,Bind docker runtime isolator with docker image provider.,"if image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. a check should be added to make sure docker runtime isolator is on if using docker as image provider.",1,val
MESOS-4832,DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted,"if the /tmp directory (where mesos tests create temporary directories) is a bind mount, the test suite will exit here:    [ run      ] dockercontainerizertest.rootdockerrecoverorphanedpersistentvolumes  i0226 03:17:26.722806  1097 leveldb.cpp:174] opened db in 12.587676ms  i0226 03:17:26.723496  1097 leveldb.cpp:181] compacted db in 636999ns  i0226 03:17:26.723536  1097 leveldb.cpp:196] created db iterator in 18271ns  i0226 03:17:26.723547  1097 leveldb.cpp:202] seeked to beginning of db in 1555ns  i0226 03:17:26.723554  1097 leveldb.cpp:271] iterated through 0 keys in the db in 363ns  i0226 03:17:26.723593  1097 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0226 03:17:26.724128  1117 recover.cpp:447] starting replica recovery  i0226 03:17:26.724367  1117 recover.cpp:473] replica is in empty status  i0226 03:17:26.725237  1117 replica.cpp:673] replica in empty status received a broadcasted recover request from (13810)@172.30.2.151:51934  i0226 03:17:26.725744  1114 recover.cpp:193] received a recover response from a replica in empty status  i0226 03:17:26.726356  1111 master.cpp:376] master 5cc57c0ef1ad4107893f420ed1a1db1a (ip172302151.mesosphere.io) started on 172.30.2.151:51934  i0226 03:17:26.726369  1118 recover.cpp:564] updating replica status to starting  i0226 03:17:26.726378  1111 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/djhtvq/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/djhtvq/master"" zksessiontimeout=""10secs""  i0226 03:17:26.726605  1111 master.cpp:423] master only allowing authenticated frameworks to register  i0226 03:17:26.726616  1111 master.cpp:428] master only allowing authenticated slaves to register  i0226 03:17:26.726632  1111 credentials.hpp:35] loading credentials for authentication from '/tmp/djhtvq/credentials'  i0226 03:17:26.726860  1111 master.cpp:468] using default 'crammd5' authenticator  i0226 03:17:26.726977  1111 master.cpp:537] using default 'basic' http authenticator  i0226 03:17:26.727092  1111 master.cpp:571] authorization enabled  i0226 03:17:26.727243  1118 hierarchical.cpp:144] initialized hierarchical allocator process  i0226 03:17:26.727285  1116 whitelistwatcher.cpp:77] no whitelist given  i0226 03:17:26.728852  1114 master.cpp:1712] the newly elected leader is master@172.30.2.151:51934 with id 5cc57c0ef1ad4107893f420ed1a1db1a  i0226 03:17:26.728876  1114 master.cpp:1725] elected as the leading master!  i0226 03:17:26.728891  1114 master.cpp:1470] recovering from registrar  i0226 03:17:26.728977  1117 registrar.cpp:307] recovering registrar  i0226 03:17:26.731503  1112 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 4.977811ms  i0226 03:17:26.731539  1112 replica.cpp:320] persisted replica status to starting  i0226 03:17:26.731711  1111 recover.cpp:473] replica is in starting status  i0226 03:17:26.732501  1114 replica.cpp:673] replica in starting status received a broadcasted recover request from (13812)@172.30.2.151:51934  i0226 03:17:26.732862  1111 recover.cpp:193] received a recover response from a replica in starting status  i0226 03:17:26.733264  1117 recover.cpp:564] updating replica status to voting  i0226 03:17:26.733836  1118 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 388246ns  i0226 03:17:26.733855  1118 replica.cpp:320] persisted replica status to voting  i0226 03:17:26.733979  1113 recover.cpp:578] successfully joined the paxos group  i0226 03:17:26.734149  1113 recover.cpp:462] recover process terminated  i0226 03:17:26.734478  1111 log.cpp:659] attempting to start the writer  i0226 03:17:26.735523  1114 replica.cpp:493] replica received implicit promise request from (13813)@172.30.2.151:51934 with proposal 1  i0226 03:17:26.736130  1114 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 576451ns  i0226 03:17:26.736150  1114 replica.cpp:342] persisted promised to 1  i0226 03:17:26.736709  1115 coordinator.cpp:238] coordinator attempting to fill missing positions  i0226 03:17:26.737771  1114 replica.cpp:388] replica received explicit promise request from (13814)@172.30.2.151:51934 for position 0 with proposal 2  i0226 03:17:26.738386  1114 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 583184ns  i0226 03:17:26.738404  1114 replica.cpp:712] persisted action at 0  i0226 03:17:26.739312  1118 replica.cpp:537] replica received write request for position 0 from (13815)@172.30.2.151:51934  i0226 03:17:26.739367  1118 leveldb.cpp:436] reading position from leveldb took 26157ns  i0226 03:17:26.740638  1118 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.238477ms  i0226 03:17:26.740669  1118 replica.cpp:712] persisted action at 0  i0226 03:17:26.741158  1118 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0226 03:17:26.742878  1118 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.697254ms  i0226 03:17:26.742902  1118 replica.cpp:712] persisted action at 0  i0226 03:17:26.742916  1118 replica.cpp:697] replica learned nop action at position 0  i0226 03:17:26.743393  1117 log.cpp:675] writer started with ending position 0  i0226 03:17:26.744370  1112 leveldb.cpp:436] reading position from leveldb took 34329ns  i0226 03:17:26.745240  1117 registrar.cpp:340] successfully fetched the registry (0b) in 16.21888ms  i0226 03:17:26.745350  1117 registrar.cpp:439] applied 1 operations in 30460ns; attempting to update the 'registry'  i0226 03:17:26.746016  1111 log.cpp:683] attempting to append 210 bytes to the log  i0226 03:17:26.746119  1116 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0226 03:17:26.746798  1114 replica.cpp:537] replica received write request for position 1 from (13816)@172.30.2.151:51934  i0226 03:17:26.747251  1114 leveldb.cpp:341] persisting action (229 bytes) to leveldb took 411333ns  i0226 03:17:26.747269  1114 replica.cpp:712] persisted action at 1  i0226 03:17:26.747808  1113 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0226 03:17:26.749511  1113 leveldb.cpp:341] persisting action (231 bytes) to leveldb took 1.673488ms  i0226 03:17:26.749534  1113 replica.cpp:712] persisted action at 1  i0226 03:17:26.749550  1113 replica.cpp:697] replica learned append action at position 1  i0226 03:17:26.750422  1111 registrar.cpp:484] successfully updated the 'registry' in 5.021952ms  i0226 03:17:26.750560  1111 registrar.cpp:370] successfully recovered registrar  i0226 03:17:26.750635  1112 log.cpp:702] attempting to truncate the log to 1  i0226 03:17:26.750751  1113 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0226 03:17:26.751096  1116 master.cpp:1522] recovered 0 slaves from the registry (171b) ; allowing 10mins for slaves to reregister  i0226 03:17:26.751126  1111 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0226 03:17:26.751561  1118 replica.cpp:537] replica received write request for position 2 from (13817)@172.30.2.151:51934  i0226 03:17:26.751999  1118 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 406823ns  i0226 03:17:26.752018  1118 replica.cpp:712] persisted action at 2  i0226 03:17:26.752521  1113 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0226 03:17:26.754161  1113 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 1.614888ms  i0226 03:17:26.754210  1113 leveldb.cpp:399] deleting ~1 keys from leveldb took 26384ns  i0226 03:17:26.754225  1113 replica.cpp:712] persisted action at 2  i0226 03:17:26.754240  1113 replica.cpp:697] replica learned truncate action at position 2  i0226 03:17:26.765103  1115 slave.cpp:193] slave started on 399)@172.30.2.151:51934  i0226 03:17:26.765130  1115 slave.cpp:194] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/dockercontainerizertestrootdockerrecoverorphanedpersistentvolumesajoesp/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/dockercontainerizertestrootdockerrecoverorphanedpersistentvolumesajoesp/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpu:2;mem:2048;disk(role1):2048"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/dockercontainerizertestrootdockerrecoverorphanedpersistentvolumesajoesp""  i0226 03:17:26.765403  1115 credentials.hpp:83] loading credential for authentication from '/tmp/dockercontainerizertestrootdockerrecoverorphanedpersistentvolumesajoesp/credential'  i0226 03:17:26.765573  1115 slave.cpp:324] slave using credential for: testprincipal  i0226 03:17:26.765733  1115 resources.cpp:576] parsing resources as json failed: cpu:2;mem:2048;disk(role1):2048  trying semicolondelimited string format instead  i0226 03:17:26.766185  1115 slave.cpp:464] slave resources: cpu():2; mem():2048; disk(role1):2048; cpus():8; ports():[3100032000]  i0226 03:17:26.766242  1115 slave.cpp:472] slave attributes: [  ]  i0226 03:17:26.766250  1115 slave.cpp:477] slave hostname: ip172302151.mesosphere.io  i0226 03:17:26.767325  1097 sched.cpp:222] version: 0.28.0  i0226 03:17:26.767390  1111 state.cpp:58] recovering state from '/tmp/dockercontainerizertestrootdockerrecoverorphanedpersistentvolumesajoesp/meta'  i0226 03:17:26.767603  1115 statusupdatemanager.cpp:200] recovering status update manager  i0226 03:17:26.767865  1113 docker.cpp:726] recovering docker containers  i0226 03:17:26.767971  1111 sched.cpp:326] new master detected at master@172.30.2.151:51934  i0226 03:17:26.768045  1111 sched.cpp:382] authenticating with master master@172.30.2.151:51934  i0226 03:17:26.768059  1111 sched.cpp:389] using default crammd5 authenticatee  i0226 03:17:26.768070  1118 slave.cpp:4565] finished recovery  i0226 03:17:26.768273  1112 authenticatee.cpp:121] creating new client sasl connection  i0226 03:17:26.768435  1118 slave.cpp:4737] querying resource estimator for oversubscribable resources  i0226 03:17:26.768565  1111 master.cpp:5526] authenticating schedulerc59020d6385e48a38a109e5c3f1dbd92@172.30.2.151:51934  i0226 03:17:26.768661  1118 slave.cpp:796] new master detected at master@172.30.2.151:51934  i0226 03:17:26.768659  1115 authenticator.cpp:413] starting authentication session for crammd5authenticatee(839)@172.30.2.151:51934  i0226 03:17:26.768679  1113 statusupdatemanager.cpp:174] pausing sending status updates  i0226 03:17:26.768728  1118 slave.cpp:859] authenticating with master master@172.30.2.151:51934  i0226 03:17:26.768743  1118 slave.cpp:864] using default crammd5 authenticatee  i0226 03:17:26.768865  1118 slave.cpp:832] detecting new master  i0226 03:17:26.768868  1112 authenticator.cpp:98] creating new server sasl connection  i0226 03:17:26.768908  1114 authenticatee.cpp:121] creating new client sasl connection  i0226 03:17:26.769003  1118 slave.cpp:4751] received oversubscribable resources  from the resource estimator  i0226 03:17:26.769103  1115 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0226 03:17:26.769131  1115 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0226 03:17:26.769209  1116 master.cpp:5526] authenticating slave(399)@172.30.2.151:51934  i0226 03:17:26.769253  1114 authenticator.cpp:203] received sasl authentication start  i0226 03:17:26.769295  1115 authenticator.cpp:413] starting authentication session for crammd5authenticatee(840)@172.30.2.151:51934  i0226 03:17:26.769307  1114 authenticator.cpp:325] authentication requires more steps  i0226 03:17:26.769403  1117 authenticatee.cpp:258] received sasl authentication step  i0226 03:17:26.769495  1114 authenticator.cpp:98] creating new server sasl connection  i0226 03:17:26.769531  1115 authenticator.cpp:231] received sasl authentication step  i0226 03:17:26.769554  1115 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302151.mesosphere.io' server fqdn: 'ip172302151.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0226 03:17:26.769562  1115 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0226 03:17:26.769608  1115 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0226 03:17:26.769629  1115 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302151.mesosphere.io' server fqdn: 'ip172302151.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0226 03:17:26.769637  1115 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0226 03:17:26.769642  1115 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0226 03:17:26.769654  1115 authenticator.cpp:317] authentication success  i0226 03:17:26.769728  1117 authenticatee.cpp:298] authentication success  i0226 03:17:26.769769  1112 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0226 03:17:26.769767  1118 master.cpp:5556] successfully authenticated principal 'testprincipal' at schedulerc59020d6385e48a38a109e5c3f1dbd92@172.30.2.151:51934  i0226 03:17:26.769803  1112 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0226 03:17:26.769798  1114 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(839)@172.30.2.151:51934  i0226 03:17:26.769881  1112 authenticator.cpp:203] received sasl authentication start  i0226 03:17:26.769932  1112 authenticator.cpp:325] authentication requires more steps  i0226 03:17:26.769981  1117 sched.cpp:471] successfully authenticated with master master@172.30.2.151:51934  i0226 03:17:26.770004  1117 sched.cpp:776] sending subscribe call to master@172.30.2.151:51934  i0226 03:17:26.770064  1118 authenticatee.cpp:258] received sasl authentication step  i0226 03:17:26.770102  1117 sched.cpp:809] will retry registration in 1.937819802secs if necessary  i0226 03:17:26.770165  1115 authenticator.cpp:231] received sasl authentication step  i0226 03:17:26.770193  1115 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302151.mesosphere.io' server fqdn: 'ip172302151.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0226 03:17:26.770207  1115 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0226 03:17:26.770213  1116 master.cpp:2280] received subscribe call for framework 'default' at schedulerc59020d6385e48a38a109e5c3f1dbd92@172.30.2.151:51934  i0226 03:17:26.770241  1115 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0226 03:17:26.770274  1115 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302151.mesosphere.io' server fqdn: 'ip172302151.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0226 03:17:26.770277  1116 master.cpp:1751] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i0226 03:17:26.770298  1115 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0226 03:17:26.770331  1115 auxprop.cpp:129] skipping auxiliary property ' cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0226 03:17:26.770349  1115 authenticator.cpp:317] authentication success  i0226 03:17:26.770428  1118 authenticatee.cpp:298] authentication success  i0226 03:17:26.770442  1116 master.cpp:5556] successfully authenticated principal 'testprincipal' at slave(399)@172.30.2.151:51934  i0226 03:17:26.770547  1116 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(840)@172.30.2.151:51934  i0226 03:17:26.770846  1116 master.cpp:2351] subscribing framework default with checkpointing enabled and capabilities [  ]  i0226 03:17:26.770866  1118 slave.cpp:927] successfully authenticated with master master@172.30.2.151:51934  i0226 03:17:26.770966  1118 slave.cpp:1321] will retry registration in 1.453415ms if necessary  i0226 03:17:26.771225  1115 hierarchical.cpp:265] added framework 5cc57c0ef1ad4107893f420ed1a1db1a0000  i0226 03:17:26.771275  1118 sched.cpp:703] framework registered with 5cc57c0ef1ad4107893f420ed1a1db1a0000  i0226 03:17:26.771299  1115 hierarchical.cpp:1434] no resources available to allocate!  i0226 03:17:26.771328  1115 hierarchical.cpp:1529] no inverse offers to send out!  i0226 03:17:26.771344  1118 sched.cpp:717] scheduler::registered took 50146ns  i0226 03:17:26.771356  1116 master.cpp:4240] registering slave at slave(399)@172.30.2.151:51934 (ip172302151.mesosphere.io) with id 5cc57c0ef1ad4107893f420ed1a1db1as0  i0226 03:17:26.771348  1115 hierarchical.cpp:1127] performed allocation for 0 slaves in 101438ns  i0226 03:17:26.771860  1114 registrar.cpp:439] applied 1 operations in 59672ns; attempting to update the 'registry'  i0226 03:17:26.772645  1117 log.cpp:683] attempting to append 423 bytes to the log  i0226 03:17:26.772758  1112 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0226 03:17:26.773435  1117 replica.cpp:537] replica received write request for position 3 from (13824)@172.30.2.151:51934  i0226 03:17:26.773586  1111 slave.cpp:1321] will retry registration in 2.74261ms if necessary  i0226 03:17:26.773682  1115 master.cpp:4228] ignoring register slave message from slave(399)@172.30.2.151:51934 (ip172302 151.mesosphere.io) as admission is already in progress  i0226 03:17:26.773937  1117 leveldb.cpp:341] persisting action (442 bytes) to level...",2,val
MESOS-4833,Poor allocator performance with labeled resources and/or persistent volumes,"modifying the hierarchicalallocatorbenchmarktest.resourcelabels benchmark from https:/reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from 2 seconds to 3 minutes. the culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume ids) inhibit merging, which causes performance to be much worse.",5,val
MESOS-4834,Add 'file' fetcher plugin.,"add support for ""file"" based uri fetcher. this could be useful for container image provisioning from local file system.",2,val
MESOS-4835,CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky,"verbose logs:     [ run      ] cgroupsanyhierarchywithfreezertest.rootcgroupsdestroytracedprocess  i0302 00:43:14.127846 11755 cgroups.cpp:2427] freezing cgroup /sys/fs/cgroup/freezer/mesostest  i0302 00:43:14.267411 11758 cgroups.cpp:1409] successfully froze cgroup /sys/fs/cgroup/freezer/mesostest after 139.46496ms  i0302 00:43:14.409395 11751 cgroups.cpp:2445] thawing cgroup /sys/fs/cgroup/freezer/mesostest  i0302 00:43:14.551304 11751 cgroups.cpp:1438] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesostest after 141.811968ms  ../../src/tests/containerizer/cgroupstests.cpp:949: failure  value of: ::waitpid(pid, &status, 0)    actual: 23809  expected:  1  ../../src/tests/containerizer/cgroupstests.cpp:950: failure  value of: ( errnolocation ())    actual: 0  expected: 10  [  failed  ] cgroupsanyhierarchywithfreezertest.rootcgroups_destroytracedprocess (1055 ms)  ",2,val
MESOS-4836,Fix rmdir for windows,this is due to a bug in mesos 4415 that landed for 0.27.0.,1,val
MESOS-4839,Move placement new processes into the freezer cgroup into a parent hook.,the linux launcher places new processes into the freezer cgroup.  this is currently done by a combination of childsetup function (blocking the new process until parent is done) and the parent (placing child process into the cgroup and then signaling child to continue).  parenthooks support this behavior (blocking child until some work is done in the parent) in a much cleaner way.   ,3,val
MESOS-4840,Remove internal usage of deprecated ShutdownFramework ACL,shutdownframework acl was deprecated a couple of versions ago in favor of the teardownframework message. its deprecation cycle came with 0.27. that means we should remove the message and its references in the code base.,2,val
MESOS-4844,Add authentication to master endpoints,"before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when authenticate_http is enabled, and so that the principal is passed into `route()`.",2,val
MESOS-4848,Agent Authn Research Spike,research the master authentication flags to see what changes will be necessary for agent http authentication.  write up a 1 2 page summary/design doc.,2,val
MESOS-4849,Add agent flags for HTTP authentication,flags should be added to the agent to:  1. enable http authentication (authenticatehttp)  2. specify credentials (httpcredentials)  3. specify http authenticators (authenticators),2,val
MESOS-4850,Add authentication to agent endpoints /state and /flags,"the /state and /flags endpoints are installed in src/slave/slave.cpp, and thus are straightforward to make authenticated. other agent endpoints require a bit more consideration, and are tracked in mesos4902.    for more information on agent endpoints, see http:/mesos.apache.org/documentation/latest/endpoints/  or search for `route(` in the source code:    $ grep rn ""route("" src/ grep v tests grep v tests grep v examples grep  v ""process..pp""  3rdparty/libprocess/include/process/profiler.hpp:34:    route(""/start"", starthelp(), &profiler::start);  3rdparty/libprocess/include/process/profiler.hpp:35:    route(""/stop"", stophelp(), &profiler::stop);  3rdparty/libprocess/include/process/system.hpp:70:    route(""/stats.json"", statshelp(), &system::stats);  3rdparty/libprocess/include/process/logging.hpp:44:    route(""/toggle"", togglehelp(), &this::toggle);  ",3,val
MESOS-4854,Update CHANGELOG with net_cls isolator,need to update the changelog for 0.28 release.,1,val
MESOS-4858,Make changes to executor v1 library around managing connections.,"while implementing pipelining changes for the scheduler library (mesos3570), we noticed a couple of small bugs that we would like to fix in the executor library:     don't pass connection objects to defer callbacks as they can sometimes lead to deadlocks.   minor cleanups around not accepting subscribe call if one is currently in progress.   create a random uuid (connectionid) before we initiate a connection to the agent, as in some scenarios, we can accept connection attempts from stale connections.",3,val
MESOS-4859,Add explicit upgrade instructions to the docs,"the documentation currently contains perversion upgrade guidelines, which for recent releases only outlines the upgrade concerns for that version, without detailing explicit upgrade instructions.    we should add explicit upgrade instructions to the top of the upgrades documentation, which can be supplemented by the perversion concerns.    this is done within the upgrade docs for some early versions, with text like:      in order to upgrade a running cluster:    install the new master binaries and restart the masters.  upgrade the schedulers by linking the latest native library and mesos jar (if necessary).  restart the schedulers.  install the new slave binaries and restart the slaves.  upgrade the executors by linking the latest native library and mesos jar (if necessary).      instructions to this effect should be featured prominently in the doc.",1,val
MESOS-4860,Add a script to install the Nvidia GDK on a host.,"this script can be used to install the nvidia gdk for cuda 7.5 on a  mesos development machine. the purpose of the nvidia gdk is to provide  all the necessary header files (nvml.h) and library files  (libnvidiaml.so) necessary to build mesos with nvidia gpu support.    if the machine on which mesos is being compiled doesn't have any gpus,  then libnvidiaml.so consists only of stubs, allowing mesos to build  and run, but not actually do anything useful under the hood. this  enables us to build a gpu enabled mesos on a development machine  without gpus and then deploy it to a production machine with gpus and  be reasonably sure it will work.",2,val
MESOS-4861,Add configure flags to build with Nvidia GPU support.,"the configure flags can be used to enable nvidia gpu support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.    they will also be used to conditionally build support for nvidia gpus into mesos.",2,val
MESOS-4863,Add Nvidia GPU isolator tests.,"we need to be able to run unit tests that verify gpu isolation, as well as run full blown tests that actually exercise the gpus.    these tests should only build when the proper configure flags are set for enabling nvidia gpu support.",2,val
MESOS-4864,Add flag to specify available Nvidia GPUs on an agent's command line.,"in the initial gpu support we will not do auto discovery of gpus on an agent.  as such, an operator will need to specify a flag on the command line, listing all of the gpus available on the system.",3,val
MESOS-4865,Add GPUs as an explicit resource.,"we will add ""gpus"" as an explicitly recognized resource in mesos, akin to cpus, memory, ports, and disk.  in the containerizer, we will verify that the number of gpu resources passed in via the resources flag matches the list of gpus passed in via the nvidia_gpus flag.  in the future we will add autodiscovery so this matching is unnecessary.  however, we will always have to pass ""gpus"" as a resource to make any gpu available on the system (unlike for cpus and memory, where the default is probed).",3,val
MESOS-4868,PersistentVolumeTests do not need to set up ACLs.,"the persistentvolumetest s have a custom helper for setting up acls in the master::flags:    acls acls;      hashset/ roles;        foreach (const frameworkinfo& framework, frameworks)         flags.acls = acls;      flags.roles = strings::join("","", roles);      this is no longer necessary with implicit roles.",1,val
MESOS-4872,Dump the contents of the sandbox when a test fails," added this logic for extra info about a rare flaky test:  https:/github.com/apache/mesos/blob/d26baee1f377aedb148ad04cc004bb38b85ee4f6/src/tests/fetchercachetests.cpp#l249l259    this information is useful regardless of the test type and should be generalized for cluster::slave.  i.e.   # when a cluster::slave is destructed, it can detect if the test has failed.    # if so, navigate through its own work_dir and print sandboxes and/or other useful debugging info.  also see the refactor in [mesos 4634].",3,val
MESOS-4873,Add documentation about container image support.,nan,5,val
MESOS-4877,"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")","this can be demonstrated with the mesosexecute command:    # docker containerizer with image alpine: success    sudo ./build/src/mesosexecute dockerimage=alpine containerizer=docker name=justatest command=""sleep 1000"" master=localhost:5050    # mesos containerizer with image alpine: failure    sudo ./build/src/mesosexecute dockerimage=alpine containerizer=mesos name=justatest command=""sleep 1000"" master=localhost:5050    # mesos containerizer with image library/alpine: success    sudo ./build/src/mesosexecute dockerimage=library/alpine containerizer=mesos name=justatest command=""sleep 1000"" master=localhost:5050      in the slave logs:      ea446083  9c838da86af34c0007'  i0306 16:32:41.418269  3403 metadatamanager.cpp:159] looking for image 'alpine:latest'  i0306 16:32:41.418699  3403 registrypuller.cpp:194] pulling image 'alpine:latest' from 'dockermanifest:/registry1.docker.io:443alpine?latest#https' to '/tmp/mesostest  /store/docker/staging/ka7mlq'  e0306 16:32:43.098131  3400 slave.cpp:3773] container '4bf9132d9a574baaa78ce7164e93ace6' for executor 'justatest' of framework 4f055c6f1bea4460839c838da86af34c0  007 failed to start: collect failed: unexpected http response '401 unauthorized      curl command executed:      $ sudo sysdig a p "" %evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl s s l d  https:/registry1.docker.io:443/v2/alpine/manifests/latest  16:42:53.784958541 curl s s l d  https:/auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull  16:42:54.294192024 curl s s l d  h authorization: bearer eyjhbgcioijfuzi1niisinr5cci6ikpxvcising1yyi6wyjnsuldthpdq0fku2dbd0lcqwdjqkfequtcz2dxagtqt1bruurbakjhtvvrd1fnwurwuvferxp0uk5gb3ppa2rytjbrnldgulfsrhbjvfrsuk9rovvwrmc2tmtgrlf6cfnuve5et2tgu01rttzumfkztnpwq1zrvkjpa2xhulvrnlexazftekflrncwee5uqtjnalv4t1rvmu5ewmfgdzb4tmpbmk1quxhpvfuxtkrayu1fwxhsrejdqmdovkjbtvrpmghhu1uwnldgzfzwam8yuvzksu9swlpuvek2ttfnmvrecfnwrekxt2s5vfnrbzztmvexumpwwvrssklpbfjmtmtnnlmxukxoanbcuvv0vu1ga3dfd1lis29asxpqmenbuvljs29asxpqmerbuwneuwdbrxl2uzivdei3t3jlmkvxcgrdefdts1nqv1n2vmj2twurwgvftunvmdbyqji0akniuvhrefdmoss0muxqmlznq29bk0rmrkiwvjbgzgdwajlowu5rl2pxt0jzaknccnpbt0jntlziuthcqwy4rujbtunbsuf3rhdzrfzsmgxcqwd3qmdzrvzsmgxbrejfqmdovkhrnevquve3u0vaslrucflwmvzxt2paqlywzzzwbgxotwpveldevk1pbepvtwpvnlqxtkttanbmvkrwr09sae9va2c2vkvzmlnecexwrxmyt2tgqlmxuxdsz1levliwakjeohdqwue3vvrsyu16cehwemrkt2xovvvfutztrtawvvrwufzgullpalpculvnnlvrmhprenbcvwpkre9roudoemm2uwxarlfucepsa1zkt2towk5vc3ddz1ljs29asxpqmevbd0leu1fbd1jnswhbtxzit2h4chhrtktqsdrhmfbns0lfdxrmtjztrdfvmws4zejovgxuwvfudkfpruf0yvjgsgjsr2o4zlvsszz4uvjhrurvqm1zz3dzelr3z3bmagjbzznoumfvpsjdfq.eyjhy2nlc3mioltdlcjhdwqioijyzwdpc3ryes5kb2nrzxiuaw8ilcjlehaioje0ntcyodi4nzqsimlhdci6mtq1nzi4mju3ncwiaxnzijoiyxv0ac5kb2nrzxiuaw8ilcjqdgkioijaogtynxzxnejmwknirs1icvjiacisim5izii6mtq1nzi4mju3ncwic3viijoiin0.c2wtjqpm0buparhmqjdfh6ztiahcvgn3tfwizeclsgxlvqsaqxaalnzkwaql2chj7nphx0gwael28aw https:/registry1.docker.io:443/v2/alpine/manifests/latest      also got the same result with ubuntu docker image.",3,val
MESOS-4879,Update glog patch to support PowerPC LE,this is a part of powerpc le porting,1,val
MESOS-4881,Rescind all outstanding offers after changing some weights.,nan,2,val
MESOS-4882,Add support for command and arguments to mesos-execute.,"commandinfo protobuf support two kinds of command:    / there are two ways to specify the command:    / 1) if 'shell == true', the command will be launched via shell    /  (i.e., /bin/sh c 'value'). the 'value' specified will be    /  treated as the shell command. the 'arguments' will be ignored.    / 2) if 'shell == false', the command will be launched by passing    /  arguments to an executable. the 'value' specified will be    /  treated as the filename of the executable. the 'arguments'    /  will be treated as the arguments to the executable. this is    /  similar to how posix exec families launch processes (i.e.,    /  execlp(value, arguments(0), arguments(1), ...)).      the mesosexecute cannot handle 2) now, enabling 2) can help with testing and running one off tasks.",5,val
MESOS-4886,Support mesos containerizer force_pull_image option.,"currently for unified containerizer, images that are already cached by metadata manager cannot be updated. user has to delete corresponding images in store if an update is need. we should support `forcepullimage` option for unified containerizer, to provide override option if existed.",3,val
MESOS-4888,Default cmd is executed as an incorrect command.,"when mesos containerizer launch a container using a docker image, which only container default cmd. the executable command is is a incorrect sequence. for example:    if an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [c, echo 'hello world']. the executable command is `[sh, c, echo 'hello world', sh]`, which is incorrect. it should be `[sh, sh,  c, echo 'hello world']` instead.    this problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",2,val
MESOS-4889,Implement runtime isolator tests.,"there different cases in docker runtime isolator. some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",5,val
MESOS-4891,Add a '/containers' endpoint to the agent to list all the active containers.,"this endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see containerstatus in mesos.proto). we'll eventually deprecate the /monitor/statistics.json endpoint.",8,val
MESOS-4902,Add authentication to libprocess endpoints,"in addition to the endpoints addressed by mesos4850 and mesos5152, the following endpoints would also benefit from http authentication:   /profiler/   /logging/toggle   /metrics/snapshot    adding http authentication to these endpoints is a bit more complicated because they are defined at the libprocess level.    while working on mesos4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that http authentication can be independently enabled/disabled for each.    we should establish a mechanism for making an endpoint authenticated that allows us to:  1) install an endpoint like /files, whose code is shared by the master and agent, with different authentication realms for the master and agent  2) avoid hardcoding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application level concerns from leaking into libprocess    another option would be to use a single default authentication realm and always enable or disable http authentication for both the master and agent in tests. however, this wouldn't allow us to test scenarios where http authentication is enabled on one but disabled on the other.",5,val
MESOS-4903,Allow multiple loads of module manifests,"the modulemanager::load() is designed to be called exactly once during a process lifetime. this works well for master/agent environments. however, it can fail in scheduler environments. for example, a single scheduler binary might implement multiple scheduler drivers causing multiple calls to modulemanager::load() leading to a failure.",3,val
MESOS-4908,Tasks cannot be killed forcefully.,"currently there is no way for a scheduler to instruct the executor to kill a certain task immediately, skipping any possible timeouts and / or kill policies. this may be desirable in cases like, e.g., the kill policy is 10 minutes but something went wrong, so the scheduler decides to issue a forceful kill.",5,val
MESOS-4909,Introduce kill policy for tasks.,a task may require some time to clean up or even a special mechanism to issue a kill request (currently it's a sigterm followed by sigkill). introducing kill policies per task will help address these issue.,5,val
MESOS-4910,Deprecate the --docker_stop_timeout agent flag.,"instead, a combination of executorshutdowngrace_period  agent flag and optionally task kill policies should be used.",1,val
MESOS-4911,Executor driver does not respect executor shutdown grace period.,"executor shutdown grace period, configured on the agent, is  propagated to executors via the `mesosexecutorshutdowngraceperiod`  environment variable. the executor driver must use this timeout to delay  the hard shutdown of the related executor.",1,val
MESOS-4912,LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.,observed on our ci:    [09:34:15] :  [step 11/11] [ run      ] linuxfilesystemisolatortest.rootmultiplecontainers  [09:34:19]w:  [step 11/11] i0309 09:34:19.906719  2357 linux.cpp:81] making '/tmp/mlvlnv' a shared mount  [09:34:19]w:  [step 11/11] i0309 09:34:19.923548  2357 linuxlauncher.cpp:101] using /sys/fs/cgroup/freezer as the freezer hierarchy for the linux launcher  [09:34:19]w:  [step 11/11] i0309 09:34:19.924705  2376 containerizer.cpp:666] starting container 'da610f7fa7094de894d374f4a520619b' for executor 'testexecutor1' of framework ''  [09:34:19]w:  [step 11/11] i0309 09:34:19.925355  2371 provisioner.cpp:285] provisioning image rootfs '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0' for container da610f7fa7094de894d374f4a520619b  [09:34:19]w:  [step 11/11] i0309 09:34:19.925881  2377 copy.cpp:127] copying layer path '/tmp/mlvlnv/testimage1' to rootfs '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0'  [09:34:30]w:  [step 11/11] i0309 09:34:30.835127  2376 linux.cpp:355] bind mounting work directory from '/tmp/mlvlnv/slaves/testslave/frameworks/executors/testexecutor1/runs/da610f7fa7094de894d374f4a520619b' to '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0/mnt/mesos/sandbox' for container da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.835392  2376 linux.cpp:683] changing the ownership of the persistent volume at '/tmp/mlvlnv/volumes/roles/testrole/persistentvolumeid' with uid 0 and gid 0  [09:34:30]w:  [step 11/11] i0309 09:34:30.840425  2376 linux.cpp:723] mounting '/tmp/mlvlnv/volumes/roles/testrole/persistentvolumeid' to '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(testrole)[persistentvolumeid:volume]:32 of container da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.843878  2374 linuxlauncher.cpp:304] cloning child process with flags = clonenewns  [09:34:30]w:  [step 11/11] i0309 09:34:30.848302  2371 containerizer.cpp:666] starting container 'fe4729c51e634cc6a2e3fe5006ffe087' for executor 'testexecutor2' of framework ''  [09:34:30]w:  [step 11/11] i0309 09:34:30.848758  2371 containerizer.cpp:1392] destroying container 'da610f7fa7094de894d374f4a520619b'  [09:34:30]w:  [step 11/11] i0309 09:34:30.848865  2373 provisioner.cpp:285] provisioning image rootfs '/tmp/mlvlnv/provisioner/containers/fe4729c51e634cc6a2e3fe5006ffe087/backends/copy/rootfses/518b246443dd47b09648e78aedde6917' for container fe4729c51e634cc6a2e3fe5006ffe087  [09:34:30]w:  [step 11/11] i0309 09:34:30.849449  2375 copy.cpp:127] copying layer path '/tmp/mlvlnv/testimage2' to rootfs '/tmp/mlvlnv/provisioner/containers/fe4729c51e634cc6a2e3fe5006ffe087/backends/copy/rootfses/518b246443dd47b09648e78aedde6917'  [09:34:30]w:  [step 11/11] i0309 09:34:30.854038  2374 cgroups.cpp:2427] freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.856693  2372 cgroups.cpp:1409] successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7fa7094de894d374f4a520619b after 2.608128ms  [09:34:30]w:  [step 11/11] i0309 09:34:30.859237  2377 cgroups.cpp:2445] thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.861454  2377 cgroups.cpp:1438] successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7fa7094de894d374f4a520619b after 2176us  [09:34:30]w:  [step 11/11] i0309 09:34:30.934608  2378 containerizer.cpp:1608] executor for container 'da610f7fa7094de894d374f4a520619b' has exited  [09:34:30]w:  [step 11/11] i0309 09:34:30.937692  2372 linux.cpp:798] unmounting volume '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.937742  2372 linux.cpp:817] unmounting sandbox/work directory '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0/mnt/mesos/sandbox' for container da610f7fa7094de894d374f4a520619b  [09:34:30]w:  [step 11/11] i0309 09:34:30.938129  2375 provisioner.cpp:330] destroying container rootfs at '/tmp/mlvlnv/provisioner/containers/da610f7fa7094de894d374f4a520619b/backends/copy/rootfses/0d7e047a50f1490bbb5800e9c49628d0' for container da610f7fa7094de894d374f4a520619b  [09:34:45] :  [step 11/11] ../../src/tests/containerizer/filesystemisolatortests.cpp:1318: failure  [09:34:45] :  [step 11/11] failed to wait 15secs for wait1  [09:34:48] :  [step 11/11] [  failed  ] linuxfilesystemisolatortest.rootmultiplecontainers (32341 ms)  ,3,val
MESOS-4914,"ProcessorManager delegate should be an Option<string>, not just a string.","currently, the delegate field in the processmanager is just a string type. we check for 'existence' of a delegate by comparing (delegate != """"). using an option is the preferred method for things like this.",1,val
MESOS-4916,Allow modules to express if they are multi-instantiable and thread safe.,"a module might be instantiated multiple time (e.g., multiple schedulers in the same java process instantiating an authenticator module) within the same process. the current mechanism doesn't provide a way through the module api to forbid multiple instantiations. it is up to the module to check and return error on prior instantiation.    along similar lines, a module should be able to express thread safety concerns. typically, a module running in master/agent doesn't have to be concerned about thread safety if it uses libprocess api. however, we should investigate how it plays in the scheduler environment.",8,val
MESOS-4917,Replace non-pod static variables in module/manager.[ch]pp with pod eqivalents.,nan,3,val
MESOS-4918,Cache module manifests while loading in ModuleManager.,"since the module managers are allowed to load the same module multiple times, we should be caching the module manifests to avoid cases where the module tries to trick the module manager by changing `modulebase` fields before the next call to `modulemanager::load`.",3,val
MESOS-4922,"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.","the network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned ip returned by cni plugin.  we should consider the following cases:  1) container is using host filesystem  2) container is using a different filesystem  3) custom executor and command executor",5,val
MESOS-4926,Add a list parser for comma separated integers in flags.,some flags require lists of integers to be passed in.  we should have an explicit parser for this instead of relying on ad hoc solutions.,2,val
MESOS-4927,"The flag parser for `hashmap<string, string>` should live in stout, not mesos.",the title says it all.,1,val
MESOS-4928,Remove all '.get().' calls on Option / Try variables in the resources abstraction.,"when possible, .get() calls should be replaced by  > for option / try variables.  this ticket only proposes a blanket change for this in the resource abstraction files, not the code base as a whole.  this is in preparation for introducing the new gpu resource.  without this change, i would need to use the old .get() calls.  instead, i propose to fix the old code surrounding it so that consistency has me doing it the right way.  ",1,val
MESOS-4932,Propose Design for Authorization based filtering for endpoints.,the design doc can be found here:  https:/docs.google.com/document/d/1m27s7otsfj8afzckloz00g_wcvrl32i9lyl6g22gwey,5,val
MESOS-4933,Registrar HTTP Authentication.,now that the master (and agents in progress) provide http authentication the registrar should do the same.     see http:/mesos.apache.org/documentation/latest/endpoints/registrar/registry/,3,val
MESOS-4934,Enable HELP to include authentication status of endpoint.,as we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones don't.,2,val
MESOS-4937,Investigate container security options for Mesos containerizer,we should investigate the following to improve the container security for mesos containerizer and come up with a list of features that we want to support in mvp.    1) capabilities  2) user namespace  3) seccomp  4) selinux  5) apparmor    we should investigate what other container systems are doing regarding security:  1)  https:/github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#l2905  2) https:/docs.docker.com/engine/security/security/  3) https:/github.com/opencontainers/specs/blob/master/config.md,5,val
MESOS-4938,Support docker registry authentication,nan,5,val
MESOS-4939,Support specifying per-container docker registry.,"currently, we only support a per agent flag to specify the docker registry. we should instead, allow people to specify the registry as part of the docker image name (like `docker pull` does).",3,val
MESOS-4941,Support update existing quota.,"we want to support updating an existing quota without the cycle of delete and recreate. this avoids the possible starvation risk of losing the quota between delete and recreate, and also makes the interface friendly.    design doc:  https:/docs.google.com/document/d/1c8fjy9n0w04ftuqb_kzm6s0eepu7eyvyfup14dsys",8,test
MESOS-4942,Docker runtime isolator tests may cause disk issue.,"currently slave working directory is used as docker store dir and archive dir, which is problematic. because slave work dir is exactly `environment >mkdtemp()`, it will get cleaned up until the end of the whole test. but the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. cleanup has to be done by each test tear down. ",2,test
MESOS-4943,Reduce the size of LinuxRootfs in tests.,"right now, linuxrootfs copies files from the host filesystem to construct a chroot able rootfs. we copy a lot of unnecessary files, making it very large. we can potentially strip a lot files.",13,test
MESOS-4944,Improve overlay backend so that it's writable,"currently, the overlay backend will provision a read only fs. we can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",5,test
MESOS-4949,Executor shutdown grace period should be configurable.,"currently, executor shutdown grace period is specified by an agent flag, which is propagated to executors via the mesosexecutorshutdowngraceperiod environment variable. there is no way to adjust this timeout for the needs of a particular executor.    to tackle this problem, we propose to introduce an optional shutdowngraceperiod field in executorinfo.",3,test
MESOS-4950,Implement reconnect funtionality in the scheduler library.,"currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library src/scheduler/scheduler.cpp. it is specifically useful in scenarios where there is a one way network partition with the master. due to this, the scheduler has not received any heartbeat events from the master. in this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the disconnected callback.",3,test
MESOS-4951,Enable actors to pass an authentication realm to libprocess,"to prepare for mesos4902, the mesos master and agent need a way to pass the desired authentication realm to libprocess. since some endpoints (like /profiler/ ) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocesslevel endpoints will be authenticated under.",2,test
MESOS-4956,Add authentication to /files endpoints,"to protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.    adding http authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.    while working on mesos4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that http authentication can be independently enabled/disabled for each.    we should establish a mechanism for making an endpoint authenticated that allows us to:  1) install an endpoint like /files, whose code is shared by the master and agent, with different authentication realms for the master and agent  2) avoid hardcoding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application level concerns from leaking into libprocess    another option would be to use a single default authentication realm and always enable or disable http authentication for both the master and agent in tests. however, this wouldn't allow us to test scenarios where http authentication is enabled on one but disabled on the other.",5,test
MESOS-4961,ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,"the logger subprocesses may exit before we reach the waitpid in the test.  if this happens, waitpid will return a 1 as the process no longer exists.    verbose logs:    [ run      ] containerloggertest.logrotaterotateinsandbox  i0316 14:28:51.329337  1242 cluster.cpp:139] creating default 'local' authorizer  i0316 14:28:51.332823  1242 leveldb.cpp:174] opened db in 3.079559ms  i0316 14:28:51.333916  1242 leveldb.cpp:181] compacted db in 1.054247ms  i0316 14:28:51.333979  1242 leveldb.cpp:196] created db iterator in 21450ns  i0316 14:28:51.334005  1242 leveldb.cpp:202] seeked to beginning of db in 2205ns  i0316 14:28:51.334025  1242 leveldb.cpp:271] iterated through 0 keys in the db in 410ns  i0316 14:28:51.334089  1242 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0316 14:28:51.334661  1275 recover.cpp:447] starting replica recovery  i0316 14:28:51.335044  1275 recover.cpp:473] replica is in empty status  i0316 14:28:51.336207  1262 replica.cpp:673] replica in empty status received a broadcasted recover request from (484)@172.17.0.3:45919  i0316 14:28:51.336730  1270 recover.cpp:193] received a recover response from a replica in empty status  i0316 14:28:51.337257  1275 recover.cpp:564] updating replica status to starting  i0316 14:28:51.338001  1267 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 537200ns  i0316 14:28:51.338032  1267 replica.cpp:320] persisted replica status to starting  i0316 14:28:51.338183  1261 master.cpp:376] master c7653f6033e944069f62dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919  i0316 14:28:51.338295  1263 recover.cpp:473] replica is in starting status  i0316 14:28:51.338213  1261 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/xtqwks/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.29.0/inst/share/mesos/webui"" workdir=""/tmp/xtqwks/master"" zksessiontimeout=""10secs""  i0316 14:28:51.338562  1261 master.cpp:423] master only allowing authenticated frameworks to register  i0316 14:28:51.338572  1261 master.cpp:428] master only allowing authenticated slaves to register  i0316 14:28:51.338580  1261 credentials.hpp:35] loading credentials for authentication from '/tmp/xtqwks/credentials'  i0316 14:28:51.338877  1261 master.cpp:468] using default 'crammd5' authenticator  i0316 14:28:51.339030  1262 replica.cpp:673] replica in starting status received a broadcasted recover request from (485)@172.17.0.3:45919  i0316 14:28:51.339246  1261 master.cpp:537] using default 'basic' http authenticator  i0316 14:28:51.339393  1261 master.cpp:571] authorization enabled  i0316 14:28:51.339390  1266 recover.cpp:193] received a recover response from a replica in starting status  i0316 14:28:51.339606  1271 whitelistwatcher.cpp:77] no whitelist given  i0316 14:28:51.339607  1275 hierarchical.cpp:144] initialized hierarchical allocator process  i0316 14:28:51.340077  1268 recover.cpp:564] updating replica status to voting  i0316 14:28:51.340533  1270 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 331558ns  i0316 14:28:51.340558  1270 replica.cpp:320] persisted replica status to voting  i0316 14:28:51.340672  1270 recover.cpp:578] successfully joined the paxos group  i0316 14:28:51.340827  1270 recover.cpp:462] recover process terminated  i0316 14:28:51.341684  1270 master.cpp:1806] the newly elected leader is master@172.17.0.3:45919 with id c7653f6033e944069f62dc74c906bf83  i0316 14:28:51.341717  1270 master.cpp:1819] elected as the leading master!  i0316 14:28:51.341740  1270 master.cpp:1508] recovering from registrar  i0316 14:28:51.341954  1263 registrar.cpp:307] recovering registrar  i0316 14:28:51.342499  1273 log.cpp:659] attempting to start the writer  i0316 14:28:51.343616  1266 replica.cpp:493] replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1  i0316 14:28:51.344183  1266 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 536941ns  i0316 14:28:51.344208  1266 replica.cpp:342] persisted promised to 1  i0316 14:28:51.344825  1267 coordinator.cpp:238] coordinator attempting to fill missing positions  i0316 14:28:51.346009  1276 replica.cpp:388] replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2  i0316 14:28:51.346371  1276 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 327890ns  i0316 14:28:51.346393  1276 replica.cpp:712] persisted action at 0  i0316 14:28:51.347363  1267 replica.cpp:537] replica received write request for position 0 from (489)@172.17.0.3:45919  i0316 14:28:51.347414  1267 leveldb.cpp:436] reading position from leveldb took 24861ns  i0316 14:28:51.347774  1267 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 323654ns  i0316 14:28:51.347796  1267 replica.cpp:712] persisted action at 0  i0316 14:28:51.348323  1276 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0316 14:28:51.348714  1276 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 361981ns  i0316 14:28:51.348738  1276 replica.cpp:712] persisted action at 0  i0316 14:28:51.348760  1276 replica.cpp:697] replica learned nop action at position 0  i0316 14:28:51.349318  1274 log.cpp:675] writer started with ending position 0  i0316 14:28:51.350275  1267 leveldb.cpp:436] reading position from leveldb took 23849ns  i0316 14:28:51.351171  1271 registrar.cpp:340] successfully fetched the registry (0b) in 9.173248ms  i0316 14:28:51.351300  1271 registrar.cpp:439] applied 1 operations in 32119ns; attempting to update the 'registry'  i0316 14:28:51.351989  1272 log.cpp:683] attempting to append 170 bytes to the log  i0316 14:28:51.352108  1266 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0316 14:28:51.352802  1263 replica.cpp:537] replica received write request for position 1 from (490)@172.17.0.3:45919  i0316 14:28:51.353313  1263 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 474854ns  i0316 14:28:51.353338  1263 replica.cpp:712] persisted action at 1  i0316 14:28:51.354101  1273 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0316 14:28:51.354483  1273 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 338210ns  i0316 14:28:51.354507  1273 replica.cpp:712] persisted action at 1  i0316 14:28:51.354529  1273 replica.cpp:697] replica learned append action at position 1  i0316 14:28:51.355444  1275 registrar.cpp:484] successfully updated the 'registry' in 4.084224ms  i0316 14:28:51.355569  1275 registrar.cpp:370] successfully recovered registrar  i0316 14:28:51.355697  1268 log.cpp:702] attempting to truncate the log to 1  i0316 14:28:51.355870  1269 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0316 14:28:51.356016  1274 master.cpp:1616] recovered 0 slaves from the registry (131b) ; allowing 10mins for slaves to reregister  i0316 14:28:51.356032  1272 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0316 14:28:51.356761  1273 replica.cpp:537] replica received write request for position 2 from (491)@172.17.0.3:45919  i0316 14:28:51.357203  1273 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 406053ns  i0316 14:28:51.357226  1273 replica.cpp:712] persisted action at 2  i0316 14:28:51.357718  1270 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0316 14:28:51.358093  1270 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 345370ns  i0316 14:28:51.358175  1270 leveldb.cpp:399] deleting 1 keys from leveldb took 57us  i0316 14:28:51.358201  1270 replica.cpp:712] persisted action at 2  i0316 14:28:51.358220  1270 replica.cpp:697] replica learned truncate action at position 2  i0316 14:28:51.368399  1242 containerizer.cpp:149] using isolation: posix/cpu,posix/mem,filesystem/posix  w0316 14:28:51.406371  1242 backend.cpp:66] failed to create 'bind' backend: bindbackend requires root privileges  i0316 14:28:51.410480  1266 slave.cpp:193] slave started on 12)@172.17.0.3:45919  i0316 14:28:51.410518  1266 slave.cpp:194] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerlogger=""orgapachemesoslogrotatecontainerlogger"" containerizers=""mesos"" credential=""/tmp/containerloggertestlogrotaterotateinsandboxjhp0gy/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/containerloggertestlogrotaterotateinsandboxjhp0gy/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.29.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/containerloggertestlogrotaterotateinsandboxjhp0gy""  i0316 14:28:51.411118  1266 credentials.hpp:83] loading credential for authentication from '/tmp/containerloggertestlogrotaterotateinsandboxjhp0gy/credential'  i0316 14:28:51.411381  1266 slave.cpp:324] slave using credential for: testprincipal  i0316 14:28:51.411696  1266 resources.cpp:572] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ports:[3100032000]  trying semicolondelimited string format instead  i0316 14:28:51.412075  1266 slave.cpp:464] slave resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0316 14:28:51.412148  1266 slave.cpp:472] slave attributes: [  ]  i0316 14:28:51.412160  1266 slave.cpp:477] slave hostname: 2cbb23302fe5  i0316 14:28:51.413516  1263 state.cpp:58] recovering state from '/tmp/containerloggertestlogrotaterotateinsandboxjhp0gy/meta'  i0316 14:28:51.413774  1266 statusupdatemanager.cpp:200] recovering status update manager  i0316 14:28:51.414029  1276 containerizer.cpp:407] recovering containerizer  i0316 14:28:51.415222  1269 provisioner.cpp:245] provisioner recovery complete  i0316 14:28:51.415650  1268 slave.cpp:4565] finished recovery  i0316 14:28:51.416115  1268 slave.cpp:4737] querying resource estimator for oversubscribable resources  i0316 14:28:51.416365  1268 slave.cpp:796] new master detected at master@172.17.0.3:45919  i0316 14:28:51.416448  1276 statusupdatemanager.cpp:174] pausing sending status updates  i0316 14:28:51.416445  1268 slave.cpp:859] authenticating with master master@172.17.0.3:45919  i0316 14:28:51.416522  1268 slave.cpp:864] using default crammd5 authenticatee  i0316 14:28:51.416671  1268 slave.cpp:832] detecting new master  i0316 14:28:51.416731  1275 authenticatee.cpp:121] creating new client sasl connection  i0316 14:28:51.416807  1268 slave.cpp:4751] received oversubscribable resources  from the resource estimator  i0316 14:28:51.417006  1263 master.cpp:5659] authenticating slave(12)@172.17.0.3:45919  i0316 14:28:51.417103  1262 authenticator.cpp:413] starting authentication session for crammd5authenticatee(38)@172.17.0.3:45919  i0316 14:28:51.417348  1273 authenticator.cpp:98] creating new server sasl connection  i0316 14:28:51.417548  1266 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0316 14:28:51.417582  1266 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0316 14:28:51.417696  1264 authenticator.cpp:203] received sasl authentication start  i0316 14:28:51.417753  1264 authenticator.cpp:325] authentication requires more steps  i0316 14:28:51.417948  1265 authenticatee.cpp:258] received sasl authentication step  i0316 14:28:51.418107  1267 authenticator.cpp:231] received sasl authentication step  i0316 14:28:51.418159  1267 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '2cbb23302fe5' server fqdn: '2cbb23302fe5' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0316 14:28:51.418180  1267 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0316 14:28:51.418233  1267 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0316 14:28:51.418270  1267 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '2cbb23302fe5' server fqdn: '2cbb23302fe5' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0316 14:28:51.418289  1267 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0316 14:28:51.418300  1267 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0316 14:28:51.418323  1267 authenticator.cpp:317] authentication success  i0316 14:28:51.418414  1264 authenticatee.cpp:298] authentication success  i0316 14:28:51.418473  1269 master.cpp:5689] successfully authenticated principal 'testprincipal' at slave(12)@172.17.0.3:45919  i0316 14:28:51.418514  1275 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(38)@172.17.0.3:45919  i0316 14:28:51.418781  1276 slave.cpp:927] successfully authenticated with master master@172.17.0.3:45919  i0316 14:28:51.418937  1276 slave.cpp:1321] will retry registration in 1.983001ms if necessary  i0316 14:28:51.419108  1262 master.cpp:4370] registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f6033e944069f62dc74c906bf83s0  i0316 14:28:51.419643  1266 registrar.cpp:439] applied 1 operations in 75642ns; attempting to update the 'registry'  i0316 14:28:51.420670  1272 log.cpp:683] attempting to append 339 bytes to the log  i0316 14:28:51.420820  1269 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0316 14:28:51.421495  1270 slave.cpp:1321] will retry registration in 1.437257ms if necessary  i0316 14:28:51.421716  1275 master.cpp:4358] ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress  i0316 14:28:51.422107  1267 replica.cpp:537] replica received write request for position 3 from (505)@172.17.0.3:45919  i0316 14:28:51.423033  1267 leveldb.cpp:341] persisting action (358 bytes) to leveldb took 762815ns  i0316 14:28:51.423066  1267 replica.cpp:712] persisted action at 3  i0316 14:28:51.424069  1267 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0316 14:28:51.424232  1264 slave.cpp:1321] will retry registration in 66.01292ms if necessary  i0316 14:28:51.424342  1269 master.cpp:4358] ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress  i0316 14:28:51.424686  1267 leveldb.cpp:341] persisting action (360 bytes) to leveldb took 574743ns  i0316 14:28:51.424757  1267 replica.cpp:712] persisted action at 3  i0316 14:28:51.424792  1267 replica.cpp:697] replica learned append action at position 3  i0316 14:28:51.426441  1272 registrar.cpp:484] successfully updated the 'registry' in 6.721024ms  i0316 14:28:51.426677  1262 log.cpp:702] attempting to truncate the log to 3  i0316 14:28:51.426808  1264 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0316 14:28:51.427584  1261 slave.cpp:3482] received ping from slaveobserver(11)@172.17.0.3:45919  i0316 14:28:51.428213  1262 hierarchical.cpp:473] added slave c7653f6033e944069f62dc74c906bf83s0 (2cbb23302fe5) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0316 14:28:51.427865  1266 master.cpp:4438] registered slave c7653f6033e944069f62dc74c906bf83s0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0316 14:28:51.428270  1267 slave.cpp:971] registered with master master@172.17.0.3:45919; given slave id c7653f6033e944069f62dc74c906bf83s0  i0316 14:28:51.428412  1265 replica.cpp:537] replica received write request for position 4 from (506)@172.17.0.3:45919  i0316 14:28:51.428443  1267 fetcher.cpp:81] clearing fetcher cache  i0316 14:28:51.428503  1262 hierarchical.cpp:1453] no resources available to allocate!  i0316 14:28:51.428535  1262 hierarchical.cpp:1150] performed allocation for slave c7653f6033e944069f62dc74c906bf83s0 in 205421ns  i0316 14:28:51.428750  1273 statusupdatemanager.cpp:181] resuming sending status updates  i0316 14:28:51.429157  1265 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 695258ns  i0316 14:28:51.429225  1267 slave.cpp:994] checkpointing slaveinfo to '/tmp/containerloggertestlogrotaterotateinsandbox_jhp0gy/meta/slaves/c7653f6033e944069f62dc74c906bf83s0/slave.info'  i0316 14:28:51.429275  1265 replica.cpp:712] persisted action at 4  i0316 14:28:51.429759  1267 slave.cpp:1030] forwarding total oversubscribed resources   i0316 14:28:51.430055  1265 master.cpp:4782] received update of slave c7653f6033e944069f62dc74c906bf83s0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources   i0316 14:28:51.430614  1271 replica.cpp:691] replica received learned notice for position 4 from @0.0.0.0:0  i0316 14:28:51.430891  1242 sched.cpp:222] version: 0.29.0  i0316 14:28:51.431043  1265 hierarchical.cpp:531] slave c7653f6033e944069f62dc74c906bf83s0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports( ):[3100032000], allocated: )  i0316 14:28:51.431236  1271 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 536892ns  i0316 14:28:51.431267  1265 hierarchical.cpp:1453] no resources available to allocate!  i0316 14:28:51.431584  1271 leveldb.cpp:399] deleting 2 keys from leveldb took 66904ns  i0316 14:28:51.431538  1273 sched.cpp:326] new master detected at master@172.17.0.3:45919  i0316 14:28:51.431622  1271 replica.cpp:712] persisted action at 4  i0316 14:28:51.431623  1265 hierarchical.cpp:1150] performed allocation for slave c7653f6033e944069f62dc74c906bf83s0 in 518588ns  i0316 14:28:51.431660  1271 replica.cpp:697] replica learned truncate action at position 4  i0316 14:28:51.431711  1273 sched.cpp:382]...",1,test
MESOS-4962,Support for Mesos releases,"as part of mesos reaching 1.0, we need to formalize the policy of supporting mesos releases.    some specific questions we need to answer:    > what fixes should we backports to older releases.    > how many old releases are supported.    > should we have a lts version?    > what is the cadence of major, minor and patch releases?",8,test
MESOS-4970,Add more examples of JSON resources to docs,"the configuration documentation currently only shows examples of scalar resource types in json format. the structures of json resources are a bit complicated, so it would be very helpful to include examples of ranges, sets, and text resource types as well.",1,test
MESOS-4978,Update mesos-execute with Appc changes.,mesos execute cli application currently does not have support for appc images. adding support would make integration tests easier.,3,test
MESOS-4982,Update example long running to use v1 API.,we need to modify the long running test framework similar to src/examples/longlivedframework.cpp to use the v1 api.    this would allow us to vet the v1 api and the scheduler library in test clusters.,5,test
MESOS-4984,MasterTest.SlavesEndpointTwoSlaves is flaky,"observed on arch linux with gcc 6, running in a virtualbox vm:    [ run      ] mastertest.slavesendpointtwoslaves  /mesos2/src/tests/master_tests.cpp:1710: failure  value of: array.get().values.size()    actual: 1  expected: 2u  which is: 2  [  failed  ] mastertest.slavesendpointtwoslaves (86 ms)    seems to fail nondeterministically, perhaps more often when there is concurrent cpu load on the machine.",2,test
MESOS-4985,Destroy a container while it's provisioning can lead to leaked provisioned directories.,"here is the possible sequence of events:  1) containerizer>launch  2) provisioner>provision is called. it is fetching the image  3) executor registration timed out  4) containerizer>destroy is called  5) container>state is still in preparing  6) provisioner>destroy is called    so we can be calling provisioner>destory while provisioner>provision hasn't finished yet. provisioner>destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. this root filesystem will not be destroyed as destroy already finishes.",3,test
MESOS-4992,sandbox uri does not work outisde mesos http server,"the sandbox uri of a framework does not work if i just copy paste it to the browser.    for example the following sandbox uri:  http:/172.17.0.1:5050/#/slaves/50f87c7379ef4f2a95f0b2b4062b2de6s0/frameworks/50f87c7379ef4f2a95f0b2b4062b2de60009/executors/driver201603211550160001/browse    should redirect to:  http:/172.17.0.1:5050/#/slaves/50f87c7379ef4f2a95f0b2b4062b2de6s0/browse?path=%2ftmp%2fmesos%2fslaves%2f50f87c7379ef4f2a95f0b2b4062b2de6s0%2fframeworks%2f50f87c7379ef4f2a95f0b2b4062b2de60009%2fexecutors%2fdriver201603211550160001%2fruns%2f6053348331fb4353987df3393911cc80    yet it fails with the message:  ""failed to find slaves.  navigate to the slave's sandbox via the mesos ui.""  and redirects to:  http:/172.17.0.1:5050/#/    it is an issue for me because im working on expanding the mesos spark ui with sandbox uri, the other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though.    moreover i dont see the runs/container_id in the mesos proto api. i guess this is hidden info, this is the needed piece of info to rewrite the uri without redirection.  ",3,test
MESOS-4998,Problematic fork/clone performance at high load.,"creating a new subprocess in mesos involves forking/cloning a new process. in most cases (executors, perf, ..) the parent of the new process is the agent/slave process. this can lead to problematic behavior especially when creating several new processes at the same time.    the problem here is that the normal fork() (or clone syscall used by libprocess) provides a copyonwrite (cow) view of the parents address space until the child execs its new binary. note that during the time between fork and exec mesos does several setup actions such as placing the new processes in systemd units or assigning them to the freezer cgroup.  this cow property of the address space implies that existing memory is marked as readonly and any write will trigger a pagefault and a newly created page. note this behavior also extends to the parent process and hence any write will be very costly.    we simulated the number of pagefaults when forking/cloning new processes by this benchmark:  https:/github.com/joerg84/forking benchmark    results can be seen here:   https:/docs.google.com/presentation/d/1sujkavhdrutlppfjy3q1yhing5fomw3hbbedzuhz7a8",8,test
MESOS-5004,Clarify docs on '/reserve' and '/create-volumes' without authentication,"for both reservations and persistent volume creation, the behavior of the http endpoints differs slightly from that of the framework operations. due to the implementation of http authentication, it is not possible for a framework/operator to provide a principal when http authentication is disabled. this means that when http authentication is disabled, the endpoint handlers will always receive none() as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the none principal is authorized to do stuff.    the docs should be updated to explain this behavior explicitly.",1,test
MESOS-5005,Enforce that DiskInfo principal is equal to framework/operator principal,"currently, we require that reservationinfo.principal be equal to the principal provided for authentication, which means that when http authentication is disabled this field cannot be set. based on comments in 'mesos.proto', the original intention was to enforce this same constraint for persistence.principal, but it seems that we don't enforce it. this should be changed to make the two fields equivalent, with one exception: when the framework/operator principal is none, we should allow the principal in diskinfo to take any value, along the same lines as mesos 5212.",3,test
MESOS-5006,Add example for mesos-execute usage of Appc images in container-image.md.,example usage for appc flags and images needs to be added to container image.md.,3,test
MESOS-5010,Installation of mesos python package is incomplete,"the installation of mesos python package is incomplete, i.e., the files cli.py, futures.py, and http.py are not installed.      % ../configure enablepython  % make install destdir=$pwd/d  % pythonpath=$pwd/d/usr/local/lib/python2.7/sitepackages:$pythonpath python c 'from mesos import http'    traceback (most recent call last):    file ""/"", line 1, in /  importerror: cannot import name http      this appears to be first broken with d1d70b9 (mesos3969, https:/reviews.apache.org/r/40630). bisecting in pipland shows that our install becomes broken for pip6.0.1 and later (we are using pip 7.1.2).  ",2,test
MESOS-5013,Add docker volume driver isolator for Mesos containerizer.,the isolator will interact with docker volume driver plugins to mount and unmount external volumes to container.  ,8,test
MESOS-5014,Call and Event Type enums in scheduler.proto should be optional,having a 'required' type enum has backwards compatibility issues when adding new enum types. see mesos 4997 for details.,2,test
MESOS-5015,Call and Event Type enums in executor.proto should be optional,having a 'required' type enum has backwards compatibility issues when adding new enum types. see mesos 4997 for details.,2,test
MESOS-5016,Add a reconnect() method to the C++ scheduler library,a reconnect() method on the library would allow the scheduler to force a reconnection (disconnect and reconnect) by the library. this might be used by the scheduler to react to lack of heartbeats.,3,test
MESOS-5023,MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.,observed on the apache jenkins.      [ run      ] mesoscontainerizerprovisionertest.provisionfailed  i0324 13:38:56.284261  2948 containerizer.cpp:666] starting container 'testcontainer' for executor 'executor' of framework ''  i0324 13:38:56.285825  2939 containerizer.cpp:1421] destroying container 'testcontainer'  i0324 13:38:56.285854  2939 containerizer.cpp:1424] waiting for the provisioner to complete for container 'testcontainer'  [       ok ] mesoscontainerizerprovisionertest.provisionfailed (7 ms)  [ run      ] mesoscontainerizerprovisionertest.destroywhileprovisioning  i0324 13:38:56.291187  2944 containerizer.cpp:666] starting container 'c2316963c6cb4c7fa3b917ca5931e5b2' for executor 'executor' of framework ''  i0324 13:38:56.292157  2944 containerizer.cpp:1421] destroying container 'c2316963c6cb4c7fa3b917ca5931e5b2'  i0324 13:38:56.292179  2944 containerizer.cpp:1424] waiting for the provisioner to complete for container 'c2316963c6cb4c7fa3b917ca5931e5b2'  f0324 13:38:56.292899  2944 containerizer.cpp:752] check failed: containers.contains(containerid)   check failure stack trace:       @     0x2ac9973d0ae4  google::logmessage::fail()      @     0x2ac9973d0a30  google::logmessage::sendtolog()      @     0x2ac9973d0432  google::logmessage::flush()      @     0x2ac9973d3346  google::logmessagefatal::~logmessagefatal()      @     0x2ac996af897c  mesos::internal::slave::mesoscontainerizerprocess::launch()      @     0x2ac996b1f18a  zzn7process8dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns111containeriderk6optionins18taskinfoeerkns112executorinfoerkssrks8isserkns17slaveiderkns3pidins35slaveeeebrks8ins313provisioninfoees5sasdsssislsqbsueens6futureiteerknsoit0eems10fszt1t2t3t4t5t6t7t8t9et10t11t12t13t14t15t16t17t18enkulpns11processbaseeecles1p      @     0x2ac996b479d9  znst17functionhandlerifvpn7process11processbaseeezns08dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns511containeriderk6optionins58taskinfoeerkns512executorinfoerkssrkscisserkns57slaveiderkns03pidins75slaveeeebrkscins713provisioninfoees9seshsssmspsubsyeens06futureiteerknssit0eems14fs13t1t2t3t4t5t6t7t8t9et10t11t12t13t14t15t16t17t18euls2ee9minvokeerkst9anydatas2      @     0x2ac997334fef  std::function/::operator()()      @     0x2ac99731b1c7  process::processbase::visit()      @     0x2ac997321154  process::dispatchevent::visit()      @           0x9a699c  process::processbase::serve()      @     0x2ac9973173c0  process::processmanager::resume()      @     0x2ac99731445a  zzn7process14processmanager12initthreadsevenkulrkst11atomicboolecles3      @     0x2ac997320916  znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eee6callivieilm0eeeetost5tupleiidpt0eest12indextupleiixspt1eee      @     0x2ac9973208c6  znst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis3eeecliieveet0dpot      @     0x2ac997320858  znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeevee9minvokeiieeevst12indextupleiixspteee      @     0x2ac9973207af  znst12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis4eeeveeclev      @     0x2ac997320748  znst6thread5implist12bindsimpleifst5bindifzn7process14processmanager12initthreadseveulrkst11atomicboolest17referencewrapperis6eeeveee6mrunev      @     0x2ac9989aea60  (unknown)      @     0x2ac999125182  startthread      @     0x2ac99943547d  (unknown)  make[4]: leaving directory `/mesos/mesos0.29.0/build/src'  make[4]:  [checklocal] aborted  make[3]:  [checkam] error 2  make[3]: leaving directory `/mesos/mesos0.29.0/build/src'  make[2]:  [check] error 2  make[2]: leaving directory `/mesos/mesos0.29.0/build/src'  make[1]:  [checkrecursive] error 1  make[1]: leaving directory `/mesos/mesos 0.29.0/build'  make:   [distcheck] error 1  build step 'execute shell' marked build as failure  ,2,test
MESOS-5027,Enable authenticated login in the webui,"the webui hits a number of endpoints to get the data that it displays: /state, /metrics/snapshot, /files/browse, /files/read, and maybe others? once authentication is enabled on these endpoints, we need to add a login prompt to the webui so that users can provide credentials.",2,test
MESOS-5028,Copy provisioner cannot replace directory with symlink,"i'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.    error log with glogv=1:      i0324 05:42:48.926678 15067 copy.cpp:127] copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6cc9704539aa64fd0eef2ec7ae/backends/copy/rootfses/507173f3e31648a3a96e5fdea9ffe9f6'  e0324 05:42:49.028506 15062 slave.cpp:3773] container '5f05be6cc9704539aa64fd0eef2ec7ae' for executor 'test' of framework 75932a8915144011bafebeb6a208bb2d0004 failed to start: collect failed: collect failed: failed to copy layer: cp: cannot overwrite directory /var/lib/mesos/provisioner/containers/5f05be6cc9704539aa64fd0eef2ec7ae/backends/copy/rootfses/507173f3e31648a3a96e5fdea9ffe9f6/etc/apt with nondirectory      content of /tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a nonexisting absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).    i believe what happened is that we executed a script at build time, which contains equivalent of:    rm rf /etc/apt/  && ln sf /buildmount point/ /etc/apt    ",3,test
MESOS-5031,Authorization Action enum does not support upgrades.,"we need to make the action enum optional in authorization::request, and add an `unknown = 0;` enum value. see mesos 4997 for details.",2,test
MESOS-5032,Remove plain text Credential format (after deprecation cycle),"currently two formats of credentials are supported: json        ""credentials"": [            and a deprecated new line file:    principal1 secret1  pricipal2 secret2      we deprecated the new line format in 0.29, and should remove it after the deprecation cycle ends.",3,test
MESOS-5034,Design doc for ordered message delivery in libprocess,nan,3,test
MESOS-5044,Temporary directories created by environment->mkdtemp cleanup can be problematic.,"currently in mesos test, we have the temporary directories created by `environment>mkdtemp()` cleaned up until the end of the test suite, which can be problematic. for instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines.     we should have these temp dir created by `environment>mkdtemp` cleaned up during each test teardown. currently we only clean up the sandbox for each test.",1,test
MESOS-5049,Refactore subproces setup functions.,"executing arbitrary setup functions while creating new processes is  dangerous as all functions called have to be async safe. as setup  functions are used for only very few purposes (setsid, chdir, monitoring  and killing a process (see upcoming review) it makes sense to support  them safely via parameters to subprocess.   another common use of child setup are is to block the child while doing some work in the parent. this pattern can be more cleanly expressed with parenthooks. ",3,test
MESOS-5050,Design Linux capability support for Mesos containerizer,"we should at least support the following cases:  1) a root user has reduced capability  2) a non root user has the capability of capnetadmin (to do e.g., tcpdump)",5,test
MESOS-5051,Create helpers for manipulating Linux capabilities.,"these helpers can either based on some existing library (e.g. libcap), or use system calls directly.",5,test
MESOS-5054,Namespace the stout flags,a recent name collision occurred when updating the 3rdparty http parser library: https:/github.com/apache/mesos/commit/94df63f72146501872a06c6487e94bdfd0f23025    we should put stout's flags namespace within another suitable namespace (perhaps stout::flags) to avoid such collisions.,2,test
MESOS-5055,Slave/Agent Rename Phase I - Update strings in the log message and standard output,"this is a sub ticket of mesos 3780. in this ticket, we will rename all the slave to agent in the log messages and standard output.",2,test
MESOS-5056,Replace Master/Slave Terminology Phase I - Update strings in the shell scripts outputs,"this is a sub ticket of mesos 3780. in this ticket, we will rename slave to agent in the shell script outputs",1,test
MESOS-5057,Slave/Agent Rename Phase I - Update strings in error messages and other strings,"this is a sub ticket of mesos 3780. in this ticket, we will update all the slave to agent in the error messages and other strings in the code",3,test
MESOS-5062,Update the long-lived-framework example to run on test clusters,there are a couple of problems with the longlived framework that prevent it from being deployed (easily) on an actual cluster:     the framework will greedily accept all offers; it runs one executor per agent in the cluster.   the framework assumes the longlivedexecutor binary is available on each agent.  this is generally only true in the build environment or in singleagent test environments.   the framework does not specify an resources with the executor.  this is required by many isolators.   the framework has no metrics.,3,test
MESOS-5064,Remove default value for the agent `work_dir`,"following a crash report from the user we need to be more explicit about the dangers of using /tmp as agent workdir. in addition, we can remove the default value for the workdir flag, forcing users to explicitly set the work directory for the agent.",2,test
MESOS-5065,Support docker private registry default docker config.,"for docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is prehandled by operators. the default docker config file should be exposed by a new agent flag ` docker_config`. ",3,test
MESOS-5069,Upgrade http-parser to v2.6.2,nan,3,test
MESOS-5070,Introduce more flexible subprocess interface for child options.,"we introduced a number of parameters to the subprocess interface with mesos 5049.  adding all options explicitly to the subprocess interface makes it inflexible.   we should investigate a flexible options, which still prevents arbitrary code to be executed.",2,test
MESOS-5071,Refactor the clone option to subprocess.,the clone option in subprocess is only used (at least in the mesos codebase) to specify custom namespace flags to clone. it feels having the clone function in the subprocess interface is too explicit for this functionality.  ,2,test
MESOS-5078,Document TaskStatus reasons,we should document the possible reason values that can be found in the taskstatus message.,1,test
MESOS-5082,Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,"there appears to be a discrepancy between clang and gcc, which allows  clang to accept `using` declarations of the form `using nsname::name;`  that contain nested classes, structs, and enums after the `name` field  in the declaration (e.g. `using nsname::name::enum;`).    the language for describing this functionality is ambiguous in the  c11 specification as referenced here:  http:/en.cppreference.com/w/cpp/language/namespace#using declarations",1,test
MESOS-5101,Add CMake build to docker_build.sh,add the cmake build system to docker_build.sh to automatically test the build on jenkins alongside gcc and clang.,2,test
MESOS-5108,Design a short-term solution for a typed error handling mechanism.,nan,2,test
MESOS-5109,Capture the error code in `ErrnoError` and `WindowsError`.,"the errnoerror and windowserror classes simply construct the error string via a mechanism such as strerror. they should also capture the error code, as it is an essential piece of information for such an error type.",2,test
MESOS-5110,Introduce an additional template parameter to `Try` for typed error.,add an additional template parameter e to the try class template.      template /  class try ;  ,3,test
MESOS-5111,Update `network::connect` to use the typed error state of `Try`.,network::connect function returns a try/ currently and the caller is required to inspect the state of errno outofband. network::connect should really return something like a try/.,2,test
MESOS-5112,Introduce `WindowsSocketError`.,"windowserror invokes ::getlasterror to retrieve the error code. windows has a ::wsagetlasterror function which at the interface level, is intended for failed socket operations. we should introduce a windowssocketerror which invokes ::wsagetlasterror and use them accordingly.",2,test
MESOS-5113,`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,"if we start the agent with the isolation='network/cni' but do not specify the networkcnipluginsdir flag, the agent crashes with the following stack dump:  0x00007ffff2324cc9 in giraise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56  56      ../nptl/sysdeps/unix/sysv/linux/raise.c: no such file or directory.  (gdb) bt  #0  0x00007ffff2324cc9 in giraise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56  #1  0x00007ffff23280d8 in giabort () at abort.c:89  #2  0x00007ffff231db86 in assertfailbase (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sassertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""issome()"",      file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,      function=function@entry=0x45294a ""const t &option/ >::get() const & [t = std::basicstring/]"") at assert.c:92  #3  0x00007ffff231dc32 in giassertfail (assertion=0x451f5c ""issome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,      function=0x45294a ""const t &option/ >::get() const & [t = std::basicstring/]"") at assert.c:101  #4  0x0000000000432c0d in option/::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111  python exception / list index out of range:  #5  0x00007ffff63ef7cc in mesos::internal::slave::networkcniisolatorprocess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331  #6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239  #7  0x00007ffff60cd972 in std::functionhandler/ process::dispatch/ > const&, hashset/, std::equalto/ > const&, std::list/ >, hashset/, std::equalto/ > >(process::pid/ const&, process::future/ (mesos::internal::slave::mesosisolatorprocess::)(std::list/ > const&, hashset/, std::equalto/ > const&), std::list/ >, hashset/, std::equalto/ >)::>::minvoke(std::anydata const&, process::processbase) (functor=..., args=0x6c1ef8) at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:2071  #8  0x00007ffff6a6bf38 in std::function/::operator()(process::processbase) const (this=0x7fffc0001d70, args=0x6c1ef8)      at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:2471  #9  0x00007ffff6a561b4 in process::processbase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130  #10 0x00007ffff6aac5fe in process::dispatchevent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161  #11 0x00007ffff55e9c91 in process::processbase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82  #12 0x00007ffff6a53ed4 in process::processmanager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570  #13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218  #14 0x00007ffff6a5bf33 in std::bind/)>::call/(std::tuple/&&, std::indextuple/) (this=0x697d70,      args=/) at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:1295  #15 0x00007ffff6a5bee6 in std::bind/)>::operator()/() (this=0x697d70)      at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:1353  #16 0x00007ffff6a5be95 in std::bindsimple/)> ()>::minvoke/(std::indextuple/) (this=0x697d70)      at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:1731  #17 0x00007ffff6a5be65 in std::bindsimple/)> ()>::operator()() (this=0x697d70)      at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/functional:1720  #18 0x00007ffff6a5be3c in std::thread::impl/)> ()> >::mrun() (this=0x697d58)      at /usr/bin/../lib/gcc/x8664linuxgnu/4.8/../../../../include/c/4.8/thread:115  #19 0x00007ffff2b98a60 in ?? () from /usr/lib/x8664linux gnu/libstdc.so.6  #20 0x00007ffff26bb182 in startthread (arg=0x7fffeb92d700) at pthreadcreate.c:312  #21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.s:111  (gdb) frame 4  #4  0x0000000000432c0d in option/::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",1,test
MESOS-5114,Flags::parse does not handle empty string correctly.,"a missing default for quorum size has generated the following master config     mesosworkdir=""/var/lib/mesos/master""  mesoszk=""zk:/zk1:2181,zk2:2181,zk3:2181/mesos""  mesosquorum=    mesosport=5050  mesoscluster=""mesos""  mesoslogdir=""/var/log/mesos""  mesoslogbufsecs=1  mesoslogginglevel=""info""      this was causing each elected leader to attempt replica recovery.    e.g. group.cpp:700] trying to get '/mesos/logreplicas/0000000012' in zookeeper    and eventually:  master.cpp:1458] recovery failed: failed to recover registrar: failed to perform fetch within 1mins    full log on one of the masters https:/gist.github.com/clehene/09a9ddfe49b92a5deb4c1b421f63479e    all masters and zk nodes were reachable over the network.   also once the quorum was configured the master recovery protocol finished gracefully.   ",2,test
MESOS-5115,Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.," calls to 'nvidiasmi'  fail inside a container even if access to a gpu has been granted. moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a gpu even if it has access to it.        we should grant/revoke access to /dev/nvidiactl and /dev/nvidiauvm as gpus are added and removed from a container in the nvidia gpu isolator.",2,test
MESOS-5121,pivot_root is not available on PowerPC,"when compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error ""pivotroot is not available""    the current code logic in src/linux/fs.cpp is:      #ifdef nrpivotroot    int ret = ::syscall(nrpivotroot, newroot.cstr(), putold.cstr());  #elif x8664    / a workaround for systems that have an old glib but have a new    / kernel. the magic number '155' is the syscall number for    / 'pivotroot' on the x8664 architecture, see    / arch/x86/syscalls/syscall64.tbl    int ret = ::syscall(155, newroot.cstr(), putold.cstr());  #else  #error ""pivotroot is not available""  #endif      there is no old glib version and the new kernel version, it will never run code in #ifdef nrpivotroot condition, and when i build on ubuntu 16.04(it has the latest linux kernel and glibc), it still can't step into the #ifdef nrpivotroot condition.    for powerpc case, i added another condition:      #elif powerpc  powerpc64  ppc64    / a workaround for powerpc. the magic number '203' is the syscall    / number for 'pivotroot' on the powerpc architecture, see    / https:/w3challs.com/syscalls/?arch=powerpc64    int ret = ::syscall(203, newroot.cstr(), putold.cstr());  ",1,test
MESOS-5124,TASK_KILLING is not supported by mesos-execute.,recently task_killing state (mesos4547) have been introduced to mesos. we should add support for this feature to mesosexecute.,3,test
MESOS-5125,"Commit message hook iterates over words, rather than lines.","for line in $commitmessage iterates over one word at a time, rather than one line at a time. we should use the following pattern instead:    while read line;  do    ...  done <<< ""$commitmessage""  ",2,test
MESOS-5126,Commit message hook iterates over the commented lines.,"currently, the commit message hook iterates over the commented lines.  for example, if there is a modified file for which its path is longer than 72 characters, the commit hook errors out. we should skip over the commented lines.",2,test
MESOS-5127,Reset `LIBPROCESS_IP` in `network\cni` isolator.,"currently the `libprocessip` environment variable was being set to      the agent ip if the environment variable has not be defined by the      `framework`. for containers having their own ip address (as with      containers on cni networks) this becomes a problem since the command      executor tries to bind to the `libprocessip` that does not exist in      its network namespace, and fails. thus, for containers launched on cni      networks the `libprocess_ip` should not be set, or rather is set to      ""0.0.0.0"", allowing the container to bind to the ip address provided      by the cni network.",1,test
MESOS-5128,PersistentVolumeTest.AccessPersistentVolume is flaky,"observed on asf ci:      [ run      ] diskresource/persistentvolumetest.accesspersistentvolume/0  i0405 17:29:19.134435 31837 cluster.cpp:139] creating default 'local' authorizer  i0405 17:29:19.251143 31837 leveldb.cpp:174] opened db in 116.386403ms  i0405 17:29:19.310050 31837 leveldb.cpp:181] compacted db in 58.80688ms  i0405 17:29:19.310180 31837 leveldb.cpp:196] created db iterator in 37145ns  i0405 17:29:19.310199 31837 leveldb.cpp:202] seeked to beginning of db in 4212ns  i0405 17:29:19.310210 31837 leveldb.cpp:271] iterated through 0 keys in the db in 410ns  i0405 17:29:19.310279 31837 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0405 17:29:19.311069 31861 recover.cpp:447] starting replica recovery  i0405 17:29:19.311362 31861 recover.cpp:473] replica is in empty status  i0405 17:29:19.312641 31861 replica.cpp:673] replica in empty status received a broadcasted recover request from (14359)@172.17.0.4:43972  i0405 17:29:19.313045 31860 recover.cpp:193] received a recover response from a replica in empty status  i0405 17:29:19.313608 31860 recover.cpp:564] updating replica status to starting  i0405 17:29:19.316416 31867 master.cpp:376] master 9565ff6ff1b642598430690e635c391f (4090d10eba90) started on 172.17.0.4:43972  i0405 17:29:19.316470 31867 master.cpp:378] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/0a9elu/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.29.0/inst/share/mesos/webui"" workdir=""/tmp/0a9elu/master"" zksessiontimeout=""10secs""  i0405 17:29:19.316938 31867 master.cpp:427] master only allowing authenticated frameworks to register  i0405 17:29:19.316951 31867 master.cpp:432] master only allowing authenticated agents to register  i0405 17:29:19.316961 31867 credentials.hpp:37] loading credentials for authentication from '/tmp/0a9elu/credentials'  i0405 17:29:19.317402 31867 master.cpp:474] using default 'crammd5' authenticator  i0405 17:29:19.317643 31867 master.cpp:545] using default 'basic' http authenticator  i0405 17:29:19.317854 31867 master.cpp:583] authorization enabled  i0405 17:29:19.318081 31864 whitelistwatcher.cpp:77] no whitelist given  i0405 17:29:19.318079 31861 hierarchical.cpp:144] initialized hierarchical allocator process  i0405 17:29:19.320838 31864 master.cpp:1826] the newly elected leader is master@172.17.0.4:43972 with id 9565ff6ff1b642598430690e635c391f  i0405 17:29:19.320888 31864 master.cpp:1839] elected as the leading master!  i0405 17:29:19.320909 31864 master.cpp:1526] recovering from registrar  i0405 17:29:19.321218 31871 registrar.cpp:331] recovering registrar  i0405 17:29:19.347045 31860 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 33.164133ms  i0405 17:29:19.347126 31860 replica.cpp:320] persisted replica status to starting  i0405 17:29:19.347611 31869 recover.cpp:473] replica is in starting status  i0405 17:29:19.349215 31871 replica.cpp:673] replica in starting status received a broadcasted recover request from (14361)@172.17.0.4:43972  i0405 17:29:19.349653 31870 recover.cpp:193] received a recover response from a replica in starting status  i0405 17:29:19.350236 31866 recover.cpp:564] updating replica status to voting  i0405 17:29:19.388882 31864 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 38.38299ms  i0405 17:29:19.388993 31864 replica.cpp:320] persisted replica status to voting  i0405 17:29:19.389369 31856 recover.cpp:578] successfully joined the paxos group  i0405 17:29:19.389735 31856 recover.cpp:462] recover process terminated  i0405 17:29:19.390476 31868 log.cpp:659] attempting to start the writer  i0405 17:29:19.392125 31862 replica.cpp:493] replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1  i0405 17:29:19.430706 31862 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 38.505062ms  i0405 17:29:19.430816 31862 replica.cpp:342] persisted promised to 1  i0405 17:29:19.431918 31856 coordinator.cpp:238] coordinator attempting to fill missing positions  i0405 17:29:19.433725 31861 replica.cpp:388] replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2  i0405 17:29:19.472491 31861 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 38.659492ms  i0405 17:29:19.472595 31861 replica.cpp:712] persisted action at 0  i0405 17:29:19.474556 31864 replica.cpp:537] replica received write request for position 0 from (14364)@172.17.0.4:43972  i0405 17:29:19.474652 31864 leveldb.cpp:436] reading position from leveldb took 49423ns  i0405 17:29:19.528175 31864 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 53.443616ms  i0405 17:29:19.528300 31864 replica.cpp:712] persisted action at 0  i0405 17:29:19.529389 31865 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0405 17:29:19.571137 31865 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 41.676495ms  i0405 17:29:19.571254 31865 replica.cpp:712] persisted action at 0  i0405 17:29:19.571302 31865 replica.cpp:697] replica learned nop action at position 0  i0405 17:29:19.572322 31856 log.cpp:675] writer started with ending position 0  i0405 17:29:19.574060 31861 leveldb.cpp:436] reading position from leveldb took 83200ns  i0405 17:29:19.575417 31864 registrar.cpp:364] successfully fetched the registry (0b) in 0ns  i0405 17:29:19.575565 31864 registrar.cpp:463] applied 1 operations in 46419ns; attempting to update the 'registry'  i0405 17:29:19.576517 31857 log.cpp:683] attempting to append 170 bytes to the log  i0405 17:29:19.576849 31857 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0405 17:29:19.578390 31857 replica.cpp:537] replica received write request for position 1 from (14365)@172.17.0.4:43972  i0405 17:29:19.780277 31857 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 201.808617ms  i0405 17:29:19.780366 31857 replica.cpp:712] persisted action at 1  i0405 17:29:19.782024 31857 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0405 17:29:19.823770 31857 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 41.667662ms  i0405 17:29:19.823851 31857 replica.cpp:712] persisted action at 1  i0405 17:29:19.823889 31857 replica.cpp:697] replica learned append action at position 1  i0405 17:29:19.825701 31867 registrar.cpp:508] successfully updated the 'registry' in 0ns  i0405 17:29:19.825929 31867 registrar.cpp:394] successfully recovered registrar  i0405 17:29:19.826015 31857 log.cpp:702] attempting to truncate the log to 1  i0405 17:29:19.826262 31867 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0405 17:29:19.827647 31867 replica.cpp:537] replica received write request for position 2 from (14366)@172.17.0.4:43972  i0405 17:29:19.828018 31857 master.cpp:1634] recovered 0 agents from the registry (131b) ; allowing 10mins for agents to reregister  i0405 17:29:19.828065 31861 hierarchical.cpp:171] skipping recovery of hierarchical allocator: nothing to recover  i0405 17:29:19.865555 31867 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 37.822178ms  i0405 17:29:19.865661 31867 replica.cpp:712] persisted action at 2  i0405 17:29:19.866921 31867 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0405 17:29:19.907341 31867 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 40.356649ms  i0405 17:29:19.907531 31867 leveldb.cpp:399] deleting ~1 keys from leveldb took 91109ns  i0405 17:29:19.907560 31867 replica.cpp:712] persisted action at 2  i0405 17:29:19.907599 31867 replica.cpp:697] replica learned truncate action at position 2  i0405 17:29:19.923305 31837 resources.cpp:572] parsing resources as json failed: cpus:2;mem:2048  trying semicolondelimited string format instead  i0405 17:29:19.926491 31837 containerizer.cpp:155] using isolation: posix/cpu,posix/mem,filesystem/posix  w0405 17:29:19.927836 31837 backend.cpp:66] failed to create 'bind' backend: bindbackend requires root privileges  i0405 17:29:19.932029 31862 slave.cpp:200] agent started on 441)@172.17.0.4:43972  i0405 17:29:19.932086 31862 slave.cpp:201] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcredentials=""/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.29.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""[,""type"":""scalar""},,""type"":""scalar""},,""type"":""scalar""}]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac""  i0405 17:29:19.932665 31862 credentials.hpp:86] loading credential for authentication from '/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/credential'  i0405 17:29:19.932934 31862 slave.cpp:338] agent using credential for: testprincipal  i0405 17:29:19.932968 31862 credentials.hpp:37] loading credentials for authentication from '/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/httpcredentials'  i0405 17:29:19.933284 31862 slave.cpp:390] using default 'basic' http authenticator  i0405 17:29:19.934916 31837 sched.cpp:222] version: 0.29.0  i0405 17:29:19.935566 31862 slave.cpp:589] agent resources: cpus():2; mem():2048; disk(role1):4096; ports():[3100032000]  i0405 17:29:19.935664 31862 slave.cpp:597] agent attributes: [  ]  i0405 17:29:19.935679 31862 slave.cpp:602] agent hostname: 4090d10eba90  i0405 17:29:19.938390 31864 state.cpp:57] recovering state from '/tmp/diskresourcepersistentvolumetestaccesspersistentvolume0fjs7ac/meta'  i0405 17:29:19.940608 31869 sched.cpp:326] new master detected at master@172.17.0.4:43972  i0405 17:29:19.940749 31869 sched.cpp:382] authenticating with master master@172.17.0.4:43972  i0405 17:29:19.940773 31869 sched.cpp:389] using default crammd5 authenticatee  i0405 17:29:19.942371 31869 authenticatee.cpp:121] creating new client sasl connection  i0405 17:29:19.942873 31859 master.cpp:5679] authenticating schedulerbdf68f7fd93847eda132bb3f218628bf@172.17.0.4:43972  i0405 17:29:19.943156 31859 authenticator.cpp:413] starting authentication session for crammd5authenticatee(896)@172.17.0.4:43972  i0405 17:29:19.943507 31863 authenticator.cpp:98] creating new server sasl connection  i0405 17:29:19.943740 31859 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0405 17:29:19.943783 31859 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0405 17:29:19.943892 31859 authenticator.cpp:203] received sasl authentication start  i0405 17:29:19.943977 31859 authenticator.cpp:325] authentication requires more steps  i0405 17:29:19.944066 31859 authenticatee.cpp:258] received sasl authentication step  i0405 17:29:19.944164 31859 authenticator.cpp:231] received sasl authentication step  i0405 17:29:19.944193 31859 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4090d10eba90' server fqdn: '4090d10eba90' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0405 17:29:19.944206 31859 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0405 17:29:19.944268 31859 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0405 17:29:19.944300 31859 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4090d10eba90' server fqdn: '4090d10eba90' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0405 17:29:19.944313 31859 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0405 17:29:19.944321 31859 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0405 17:29:19.944339 31859 authenticator.cpp:317] authentication success  i0405 17:29:19.944541 31859 authenticatee.cpp:298] authentication success  i0405 17:29:19.944655 31859 master.cpp:5709] successfully authenticated principal 'testprincipal' at schedulerbdf68f7fd93847eda132bb3f218628bf@172.17.0.4:43972  i0405 17:29:19.944737 31859 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(896)@172.17.0.4:43972  i0405 17:29:19.945111 31859 sched.cpp:472] successfully authenticated with master master@172.17.0.4:43972  i0405 17:29:19.945132 31859 sched.cpp:777] sending subscribe call to master@172.17.0.4:43972  i0405 17:29:19.945591 31859 sched.cpp:810] will retry registration in 372.80738ms if necessary  i0405 17:29:19.945744 31865 master.cpp:2346] received subscribe call for framework 'default' at schedulerbdf68f7fd93847eda132bb3f218628bf@172.17.0.4:43972  i0405 17:29:19.945838 31865 master.cpp:1865] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i0405 17:29:19.946194 31865 master.cpp:2417] subscribing framework default with checkpointing disabled and capabilities [  ]  i0405 17:29:19.946866 31866 hierarchical.cpp:266] added framework 9565ff6ff1b642598430690e635c391f0000  i0405 17:29:19.946974 31866 hierarchical.cpp:1490] no resources available to allocate!  i0405 17:29:19.947010 31866 hierarchical.cpp:1585] no inverse offers to send out!  i0405 17:29:19.947054 31865 sched.cpp:704] framework registered with 9565ff6ff1b642598430690e635c391f0000  i0405 17:29:19.947074 31866 hierarchical.cpp:1141] performed allocation for 0 agents in 178242ns  i0405 17:29:19.947124 31865 sched.cpp:718] scheduler::registered took 38907ns  i0405 17:29:19.948712 31866 statusupdatemanager.cpp:200] recovering status update manager  i0405 17:29:19.948901 31866 containerizer.cpp:416] recovering containerizer  i0405 17:29:19.951021 31866 provisioner.cpp:245] provisioner recovery complete  i0405 17:29:19.951802 31866 slave.cpp:4773] finished recovery  i0405 17:29:19.952518 31866 slave.cpp:4945] querying resource estimator for oversubscribable resources  i0405 17:29:19.953248 31866 slave.cpp:928] new master detected at master@172.17.0.4:43972  i0405 17:29:19.953305 31865 statusupdatemanager.cpp:174] pausing sending status updates  i0405 17:29:19.953626 31866 slave.cpp:991] authenticating with master master@172.17.0.4:43972  i0405 17:29:19.953716 31866 slave.cpp:996] using default crammd5 authenticatee  i0405 17:29:19.954074 31866 slave.cpp:964] detecting new master  i0405 17:29:19.954167 31861 authenticatee.cpp:121] creating new client sasl connection  i0405 17:29:19.954372 31866 slave.cpp:4959] received oversubscribable resources  from the resource estimator  i0405 17:29:19.954756 31866 master.cpp:5679] authenticating slave(441)@172.17.0.4:43972  i0405 17:29:19.954944 31861 authenticator.cpp:413] starting authentication session for crammd5authenticatee(897)@172.17.0.4:43972  i0405 17:29:19.955368 31863 authenticator.cpp:98] creating new server sasl connection  i0405 17:29:19.955687 31861 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0405 17:29:19.955801 31861 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0405 17:29:19.956075 31861 authenticator.cpp:203] received sasl authentication start  i0405 17:29:19.956279 31861 authenticator.cpp:325] authentication requires more steps  i0405 17:29:19.956455 31861 authenticatee.cpp:258] received sasl authentication step  i0405 17:29:19.956676 31861 authenticator.cpp:231] received sasl authentication step  i0405 17:29:19.956815 31861 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4090d10eba90' server fqdn: '4090d10eba90' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0405 17:29:19.956907 31861 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0405 17:29:19.957044 31861 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0405 17:29:19.957166 31861 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '4090d10eba90' server fqdn: '4090d10eba90' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0405 17:29:19.957264 31861 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0405 17:29:19.957353 31861 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0405 17:29:19.957449 31861 authenticator.cpp:317] authentication success  i0405 17:29:19.957664 31857 authenticatee.cpp:298] authentication success  i0405 17:29:19.957813 31857 master.cpp:5709] successfully authenticated principal 'testprincipal' at slave(441)@172.17.0.4:43972  i0405 17:29:19.958008 31861 authenticator.cpp:431] authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972  i0405 17:29:19.958732 31857 slave.cpp:1061] successfully authenticated with master master@172.17.0.4:43972  i0405 17:29:19.958930 31857 slave.cpp:1457] will retry registration in 18.568334ms if necessary  i0405 17:29:19.959262 31857 master.cpp:4390] registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6ff1b642598430690e635c391f s0  i0405 17:29:19.959934 31857 registrar.cpp:463] applied 1 operations in 99197ns; attempting to update the 'registry'  i0405 17:29:19.961587 31857 log.cpp:683] attempting to append 343 bytes to the log  i0405 17:29:19.9...",3,test
MESOS-5130,Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.,"currently there are no default `network` isolators for `mesoscontainerizer`. with the development of the `network/cni` isolator we have an interface to run mesos on multitude of ip networks. given that its based on an open standard (the cni spec) which is gathering a lot of traction from vendors (calico, weave, coreos) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.",1,test
MESOS-5132,Commit message hook lints the diff in verbose mode.,"in verbose mode (i.e., git commit verbose), the commit message includes the diff of the commit at the bottom, delimited by the following lines:      #  >8   # do not touch the line above.  # everything below will be removed.      we should break once we encounter such a line.",2,test
MESOS-5133,Expose TaskStatus source & reason in master's '/state' output,it would be helpful if the taskstatus lists provided by the master's /state endpoint included the source and reason associated with the status message. the json modeling function for taskstatus should be extended to include these fields.,1,test
MESOS-5135,Update existing documentation to Include references to GPUs as a first class resource.,"specifically, the documentation in the following files should be udated:      docs/attributes resources.md  docs/monitoring.md  ",1,test
MESOS-5136,Update the default JSON representation of a Resource to include GPUs,"the default json representation of a resource currently lists a value of ""0"" if no value is set on a first class scalar resource (i.e. cpus, mem, disk).  we should add gpus in here as well.  ",1,test
MESOS-5137,Remove 'dashboard.js' from the webui.,this file is no longer in use anywhere.,1,test
MESOS-5138,Fix Nvidia GPU test build for namespace change of MasterDetector,an update to master the day after all of the nvidia gpu stuff landed has a build error in the nvidia gpu tests. the namespace that masterdetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now.,1,test
MESOS-5139,ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar is flaky,"found this on asf ci while testing 0.28.1rc2      [ run      ] provisionerdockerlocalstoretest.localstoretestwithtar  e0406 18:29:30.870481   520 shell.hpp:93] command 'hadoop version 2>&1' failed; this is the output:  sh: 1: hadoop: not found  e0406 18:29:30.870576   520 fetcher.cpp:59] failed to create uri fetcher plugin 'hadoop': failed to create hdfs client: failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a nonzero exit status: 127  i0406 18:29:30.871052   520 localpuller.cpp:90] creating local puller with docker registry '/tmp/3l8zbv/images'  i0406 18:29:30.873325   539 metadatamanager.cpp:159] looking for image 'abc'  i0406 18:29:30.874438   539 localpuller.cpp:142] untarring image 'abc' from '/tmp/3l8zbv/images/abc.tar' to '/tmp/3l8zbv/store/staging/5tw8bd'  i0406 18:29:30.901916   547 localpuller.cpp:162] the repositories json file for image 'abc' is '}'  i0406 18:29:30.902304   547 localpuller.cpp:290] extracting layer tar ball '/tmp/3l8zbv/store/staging/5tw8bd/123/layer.tar to rootfs '/tmp/3l8zbv/store/staging/5tw8bd/123/rootfs'  i0406 18:29:30.909144   547 localpuller.cpp:290] extracting layer tar ball '/tmp/3l8zbv/store/staging/5tw8bd/456/layer.tar to rootfs '/tmp/3l8zbv/store/staging/5tw8bd/456/rootfs'  ../../src/tests/containerizer/provisionerdockertests.cpp:183: failure  (imageinfo).failure(): collect failed: subprocess 'tar, tar, x, f, /tmp/3l8zbv/store/staging/5tw8bd/456/layer.tar,  c, /tmp/3l8zbv/store/staging/5tw8bd/456/rootfs' failed: tar: this does not look like a tar archive  tar: exiting with failure status due to previous errors    [  failed  ] provisionerdockerlocalstoretest.localstoretestwithtar (243 ms)  ",2,test
MESOS-5142,Add agent flags for HTTP authorization.,flags should be added to the agent to:  1. enable authorization (authorizers)  2. provide acls (acls),2,test
MESOS-5144,Cleanup memory leaks in libprocess finalize(),"libprocess's finalize function currently leaks memory for a few different reasons. cleaning up the socketmanager will be somewhat involved (mesos 3910), but the remaining memory leaks should be fairly easy to address.",2,test
MESOS-5146,MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,"observed on the asf ci:      [ run      ] masterallocatortest/1.rebalancedforupdatedweights  i0407 22:34:10.330394 29278 cluster.cpp:149] creating default 'local' authorizer  i0407 22:34:10.466182 29278 leveldb.cpp:174] opened db in 135.608207ms  i0407 22:34:10.516398 29278 leveldb.cpp:181] compacted db in 50.159558ms  i0407 22:34:10.516464 29278 leveldb.cpp:196] created db iterator in 34959ns  i0407 22:34:10.516484 29278 leveldb.cpp:202] seeked to beginning of db in 10195ns  i0407 22:34:10.516496 29278 leveldb.cpp:271] iterated through 0 keys in the db in 7324ns  i0407 22:34:10.516547 29278 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0407 22:34:10.517277 29298 recover.cpp:447] starting replica recovery  i0407 22:34:10.517693 29300 recover.cpp:473] replica is in empty status  i0407 22:34:10.520251 29310 replica.cpp:673] replica in empty status received a broadcasted recover request from (4775)@172.17.0.3:35855  i0407 22:34:10.520611 29311 recover.cpp:193] received a recover response from a replica in empty status  i0407 22:34:10.521164 29299 recover.cpp:564] updating replica status to starting  i0407 22:34:10.523435 29298 master.cpp:382] master f59f9057a5c743e1b12996862e640a12 (129e11060069) started on 172.17.0.3:35855  i0407 22:34:10.523473 29298 master.cpp:384] flags at startup: acls="""" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticate=""true"" authenticatehttp=""true"" authenticateslaves=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/3rzy8c/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" maxslavepingtimeouts=""5"" quiet=""false"" recoveryslaveremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" slavepingtimeout=""15secs"" slavereregistertimeout=""10mins"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos0.29.0/inst/share/mesos/webui"" workdir=""/tmp/3rzy8c/master"" zksessiontimeout=""10secs""  i0407 22:34:10.523885 29298 master.cpp:433] master only allowing authenticated frameworks to register  i0407 22:34:10.523901 29298 master.cpp:438] master only allowing authenticated agents to register  i0407 22:34:10.523913 29298 credentials.hpp:37] loading credentials for authentication from '/tmp/3rzy8c/credentials'  i0407 22:34:10.524298 29298 master.cpp:480] using default 'crammd5' authenticator  i0407 22:34:10.524441 29298 master.cpp:551] using default 'basic' http authenticator  i0407 22:34:10.524564 29298 master.cpp:589] authorization enabled  i0407 22:34:10.525269 29305 hierarchical.cpp:145] initialized hierarchical allocator process  i0407 22:34:10.525333 29305 whitelistwatcher.cpp:77] no whitelist given  i0407 22:34:10.527331 29298 master.cpp:1832] the newly elected leader is master@172.17.0.3:35855 with id f59f9057a5c743e1b12996862e640a12  i0407 22:34:10.527441 29298 master.cpp:1845] elected as the leading master!  i0407 22:34:10.527545 29298 master.cpp:1532] recovering from registrar  i0407 22:34:10.527889 29298 registrar.cpp:331] recovering registrar  i0407 22:34:10.549734 29299 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 28.25177ms  i0407 22:34:10.549782 29299 replica.cpp:320] persisted replica status to starting  i0407 22:34:10.550010 29299 recover.cpp:473] replica is in starting status  i0407 22:34:10.551352 29299 replica.cpp:673] replica in starting status received a broadcasted recover request from (4777)@172.17.0.3:35855  i0407 22:34:10.551676 29299 recover.cpp:193] received a recover response from a replica in starting status  i0407 22:34:10.552315 29308 recover.cpp:564] updating replica status to voting  i0407 22:34:10.574865 29308 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 22.413614ms  i0407 22:34:10.574928 29308 replica.cpp:320] persisted replica status to voting  i0407 22:34:10.575103 29308 recover.cpp:578] successfully joined the paxos group  i0407 22:34:10.575346 29308 recover.cpp:462] recover process terminated  i0407 22:34:10.575913 29308 log.cpp:659] attempting to start the writer  i0407 22:34:10.577512 29308 replica.cpp:493] replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1  i0407 22:34:10.599984 29308 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 22.453613ms  i0407 22:34:10.600026 29308 replica.cpp:342] persisted promised to 1  i0407 22:34:10.601773 29304 coordinator.cpp:238] coordinator attempting to fill missing positions  i0407 22:34:10.603757 29307 replica.cpp:388] replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2  i0407 22:34:10.634392 29307 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 30.269987ms  i0407 22:34:10.634829 29307 replica.cpp:712] persisted action at 0  i0407 22:34:10.637017 29297 replica.cpp:537] replica received write request for position 0 from (4780)@172.17.0.3:35855  i0407 22:34:10.637099 29297 leveldb.cpp:436] reading position from leveldb took 52948ns  i0407 22:34:10.676170 29297 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 38.917487ms  i0407 22:34:10.676352 29297 replica.cpp:712] persisted action at 0  i0407 22:34:10.677564 29306 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0407 22:34:10.717959 29306 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 40.306229ms  i0407 22:34:10.718202 29306 replica.cpp:712] persisted action at 0  i0407 22:34:10.718399 29306 replica.cpp:697] replica learned nop action at position 0  i0407 22:34:10.719883 29306 log.cpp:675] writer started with ending position 0  i0407 22:34:10.721688 29305 leveldb.cpp:436] reading position from leveldb took 75934ns  i0407 22:34:10.723640 29306 registrar.cpp:364] successfully fetched the registry (0b) in 195648us  i0407 22:34:10.723999 29306 registrar.cpp:463] applied 1 operations in 108099ns; attempting to update the 'registry'  i0407 22:34:10.725077 29311 log.cpp:683] attempting to append 170 bytes to the log  i0407 22:34:10.725328 29308 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0407 22:34:10.726552 29299 replica.cpp:537] replica received write request for position 1 from (4781)@172.17.0.3:35855  i0407 22:34:10.759747 29299 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 33.089719ms  i0407 22:34:10.759976 29299 replica.cpp:712] persisted action at 1  i0407 22:34:10.761739 29299 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0407 22:34:10.801522 29299 leveldb.cpp:341] persisting action (191 bytes) to leveldb took 39.694064ms  i0407 22:34:10.801602 29299 replica.cpp:712] persisted action at 1  i0407 22:34:10.801638 29299 replica.cpp:697] replica learned append action at position 1  i0407 22:34:10.803371 29311 registrar.cpp:508] successfully updated the 'registry' in 79.163904ms  i0407 22:34:10.803829 29311 registrar.cpp:394] successfully recovered registrar  i0407 22:34:10.804585 29311 master.cpp:1640] recovered 0 agents from the registry (131b) ; allowing 10mins for agents to reregister  i0407 22:34:10.805269 29308 log.cpp:702] attempting to truncate the log to 1  i0407 22:34:10.805721 29310 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0407 22:34:10.805276 29296 hierarchical.cpp:172] skipping recovery of hierarchical allocator: nothing to recover  i0407 22:34:10.806529 29307 replica.cpp:537] replica received write request for position 2 from (4782)@172.17.0.3:35855  i0407 22:34:10.843320 29307 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 36.77593ms  i0407 22:34:10.843531 29307 replica.cpp:712] persisted action at 2  i0407 22:34:10.845369 29311 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0407 22:34:10.885098 29311 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 39.641102ms  i0407 22:34:10.885401 29311 leveldb.cpp:399] deleting ~1 keys from leveldb took 88701ns  i0407 22:34:10.885745 29311 replica.cpp:712] persisted action at 2  i0407 22:34:10.885862 29311 replica.cpp:697] replica learned truncate action at position 2  i0407 22:34:10.900660 29278 containerizer.cpp:155] using isolation: posix/cpu,posix/mem,filesystem/posix  w0407 22:34:10.901793 29278 backend.cpp:66] failed to create 'bind' backend: bindbackend requires root privileges  i0407 22:34:10.905488 29302 slave.cpp:201] agent started on 111)@172.17.0.3:35855  i0407 22:34:10.905553 29302 slave.cpp:202] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcredentials=""/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos0.29.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;mem:1024;disk:4096;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/masterallocatortest1rebalancedforupdatedweights9acaya""  i0407 22:34:10.906365 29302 credentials.hpp:86] loading credential for authentication from '/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/credential'  i0407 22:34:10.906787 29302 slave.cpp:339] agent using credential for: testprincipal  i0407 22:34:10.907202 29302 credentials.hpp:37] loading credentials for authentication from '/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/httpcredentials'  i0407 22:34:10.907713 29302 slave.cpp:391] using default 'basic' http authenticator  i0407 22:34:10.908499 29302 resources.cpp:572] parsing resources as json failed: cpus:2;mem:1024;disk:4096;ports:[3100032000]  trying semicolondelimited string format instead  i0407 22:34:10.910189 29302 slave.cpp:590] agent resources: cpus():2; mem():1024; disk():4096; ports():[3100032000]  i0407 22:34:10.910362 29302 slave.cpp:598] agent attributes: [  ]  i0407 22:34:10.910465 29302 slave.cpp:603] agent hostname: 129e11060069  i0407 22:34:10.913280 29303 state.cpp:57] recovering state from '/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/meta'  i0407 22:34:10.914621 29303 statusupdatemanager.cpp:200] recovering status update manager  i0407 22:34:10.915226 29303 containerizer.cpp:416] recovering containerizer  i0407 22:34:10.917246 29301 provisioner.cpp:245] provisioner recovery complete  i0407 22:34:10.917733 29301 slave.cpp:4784] finished recovery  i0407 22:34:10.918226 29301 slave.cpp:4956] querying resource estimator for oversubscribable resources  i0407 22:34:10.918529 29301 slave.cpp:4970] received oversubscribable resources  from the resource estimator  i0407 22:34:10.918908 29304 slave.cpp:939] new master detected at master@172.17.0.3:35855  i0407 22:34:10.918988 29304 slave.cpp:1002] authenticating with master master@172.17.0.3:35855  i0407 22:34:10.919098 29301 statusupdatemanager.cpp:174] pausing sending status updates  i0407 22:34:10.919309 29304 slave.cpp:1007] using default crammd5 authenticatee  i0407 22:34:10.919535 29304 slave.cpp:975] detecting new master  i0407 22:34:10.919747 29308 authenticatee.cpp:121] creating new client sasl connection  i0407 22:34:10.920413 29308 master.cpp:5695] authenticating slave(111)@172.17.0.3:35855  i0407 22:34:10.920650 29308 authenticator.cpp:413] starting authentication session for crammd5authenticatee(278)@172.17.0.3:35855  i0407 22:34:10.921020 29308 authenticator.cpp:98] creating new server sasl connection  i0407 22:34:10.921308 29308 authenticatee.cpp:212] received sasl authentication mechanisms: crammd5  i0407 22:34:10.921424 29308 authenticatee.cpp:238] attempting to authenticate with mechanism 'crammd5'  i0407 22:34:10.921596 29308 authenticator.cpp:203] received sasl authentication start  i0407 22:34:10.921752 29308 authenticator.cpp:325] authentication requires more steps  i0407 22:34:10.921957 29307 authenticatee.cpp:258] received sasl authentication step  i0407 22:34:10.922178 29308 authenticator.cpp:231] received sasl authentication step  i0407 22:34:10.922214 29308 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '129e11060069' server fqdn: '129e11060069' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0407 22:34:10.922229 29308 auxprop.cpp:179] looking up auxiliary property 'userpassword'  i0407 22:34:10.922281 29308 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  i0407 22:34:10.922309 29308 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: '129e11060069' server fqdn: '129e11060069' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0407 22:34:10.922322 29308 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0407 22:34:10.922332 29308 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0407 22:34:10.922353 29308 authenticator.cpp:317] authentication success  i0407 22:34:10.922436 29307 authenticatee.cpp:298] authentication success  i0407 22:34:10.922587 29308 master.cpp:5725] successfully authenticated principal 'testprincipal' at slave(111)@172.17.0.3:35855  i0407 22:34:10.922668 29299 authenticator.cpp:431] authentication session cleanup for crammd5authenticatee(278)@172.17.0.3:35855  i0407 22:34:10.923256 29307 slave.cpp:1072] successfully authenticated with master master@172.17.0.3:35855  i0407 22:34:10.923429 29307 slave.cpp:1468] will retry registration in 3.220345ms if necessary  i0407 22:34:10.923707 29302 master.cpp:4406] registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057a5c743e1b12996862e640a12s0  i0407 22:34:10.924239 29309 registrar.cpp:463] applied 1 operations in 105794ns; attempting to update the 'registry'  i0407 22:34:10.925787 29309 log.cpp:683] attempting to append 339 bytes to the log  i0407 22:34:10.926028 29309 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0407 22:34:10.927139 29309 replica.cpp:537] replica received write request for position 3 from (4797)@172.17.0.3:35855  i0407 22:34:10.929083 29305 slave.cpp:1468] will retry registration in 39.293556ms if necessary  i0407 22:34:10.929363 29305 master.cpp:4394] ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress  i0407 22:34:10.968843 29309 leveldb.cpp:341] persisting action (358 bytes) to leveldb took 41.68025ms  i0407 22:34:10.969005 29309 replica.cpp:712] persisted action at 3  i0407 22:34:10.969741 29309 slave.cpp:1468] will retry registration in 54.852242ms if necessary  i0407 22:34:10.970118 29309 master.cpp:4394] ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress  i0407 22:34:10.970852 29306 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0407 22:34:11.010634 29306 leveldb.cpp:341] persisting action (360 bytes) to leveldb took 39.680272ms  i0407 22:34:11.010840 29306 replica.cpp:712] persisted action at 3  i0407 22:34:11.011014 29306 replica.cpp:697] replica learned append action at position 3  i0407 22:34:11.014020 29306 registrar.cpp:508] successfully updated the 'registry' in 89.684224ms  i0407 22:34:11.014181 29296 log.cpp:702] attempting to truncate the log to 3  i0407 22:34:11.014606 29296 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0407 22:34:11.015836 29298 replica.cpp:537] replica received write request for position 4 from (4798)@172.17.0.3:35855  i0407 22:34:11.016973 29296 master.cpp:4474] registered agent f59f9057a5c743e1b12996862e640a12s0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus():2; mem():1024; disk():4096; ports():[3100032000]  i0407 22:34:11.017518 29304 hierarchical.cpp:476] added agent f59f9057a5c743e1b12996862e640a12s0 (129e11060069) with cpus():2; mem():1024; disk():4096; ports():[3100032000] (allocated: )  i0407 22:34:11.017763 29311 slave.cpp:1116] registered with master master@172.17.0.3:35855; given agent id f59f9057a5c743e1b12996862e640a12s0  i0407 22:34:11.018362 29311 fetcher.cpp:81] clearing fetcher cache  i0407 22:34:11.018870 29311 slave.cpp:1139] checkpointing slaveinfo to '/tmp/masterallocatortest1rebalancedforupdatedweights9acaya/meta/slaves/f59f9057a5c743e1b12996862e640a12s0/slave.info'  i0407 22:34:11.018890 29307 statusupdatemanager.cpp:181] resuming sending status updates  i0407 22:34:11.019182 29304 hierarchical.cpp:1491] no resources available to allocate!  i0407 22:34:11.019304 29304 hierarchical.cpp:1165] performed allocation for agent f59f9057a5c743e1b12996862e640a12s0 in 1.077349ms  i0407 22:34:11.019493 29311 slave.cpp:1176] forwarding total oversubscribed resources   i0407 22:34:11.019726 29311 slave.cpp:3675] received ping from slaveobserver(112)@172.17.0.3:35855  i0407 22:34:11.019878 29299 master.cpp:4818] received update of agent f59f9057a5c743e1b12996862e640a12s0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources   i0407 22:34:11.020845 29305 hierarchical.cpp:534] agent f59f9057a5c743e1b12996862e640a12s0 (129e11060069) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():4096; ports( ):[3100032000], allocated: )  i0407 22:34:11.021005 29305 hierarchical.cpp:1491] no resources available to allocate!  i0407 22:34:11.021065 29305 hierarchical.cpp:1165] performed allocation for agent f59f9057a5c743e1b12996862e640a12s0 in 173907ns  i0407 22:34:11.022289 29278 containerizer.cpp:155] using isolation: posix/cpu,posix/mem,filesystem/posix  w0407 22:34:11.023422 29278 backend.cpp:66] failed to create 'bind' backend: bindbackend requires root privileges  i0407 22:34:11.026309 29309 slave.cpp:201] agent started on 112)@172.17.0.3:35855  i0407 22:34:11.026410 29309 slave.cpp:202] flags at startup: appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5""  cgroupscpue...",1,test
MESOS-5152,Add authentication to agent's /monitor/statistics endpoint,"operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per executor resource usage statistics.  since this endpoint is handled by the resourcemonitorprocess, i would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",2,test
MESOS-5153,Sandboxes contents should be protected from unauthorized users,"mesos 4956 introduced authentication support for the sandboxes. however, authentication can only go as far as to tell whether an user is known to mesos or not. an extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",8,test
MESOS-5155,Consolidate authorization actions for quota.,"we should have just a single authz action: updatequotawith_role. it was a mistake in retrospect to introduce multiple actions.    actions that are not symmetrical are register/teardown and dynamic reservations. the way they are implemented in this way is because entities that do one action differ from entities that do the other. for example, register framework is issued by a framework, teardown by an operator. what is a good way to identify a framework? a role it runs in, which may be different each launch and makes no sense in multi role frameworks setup or better a sort of a group id, which is its principal. for dynamic reservations and persistent volumes, they can be both issued by frameworks and operators, hence similar reasoning applies.     now, quota is associated with a role and set only by operators. do we need to care about principals that set it? not that much. ",5,test
MESOS-5156,Run mesos builds on PowerPC platform in ASF CI,this is the last step to declare official support for powerpc.    this is currently blocked on asf infra adding powerpc based jenkins machines to the asf ci.  ,1,test
MESOS-5157,Update webui for GPU metrics,"after adding the gpu metrics and updating the resources json to include gpu information, the webui should be updated accordingly.",1,test
MESOS-5159,Add test to verify error when requesting fractional GPUs,fractional gpu requests should immediately cause a task_failed without ever launching the task.,1,test
MESOS-5160,Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.,"currently there are no default `network` isolators for `mesoscontainerizer`. with the development of the `network/cni` isolator we have an interface to run mesos on multitude of ip networks. given that its based on an open standard (the cni spec) which is gathering a lot of traction from vendors (calico, weave, coreos) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",1,test
MESOS-5162,"Commit message hook behaves incorrectly when a message includes a ""*"".","if there is a ""\"" in a commit message (there often is when we have bulleted lists), due to the current use of echo $line, the $line gets expanded with a """" in it, which becomes a matcher in bash and therefore subsequently gets expanded into the list of files/directories in the current directory.    in order to avoid this mess, we need to wrap such variables in quotes, like so: echo ""$line"".",2,test
MESOS-5164,Add authorization to agent's /monitor/statistics endpoint.,"operators may want to enforce that only specific authorized users be able to view perexecutor resource usage statistics. for 0.29 mvp, we can make this coarsegrained, and assume that only the operator or a operatorprivileged monitoring service will be accessing the endpoint.  for a future release, we can consider finegrained authz that filters statistics like we plan to do for /tasks.",5,test
MESOS-5167,Add tests for `network/cni` isolator,we need to add tests to verify the functionality of `network/cni` isolator.,5,test
MESOS-5168,Benchmark overhead of authorization based filtering.,when adding authorization based filtering as outlined in mesos 4931 we need to be careful especially for performance critical endpoints such as /state.    we should ensure via a benchmark that performance does not degreade below an acceptable state.,3,test
MESOS-5169,Introduce new Authorizer Actions for Authorized based filtering of endpoints.,for authorization based endpoint filtering we need to introduce the authorizer actions outlined via mesos 4932.,3,test
MESOS-5170,Adapt json creation for authorization based endpoint filtering.,for authorization based endpoint filtering we need to adapt the json endpoint creation as discussed in mesos 4931.,5,test
MESOS-5171,Expose state/state.hpp to public headers,"we want the modules to be able to use replicated log along with the apis to communicate with zookeeper. this change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",3,test
MESOS-5172,Registry puller cannot fetch blobs correctly from some private repos.,"when the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. the error message is `unexpected http response '400 bad request' when trying to download the blob`. this may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",3,test
MESOS-5173,Allow master/agent to take multiple modules manifest files,"when loading multiple modules into master/agent, one has to merge all module metadata (library name, module name, parameters, etc.) into a single json file which is then passed on to the modules flag. this quickly becomes cumbersome especially if the modules are coming from different vendors/developers.    an alternate would be to allow multiple invocations of modules flag that can then be passed on to the module manager. that way, each flag corresponds to just one module library and modules from that library.    another approach is to create a new flag (e.g., modules dir) that contains a path to a directory that would contain multiple json files. one can think of it as an analogous to systemd units. the operator that drops a new file into this directory and the file would automatically be picked up by the master/agent module manager. further, the naming scheme can also be inherited to prefix the filename with an ""nn_"" to signify oad order.",3,test
MESOS-5174,Update the balloon-framework to run on test clusters,"there are a couple of problems with the balloon framework that prevent it from being deployed (easily) on an actual cluster:     the framework accepts 100% of memory in an offer.  this means the expected behavior (finish or oom) is dependent on the offer size.   the framework assumes the balloonexecutor binary is available on each agent.  this is generally only true in the build environment or in singleagent test environments.   the framework does not specify cpus with the executor.  this is required by many isolators.   the executor's task_finished logic path was untested and is flaky.   the framework has no metrics.   the framework only launches a single task and then exits.  with this behavior, we can't have useful metrics.  ",3,test
MESOS-5178,Add logic to validate for non-fractional GPU requests in the master,"we should not put this logic directly into the  'resources::validate()' function.  the primary reason is that the existing 'resources::validate()' function doesn't consider the semantics of any particular resource when performing its validation (it only makes sure that the fields in the 'resource' protobuf message are correctly formed). since a fractional 'gpus' resources is actually wellformed (and only semantically incorrect), we should push this validation logic up into the master.        moreover, the existing logic to construct a 'resources' object from a 'repeatedptrfield/' silently drops any resources that don't pass 'resources::validate()'. this means that if we were to push the nonfractional 'gpus' validation into 'resources::validate()', the 'gpus' resources would just be silently dropped rather than causing a task_error in the master. this is obviously not the desired behaviour.",2,test
MESOS-5179,Enhance the error message for Duration flag.,enhance the error message for  https:/github.com/apache/mesos/blob/4dfa91fc21f80204f5125b2e2f35c489f8fb41d8/3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp#l70 to list all of the supported duration unit.,1,test
MESOS-5180,Scheduler driver does not detect disconnection with master and reregister.,"the existing implementation of the scheduler driver does not reregister with the master under some network partition cases.    when a scheduler registers with the master:  1) master links to the framework  2) framework links to the master    it is possible for either of these links to break without the master changing.  (currently, the scheduler driver will only reregister if the master changes).    if both links break or if just link (1) breaks, the master views the framework as inactive and disconnected.  this means the framework will not receive any more events (such as offers) from the master until it reregisters.  there is currently no way for the scheduler to detect a oneway link breakage.    if link (2) breaks, it makes (almost) no difference to the scheduler.  the scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available.    to fix link breakages for (1+2) and (2), the scheduler driver should implement a `::exited` event handler for the master's pid and trigger a master (re)detection upon a disconnection. this in turn should make the driver (re)register with the master. the scheduler library already does this: https:/github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#l395    see the related issue mesos 5181 for link (1) breakage.",3,test
MESOS-5181,Master should reject calls from the scheduler driver if the scheduler is not connected.,"when a scheduler registers, the master will create a link from master to scheduler.  if this link breaks, the master will consider the scheduler inactive and mark it as disconnected.    this causes a couple problems:  1) master does not send offers to inactive schedulers.  but these schedulers might consider themselves ""registered"" in a oneway network partition scenario.  2) any calls from the inactive scheduler is still accepted, which leaves the scheduler in a starved, but semifunctional state.    see the related issue for more context: mesos 5180    there should be an additional guard for registered, but inactive schedulers here:  https:/github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#l1977    the http api already does this:  https:/github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#l459    since the scheduler driver cannot return a 403, it may be necessary to return a event::error and force the scheduler to abort.",1,test
MESOS-5199,The mesos-execute prints confusing message when launching tasks.,"  root@mesos002:/src/mesos/m2/mesos/build# src/mesosexecute master=192.168.56.12:5050 name=test dockerimage=ubuntu:14.04 command=""ls /root""  i0413 07:28:03.833521  2295 scheduler.cpp:175] version: 0.29.0  subscribed with id '3a1af11ecf664ce2826d48b3329779990001'  submitted task 'test' to agent '3a1af11ecf664ce2826d48b332977999 s0'  received status update taskrunning for task 'test'    source: sourceexecutor    reason: reasoncommandexecutorfailed <<<   received status update taskfinished for task 'test'    message: 'command exited with status 0'    source: sourceexecutor    reason: reasoncommandexecutor_failed <<<  root@mesos002:/src/mesos/m2/mesos/build#  ",1,test
MESOS-5212,Allow any principal in ReservationInfo when HTTP authentication is off,"mesos currently provides no way for operators to pass their principal to http endpoints when http authentication is off. since we enforce that reservationinfo.principal be equal to the operator principal in requests to /reserve, this means that when http authentication is disabled, the reservationinfo.principal field cannot be set.    to address this in the short term, we should allow reservationinfo.principal to hold any value when http authentication is disabled.",1,test
MESOS-5213,Operator endpoints should accept a principal without HTTP authentication,"mesos currently provides no way for operators to include their principal with http endpoint requests when http authentication is disabled. to remedy this, we should add optional principal fields to the relevant protobuf messages. when http authentication is enabled, we can allow the user to leave this field empty and populate it with the principal from their http auth header.",3,test
MESOS-5214,Populate FrameworkInfo.principal for authenticated frameworks,"if a framework authenticates and then does not provide a principal in its frameworkinfo, we currently allow this and leave frameworkinfo.principal unset. instead, we should populate frameworkinfo.principal for them automatically in that case to ensure that the two principals are equal.",2,test
MESOS-5215,Update the documentation for '/reserve' and '/create-volumes',there are a couple issues related to the principal field in diskinfo and reservationinfo (see linked jiras) that should be better documented. we need to help users understand the purpose of these fields and how they interact with the principal provided in the http authentication header. see linked tickets for background.,1,test
MESOS-5216,Document docker volume driver isolator.,"should include the followings:    1. what features (driver options) are supported in docker volume driver isolator.  2. how to use docker volume driver isolator.      related agent flags introduction and usage.      isolator dependency clarification (e.g., filesystem/linux).      related driver daemon preprocess.      volumes pre specified by users and volume cleanup.",5,test
MESOS-5221,Add Documentation for Nvidia GPU support,https:/reviews.apache.org/r/46220/,5,test
MESOS-5222,Create a benchmark for scale testing HTTP frameworks,it would be good to add a benchmark for scale testing the http frameworks wrt driver based frameworks. the benchmark can be as simple as trying to launch n tasks (parameterized) with the old/new api. we can then focus on fixing performance issues that we find as a result of this exercise.,3,test
MESOS-5227,Implement HTTP Docker Executor that uses the Executor Library,similar to what we did with the http command executor in mesos 3558 we should have a http docker executor that can speak the v1 executor api.,5,test
MESOS-5228,Add tests for Capability API.,add basic tests for the capability api.,3,test
MESOS-5232,Add capability information to ContainerInfo protobuf message.,"to enable support for capability as first class framework entity, we need to add capabilities related information to the containerinfo protobuf.",1,test
MESOS-5237,The windows version of `os::access` has differing behavior than the POSIX version.,"the posix version of os::access looks like this:      inline try/ access(const std::string& path, int how)   else     }    return true;  }      compare this to the windows version of os::access which looks like this following:      inline try/ access(const std::string& filename, int how)        return true;  }      as we can see, the case where errno is set to eacces is handled differently between the 2 functions.    we can actually consolidate the 2 functions by simply using the posix version. the challenge is that on posix, we should use ::access and ::access on windows. note however, that this problem is already solved, as we have an implementation of ::access for windows in 3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp which simply defers to ::_access.    thus, i propose to simply consolidate the 2 implementations.",2,test
MESOS-5238,CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest,observed on the mesosphere internal ci:      [22:56:28]w:     [step 10/10] f0420 22:56:28.056788   629 containerizer.cpp:1634] check failed: containers_.contains(containerid)      complete test log will be attached as a file.,2,test
MESOS-5239,Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.,"we recently added persistent volume support in dockercontainerizer (mesos3413). to understand the problem, we first need to understand how persistent volumes are supported in dockercontainerizer.    to support persistent volumes in dockercontainerizer, we bind mount persistent volumes under a container's sandbox ('containerpath' has to be relative for persistent volumes). when the docker container is launched, since we always add a volume (v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since docker does a 'rbind').    the assumption that the above works is that the docker daemon should see those persistent volume mounts that mesos mounts on the host mount table. it's not a problem if docker daemon itself is using the host mount namespace. however, on systemd enabled systems, docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this https:/github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a.    so what that means is that: in order for it to work, the parent mount of agent's workdir should be a shared mount when docker daemon starts. this is typically true on centos7, coreos as all mounts are shared mounts by default.    however, this causes an issue with the 'filesystem/linux' isolator. to understand why, first i need to show you a typical problem when dealing with shared mounts. let me explain that using the following commands on a centos7 machine:    [root@coredev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22  tmpfs tmpfs rw,seclabel,mode=755  [root@coredev run]# mkdir /run/netns  [root@coredev run]# mount bind /run/netns /run/netns  [root@coredev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22  tmpfs tmpfs rw,seclabel,mode=755  121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22  tmpfs tmpfs rw,seclabel,mode=755  [root@coredev run]# ip netns add test  [root@coredev run]# cat /proc/self/mountinfo  24 60 0:19 / /run rw,nosuid,nodev shared:22  tmpfs tmpfs rw,seclabel,mode=755  121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22  tmpfs tmpfs rw,seclabel,mode=755  162 121 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5  proc proc rw  163 24 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5  proc proc rw      as you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). this will confuse some systems sometimes. the reason is because when we create a self bind mount (/run/netns > /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.    the reason we need to do a self bind mount in mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. however, on some systems, mounts are private by default (e.g., ubuntu 14.04). in those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. for instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.    to avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a makeslave + makeshared so that the mount is its own shared mount peer group. in that way, any mounts underneath it will not be propagated back.    however, that operation will break the assumption that the persistent volume dockercontainerizer support makes. as a result, we're seeing problem with persistent volumes in dockercontainerizer when filesystem/linux isolator is turned on.",3,test
MESOS-5240,Command executor may escalate after the task is reaped.,"in command executor, escalated() may be scheduled before the task has been killed, i.e. reaped(), but called after. in this case escalated() should be a no op.",1,test
MESOS-5243,Remove '/system/stats.json' endpoint,the /system/stats.json endpoint was deprecated by mesos 2058. this endpoint can now be removed.,1,test
MESOS-5249,Update CMake files to reflect reorganized 3rdparty,nan,2,test
MESOS-5250,Move 3rdparty/libprocess/3rdparty/* to 3rdparty/,nan,5,test
MESOS-5253,Isolator cleanup should not be invoked if they are not prepared yet.,"if the mesos containerizer destroys a container in provisioning state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet.     in this case, there no need to clean up any isolator, call provisioner destroy directly.",2,test
MESOS-5254,Add URI parsing function/library,"the uri::fetcher theoretically supports all uris, per http:/tools.ietf.org/html/rfc3986.  to do this, we need a spec compliant parser from string to uri.    http:/uriparser.sourceforge.net/ appears to fit the bill.",2,test
MESOS-5255,Add GPUs to container resource consumption metrics.,"currently the usage callback in the nvidia gpu isolator is unimplemented:      src/slave/containerizer/mesos/isolators/cgroups/devices/gpus/nvidia.cpp      it should use functionality from nvml to gather the current gpu usage and add it to a resourcestatistics object. it is still an open question as to exactly what information we want to expose here (power, memory consumption, current load, etc.). whatever we decide on should be standard across different gpu types, different gpu vendors, etc.",3,test
MESOS-5256,Add support for per-containerizer resource enumeration,"currently the top level containerizer includes a static function for enumerating the resources available on a given agent. ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).    adding support for this will involve making the `containerizer::resources()` function virtual instead of static and then implementing it on a per containerizer basis.  we should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources.",3,test
MESOS-5257,Add autodiscovery for GPU resources,"right now, the only way to enumerate the available gpus on an agent is to use the `nvidiagpudevices` flag and explicitly list them out.  instead, we should leverage nvml to autodiscover the gpus that are available and only use this flag as a way to explicitly list out the gpus you want to make available in order to restrict access to some of them.",3,test
MESOS-5258,Turn the Nvidia GPU isolator into a module,"the nvidia gpu isolator has an external dependence on `libnvidiaml.so`. as it currently stands, this forces all binaries that link with `libmesos.so` to also link with `libnvidiaml.so` (including master, agents on machines without gpus, scheduler, exectors, etc.).    by turning the nvidia gpu isolator into a module, it will be loaded at runtime only when an agent has explicitly including the the nvidia gpu isolator in its `isolation` flag.",5,test
MESOS-5259,Refactor the mesos-fetcher binary to use the uri::Fetcher as a backend,this is an intermediate step for combining the mesos fetcher binary and uri::fetcher.      the download method should be replaced with uri::fetcher::fetch.  https:/github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/launcher/fetcher.cpp#l179    combining the two will:   attach the uri::fetcher to the existing fetcher caching logic.   remove some code duplication for downloading uris.,3,test
MESOS-5260,"Extend the uri::Fetcher::Plugin interface to include a ""fetchSize""","in order to replace the mesos fetcher binary with the uri::fetcher, each plugin must be able to determine/estimate the size of a download.  this is used by the fetcher cache when it creates cache entries and such.    the logic for each of the four fetcher::plugins can be taken and refactored from the existing fetcher.  https:/github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#l267",2,test
MESOS-5261,Combine the internal::slave::Fetcher class and mesos-fetcher binary,"after [mesos5259], the mesosfetcher will no longer need to be a separate binary and can be safely folded back into the agent process.  (it was a separate binary because libcurl has synchronous/blocking calls.)      this will likely mean:   a change to the fetch continuation chain:    https:/github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#l315   this protobuf can be deprecated (or just removed):    https:/github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/include/mesos/fetcher/fetcher.proto",3,test
MESOS-5263,pivot_root is not available on ARM,"when compile on arm, it will through error.  the current code logic in src/linux/fs.cpp is:      #ifdef nrpivotroot    int ret = ::syscall(nrpivotroot, newroot.cstr(), putold.cstr());  #elif x8664    / a workaround for systems that have an old glib but have a new    / kernel. the magic number '155' is the syscall number for    / 'pivotroot' on the x8664 architecture, see    / arch/x86/syscalls/syscall64.tbl    int ret = ::syscall(155, newroot.cstr(), putold.cstr());  #elif powerpc  powerpc64  ppc64    / a workaround for powerpc. the magic number '203' is the syscall    / number for 'pivotroot' on the powerpc architecture, see    / https:/w3challs.com/syscalls/?arch=powerpc64    int ret = ::syscall(203, newroot.cstr(), putold.cstr());  #else  #error ""pivot_root is not available""  #endif      possible sollution is to add `unistd.h` header",1,test
MESOS-5265,Update mesos-execute to support docker volume isolator.,the mesos execute needs to be updated to support docker volume isolator.,3,test
MESOS-5266,add test cases for docker volume driver,nan,5,test
MESOS-5271,Add alias support for Flags,"currently there is no support for a flag to have an alias. such support would be useful to rename/deprecate a flag.    for example, for mesos4386, we could let the flag have `authenticate` name and a `authenticate_frameworks` alias. the alias can be marked as deprecated (need to add support for this as well).    this support will also be useful for slave/agent flag rename. see mesos3781 for details.  ",5,test
MESOS-5272,Support docker image labels.,"docker image labels should be supported in unified containerizer, which can be used for applying custom metadata. image labels are necessary for mesos features to support docker in unified containerizer (e.g., for mesos gpu device isolator).",3,test
MESOS-5273,Need support for Authorization information via HELP.,we should add information about authentication to the help message and thereby endpoint documentation (similarly as mesos 4934 has done for authentication).,3,test
MESOS-5275,Add capabilities support for unified containerizer.,add capabilities support for unified containerizer.     requirements:  1. use the mesos capabilities api.  2. frameworks be able to add capability requests for containers.  3. agents be able to add maximum allowed capabilities for all containers launched.    design document: https:/docs.google.com/document/d/1yitift8tqla2vq3upqr7kriq_pqfkocosysqjrogc/edit#heading=h.rgfwelqrskmd  ,5,test
MESOS-5277,Need to add REMOVE semantics to the copy backend,"some dockerfiles run the `rm` command to remove files from the base image using the ""run"" directive in the dockerfile. an example can be found here:  https:/github.com/ngineered/nginxphpfpm.git    in the final rootfs the removed files should not be present. presence of these files in the final image can make the container misbehave. for example, the nginxphpfpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different html root. if the default nginx config is still present after the building the image, nginx will start pointing to a different html root than the one set in the dockerfile.      currently the copy backend cannot handle removal of files from intermediate layers. this can cause issues with docker images built using a dockerfile similar to the one listed here. hence, we need to add remove semantics to the copy backend.  ",5,test
MESOS-5286,Add authorization to libprocess HTTP endpoints,"now that the libprocesslevel http endpoints have had authentication added to them in mesos4902, we can add authorization to them as well. as a first step, we can implement a ""coarse grained"" approach, in which a principal is granted or denied access to a given endpoint. we will likely need to register an authorizer with libprocess.",5,test
MESOS-5294,Status updates after a health check are incomplete or invalid,"with command health checks enabled via marathon, mesosdns will resolve the task correctly until the task is reported as ""healthy"". at that point, mesosdns stops resolving the task correctly.    digging through src/docker/executor.cpp, i found that in the taskhealthupdated() function is attempting to copy the taskid to the new status instance with    status.mutabletaskid()>copyfrom(taskid);    but other instances of status updates have a similar line    status.mutabletaskid()>copyfrom(taskid.get());    my assumption is that this difference is causing the status update after a health check to not have a proper taskid, which in turn is causing an incorrect state.json output.    i'll try to get a patch together soon.    update:  none of the above assumption are correct. something else is causing the issue.",1,test
MESOS-5296,Split Resource and Inverse offer protobufs for V1 API,"the protobufs for the v1 api regarding inverse offers initially re used the existing offer / rescind / accept / decline messages for regular offers.  we should split these out the be more explicit, and provide the ability to augment the messages with particulars to either resource or inverse offers.",5,test
MESOS-5297,"Add authorization to the master's ""/flags"" endpoint.","coarse http endpoint authorization using the getendpointwith_path acl rule needs to be added to the ""/flags"" endpoint of the master.",3,test
MESOS-5301,Add synchronous validation for all types of Calls.,"currently, we do a best effort validation for all calls sent to the master from the scheduler by invoking validation::scheduler::call::validate(call, principal). this is a generic validation helper for all calls. however, for more fine grained validation for a particular call, we invoke the validation as part of the call handle itself.      option/ validationerror = roles::validate(frameworkinfo.role());      this in turn makes all validations asynchronous i.e. the framework gets them as event::error events later. it would be good if such validations can be handled while processing the call message itself synchronously.",5,test
MESOS-5302,Consider adding an Executor Shim/Adapter for the new/old API,"currently, all the business logic for http based command executor/driver based command executor lives in 2 different files. as more features are added/bugs are discovered in the executor itself, they need to be fixed in two places. it would be nice to have some kind of a shim/adapter that abstracts away the underlying library details from the executor. hence, the executor can toggle between whether it wants to use the driver or the new api via an environment variable.",5,test
MESOS-5303,Add capabilities support for mesos execute cli.,add support for `user` and `capabilities` to execute cli. this will help in testing the `capabilities` feature for unified containerizer.,3,test
MESOS-5304,/metrics/snapshot endpoint help disappeared on agent.,after https:/github.com/apache/mesos/commit/066fc4bd0df6690a5e1a929d3836e307c1e22586  the help for the /metrics/snapshot endpoint on the agent doesn't appear anymore (master endpoint help is unchanged).,1,test
MESOS-5307,Sandbox mounts should not be in the host mount namespace.,"currently, if a container uses container image, we'll do a bind mount of its sandbox (/ > /mnt/mesos/sandbox) in the host mount namespace.    however, doing the mounts in the host mount table is not ideal. that complicates both the cleanup path and the recovery path.    instead, we can do the sandbox bind mount in the container's mount namespace so that cleanup and recovery will be greatly simplified. we can setup mount propagation properly so that persistent volumes mounted at /xxx can be propagated into the container.    here is a simple proof of concept:    console 1:    vagrant@vagrantubuntutrusty64:/tmp/mesos$ ll .  total 12  drwxrwxrx 3 vagrant vagrant 4096 apr 25 16:05 ./  drwxrwxrx 6 vagrant vagrant 4096 apr 25 23:17 ../  drwxrwxrx 5 vagrant vagrant 4096 apr 25 23:17 slave/  vagrant@vagrantubuntutrusty64:/tmp/mesos$ ll slave/  total 20  drwxrwxrx  5 vagrant vagrant 4096 apr 25 23:17 ./  drwxrwxrx  3 vagrant vagrant 4096 apr 25 16:05 ../  drwxrwxrx  6 vagrant vagrant 4096 apr 26 21:06 directory/  drwxrxrx 12 vagrant vagrant 4096 apr 25 23:20 rootfs/  drwxrwxrx  2 vagrant vagrant 4096 apr 25 16:09 volume/  vagrant@vagrantubuntutrusty64:/tmp/mesos$ sudo mount bind slave/ slave/                                                                                                                                                                                                                              vagrant@vagrantubuntutrusty64:/tmp/mesos$ sudo mount makeshared slave/  vagrant@vagrantubuntutrusty64:/tmp/mesos$ cat /proc/self/mountinfo   50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered      console 2:    vagrant@vagrantubuntutrusty64:/tmp/mesos$ cd slave/  vagrant@vagrantubuntutrusty64:/tmp/mesos/slave$ sudo unshare m /bin/bash  root@vagrantubuntutrusty64:/tmp/mesos/slave# sudo mount makerslave .  root@vagrantubuntutrusty64:/tmp/mesos/slave# cat /proc/self/mountinfo  124 63 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  root@vagrantubuntutrusty64:/tmp/mesos/slave# mount rbind directory/ rootfs/mnt/mesos/sandbox/                                                                                                                                                                                          root@vagrantubuntutrusty64:/tmp/mesos/slave# mount rbind rootfs/ rootfs/  root@vagrantubuntutrusty64:/tmp/mesos/slave# mount t proc proc rootfs/proc                                                                                                                                                                                                              root@vagrantubuntutrusty64:/tmp/mesos/slave# pivot_root rootfs rootfs/tmp/.rootfs                                                                                                                                                                                                        root@vagrantubuntutrusty64:/tmp/mesos/slave# cd /  root@vagrantubuntutrusty64:/# cat /proc/self/mountinfo  126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  128 126 0:3 / /proc rw,relatime  proc proc rw      console 1:    agrant@vagrantubuntutrusty64:/tmp/mesos$ cd slave/  vagrant@vagrantubuntutrusty64:/tmp/mesos/slave$ sudo mount bind volume/ directory/v1  vagrant@vagrantubuntutrusty64:~/tmp/mesos/slave$ cat /proc/self/mountinfo  50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  129 50 8:1 /home/vagrant/tmp/mesos/slave/volume /home/vagrant/tmp/mesos/slave/directory/v1 rw,relatime shared:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered      console 2:    root@vagrantubuntutrusty64:/# cat /proc/self/mountinfo  126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a715b912e0f09b6 rw,data=ordered  128 126 0:3 / /proc rw,relatime  proc proc rw  132 127 8:1 /home/vagrant/tmp/mesos/slave/volume /mnt/mesos/sandbox/v1 rw,relatime shared:4 master:1  ext4 /dev/disk/byuuid/baf292e50bb64e588a71 5b912e0f09b6 rw,data=ordered  ",5,test
MESOS-5310,Enable `network/cni` isolator to allow modifications and deletion of CNI config,"currently the `network/cni` isolator can only load the cni configs at startup. this makes the cni networks immutable. from an operational standpoint this can make deployments painful for operators.     to make cni more flexible the `network/cni` isolator should be able to load configs at run time.     the proposal is to add an endpoint to the `network/cni` isolator, to which when the operator sends a put request the `network/cni` isolator will reload  cni configs. ",5,test
MESOS-5312,Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,"this is in the context of mesos containerizer (a.k.a., unified containerizer).    i did a simple test:    sudo sbin/mesosmaster workdir=/tmp/mesos/master  sudo glogv=1 sbin/mesosslave master=10.0.2.15:5050 isolation=docker/runtime,filesystem/linux workdir=/tmp/mesos/slave/ imageproviders=docker executorenvironmentvariables=""{}""  sudo bin/mesosexecute master=10.0.2.15:5050 name=test dockerimage=alpine command=""env""     mesosexecutorid=test  shlvl=1  mesoscheckpoint=0  mesosexecutorshutdowngraceperiod=5secs  libprocessport=0  mesosagentendpoint=10.0.2.15:5051  mesossandbox=/tmp/mesos/slave/slaves/2d7e44bb32824193bdc4eeab9e0943c2s0/frameworks/1a1cad182d8743dd97b61dbf2d2290610000/executors/test/runs/bb8dd72cfb4c426abe1851b0621339f6  mesosnativejavalibrary=/home/vagrant/dist/mesos/lib/libmesos0.29.0.so  mesosframeworkid=1a1cad182d8743dd97b61dbf2d2290610000  mesosslaveid=2d7e44bb32824193bdc4eeab9e0943c2s0  mesosnativelibrary=/home/vagrant/dist/mesos/lib/libmesos0.29.0.so  mesosdirectory=/tmp/mesos/slave/slaves/2d7e44bb32824193bdc4eeab9e0943c2s0/frameworks/1a1cad182d8743dd97b61dbf2d2290610000/executors/test/runs/bb8dd72cfb4c426abe18 51b0621339f6  pwd=/mnt/mesos/sandbox  mesosslavepid=slave(1)@10.0.2.15:5051      `mesos_sandbox` above should be `/mnt/mesos/sandbox`.",2,test
MESOS-5313,Failed to set quota and update weight according to document,"  root@mesos002:/test# curl d jsonmessagebody x post http:/192.168.56.12:5050/quota  failed to parse set quota request json 'jsonmessagebody': syntax error at line 1 near: jsonmessagebodyroot@mesos002:/test# cat jsonmessagebody     },    }]  }  root@mesos002:/test# curl d weight.json x put http:/192.168.56.12:5050/weights  failed to parse update weights request json ('weight.json'): syntax error at line 1 near: weight.js  root@mesos002:/test# cat weight.json      [        ,              ]      the right command should be adding @ before the quota json file jsonmessagebody.",1,test
MESOS-5316,Authenticate the agent's '/containers' endpoint.,the /containers endpoint was recently added to the agent. authentication should be enabled on this endpoint.,2,test
MESOS-5317,Authorize the agent's '/containers' endpoint.,"after the agent's /containers endpoint is authenticated, we should enabled authorization as well.",2,test
MESOS-5318,Make `os::close` always catch structured exceptions on Windows,nan,2,test
MESOS-5335,Add authorization to GET /weights.,"we already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. we should add authz around get operations on /weights.    easy option: getendpointwithpath /weights   pro: no new verb   con: all or nothing    complex option: getweightswithrole   pro: filters contents based on roles the user is authorized to see   con: more authorize calls (one per role in each /weights request)",3,test
MESOS-5336,Add authorization to GET /quota.,"we already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. we should add authz around get operations on /quota.",3,test
MESOS-5337,Add Master Flag to enable fine-grained filtering of HTTP endpoints.,"as the fine grained filtering of endpoints can the rather expensive, we should create a master flag to enable/disable this feature.",1,test
MESOS-5338,Add `user` to `Task` protobuf message.,the localauthorizer is supposed to use the os `user` under which tasks are running for authorization.  as the master keeps track of running and completed processes we need access to this information in task in order to authorize such tasks.,1,test
MESOS-5339,Create Tests for testing fine-grained HTTP endpoint filtering.,nan,3,test
MESOS-5343,Behavior of custom HTTP authenticators with disabled HTTP authentication is inconsistent between master and agent,"when setting a custom authenticator with httpauthenticators and also specifying authenticatehttp=false currently agents refuse to start with    a custom http authenticator was specified with the 'httpauthenticators' flag, but http authentication was not enabled via 'authenticatehttp'      masters on the other hand accept this setting.    having differing behavior between master and agents is confusing, and we should decide on whether we want to accept these settings or not, and make the implementations consistent.  ",3,test
MESOS-5345,Design doc for TASK_LOST_PENDING,"the tasklost task status describes two different situations: (a) the task was not launched because of an error (e.g., insufficient available resources), or (b) the master lost contact with a running task (e.g., due to a network partition); the master will kill the task when it can (e.g., when the network partition heals), but in the meantime the task may still be running.    this has two problems:  1. using the same task status for two fairly different situations is confusing.  2. in the partitionedbutstill running case, frameworks have no easy way to determine when a task has truly terminated.    to address these problems, we propose introducing a new task status, tasklostpending. if a framework opts into this behavior using a new capability, tasklost would mean ""the task is definitely not running"", whereas tasklostpending would mean ""the task may or may not be running (we've lost contact with the agent), but the master will try to shut it down when possible.""",5,test
MESOS-5347,Enhance the log message when launching mesos containerizer.,"log the launch flag which includes the executor command, pre launch commands and other information when launching the mesos containerizer. ",2,test
MESOS-5348,Enhance the log message when launching docker containerizer.,log the launch flag which includes the executor command and other information when launching the docker containerizer.,2,test
MESOS-5350,Add asynchronous hook for validating docker containerizer tasks,"it is possible to plug in custom validation logic for the mesoscontainerizer via an isolator module, but the same is not true of the dockercontainerizer.    basic logic can be plugged into the dockercontainerizer via hooks, but this has some notable differences compared to isolators:   hooks are synchronous.   modifications to tasks via hooks have lower priority compared to the task itself.  i.e. if both the taskinfo and slaveexecutorenvironmentdecorator define the same environment variable, the taskinfo wins.   hooks have no effect if they fail (short of segfaulting)  i.e. the slaveprelaunchdockerhook has a return type of try/:  https:/github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/include/mesos/hook.hpp#l90  but the effect of returning an error is a log message:  https:/github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/hook/manager.cpp#l227 l230    we should add a hook to the dockercontainerizer to narrow this gap.  this new hook would:   be called at roughly the same place as slaveprelaunchdockerhook  https:/github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/slave/containerizer/docker.cpp#l1022   return a future and require splitting up dockercontainerizer::launch.   prevent a task from launching if it returns a failure.",5,test
MESOS-5353,Use `Connection` abstraction to compare stale connections in scheduler library.,"previously, we had a bug in the connection abstraction in libprocess that hindered the ability to pass it onto defer callbacks since it could sometimes lead to deadlock (mesos 4658). now that it is resolved, we might consider not using uuid objects for stale connection checks but directly using the connection abstraction in the scheduler library.",3,test
MESOS-5356,Add Windows support for StopWatch,nan,2,test
MESOS-5359,The scheduler library should have a delay before initiating a connection with master.,"currently, the scheduler library src/scheduler/scheduler.cpp does have an artificially induced delay when trying to initially establish a connection with the master. in the event of a master failover or zk disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with tcp syn requests.     on a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. this compounds the issue further on the master.",3,test
MESOS-5360,Set death signal for dvdcli subprocess in docker volume isolator.,"if the slave crashes, we should kill the dvdcli subprocess. otherwise, if the dvdcli subprocess gets stuck, it'll not be cleaned up.",2,test
MESOS-5362,Add authentication to example frameworks,some example frameworks do not have the ability to authenticate with the master. adding authentication to the example frameworks that don't already have it implemented would allow us to use these frameworks for testing in authenticated/authorized scenarios.,2,test
MESOS-5365,Introduce a timeout for docker volume driver mount/unmount operation.,'dvdcli' might hang indefinitely. we should introduce timeout for both mount/unmount operation so that launch/cleanup are not blocked forever.,2,test
MESOS-5370,Add deprecation support for Flags,mesos 5271 adds support for a flag name to have an alias. this ticket captures the work need to add deprecation support. the idea is for the caller to explicitly specify deprecation via `flagsbase::add()`  and get a list of deprecation warnings when doing `flagsbase::load()`.,5,test
MESOS-5372,Add random() to os:: namespace ,"the function ""random()"" is not available in windows. after this improvement the calls to ""os::random()"" will result in calls to ""::random()"" on posix and ""::rand()"" on windows.  ",1,test
MESOS-5373,Remove `Zookeeper's` NTDDI_VERSION define,"zookeeper client library defines ntddiversion to 0x0400 in ""winconfig.h"". while this api level is suficient to compile the client library,  mesos have to use a newer api set. after this improvement the code will compile with the latest ntddiversion.     ",2,test
MESOS-5374,Add support for Console Ctrl handling in `slave.cpp`,extract supporting code to handle posix signals in a separate header and add support for ctrl handler when running on windows. ,3,test
MESOS-5375,Implement stout/os/windows/kill.hpp,implement equivalent functionality on windows ,5,test
MESOS-5378,Terminating a framework during master failover leads to orphaned tasks,"repro steps:    1) setup:    bin/mesosmaster.sh workdir=/tmp/master  bin/mesosslave.sh workdir=/tmp/slave master=localhost:5050  src/mesosexecute checkpoint command=""sleep 1000"" master=localhost:5050 name=""test""      2) kill all three from (1), in the order they were started.    3) restart the master and agent.  do not restart the framework.    result)   the agent will reconnect to an orphaned task.   the web ui will report no memory usage    curl localhost:5050/metrics/snapshot will say:  ""master/memused"": 128,    cause)   when a framework registers with the master, it provides a failovertimeout, in case the framework disconnects.  if the framework disconnects and does not reconnect within this failovertimeout, the master will kill all tasks belonging to the framework.    however, the master does not persist this failovertimeout across master failover.  the master will ""forget"" about a framework if:  1) the master dies before failovertimeout passes.  2) the framework dies while the master is dead.    when the master comes back up, the agent will reregister.  the agent will report the orphaned task(s).  because the master failed over, it does not know these tasks are orphans (i.e. it thinks the frameworks might re register).    proposed solution)  the master should save the frameworkid and failovertimeout in the registry.  upon recovery, the master should resume the failover_timeout timers.",3,test
MESOS-5380,Killing a queued task can cause the corresponding command executor to never terminate.,"we observed this in our testing environment. sequence of events:    1) a command task is queued since the executor has not registered yet.  2) the framework issues a killtask.  3) since executor is in registering state, agent calls `statusupdate(taskkilled, upid())`  4) `statusupdate` now will call `containerizer>status()` before calling `executor>terminatetask(status.taskid(), status);` which will remove the queued task. (introduced in this patch: https:/reviews.apache.org/r/43258).  5) since the above is async, it's possible that the task is still in queued task when we trying to see if we need to kill unregistered executor in `killtask`:          / todo(jieyu): here, we kill the executor if it no longer has        / any task to run and has not yet registered. this is a        / workaround for those single task executors that do not have a        / proper self terminating logic when they haven't received the        / task within a timeout.        if (executor>queuedtasks.empty())           6) consequently, the executor will never be terminated by mesos.    attaching the relevant agent log:    may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.640527  1342 slave.cpp:1361] got assigned task mesosvol.6ccd993c192011e6a7229648cb19afd6 for framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.641034  1342 slave.cpp:1480] launching task mesosvol.6ccd993c192011e6a7229648cb19afd6 for framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.641440  1342 paths.cpp:528] trying to chown '/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a' to user 'root'  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.644664  1342 slave.cpp:5389] launching executor mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 with resources cpus():0.1; mem():32 in work directory '/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a'  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.645195  1342 slave.cpp:1698] queuing task 'mesosvol.6ccd993c192011e6a7229648cb19afd6' for executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.645491  1338 containerizer.cpp:671] starting container '24762d432134475eb724caa72110497a' for executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework 'a3ad8418cb774705b3534b514ceca52c0000'  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.647897  1345 cpushare.cpp:389] updated 'cpu.shares' to 1126 (cpus 1.1) for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.648619  1345 cpushare.cpp:411] updated 'cpu.cfsperiodus' to 100ms and 'cpu.cfsquotaus' to 110ms (cpus 1.1) for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.650180  1341 mem.cpp:602] started listening for oom events for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.650718  1341 mem.cpp:722] started listening on low memory pressure events for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.651147  1341 mem.cpp:722] started listening on medium memory pressure events for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.651599  1341 mem.cpp:722] started listening on critical memory pressure events for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.652015  1341 mem.cpp:353] updated 'memory.softlimitinbytes' to 160mb for container 24762d432134475eb724caa72110497a  may 13 15:36:13 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:13.652719  1341 mem.cpp:388] updated 'memory.limitinbytes' to 160mb for container 24762d432134475eb724caa72110497a  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.508930  1342 slave.cpp:1891] asked to kill task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.509063  1342 slave.cpp:3048] handling status update taskkilled (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 from @0.0.0.0:0  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.509702  1340 disk.cpp:169] updating the disk resources for container 24762d432134475eb724caa72110497a to cpus():0.1; mem():32  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.510298  1343 mem.cpp:353] updated 'memory.softlimitinbytes' to 32mb for container 24762d432134475eb724caa72110497a  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.510349  1341 cpushare.cpp:389] updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.511102  1343 mem.cpp:388] updated 'memory.limitinbytes' to 32mb for container 24762d432134475eb724caa72110497a  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.511495  1341 cpushare.cpp:411] updated 'cpu.cfsperiodus' to 100ms and 'cpu.cfsquotaus' to 10ms (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.511715  1341 statusupdatemanager.cpp:320] received status update taskkilled (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.512032  1341 statusupdatemanager.cpp:824] checkpointing update for status update taskkilled (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.513849  1343 slave.cpp:3446] forwarding the update taskkilled (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 to master@10.0.5.79:5050  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.528929  1344 statusupdatemanager.cpp:392] received status update acknowledgement (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:25 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:25.529002  1344 statusupdatemanager.cpp:824] checkpointing ack for status update taskkilled (uuid: f9d159556c9a4a7398c397c0128510ba) for task mesosvol.6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.199105  1345 isolator.cpp:469] mounting docker volume mount point '/var/lib/rexray/volumes/jdeftest125/data' to '/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/data' for container 24762d432134475eb724caa72110497a  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.207062  1338 containerizer.cpp:1184] checkpointing executor's forked pid 5810 to '/var/lib/mesos/slave/meta/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/pids/forked.pid'  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.832330  1338 slave.cpp:2689] got registration for executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000 from executor(1)@10.0.2.74:46154  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.833149  1345 disk.cpp:169] updating the disk resources for container 24762d432134475eb724caa72110497a to cpus():0.1; mem():32  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.833804  1342 mem.cpp:353] updated 'memory.softlimitinbytes' to 32mb for container 24762d432134475eb724caa72110497a  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.833871  1340 cpushare.cpp:389] updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 15:36:28 ip100274.uswest2.compute.internal mesosslave[1304]: i0513 15:36:28.835160  1340 cpushare.cpp:411] updated 'cpu.cfsperiodus' to 100ms and 'cpu.cfsquotaus' to 10ms (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: 5804 'mesoslogrotatelogger help=false logfilename=/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/stdout logrotateoptions=rotate 9 logrotatepath=logrotate maxsize=2mb '  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: 5809 'mesoslogrotatelogger help=false logfilename=/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/stderr logrotateoptions=rotate 9 logrotatepath=logrotate maxsize=2mb '  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: 5804 'mesoslogrotatelogger help=false logfilename=/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/stdout logrotateoptions=rotate 9 logrotatepath=logrotate maxsize=2mb '  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: 5809 'mesoslogrotatelogger help=false logfilename=/var/lib/mesos/slave/slaves/a3ad8418cb774705b3534b514ceca52cs0/frameworks/a3ad8418cb774705b3534b514ceca52c0000/executors/mesosvol.6ccd993c192011e6a7229648cb19afd6/runs/24762d432134475eb724caa72110497a/stderr logrotateoptions=rotate 9 logrotatepath=logrotate maxsize=2mb '  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.374567 30993 slave.cpp:5498] recovering executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.420411 30990 statusupdatemanager.cpp:208] recovering executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.513164 30994 containerizer.cpp:467] recovering container '24762d432134475eb724caa72110497a' for executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.533478 30988 mem.cpp:602] started listening for oom events for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.534553 30988 mem.cpp:722] started listening on low memory pressure events for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.535269 30988 mem.cpp:722] started listening on medium memory pressure events for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.536198 30988 mem.cpp:722] started listening on critical memory pressure events for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.579385 30988 docker.cpp:859] skipping recovery of executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework 'a3ad8418cb774705b3534b514ceca52c0000' because it was not launched from docker containerizer  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.587158 30989 slave.cpp:4527] sending reconnect request to executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000 at executor(1)@10.0.2.74:46154  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.588287 30990 slave.cpp:2838] reregistering executor 'mesosvol.6ccd993c192011e6a7229648cb19afd6' of framework a3ad8418cb774705b3534b514ceca52c0000  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.589736 30988 disk.cpp:169] updating the disk resources for container 24762d432134475eb724caa72110497a to cpus():0.1; mem( ):32  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.590117 30990 cpushare.cpp:389] updated 'cpu.shares' to 102 (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.591284 30990 cpushare.cpp:411] updated 'cpu.cfsperiodus' to 100ms and 'cpu.cfsquotaus' to 10ms (cpus 0.1) for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.595403 30992 mem.cpp:353] updated 'memory.softlimitinbytes' to 32mb for container 24762d432134475eb724caa72110497a  may 13 16:58:30 ip100274.uswest2.compute.internal mesosslave[30985]: i0513 16:58:30.596102 30992 mem.cpp:388] updated 'memory.limitin_bytes' to 32mb for container 24762d432134475eb724caa72110497a  ",3,test
MESOS-5382,Implement os::fsync,nan,1,test
MESOS-5383,Implement os::setHostname,nan,1,test
MESOS-5386,Add `HANDLE` overloads for functions that take a file descriptor,nan,3,test
MESOS-5388,MesosContainerizerLaunch flags execute arbitrary commands via shell,"for example, the docker volume isolator's containerpath is appended (without sanitation) to a command that's executed in this manner. as such, it's possible to inject arbitrary shell commands to be executed by mesos.    https:/github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#l206    perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?",3,test
MESOS-5389,docker containerizer should prefix relative volume.container_path values with the path to the sandbox,docker containerizer currently requires absolute paths for values of volume.containerpath. this is inconsistent with the mesos containerizer which requires relative containerpath. it makes for a confusing api. both at the mesos level as well as at the marathon level.    ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.    /cc ,3,test
MESOS-5390,v1 Executor Protos not included in maven jar,according to mesos4793 the executor v1 http api was released in mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.    script to verify    wget https:/repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos0.28.1.jar && unzip lf mesos0.28.1.jar  wc  l  ,1,test
MESOS-5391,Add support for controlling resource limits in Mesos containerizer.,"currently, we dont have ability to control system resource limits. add support for :   frameworks to specify resource limits   operators to override default resource limits.",5,test
MESOS-5392,Design doc for adding resource limits support for Mesos containerizer,this will be the design doc for mesos 5391.,3,test
MESOS-5397,Slave/Agent Rename Phase 1: Update terms in the website,the following files need to be updated    site/source/index.html.md  ,1,test
MESOS-5398,Rewrite os::read() to be friendlier to reading binary files,"the existing read() implementation is based on calling getline() to  read in chunks of data from a file. this is fine for text based files,  but is a little strange for binary files.",3,test
MESOS-5399,Add utility for parsing ld.so.cache on linux.,the /etc/ld.so.cache file on linux contains a mapping of dynamic library names to their fully resolved paths for use by ld when linking.    we should write a utility that knows how to parse this file so we can find the paths to these libraries as well.  this is especially important for collecting libraries into a common location for supporting nvidia gpus in mesos.,5,test
MESOS-5400,Add preliminary support for parsing ELF files in stout.,"the upcoming nvidia gpu support for docker containers in mesos relies on consolidating all nvidia shared libraries into a common location for injecting a volume into a container.    as part of this, we need some preliminary parsing capabilities for elf file to infer things about each shared library we are consolidating.",5,test
MESOS-5401,Add ability to inject a Volume of Nvidia libraries/binaries into a docker-image container in mesos containerizer.,"in order to support nvidia gpus with docker containers in mesos, we need to be able to consolidate all nvidia libraries into a common volume and inject that volume into the container.    this tracks the support in the mesos containerizer. the docker containerizer support will be tracked separately.    more info on why this is necessary here: https:/github.com/nvidia/nvidia docker/",5,test
MESOS-5403,Introduce ObjectApprover Interface to Authorizer.,as outlined here (https:/docs.google.com/document/d/1fus79p8uj5pibycrblkjsbkotmeo8ezauinxxwia3qa) we plan to add the option of retrieving a filterobject from the authorizer with the goal of allowing for efficient authorization of a large number of (potentially large) objects. ,5,test
MESOS-5404,Allow `Task` to be authorized.,"as we need to be able to authorize `tasks` (e.g., for deciding whether to include them in the /state endpoint when applying authorization based filtering) we need to expose it to the authorizer. secondly we also need to include some additional information (`user` and `env variables`) in order to provide the authorizer  with meaning information.",3,test
MESOS-5405,Make fields in authorization::Request protobuf optional.,"currently authorization::request protobuf declares subject and object as required fields. however, in the codebase we not always set them, which renders the message in the uninitialized state, for example:    https:/github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#l603    https:/github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#l2057    i believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. however, they are still invalid protobuf messages. moreover, some external authorizers may serialize these messages.    we can either ensure all required fields are set or make both subject and object fields optional. this will also require updating local authorizer, which should properly handle the situation when these fields are absent. we may also want to notify authors of external authorizers to update their code accordingly.    it looks like no deprecation is necessary, mainly because we alreadyerroneously!treat these fields as optional.",3,test
MESOS-5406,Validate ACLs on creating an instance of local authorizer.,"some combinations of acls are not allowed, for example, specifying both setquota and updatequota. we should capture such issues and error out early.     this ticket aims to add as many validations as possible to a dedicated validate() routine, instead of having them implicitly in the codebase.",3,test
MESOS-5408,Delete the /observe HTTP endpoint,"the ""/observe"" endpoint was introduced a long time ago for supporting functionality that was never implemented. we should just kill this endpoint and associated code to avoid tech debt.",2,test
MESOS-5413,`network/cni` isolator should skip the bind mounting of the CNI network information root directory if possible,"currently in the create() method `network/cni` isolator, for the cni network information root directory (i.e., /var/run/mesos/isolators/network/cni), we do a self bind mount and make sure it is a shared mount of its own peer group. however, we should not do a self bind mount if the mount containing the cni network information root directory is already a shared mount in its own share peer group, just like what we did for `filesystem/linux` isolator in  https:/issues.apache.org/jira/browse/mesos5239.",3,test
MESOS-5419,Document all known client libraries for the Scheduler/Executor API,"previously during various community syncs, we had decided that we would only be supporting the c scheduler/executor library in the mesos code base going forward. we should however, still document the client libraries available in various languages to drive adoption/have a recommended list for users to look up.    this can be similar to the already existing frameworks doc: http:/mesos.apache.org/documentation/latest/frameworks/    other projects also seem to have been following a similar practice:  https:/docs.docker.com/engine/reference/api/remoteapiclient_libraries/  https:/github.com/kubernetes/kubernetes/blob/master/docs/devel/client libraries.md",2,test
MESOS-5420,Implement os::exists for processes,"os::exists returns true if the process identified by the parameter is still running or was running and we are able to get information about it, such us the exit code. in windows after obtaining a handle to the process it is possible perform those operations. ",1,test
MESOS-5425,Consider using IntervalSet for Port range resource math,followup jira for comments raised in mesos3051 (see comments there).    we should consider utilizing https:/github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/3rdparty/stout/include/stout/interval.hpp in https:/github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/src/common/values.cpp#l143.,3,test
MESOS-5426,Relax version compatibility requirement for some modules,"some module interfaces such as authenticatee, have not changed for a while and so we should be able to relax the version compatibility checks. this needs to be done on a casebycase basis.    i am also hoping, this change will also provide a framework for updating the version requirement for other modules as we go towards a stable module api.    [cc: [adam mesos] [tillt] ]",5,test
MESOS-5435,Add default implementations to all Isolator virtual functions,"currently, all of the virtual functions in `mesos::slave::isolator` are pure virtual (expect status()). for many isolators, however, it doesn't make sense to implement all of these virtual functions. each isolator has to provide its own default implementation of these functions even if they aren't really relying on them. this adds unnecessary extra code to many isolators that don't need them.    moreover, the `mesosisolatorprocess` has the same problem for each of its virtual functions.    we should provide defaults for these instead of making each and every isolator implement even in cases when it doesn't make sense.",1,test
MESOS-5436,GPU resource broke framework data table in webUI,"in agent_framework.html and master/static/agent.html, we add gpus (used / allocated) in table header. but we didn't add the corresponding column to the table body as well.    on the other hand, we didn't provide statistics for gpus on monitor endpoints.  to provide those data in webui, it requires we implement gpus statistics in monitor endpoints firstly. ",1,test
MESOS-5437,AppC  appc_simple_discovery_uri_prefix is lost in configuration.md,appc  appcsimplediscoveryuriprefix is lost in configuration.md,1,test
MESOS-5445,Allow libprocess/stout to build without first doing `make` in 3rdparty.,"after the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",2,test
MESOS-5450,Make the SASL dependency optional.,"right now there is a hard dependency on sasl, which probably won't work well on windows (at least) in the near future for our use cases.    in the future, it would be nice to have a pluggable authentication layer.",2,test
MESOS-5452,Agent modules should be initialized before all components except firewall.,"on mesos agents anonymous modules should not have any dependencies, by design, on any other mesos components. this implies that anonymous modules should be initialized before all other mesos components other than `firewall`. the dependency on `firewall` is primarily to enforce any policies to secure endpoints that might be owned by the anonymous module.",1,test
MESOS-5453,CNI should not store subnet of address in NetworkInfo,"when the cni isolator executes the cni plugin, that cni plugin will return an ip address and subnet (192.168.0.1/32). mesos should strip the subnet before storing the address in the task.networkinfo.ipaddress.    reason being  most current mesos components are not expecting a subnet in the task's networkinfo.ipaddress, and instead expect just the ip address. this can cause errors in those components, such as mesosdns failing to return a networkinfo address (and instead defaulting to the next configured ipsource), and marathon generating invalid links to tasks (as it includes /32 in the link)",2,test
MESOS-5456,Master anonymous modules should initialized before any other components.,"anonymous modules on the master are by design supposed to be independent of any mesos components. however, there might be a dependency in the reverse direction. for e.g., anonymous modules might want to influence the behavior of mesos components (say by generating configuration, that might be consumed later by the components).     the anonymous modules on the master therefore need to be initialized before other mesos components. ",1,test
MESOS-5459,Update RUN_TASK_WITH_USER to use additional metadata,"currently, the `authorization::action` `runtaskwithuser` will pass the user as its `object.value` string, but some authorizers may want to make authorization decisions based on additional task attributes, like role, resources, labels, container type, etc.    we should create a new action `runtask` that passes frameworkinfo and taskinfo in its object, and the localauthorizer's runtaskwithuser acl can be implemented using the user found in taskinfo/frameworkinfo.  we may need to leave the old withuser action around, but it's arguable whether we should call the authorizer once for runtask and once for runtaskwithuser, or only use the new action and deprecate the old one?",5,test
MESOS-5469,Remove hard-coded principals in `PersistentVolumeEndpointsTest.SlavesEndpointFullResources`,"in the test persistentvolumeendpointstest.slavesendpointfullresources, the value testprincipal is hardcoded into the json strings expected in http responses. it would be more durable to use default_credential.principal() instead.",1,test
MESOS-5470,Confirm errors in authorized persistent volume tests,the tests persistentvolumetest.badacldropcreateanddestroy and persistentvolumetest.badaclnoprincipal check for a failed destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation. we should also explicitly check that the operation did not succeed due to failed authorization.,1,test
MESOS-5471,Enable `Option` to handle string literals gracefully,"in flagsbase::add, mesos 5064 begins making use of template function parameters like t2 for the default flag value rather than option/&. this is because in some places in the code base, we pass string literals for this argument. if an option type is used, the compiler infers a char [x] type for t2, which breaks option::getorelse, which attempts to return that same type, since returning arrays is disallowed.    to fix this, we could employ std::decay, which would convert a return type of char [x] into const char .",2,test
MESOS-5531,Re-enable style-check for stout.,"after the 3rdparty reorg, the mesos style checker stopped checking stout.",1,test
MESOS-5532,Maven build is too verbose for batch builds,"during a noninteractive (without terminal) mesos build, maven generates several thousands of log lines when downloading artifacts. this often makes several webbased log viewers unresponsive.    further, these several thousand line long progress indicator logs don't provide any meaningful information either. from a user's point of view, just knowing that the artifact download succeeded/failed is often enough.    we should be using 'batch mode' flag to disable these additionals log lines.",1,test
MESOS-5537,http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds,"i'm writing a controller in go to monitor heartbeats. i'd like to use the interval as communicated by the master, which should be specified in the subscribed event. but it's not.      2016/06/03 18:34:04 ,heartbeatintervalseconds:nil,} offers:nil rescind:nil update:nil message:nil failure:nil error:nil}        $ dpkg l |grep e mesos  ii  mesos                               0.28.02.0.16.ubuntu1404         amd64        cluster resource manager with efficient resource isolation      i am seeing heartbeat events. just not seeing the interval specified in the subscribed event.",1,test
MESOS-5549,Document aufs provisioner backend.,we should update container image.md with the newly supported backend.,2,test
MESOS-5550,Remove Nvidia GPU Isolator's link-time dependence on `libnvidia-ml`,"the current nvidia gpu isolator has a dependence on `libnvidiaml`, and as such, pulls a hard dependence on this library into `libmesos`. the consequence of this is that any process that relies on `libmesos` has to have `libnvidiaml` available as well, even on machines where no gpus are available.  since this library is not easily installable through standard package managers, having such a hard dependence can be burdensome.    this ticket proposes to pull in `libnvidiaml` as a runtime dependence instead of a link time dependence. as such, only machines that actually have gpus installed and would like to rely on this library need to have it installed.",2,test
MESOS-5551,Move the Nvidia GPU isolator from `cgroups/devices/gpu/nvidia` to `gpu/nvidia`,"currently, the nvidia gpu isolator lives in `src/slave/containerizers/mesos/isolators/cgroups/devices/gpu/nvidia`. however, in the future this isolator will do more than simply isolate gpus using the cgroups devices subsystem (e.g. volume management for injecting machine specific nvidia libraries into a container). for this reason, we should preemptively move this isolator up to `src/slave/containerizers/mesos/isolators/gpu/nvidia`. as part of this, we should update the string we pass to the `isolator` agent flag to reflect this.",2,test
MESOS-5552,Bundle NVML headers for Nvidia GPU support.,"currently, we rely on a script to install the nvidia gdk as a build dependence for building mesos with nvidia gpu support.    a previous ticket removed the mesos build dependence on `libnvidiaml` which comes as part of the gdk. this ticket proposes bundling the nvml headers with mesos in order to completely remove the build dependence on the gdk.    with this change it will be much simpler to configure and build with nvidia gpu support.  all that will be required is:    ../configure enablenvidiagpusupport  make  j    ",1,test
MESOS-5554,Change major/minor device types for Nvidia GPUs to `unsigned int`,"currently, the gpu struct specifies the type of its `major` and `minor` fields as `devt`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. these macros return an `unsigned int` when handed a `devt`, so it makes sense for these fields to be of that type instead.",1,test
MESOS-5555,Always provide access to NVIDIA control devices within containers (if GPU isolation is enabled).,"currently, access to `/dev/nvidiactl` and `/dev/nvidiauvm` is only granted to / revoked from a container as gpus are added and removed from them. on some level, this makes sense because most jobs don't need access to these devices unless they are also using a gpu. however, there are cases when access to these files is appropriate, even when not making use of a gpu. running `nvidiasmi` to control the global state of the underlying nvidia driver, for example.        we should add `/dev/nvidiactl` and `/dev/nvidiauvm` to the default whitelist of devices to include in every container when the `gpu/nvidia` isolator is enabled. this will allow a container to run standard nvidia driver tools (such as `nvidiasmi`) without failing with abnormal errors when no gpus have been granted to it. as such, these tools will now report that no gpus are installed instead of failing abnormally.",3,test
MESOS-5556,"Fix method of populating device entries for `/dev/nvidia-uvm`, etc.","currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidiauvm` are hardcoded. this causes problems for `/dev/nvidia uvm` because its major number is part of the ""experimental"" device range on linux.    because this range is experimental, there is no guarantee which device  number will be assigned to it on a given machine.  we should use `os:stat::rdev()` to extract the major/minor numbers programatically.",2,test
MESOS-5557,Add `NvidiaGpuAllocator` component for cross-containerizer GPU allocation,we need some way of allocating gpus from a centralized location to allow both the mesos containerizer and the docker containerizer to pull from central pool.  we propose to build a `nvidiagpuallocator` for this purpose.        this component should also be overloaded to do resource enumeration of gpus based on the agent flags. this keeps all code for enumerating gpus and the resources they represent in a single centralized location.,5,test
MESOS-5558,Update `Containerizer::resources()` to use the `NvidiaGpuAllocator`,"with the introduction of the shared `nvidiagpuallocator` component, `containerizer::resources()` should be updated to use it.",2,test
MESOS-5559,Integrate the `NvidiaGpuAllocator` into the `NvidiaGpuIsolator`,nan,3,test
MESOS-5561,"Need to remove references to ""messages/messages.hpp"" from `State` API",in order to expose the `state` api for using replicated log in mesos modules it is necessary that the `state` api does not reference headers that are not exposed as part of the mesos installation.     currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `state` api unusable in a module.     we need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. this will help us remove references to messages.hpp from the `state` api.,2,test
MESOS-5562,Add class to share Nvidia-specific components between containerizers,"once we have an `nvidiagpuallocator` component, we need some way to share it across multiple containerizers.  moreover, we anticipate needing other nvidia components to share across multiple containerizers as well (e.g. an `nvidiavolumemanager` component). as such, we should add a wrapper class around these components to make it easily passable to each containerizer without having to continually add a bunch of parameters to the containerizer interface.",2,test
MESOS-5563,Rearrange Nvidia GPU files to cleanup semantics for header inclusion.,"currently, components outside of `src/slave/containerizers/mesos/isolators/gpu` have to protect their #includes for certain nvidia header files with the enablenvidiagpu_support flag. other headers strictly could not be wrapped in this flag.        we need to clean up this header madness, by creating a common ""nvidia.hpp"" header that takes care of all the dependencies. all componenents outside of `src/slave/containerizers/mesos/isolators/gpu`  should only need to #include this one header instead of managing everything themselves.",1,test
MESOS-5564,Document common use cases of authorization,"our authorization documentation covers the existing functionality, but it doesn't provide a practical how to guide to help users accomplish common authorized use cases. for example, a user recently reported that to gain full use of the web ui after upgrading to mesos 1.0, six new acl rules needed to be added: getendpoints, viewframeworks, viewtasks, viewexecutors,  accesssandboxes, and accessmesoslogs. rather than expecting users to figure this out on their own, we should document the acls needed to accomplish a common goal like this.    similarly, authorizing a stateful framework to accomplish the actions it would usually be expected to perform would involve setting rules for registerframeworks, runtasks, shutdownframeworks, reserveresources, unreserveresources, createvolumes, and destroyvolumes.",1,test
MESOS-5570,Improve CHANGELOG and upgrades.md,currently we have a lot of data duplication between the changelog and upgrades.md. we should try to improve this and potentially make the changlog a markdown file as well. for inspiration see the hadoop changelog: https:/github.com/apache/hadoop/blob/2e1d0ff4e901b8313c8d71869735b94ed8bc40a0/hadoopcommonproject/hadoop common/src/site/markdown/release/1.2.0/changes.1.2.0.md  ,3,test
MESOS-5576,Masters may drop the first message they send between masters after a network partition,"we observed the following situation in a cluster of five masters:   master 1  master 3  master 5  partitioned from cluster by downing this vm's network  elected leader by zk  voting  suicides due to lost leadership  3  replies to leader  replies to leader    performs writing  acks to leader  still down  5  follower  follower    leader  follower  comes back up  7  follower  follower    follower  follower  10  performs consensus  replies to leader    still down  acks to leader  12  leader  follower     master 2 sends a series of messages to the recentlyrestarted master 5.  the first message is dropped, but subsequent messages are not dropped.    this appears to be due to a stale link between the masters.  before leader election, the replicated log actors create a network watcher, which adds links to masters that join the zk group:  https:/github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/network.hpp#l157l159    this link does not appear to break (master 2 > 5) when master 5 goes down, perhaps due to how the network partition was induced (in the hypervisor layer, rather than in the vm itself).    when master 2 tries to send an promiserequest to master 5, we do not observe the https:/github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/replica.cpp#l493l494    instead, we see a log line in master 2:    process.cpp:2040] failed to shutdown socket with fd 27: transport endpoint is not connected      the broken link is removed by the libprocess socket_manager and the following writerequest from master 2 to master 5 succeeds via a new socket.",5,test
MESOS-5577,Modules using replicated log state API require zookeeper headers,the state api uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during mesos installation. ,1,test
MESOS-5578,Support static address allocation in CNI,"currently a framework can't specify a static ip address for the container when using the network/cni isolator.    the `ipaddress` field in the `networkinfo` protobuf was designed for this specific purpose but since the cni spec does not specify a means to allocate an ip address to the container the `network/cni` isolator cannot honor this field even when it is filled in by the framework.    creating this ticket to act as a place holder to track this limitation. as and when the cni spec allows us to specify a static ip address for the container, we can resolve this ticket. ",1,test
MESOS-5579,Support static IP address allocation with `DockerContainerizer`,"docker run supports the `ip` option to allocate a specific ipv4 address to the container. also, the `networkinfo` protobuf has an `ipaddress` field that all frameworks to specify an ip address for the container. the docker executor should therefore invoke the `docker run` command with the ip option whenever the `ipaddress` field of the `networkinfo` is set allowing frameworks to try and assign a static ip address for their services.",1,test
MESOS-5580,Implement authn/authz for the network/cni isolator,currently any framework can launch containers on any cni network irrespective of its role and principal. we need perform authn/authz in the network/cni isolator (or master) to make sure that only roles/principals specified by the operator can launch containers on a given network. ,3,test
MESOS-5581,Guarantee ordering between Isolators,"some isolators depend on other isolators. however, we currently do not have a generic method of expressing these dependencies. we special case the `filesystem/ ` isolators to make sure that dependencies on them are satisfied, but no other dependencies can be expressed.        instead, we should use a vector to represent the pairing of isolator name to isolator creator function. this way, the relative dependencies between each isolator will be implicit in the ordering of the vector. currently, a hashmap is used to hold this pairing, but this is inadequate because hashmaps are inherently unordered. the new implementation using a vector will ensure everything is processed in the order it is listed.",3,test
MESOS-5582,Create a `cgroups/devices` isolator.,"currently, all the logic for the `cgroups/devices` isolator is bundled into the nvidia gpu isolator. we should abstract it out into it's own component and remove the redundant logic from the nvidia gpu isolator. assuming the guaranteed ordering between isolators from mesos 5581, we can be sure that the dependency order between the `cgroups/devices` and `gpu/nvidia` isolators is met.",2,test
MESOS-5583,Improve authorization documentation when setting permissive flag.,"a common problem for a users starting to use acls is that once they set `permisse = false` and not add acls allowing common operations (e.g., register_framework) their mesos cluster don't  behave as expected. ",1,test
MESOS-5588,Improve error handling when parsing acls.,"during parsing of the authorizer errors are ignored. this can lead to undetected security issues.    consider the following acl with an typo (usr instead of user)       ""viewframeworks"": [                    ,                      ""usr"":                     }                  ]      when the master is started with these flags it will interprete the acl int he following way which gives any principal access to any framework.      viewframeworks   }  ",5,test
MESOS-5592,Pass NetworkInfo to CNI Plugins,"mesos has adopted the container network interface as a simple means of networking mesos tasks launched by the unified containerizer. the cni specification covers a minimum feature set, granting the flexibility to add customized networking functionality in the form of agreements made between the orchestrator and cni plugin.    this proposal is to pass networkinfo.labels to the cni plugin by injecting it into the cni network configuration json during plugin invocation.    design doc on this change: https:/docs.google.com/document/d/1rxrucccjqpppsqxqrztbhfvnnw6cgq2otieyamwl284/edit?usp=sharing    reviewboard: https:/reviews.apache.org/r/48527/",3,test
MESOS-5597,"Document Mesos ""health check"" feature",we don't talk about this feature at all.,5,test
MESOS-5605,Improve documentation for using persistent volumes. ,when using persistent volumes at a arangodb we ran into a few pitfalls.  we should document them in order for others to avoid those issues.,2,test
MESOS-5609,Put initial scaffolding in place for implementing SUBSCRIBE call on v1 Master API.,"as discussed on mesos5498, this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the api/v1 operator api endpoint. other events/support for snapshots would be done as part of mesos5498.",5,test
MESOS-5618,Added a metric indicating if replicated log for the registrar has recovered or not.,this gives operator insight about the state of the replicated log for registrar. the operator needs to know when it is safe to move on to another master in the upgrade orchestration pipeline.  ,3,test
MESOS-5629,Agent segfaults after request to '/files/browse',"we observed a number of agent segfaults today on an internal testing cluster. here is a log excerpt:    jun 16 17:12:28 ip1010087 mesosslavehttps:/reviews.apache.org/r/48563/ and https:/reviews.apache.org/r/48566/, which were done to repair a different https:/issues.apache.org/jira/browse/mesos5587 on the master and agent.    thanks go to https:/github.com/apache/mesos/blob/master/src/slave/slave.cpp#l5737l5745, where use of defer() may be necessary to keep execution in the correct context.",3,test
MESOS-5630,Change build to always enable Nvidia GPU support for Linux,see summary,2,test
MESOS-5634,Add Framework Capability for GPU_RESOURCES,"due to the scarce resource problem described in mesos5377, we plan to introduce a gpu_resources framework capability. this capability will allow the mesos allocator to make better decisions about which frameworks should receive resources from gpu capable machines.  in essence, the allocator will only allocate resources from gpu capable machines to frameworks that have this capability. this is necessary to prevent nongpu workloads from filling up the gpu machines and preventing gpu workloads to run.",3,test
MESOS-5638,Check all omissions of 'defer' for safety,"when registering callbacks with .then, .onany, etc., we sometimes omit defer() in cases where it's deemed safe; for example, when the callback uses no process state and thus could be executed in an arbitrary context. because of recent bugs due to the unsafe omission of defer(), we should do a sweep of the codebase for all such occurrences and evaluate their safety. we should also consider using defer() consistently in all such cases, as our https:/github.com/apache/mesos/tree/master/3rdparty/libprocess#defer recommends.",5,test
MESOS-5639,Add documentation about metadata for CNI plugins.,we need to document the behavior implemented in mesos 5592.,2,test
MESOS-5646,Build `network/cni` isolator with `libnl` support,"currently, the `network/cni` isolator does not have the ability to collect network statistics for containers launched on a cni network. we need to give the `network/cni` isolator the ability to query interfaces, route tables and statistics in the containers network namespace. to achieve this the `network/cni` isolator will need to talk `netlink`.    for enabling `netlink` api we need the `network/cni` isolator to be built with libnl support. ",3,test
MESOS-5647,Expose a statistics endpoint on the `network/cni` isolator.,we need a statistics endpoint in the `network/cni` isolator to expose metrics relating to a containers network traffic.     on receiving a request for a given container the `network/cni` isolator could use netlink system calls to query the kernel for interface and routing statistics for a given container's network namespace.,5,test
MESOS-5649,Build an example framework to consume GPUs,this framework should show how to build a gpu capable framework that can accept offers with gpus and launch tasks that use them.,3,test
MESOS-5650,UNRESERVE operation causes master to crash.,"reserve operation may cause a master failure:    i0619 05:02:02.298602 11194 http.cpp:312] http get for /master/slaves from 172.17.0.4:49617 with useragent='pythonrequests/2.9.1'  i0619 05:02:02.305542 11193 http.cpp:312] http post for /master/destroyvolumes from 172.17.0.4:49618 with useragent='pythonrequests/2.9.1'  i0619 05:02:02.306731 11191 master.cpp:6560] sending checkpointed resources mem(kafkatestrole, kafkatestprincipal, ):256; cpus(kafkatestrole, kafkatestprincipal, ):1.5; mem(kafkatestrole, kafkatestprincipal, ):2304; ports(kafkatestrole, kafkatestprincipal, ):[93059305, 1159611596]; cpus(kafkatestrole, kafkatestprincipal, ):0.5; disk(kafkatestrole, kafkatestprincipal, )[]:11204 to slave a80ff9dde04643abb76328365b136f6bs0 at slave(1)@10.0.0.5:5051 (10.0.0.5)  i0619 05:02:02.311069 11189 http.cpp:312] http post for /master/destroyvolumes from 172.17.0.4:49619 with useragent='pythonrequests/2.9.1'  i0619 05:02:02.312191 11187 master.cpp:6560] sending checkpointed resources cpus(kafkatestrole, kafkatestprincipal, ):1.5; mem(kafkatestrole, kafkatestprincipal, ):2304; ports(kafkatestrole, kafkatestprincipal, ):[96929692, 1182411824]; cpus(kafkatestrole, kafkatestprincipal, ):0.5; mem(kafkatestrole, kafkatestprincipal, ):256; disk(kafkatestrole, kafkatestprincipal, )[]:11204 to slave 489aa72fae074383a56f6fe9346ace37s7 at slave(1)@10.0.0.7:5051 (10.0.0.7)  i0619 05:02:02.316118 11189 http.cpp:312] http get for /master/slaves from 172.17.0.4:49620 with useragent='pythonrequests/2.9.1'  i0619 05:02:02.321527 11189 http.cpp:312] http post for /master/unreserve from 172.17.0.4:49621 with useragent='pythonrequests/2.9.1'  i0619 05:02:02.323523 11193 master.cpp:6560] sending checkpointed resources  to slave a80ff9dde04643abb76328365b136f6bs0 at slave(1)@10.0.0.5:5051 (10.0.0.5)  i0619 05:02:02.327658 11191 http.cpp:312] http post for /master/unreserve from 172.17.0.4:49622 with useragent='python requests/2.9.1'  f0619 05:02:02.329208 11190 sorter.cpp:284] check failed: total_.scalarquantities.contains(oldslavequantity)      possible reasons:   recent improvements in allocator (b4d746f)   bug in bookkeeping during the previous unreserve    network partition that happened after reserve and before unreserve",5,test
MESOS-5657,Executors should not inherit environment variables from the agent.,"currently executors are inheriting environment variables form the slave in mesos containerizer. this is problematic, because of two reasons:    1. when we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because lang and/or lc  environment variables are not set correctly.    2. when we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.    depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. instead, users should specify all environment variables they need by setting the slave flag `executorenvironment_variables` as a json format.",3,test
MESOS-5659,Design doc for TASK_UNREACHABLE,see mesos 4049.,5,test
MESOS-5660,ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited,a recent change forbits the executor to inherit environment variables from the agent's environment. as a regression this break containerizertest.rootcgroupsballoonframework.,2,test
MESOS-5661,Use snake casing for flag names consistently,"historically, we have always used snake casing for the flag variables e.g., dockerconfig etc. however, there are some instances in our .cpp code where we define the flag name in the .cpp file in camel case e.g., modulesdir but still have the flag name as modulesdir when taking arguments from the user. it would be good to audit all such occurrences and consistently uses snake casing in our .cpp/.hpp files everywhere.",1,test
MESOS-5663,Remove hard dependence on libelf for Linux,"    we recently added a hard dependency for `libelf` on linux. this was in      preparation for some upcoming nvidia gpu support for injecting volumes      into containers. since this dependence is not actually necessary for      the upcoming release, we should remove it for now, and rethink the      best way to add it back in later (possibly as a runtime dependence      instead of a linktime one).",1,test
MESOS-5664,Invalid resources sent to '/reserve' are silently dropped,"if an invalid resource is passed to the master's /reserve endpoint, it will be silently dropped and not cause an error. this can lead, for example, to a /reserve request containing a single invalid resource receiving a 200 ok response, despite the fact that no resources were reserved as a result of the request.    this is due to the fact that the += operator for resources silently drops invalid resources, and this operator is used when parsing the resources in the http request. this could be addressed by validating the resource objects one at a time as they are parsed.",1,test
MESOS-5666,Deprecate camel case proto field in isolator ContainerConfig.,"currently there are extra executorinfo and taskinfo in isolator contaienrconfig, because a deprecation cycle is needed to deprecate camel cased proto field names. this jira is used for tracking this issue, which should address the todo in isolator.proto.",2,test
MESOS-5667,CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.,"  [22:41:54] :  [step 10/10] [ run      ] cniisolatortest.rootinternetcurllaunchcommandtask  [22:41:54]w:  [step 10/10] i0619 22:41:54.348641 30896 cluster.cpp:155] creating default 'local' authorizer  [22:41:54]w:  [step 10/10] i0619 22:41:54.353384 30896 leveldb.cpp:174] opened db in 4.634552ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.354763 30896 leveldb.cpp:181] compacted db in 1.360201ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.354784 30896 leveldb.cpp:196] created db iterator in 3421ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.354790 30896 leveldb.cpp:202] seeked to beginning of db in 633ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.354797 30896 leveldb.cpp:271] iterated through 0 keys in the db in 401ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.354811 30896 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [22:41:54]w:  [step 10/10] i0619 22:41:54.354990 30913 recover.cpp:451] starting replica recovery  [22:41:54]w:  [step 10/10] i0619 22:41:54.355123 30915 recover.cpp:477] replica is in empty status  [22:41:54]w:  [step 10/10] i0619 22:41:54.355391 30915 replica.cpp:673] replica in empty status received a broadcasted recover request from (18695)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.355479 30912 recover.cpp:197] received a recover response from a replica in empty status  [22:41:54]w:  [step 10/10] i0619 22:41:54.355581 30914 recover.cpp:568] updating replica status to starting  [22:41:54]w:  [step 10/10] i0619 22:41:54.356091 30910 master.cpp:382] master 27c796db6f984d6196c0f583f22787ff (ip172302105.mesosphere.io) started on 172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.356104 30910 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/khgyrq/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/khgyrq/master"" zksessiontimeout=""10secs""  [22:41:54]w:  [step 10/10] i0619 22:41:54.356237 30910 master.cpp:434] master only allowing authenticated frameworks to register  [22:41:54]w:  [step 10/10] i0619 22:41:54.356245 30910 master.cpp:448] master only allowing authenticated agents to register  [22:41:54]w:  [step 10/10] i0619 22:41:54.356247 30910 master.cpp:461] master only allowing authenticated http frameworks to register  [22:41:54]w:  [step 10/10] i0619 22:41:54.356251 30910 credentials.hpp:37] loading credentials for authentication from '/tmp/khgyrq/credentials'  [22:41:54]w:  [step 10/10] i0619 22:41:54.356351 30910 master.cpp:506] using default 'crammd5' authenticator  [22:41:54]w:  [step 10/10] i0619 22:41:54.356389 30910 master.cpp:578] using default 'basic' http authenticator  [22:41:54]w:  [step 10/10] i0619 22:41:54.356439 30910 master.cpp:658] using default 'basic' http framework authenticator  [22:41:54]w:  [step 10/10] i0619 22:41:54.356467 30910 master.cpp:705] authorization enabled  [22:41:54]w:  [step 10/10] i0619 22:41:54.356531 30913 whitelistwatcher.cpp:77] no whitelist given  [22:41:54]w:  [step 10/10] i0619 22:41:54.356549 30912 hierarchical.cpp:142] initialized hierarchical allocator process  [22:41:54]w:  [step 10/10] i0619 22:41:54.356868 30916 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.232816ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.356884 30916 replica.cpp:320] persisted replica status to starting  [22:41:54]w:  [step 10/10] i0619 22:41:54.356945 30916 recover.cpp:477] replica is in starting status  [22:41:54]w:  [step 10/10] i0619 22:41:54.357100 30917 master.cpp:1969] the newly elected leader is master@172.30.2.105:40724 with id 27c796db6f984d6196c0f583f22787ff  [22:41:54]w:  [step 10/10] i0619 22:41:54.357115 30917 master.cpp:1982] elected as the leading master!  [22:41:54]w:  [step 10/10] i0619 22:41:54.357122 30917 master.cpp:1669] recovering from registrar  [22:41:54]w:  [step 10/10] i0619 22:41:54.357213 30910 registrar.cpp:332] recovering registrar  [22:41:54]w:  [step 10/10] i0619 22:41:54.357429 30913 replica.cpp:673] replica in starting status received a broadcasted recover request from (18698)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.357549 30914 recover.cpp:197] received a recover response from a replica in starting status  [22:41:54]w:  [step 10/10] i0619 22:41:54.357728 30913 recover.cpp:568] updating replica status to voting  [22:41:54]w:  [step 10/10] i0619 22:41:54.358937 30913 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.14792ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.358952 30913 replica.cpp:320] persisted replica status to voting  [22:41:54]w:  [step 10/10] i0619 22:41:54.358986 30913 recover.cpp:582] successfully joined the paxos group  [22:41:54]w:  [step 10/10] i0619 22:41:54.359041 30913 recover.cpp:466] recover process terminated  [22:41:54]w:  [step 10/10] i0619 22:41:54.359180 30916 log.cpp:553] attempting to start the writer  [22:41:54]w:  [step 10/10] i0619 22:41:54.359578 30917 replica.cpp:493] replica received implicit promise request from (18699)@172.30.2.105:40724 with proposal 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.360752 30917 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.157449ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.360767 30917 replica.cpp:342] persisted promised to 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.360982 30914 coordinator.cpp:238] coordinator attempting to fill missing positions  [22:41:54]w:  [step 10/10] i0619 22:41:54.361426 30910 replica.cpp:388] replica received explicit promise request from (18700)@172.30.2.105:40724 for position 0 with proposal 2  [22:41:54]w:  [step 10/10] i0619 22:41:54.362571 30910 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 1.124969ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.362587 30910 replica.cpp:712] persisted action at 0  [22:41:54]w:  [step 10/10] i0619 22:41:54.362999 30911 replica.cpp:537] replica received write request for position 0 from (18701)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.363030 30911 leveldb.cpp:436] reading position from leveldb took 14967ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.364264 30911 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.214497ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.364279 30911 replica.cpp:712] persisted action at 0  [22:41:54]w:  [step 10/10] i0619 22:41:54.364470 30910 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [22:41:54]w:  [step 10/10] i0619 22:41:54.365622 30910 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.131398ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.365636 30910 replica.cpp:712] persisted action at 0  [22:41:54]w:  [step 10/10] i0619 22:41:54.365643 30910 replica.cpp:697] replica learned nop action at position 0  [22:41:54]w:  [step 10/10] i0619 22:41:54.365769 30915 log.cpp:569] writer started with ending position 0  [22:41:54]w:  [step 10/10] i0619 22:41:54.366080 30913 leveldb.cpp:436] reading position from leveldb took 8794ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.366284 30915 registrar.cpp:365] successfully fetched the registry (0b) in 9.053952ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.366315 30915 registrar.cpp:464] applied 1 operations in 3436ns; attempting to update the 'registry'  [22:41:54]w:  [step 10/10] i0619 22:41:54.366487 30911 log.cpp:577] attempting to append 209 bytes to the log  [22:41:54]w:  [step 10/10] i0619 22:41:54.366539 30917 coordinator.cpp:348] coordinator attempting to write append action at position 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.366839 30917 replica.cpp:537] replica received write request for position 1 from (18702)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.367966 30917 leveldb.cpp:341] persisting action (228 bytes) to leveldb took 1.106053ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.367982 30917 replica.cpp:712] persisted action at 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.368201 30915 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [22:41:54]w:  [step 10/10] i0619 22:41:54.371786 30915 leveldb.cpp:341] persisting action (230 bytes) to leveldb took 3.566076ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.371803 30915 replica.cpp:712] persisted action at 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.371809 30915 replica.cpp:697] replica learned append action at position 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.372032 30910 registrar.cpp:509] successfully updated the 'registry' in 5.693952ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.372097 30910 registrar.cpp:395] successfully recovered registrar  [22:41:54]w:  [step 10/10] i0619 22:41:54.372107 30911 log.cpp:596] attempting to truncate the log to 1  [22:41:54]w:  [step 10/10] i0619 22:41:54.372151 30910 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [22:41:54]w:  [step 10/10] i0619 22:41:54.372218 30911 master.cpp:1777] recovered 0 agents from the registry (170b) ; allowing 10mins for agents to reregister  [22:41:54]w:  [step 10/10] i0619 22:41:54.372242 30915 hierarchical.cpp:169] skipping recovery of hierarchical allocator: nothing to recover  [22:41:54]w:  [step 10/10] i0619 22:41:54.372467 30914 replica.cpp:537] replica received write request for position 2 from (18703)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.373693 30914 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.207676ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.373708 30914 replica.cpp:712] persisted action at 2  [22:41:54]w:  [step 10/10] i0619 22:41:54.373920 30913 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [22:41:54]w:  [step 10/10] i0619 22:41:54.375115 30913 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 1.17978ms  [22:41:54]w:  [step 10/10] i0619 22:41:54.375145 30913 leveldb.cpp:399] deleting ~1 keys from leveldb took 14216ns  [22:41:54]w:  [step 10/10] i0619 22:41:54.375154 30913 replica.cpp:712] persisted action at 2  [22:41:54]w:  [step 10/10] i0619 22:41:54.375159 30913 replica.cpp:697] replica learned truncate action at position 2  [22:41:54]w:  [step 10/10] i0619 22:41:54.383839 30896 containerizer.cpp:201] using isolation: docker/runtime,filesystem/linux,network/cni  [22:41:54]w:  [step 10/10] i0619 22:41:54.388789 30896 linuxlauncher.cpp:101] using /sys/fs/cgroup/freezer as the freezer hierarchy for the linux launcher  [22:41:54]w:  [step 10/10] e0619 22:41:54.393234 30896 shell.hpp:106] command 'hadoop version 2>&1' failed; this is the output:  [22:41:54]w:  [step 10/10] sh: hadoop: command not found  [22:41:54]w:  [step 10/10] i0619 22:41:54.393265 30896 fetcher.cpp:62] skipping uri fetcher plugin 'hadoop' as it could not be created: failed to create hdfs client: failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a nonzero exit status: 127  [22:41:54]w:  [step 10/10] i0619 22:41:54.393316 30896 registrypuller.cpp:111] creating registry puller with docker registry 'https:/registry1.docker.io'  [22:41:54]w:  [step 10/10] i0619 22:41:54.395668 30896 cluster.cpp:432] creating default 'local' authorizer  [22:41:54]w:  [step 10/10] i0619 22:41:54.396100 30914 slave.cpp:203] agent started on 469)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.396116 30914 slave.cpp:204] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/khgyrq/store"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/httpcredentials"" imageproviders=""docker"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""docker/runtime,filesystem/linux,network/cni"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" networkcniconfigdir=""/tmp/khgyrq/configs"" networkcnipluginsdir=""/tmp/khgyrq/plugins"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi""  [22:41:54]w:  [step 10/10] i0619 22:41:54.396380 30914 credentials.hpp:86] loading credential for authentication from '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/credential'  [22:41:54]w:  [step 10/10] i0619 22:41:54.396495 30914 slave.cpp:341] agent using credential for: testprincipal  [22:41:54]w:  [step 10/10] i0619 22:41:54.396509 30914 credentials.hpp:37] loading credentials for authentication from '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/httpcredentials'  [22:41:54]w:  [step 10/10] i0619 22:41:54.396586 30914 slave.cpp:393] using default 'basic' http authenticator  [22:41:54]w:  [step 10/10] i0619 22:41:54.396698 30914 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]  [22:41:54]w:  [step 10/10] trying semicolondelimited string format instead  [22:41:54]w:  [step 10/10] i0619 22:41:54.396780 30896 sched.cpp:224] version: 1.0.0  [22:41:54]w:  [step 10/10] i0619 22:41:54.396991 30914 slave.cpp:592] agent resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  [22:41:54]w:  [step 10/10] i0619 22:41:54.397020 30914 slave.cpp:600] agent attributes: [  ]  [22:41:54]w:  [step 10/10] i0619 22:41:54.397029 30914 slave.cpp:605] agent hostname: ip172302105.mesosphere.io  [22:41:54]w:  [step 10/10] i0619 22:41:54.397040 30916 sched.cpp:328] new master detected at master@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.397068 30916 sched.cpp:394] authenticating with master master@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.397078 30916 sched.cpp:401] using default crammd5 authenticatee  [22:41:54]w:  [step 10/10] i0619 22:41:54.397188 30916 authenticatee.cpp:121] creating new client sasl connection  [22:41:54]w:  [step 10/10] i0619 22:41:54.397467 30914 state.cpp:57] recovering state from '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskgcx6xi/meta'  [22:41:54]w:  [step 10/10] i0619 22:41:54.397476 30912 master.cpp:5943] authenticating scheduleraf10d6a31ebc4377b44d8c0dfbffcb8e@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.397544 30913 authenticator.cpp:414] starting authentication session for crammd5authenticatee(953)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.397614 30915 statusupdatemanager.cpp:200] recovering status update manager  [22:41:54]w:  [step 10/10] i0619 22:41:54.397668 30912 authenticator.cpp:98] creating new server sasl connection  [22:41:54]w:  [step 10/10] i0619 22:41:54.397709 30915 containerizer.cpp:514] recovering containerizer  [22:41:54]w:  [step 10/10] i0619 22:41:54.397869 30912 authenticatee.cpp:213] received sasl authentication mechanisms: crammd5  [22:41:54]w:  [step 10/10] i0619 22:41:54.397886 30912 authenticatee.cpp:239] attempting to authenticate with mechanism 'crammd5'  [22:41:54]w:  [step 10/10] i0619 22:41:54.397927 30912 authenticator.cpp:204] received sasl authentication start  [22:41:54]w:  [step 10/10] i0619 22:41:54.397964 30912 authenticator.cpp:326] authentication requires more steps  [22:41:54]w:  [step 10/10] i0619 22:41:54.398000 30912 authenticatee.cpp:259] received sasl authentication step  [22:41:54]w:  [step 10/10] i0619 22:41:54.398052 30912 authenticator.cpp:232] received sasl authentication step  [22:41:54]w:  [step 10/10] i0619 22:41:54.398066 30912 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302105.mesosphere.io' server fqdn: 'ip172302105.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   [22:41:54]w:  [step 10/10] i0619 22:41:54.398073 30912 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [22:41:54]w:  [step 10/10] i0619 22:41:54.398087 30912 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [22:41:54]w:  [step 10/10] i0619 22:41:54.398098 30912 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip172302105.mesosphere.io' server fqdn: 'ip172302105.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   [22:41:54]w:  [step 10/10] i0619 22:41:54.398103 30912 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  [22:41:54]w:  [step 10/10] i0619 22:41:54.398108 30912 auxprop.cpp:129] skipping auxiliary property ' cmusaslsecretcrammd5' since saslauxpropauthzid == true  [22:41:54]w:  [step 10/10] i0619 22:41:54.398116 30912 authenticator.cpp:318] authentication success  [22:41:54]w:  [step 10/10] i0619 22:41:54.398162 30914 authenticatee.cpp:299] authentication success  [22:41:54]w:  [step 10/10] i0619 22:41:54.398181 30913 authenticator.cpp:432] authentication session cleanup for crammd5authenticatee(953)@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.398200 30912 master.cpp:5973] successfully authenticated principal 'testprincipal' at scheduleraf10d6a31ebc4377b44d8c0dfbffcb8e@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.398270 30914 sched.cpp:484] successfully authenticated with master master@172.30.2.105:40724  [22:41:54]w:  [step 10/10] i0619 22:41:54.398280 30914 sched.cpp:800] sending subscribe call to m...",2,test
MESOS-5668,Add CGROUP namespace to linux ns helper.,"since linux kernel 4.6, cgroup namespace is added. we need to support the handle for the cgroup namespace of the process.    this also relates to two test failures on ubuntu 16:    [22:41:26] :  [step 10/10] [ run      ] nstest.rootsetns  [22:41:26] :  [step 10/10] ../../src/tests/containerizer/nstests.cpp:75: failure  [22:41:26] :  [step 10/10] nstype: unknown namespace 'cgroup'  [22:41:26] :  [step 10/10] [  failed  ] nstest.rootsetns (1 ms)        [22:41:26] :  [step 10/10] [ run      ] nstest.rootgetns  [22:41:26] :  [step 10/10] ../../src/tests/containerizer/nstests.cpp:160: failure  [22:41:26] :  [step 10/10] nstype: unknown namespace 'cgroup'  [22:41:26] :  [step 10/10] [  failed  ] nstest.rootgetns (0 ms)  ",3,test
MESOS-5669,CNI isolator should not return failure if /etc/hostname does not exist on host.,"/etc/hostname may not necessarily exist on every system (e.g., centos 6). currently cni isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. this is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.    this issue relates to 3 failure tests:    [22:45:21] :  [step 10/10] [ run      ] cniisolatortest.rootinternetcurllaunchcommandtask  [22:45:21]w:  [step 10/10] i0619 22:45:21.647611 24647 cluster.cpp:155] creating default 'local' authorizer  [22:45:21]w:  [step 10/10] i0619 22:45:21.655230 24647 leveldb.cpp:174] opened db in 7.510408ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.657680 24647 leveldb.cpp:181] compacted db in 2.427309ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.657702 24647 leveldb.cpp:196] created db iterator in 6209ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.657709 24647 leveldb.cpp:202] seeked to beginning of db in 692ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.657713 24647 leveldb.cpp:271] iterated through 0 keys in the db in 431ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.657727 24647 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [22:45:21]w:  [step 10/10] i0619 22:45:21.657888 24662 recover.cpp:451] starting replica recovery  [22:45:21]w:  [step 10/10] i0619 22:45:21.658051 24668 recover.cpp:477] replica is in empty status  [22:45:21]w:  [step 10/10] i0619 22:45:21.658495 24664 replica.cpp:673] replica in empty status received a broadcasted recover request from (18401)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.658583 24662 recover.cpp:197] received a recover response from a replica in empty status  [22:45:21]w:  [step 10/10] i0619 22:45:21.658687 24664 recover.cpp:568] updating replica status to starting  [22:45:21]w:  [step 10/10] i0619 22:45:21.659111 24664 master.cpp:382] master 9a4a353b91c543b98c3719245c37758c (ip172302247.mesosphere.io) started on 172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.659126 24664 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/l8346z/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/l8346z/master"" zksessiontimeout=""10secs""  [22:45:21]w:  [step 10/10] i0619 22:45:21.659267 24664 master.cpp:434] master only allowing authenticated frameworks to register  [22:45:21]w:  [step 10/10] i0619 22:45:21.659276 24664 master.cpp:448] master only allowing authenticated agents to register  [22:45:21]w:  [step 10/10] i0619 22:45:21.659278 24664 master.cpp:461] master only allowing authenticated http frameworks to register  [22:45:21]w:  [step 10/10] i0619 22:45:21.659282 24664 credentials.hpp:37] loading credentials for authentication from '/tmp/l8346z/credentials'  [22:45:21]w:  [step 10/10] i0619 22:45:21.659375 24664 master.cpp:506] using default 'crammd5' authenticator  [22:45:21]w:  [step 10/10] i0619 22:45:21.659415 24664 master.cpp:578] using default 'basic' http authenticator  [22:45:21]w:  [step 10/10] i0619 22:45:21.659495 24664 master.cpp:658] using default 'basic' http framework authenticator  [22:45:21]w:  [step 10/10] i0619 22:45:21.659569 24664 master.cpp:705] authorization enabled  [22:45:21]w:  [step 10/10] i0619 22:45:21.659684 24666 hierarchical.cpp:142] initialized hierarchical allocator process  [22:45:21]w:  [step 10/10] i0619 22:45:21.659696 24665 whitelistwatcher.cpp:77] no whitelist given  [22:45:21]w:  [step 10/10] i0619 22:45:21.660269 24666 master.cpp:1969] the newly elected leader is master@172.30.2.247:42024 with id 9a4a353b91c543b98c3719245c37758c  [22:45:21]w:  [step 10/10] i0619 22:45:21.660281 24666 master.cpp:1982] elected as the leading master!  [22:45:21]w:  [step 10/10] i0619 22:45:21.660290 24666 master.cpp:1669] recovering from registrar  [22:45:21]w:  [step 10/10] i0619 22:45:21.660342 24662 registrar.cpp:332] recovering registrar  [22:45:21]w:  [step 10/10] i0619 22:45:21.661232 24669 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 2.48585ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.661254 24669 replica.cpp:320] persisted replica status to starting  [22:45:21]w:  [step 10/10] i0619 22:45:21.661326 24669 recover.cpp:477] replica is in starting status  [22:45:21]w:  [step 10/10] i0619 22:45:21.661667 24668 replica.cpp:673] replica in starting status received a broadcasted recover request from (18404)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.661758 24665 recover.cpp:197] received a recover response from a replica in starting status  [22:45:21]w:  [step 10/10] i0619 22:45:21.661893 24664 recover.cpp:568] updating replica status to voting  [22:45:21]w:  [step 10/10] i0619 22:45:21.663851 24664 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.915617ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.663866 24664 replica.cpp:320] persisted replica status to voting  [22:45:21]w:  [step 10/10] i0619 22:45:21.663899 24664 recover.cpp:582] successfully joined the paxos group  [22:45:21]w:  [step 10/10] i0619 22:45:21.663944 24664 recover.cpp:466] recover process terminated  [22:45:21]w:  [step 10/10] i0619 22:45:21.664088 24668 log.cpp:553] attempting to start the writer  [22:45:21]w:  [step 10/10] i0619 22:45:21.664556 24668 replica.cpp:493] replica received implicit promise request from (18405)@172.30.2.247:42024 with proposal 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.666551 24668 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.971938ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.666566 24668 replica.cpp:342] persisted promised to 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.666767 24667 coordinator.cpp:238] coordinator attempting to fill missing positions  [22:45:21]w:  [step 10/10] i0619 22:45:21.667230 24668 replica.cpp:388] replica received explicit promise request from (18406)@172.30.2.247:42024 for position 0 with proposal 2  [22:45:21]w:  [step 10/10] i0619 22:45:21.669271 24668 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 2.02399ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.669287 24668 replica.cpp:712] persisted action at 0  [22:45:21]w:  [step 10/10] i0619 22:45:21.669656 24669 replica.cpp:537] replica received write request for position 0 from (18407)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.669680 24669 leveldb.cpp:436] reading position from leveldb took 10808ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.671674 24669 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.977316ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.671689 24669 replica.cpp:712] persisted action at 0  [22:45:21]w:  [step 10/10] i0619 22:45:21.671907 24665 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [22:45:21]w:  [step 10/10] i0619 22:45:21.673920 24665 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.991274ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.673935 24665 replica.cpp:712] persisted action at 0  [22:45:21]w:  [step 10/10] i0619 22:45:21.673941 24665 replica.cpp:697] replica learned nop action at position 0  [22:45:21]w:  [step 10/10] i0619 22:45:21.674190 24665 log.cpp:569] writer started with ending position 0  [22:45:21]w:  [step 10/10] i0619 22:45:21.674489 24663 leveldb.cpp:436] reading position from leveldb took 9059ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.674718 24663 registrar.cpp:365] successfully fetched the registry (0b) in 14.355968ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.674747 24663 registrar.cpp:464] applied 1 operations in 3070ns; attempting to update the 'registry'  [22:45:21]w:  [step 10/10] i0619 22:45:21.674935 24665 log.cpp:577] attempting to append 209 bytes to the log  [22:45:21]w:  [step 10/10] i0619 22:45:21.674978 24665 coordinator.cpp:348] coordinator attempting to write append action at position 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.675242 24666 replica.cpp:537] replica received write request for position 1 from (18408)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.677088 24666 leveldb.cpp:341] persisting action (228 bytes) to leveldb took 1.823904ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.677103 24666 replica.cpp:712] persisted action at 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.677299 24667 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [22:45:21]w:  [step 10/10] i0619 22:45:21.679270 24667 leveldb.cpp:341] persisting action (230 bytes) to leveldb took 1.952303ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.679286 24667 replica.cpp:712] persisted action at 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.679291 24667 replica.cpp:697] replica learned append action at position 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.679481 24663 registrar.cpp:509] successfully updated the 'registry' in 4.715264ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.679503 24666 log.cpp:596] attempting to truncate the log to 1  [22:45:21]w:  [step 10/10] i0619 22:45:21.679560 24667 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [22:45:21]w:  [step 10/10] i0619 22:45:21.679581 24663 registrar.cpp:395] successfully recovered registrar  [22:45:21]w:  [step 10/10] i0619 22:45:21.679745 24664 master.cpp:1777] recovered 0 agents from the registry (170b) ; allowing 10mins for agents to reregister  [22:45:21]w:  [step 10/10] i0619 22:45:21.679774 24662 hierarchical.cpp:169] skipping recovery of hierarchical allocator: nothing to recover  [22:45:21]w:  [step 10/10] i0619 22:45:21.679986 24662 replica.cpp:537] replica received write request for position 2 from (18409)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.681895 24662 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.891877ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.681910 24662 replica.cpp:712] persisted action at 2  [22:45:21]w:  [step 10/10] i0619 22:45:21.682160 24666 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [22:45:21]w:  [step 10/10] i0619 22:45:21.684331 24666 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 2.153217ms  [22:45:21]w:  [step 10/10] i0619 22:45:21.684375 24666 leveldb.cpp:399] deleting ~1 keys from leveldb took 26973ns  [22:45:21]w:  [step 10/10] i0619 22:45:21.684383 24666 replica.cpp:712] persisted action at 2  [22:45:21]w:  [step 10/10] i0619 22:45:21.684389 24666 replica.cpp:697] replica learned truncate action at position 2  [22:45:21]w:  [step 10/10] i0619 22:45:21.691529 24647 containerizer.cpp:201] using isolation: docker/runtime,filesystem/linux,network/cni  [22:45:21]w:  [step 10/10] i0619 22:45:21.694491 24647 linuxlauncher.cpp:101] using /cgroup/freezer as the freezer hierarchy for the linux launcher  [22:45:21]w:  [step 10/10] e0619 22:45:21.699741 24647 shell.hpp:106] command 'hadoop version 2>&1' failed; this is the output:  [22:45:21]w:  [step 10/10] sh: hadoop: command not found  [22:45:21]w:  [step 10/10] i0619 22:45:21.699769 24647 fetcher.cpp:62] skipping uri fetcher plugin 'hadoop' as it could not be created: failed to create hdfs client: failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a nonzero exit status: 127  [22:45:21]w:  [step 10/10] i0619 22:45:21.699823 24647 registrypuller.cpp:111] creating registry puller with docker registry 'https:/registry1.docker.io'  [22:45:21]w:  [step 10/10] i0619 22:45:21.700865 24647 linux.cpp:146] bind mounting '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg' and making it a shared mount  [22:45:21]w:  [step 10/10] i0619 22:45:21.707801 24647 cni.cpp:286] bind mounting '/var/run/mesos/isolators/network/cni' and making it a shared mount  [22:45:21]w:  [step 10/10] i0619 22:45:21.714337 24647 cluster.cpp:432] creating default 'local' authorizer  [22:45:21]w:  [step 10/10] i0619 22:45:21.714825 24668 slave.cpp:203] agent started on 468)@172.30.2.247:42024  [22:45:21]w:  [step 10/10] i0619 22:45:21.714839 24668 slave.cpp:204] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg/credential"" defaultrole="" "" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/l8346z/store"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg/httpcredentials"" imageproviders=""docker"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""docker/runtime,filesystem/linux,network/cni"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" networkcniconfigdir=""/tmp/l8346z/configs"" networkcnipluginsdir=""/tmp/l8346z/plugins"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg""  [22:45:21]w:  [step 10/10] i0619 22:45:21.715116 24668 credentials.hpp:86] loading credential for authentication from '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg/credential'  [22:45:21]w:  [step 10/10] i0619 22:45:21.715195 24668 slave.cpp:341] agent using credential for: testprincipal  [22:45:21]w:  [step 10/10] i0619 22:45:21.715214 24668 credentials.hpp:37] loading credentials for authentication from '/mnt/teamcity/temp/buildtmp/cniisolatortestrootinternetcurllaunchcommandtaskcvawpg/httpcredentials'  [22:45:21]w:  [step 10/10] i0619 22:45:21.715296 24668 slave.cpp:393] using default 'basic' http authenticator  [22:45:21]w:  [step 10/10] i0619 22:45:21.715400 24668 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]        [22:45:38] :  [step 10/10] [ run      ] cniisolatortest.rootverifycheckpointedinfo  [22:45:38]w:  [step 10/10] i0619 22:45:38.459836 24647 cluster.cpp:155] creating default 'local' authorizer  [22:45:38]w:  [step 10/10] i0619 22:45:38.470319 24647 leveldb.cpp:174] opened db in 10.34226ms  [22:45:38]w:  [step 10/10] i0619 22:45:38.472771 24647 leveldb.cpp:181] compacted db in 2.403554ms  [22:45:38]w:  [step 10/10] i0619 22:45:38.472795 24647 leveldb.cpp:196] created db iterator in 4446ns  [22:45:38]w:  [step 10/10] i0619 22:45:38.472801 24647 leveldb.cpp:202] seeked to beginning of db in 810ns  [22:45:38]w:  [step 10/10] i0619 22:45:38.472806 24647 leveldb.cpp:271] iterated through 0 keys in the db in 393ns  [22:45:38]w:  [step 10/10] i0619 22:45:38.472822 24647 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [22:45:38]w:  [step 10/10] i0619 22:45:38.473093 24665 recover.cpp:451] starting replica recovery  [22:45:38]w:  [step 10/10] i0619 22:45:38.473260 24663 recover.cpp:477] replica is in empty status  [22:45:38]w:  [step 10/10] i0619 22:45:38.473647 24663 replica.cpp:673] replica in empty status received a broadcasted recover request from (18464)@172.30.2.247:42024  [22:45:38]w:  [step 10/10] i0619 22:45:38.473752 24665 recover.cpp:197] received a recover response from a replica in empty status  [22:45:38]w:  [step 10/10] i0619 22:45:38.473896 24667 recover.cpp:568] updating replica status to starting  [22:45:38]w:  [step 10/10] i0619 22:45:38.474319 24663 master.cpp:382] master 64f1f7ace8104fb1b5496e29fc62622b (ip172302247.mesosphere.io) started on 172.30.2.247:42024  [22:45:38]w:  [step 10/10] i0619 22:45:38.474329 24663 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/qjwqsy/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/qjwqsy/master""  zksession_timeout=""10secs""  [22:45:38]w:  [step 10/10] i0619 22:45:38.474452 24663 master.cpp:434] master only allowing authenticated frameworks to register  [22:45:38]w:  [step 10/10] i0619 22:45:38.474457 24663 master.cpp:448] master only allowing authenticated agents to register  [22:45:38]w:  [step 10/10] i0619 22:45:38.474459 24663 master.cpp:461] master only allowing authenticated http frameworks to register  [22:45:38]w:  [step 10/10] i0619 22:45:38.474463 24663 credentials.hpp:37] loading credentials for authentication from '/tmp/qjwqsy/credentials'  [22:45:38]w:  [step 10/10] i0619 22:45:38.474551 24663 master.cpp:506] using default 'crammd5' authenticator  [22:45:38]w:  [step 10/10] i0619 22:45:38.474598 24663 master.cpp:578] using default 'basic' http authenticator  [22:45:38]w:  [step 10/10] i0619 22:45:38.474643 24663 master.cpp:658] using default 'basic' http framework authenticator  [22:45:38]w:  [step 10/10] i0619 22:45:38.474674 2466...",3,test
MESOS-5670,MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.,"  [03:36:29] :  [step 10/10] [ run      ] memorypressuremesostest.cgroupsrootslaverecovery  [03:36:29]w:  [step 10/10] i0618 03:36:29.461802  2797 cluster.cpp:155] creating default 'local' authorizer  [03:36:29]w:  [step 10/10] i0618 03:36:29.469468  2797 leveldb.cpp:174] opened db in 7.527163ms  [03:36:29]w:  [step 10/10] i0618 03:36:29.470188  2797 leveldb.cpp:181] compacted db in 699544ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.470206  2797 leveldb.cpp:196] created db iterator in 4293ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.470211  2797 leveldb.cpp:202] seeked to beginning of db in 535ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.470216  2797 leveldb.cpp:271] iterated through 0 keys in the db in 321ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.470230  2797 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [03:36:29]w:  [step 10/10] i0618 03:36:29.470510  2815 recover.cpp:451] starting replica recovery  [03:36:29]w:  [step 10/10] i0618 03:36:29.470592  2817 recover.cpp:477] replica is in empty status  [03:36:29]w:  [step 10/10] i0618 03:36:29.471029  2813 replica.cpp:673] replica in empty status received a broadcasted recover request from (19800)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.471139  2816 recover.cpp:197] received a recover response from a replica in empty status  [03:36:29]w:  [step 10/10] i0618 03:36:29.471271  2818 recover.cpp:568] updating replica status to starting  [03:36:29]w:  [step 10/10] i0618 03:36:29.471606  2811 master.cpp:382] master 6d44b7c1ac0b440997dfa53fa2e39d09 (ip17230229.mesosphere.io) started on 172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.471619  2811 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/baxwq5/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/baxwq5/master"" zksessiontimeout=""10secs""  [03:36:29]w:  [step 10/10] i0618 03:36:29.471745  2811 master.cpp:434] master only allowing authenticated frameworks to register  [03:36:29]w:  [step 10/10] i0618 03:36:29.471753  2811 master.cpp:448] master only allowing authenticated agents to register  [03:36:29]w:  [step 10/10] i0618 03:36:29.471757  2811 master.cpp:461] master only allowing authenticated http frameworks to register  [03:36:29]w:  [step 10/10] i0618 03:36:29.471761  2811 credentials.hpp:37] loading credentials for authentication from '/tmp/baxwq5/credentials'  [03:36:29]w:  [step 10/10] i0618 03:36:29.471829  2811 master.cpp:506] using default 'crammd5' authenticator  [03:36:29]w:  [step 10/10] i0618 03:36:29.471868  2811 master.cpp:578] using default 'basic' http authenticator  [03:36:29]w:  [step 10/10] i0618 03:36:29.471941  2811 master.cpp:658] using default 'basic' http framework authenticator  [03:36:29]w:  [step 10/10] i0618 03:36:29.471977  2811 master.cpp:705] authorization enabled  [03:36:29]w:  [step 10/10] i0618 03:36:29.472034  2817 hierarchical.cpp:142] initialized hierarchical allocator process  [03:36:29]w:  [step 10/10] i0618 03:36:29.472038  2814 whitelistwatcher.cpp:77] no whitelist given  [03:36:29]w:  [step 10/10] i0618 03:36:29.472506  2811 master.cpp:1969] the newly elected leader is master@172.30.2.29:37328 with id 6d44b7c1ac0b440997dfa53fa2e39d09  [03:36:29]w:  [step 10/10] i0618 03:36:29.472522  2811 master.cpp:1982] elected as the leading master!  [03:36:29]w:  [step 10/10] i0618 03:36:29.472527  2811 master.cpp:1669] recovering from registrar  [03:36:29]w:  [step 10/10] i0618 03:36:29.472573  2812 registrar.cpp:332] recovering registrar  [03:36:29]w:  [step 10/10] i0618 03:36:29.473511  2816 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 2.195002ms  [03:36:29]w:  [step 10/10] i0618 03:36:29.473527  2816 replica.cpp:320] persisted replica status to starting  [03:36:29]w:  [step 10/10] i0618 03:36:29.473578  2816 recover.cpp:477] replica is in starting status  [03:36:29]w:  [step 10/10] i0618 03:36:29.473877  2815 replica.cpp:673] replica in starting status received a broadcasted recover request from (19803)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.473989  2814 recover.cpp:197] received a recover response from a replica in starting status  [03:36:29]w:  [step 10/10] i0618 03:36:29.474126  2817 recover.cpp:568] updating replica status to voting  [03:36:29]w:  [step 10/10] i0618 03:36:29.474735  2811 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 547332ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.474748  2811 replica.cpp:320] persisted replica status to voting  [03:36:29]w:  [step 10/10] i0618 03:36:29.474783  2811 recover.cpp:582] successfully joined the paxos group  [03:36:29]w:  [step 10/10] i0618 03:36:29.474829  2811 recover.cpp:466] recover process terminated  [03:36:29]w:  [step 10/10] i0618 03:36:29.474969  2818 log.cpp:553] attempting to start the writer  [03:36:29]w:  [step 10/10] i0618 03:36:29.475361  2811 replica.cpp:493] replica received implicit promise request from (19804)@172.30.2.29:37328 with proposal 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.475944  2811 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 559444ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.475956  2811 replica.cpp:342] persisted promised to 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.476215  2815 coordinator.cpp:238] coordinator attempting to fill missing positions  [03:36:29]w:  [step 10/10] i0618 03:36:29.476660  2816 replica.cpp:388] replica received explicit promise request from (19805)@172.30.2.29:37328 for position 0 with proposal 2  [03:36:29]w:  [step 10/10] i0618 03:36:29.477262  2816 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 584333ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.477273  2816 replica.cpp:712] persisted action at 0  [03:36:29]w:  [step 10/10] i0618 03:36:29.477699  2815 replica.cpp:537] replica received write request for position 0 from (19806)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.477726  2815 leveldb.cpp:436] reading position from leveldb took 8842ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.478277  2815 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 537361ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.478291  2815 replica.cpp:712] persisted action at 0  [03:36:29]w:  [step 10/10] i0618 03:36:29.478569  2811 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [03:36:29]w:  [step 10/10] i0618 03:36:29.479132  2811 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 545208ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.479146  2811 replica.cpp:712] persisted action at 0  [03:36:29]w:  [step 10/10] i0618 03:36:29.479152  2811 replica.cpp:697] replica learned nop action at position 0  [03:36:29]w:  [step 10/10] i0618 03:36:29.479317  2814 log.cpp:569] writer started with ending position 0  [03:36:29]w:  [step 10/10] i0618 03:36:29.479568  2811 leveldb.cpp:436] reading position from leveldb took 8325ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.479786  2814 registrar.cpp:365] successfully fetched the registry (0b) in 7.192064ms  [03:36:29]w:  [step 10/10] i0618 03:36:29.479822  2814 registrar.cpp:464] applied 1 operations in 3018ns; attempting to update the 'registry'  [03:36:29]w:  [step 10/10] i0618 03:36:29.479995  2818 log.cpp:577] attempting to append 205 bytes to the log  [03:36:29]w:  [step 10/10] i0618 03:36:29.480044  2818 coordinator.cpp:348] coordinator attempting to write append action at position 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.480309  2811 replica.cpp:537] replica received write request for position 1 from (19807)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.480928  2811 leveldb.cpp:341] persisting action (224 bytes) to leveldb took 596433ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.480942  2811 replica.cpp:712] persisted action at 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.481148  2815 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [03:36:29]w:  [step 10/10] i0618 03:36:29.481710  2815 leveldb.cpp:341] persisting action (226 bytes) to leveldb took 545656ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.481722  2815 replica.cpp:712] persisted action at 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.481727  2815 replica.cpp:697] replica learned append action at position 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.481958  2816 registrar.cpp:509] successfully updated the 'registry' in 2.119168ms  [03:36:29]w:  [step 10/10] i0618 03:36:29.482014  2816 registrar.cpp:395] successfully recovered registrar  [03:36:29]w:  [step 10/10] i0618 03:36:29.482045  2817 log.cpp:596] attempting to truncate the log to 1  [03:36:29]w:  [step 10/10] i0618 03:36:29.482117  2817 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [03:36:29]w:  [step 10/10] i0618 03:36:29.482166  2816 master.cpp:1777] recovered 0 agents from the registry (166b) ; allowing 10mins for agents to reregister  [03:36:29]w:  [step 10/10] i0618 03:36:29.482177  2817 hierarchical.cpp:169] skipping recovery of hierarchical allocator: nothing to recover  [03:36:29]w:  [step 10/10] i0618 03:36:29.482404  2817 replica.cpp:537] replica received write request for position 2 from (19808)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.482975  2817 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 552763ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.482986  2817 replica.cpp:712] persisted action at 2  [03:36:29]w:  [step 10/10] i0618 03:36:29.483301  2813 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [03:36:29]w:  [step 10/10] i0618 03:36:29.483870  2813 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 547529ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.483896  2813 leveldb.cpp:399] deleting ~1 keys from leveldb took 12161ns  [03:36:29]w:  [step 10/10] i0618 03:36:29.483904  2813 replica.cpp:712] persisted action at 2  [03:36:29]w:  [step 10/10] i0618 03:36:29.483911  2813 replica.cpp:697] replica learned truncate action at position 2  [03:36:29]w:  [step 10/10] i0618 03:36:29.492995  2797 containerizer.cpp:201] using isolation: cgroups/mem,filesystem/posix,network/cni  [03:36:29]w:  [step 10/10] i0618 03:36:29.496548  2797 linuxlauncher.cpp:101] using /sys/fs/cgroup/freezer as the freezer hierarchy for the linux launcher  [03:36:29]w:  [step 10/10] i0618 03:36:29.503572  2797 cluster.cpp:432] creating default 'local' authorizer  [03:36:29]w:  [step 10/10] i0618 03:36:29.503936  2817 slave.cpp:203] agent started on 488)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.503952  2817 slave.cpp:204] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesostestecfecccd67144ec7b5eba3071b772617"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""cgroups/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl""  [03:36:29]w:  [step 10/10] i0618 03:36:29.504148  2817 credentials.hpp:86] loading credential for authentication from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/credential'  [03:36:29]w:  [step 10/10] i0618 03:36:29.504189  2817 slave.cpp:341] agent using credential for: testprincipal  [03:36:29]w:  [step 10/10] i0618 03:36:29.504199  2817 credentials.hpp:37] loading credentials for authentication from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/httpcredentials'  [03:36:29]w:  [step 10/10] i0618 03:36:29.504245  2817 slave.cpp:393] using default 'basic' http authenticator  [03:36:29]w:  [step 10/10] i0618 03:36:29.504410  2797 sched.cpp:224] version: 1.0.0  [03:36:29]w:  [step 10/10] i0618 03:36:29.504416  2817 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]  [03:36:29]w:  [step 10/10] trying semicolondelimited string format instead  [03:36:29]w:  [step 10/10] i0618 03:36:29.504580  2818 sched.cpp:328] new master detected at master@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.504613  2818 sched.cpp:394] authenticating with master master@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.504622  2818 sched.cpp:401] using default crammd5 authenticatee  [03:36:29]w:  [step 10/10] i0618 03:36:29.504649  2817 slave.cpp:592] agent resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  [03:36:29]w:  [step 10/10] i0618 03:36:29.504673  2817 slave.cpp:600] agent attributes: [  ]  [03:36:29]w:  [step 10/10] i0618 03:36:29.504678  2817 slave.cpp:605] agent hostname: ip17230229.mesosphere.io  [03:36:29]w:  [step 10/10] i0618 03:36:29.504703  2816 authenticatee.cpp:121] creating new client sasl connection  [03:36:29]w:  [step 10/10] i0618 03:36:29.504830  2818 master.cpp:5943] authenticating scheduler3e992438052b45f0af6a851091145739@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.504887  2816 authenticator.cpp:414] starting authentication session for crammd5authenticatee(991)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.504982  2811 authenticator.cpp:98] creating new server sasl connection  [03:36:29]w:  [step 10/10] i0618 03:36:29.505004  2816 state.cpp:57] recovering state from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootslaverecoverymbzwwl/meta'  [03:36:29]w:  [step 10/10] i0618 03:36:29.505105  2813 authenticatee.cpp:213] received sasl authentication mechanisms: crammd5  [03:36:29]w:  [step 10/10] i0618 03:36:29.505131  2813 authenticatee.cpp:239] attempting to authenticate with mechanism 'crammd5'  [03:36:29]w:  [step 10/10] i0618 03:36:29.505138  2818 statusupdatemanager.cpp:200] recovering status update manager  [03:36:29]w:  [step 10/10] i0618 03:36:29.505167  2813 authenticator.cpp:204] received sasl authentication start  [03:36:29]w:  [step 10/10] i0618 03:36:29.505200  2813 authenticator.cpp:326] authentication requires more steps  [03:36:29]w:  [step 10/10] i0618 03:36:29.505200  2814 containerizer.cpp:514] recovering containerizer  [03:36:29]w:  [step 10/10] i0618 03:36:29.505241  2813 authenticatee.cpp:259] received sasl authentication step  [03:36:29]w:  [step 10/10] i0618 03:36:29.505300  2812 authenticator.cpp:232] received sasl authentication step  [03:36:29]w:  [step 10/10] i0618 03:36:29.505317  2812 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip17230229.mesosphere.io' server fqdn: 'ip17230229.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   [03:36:29]w:  [step 10/10] i0618 03:36:29.505323  2812 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [03:36:29]w:  [step 10/10] i0618 03:36:29.505331  2812 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [03:36:29]w:  [step 10/10] i0618 03:36:29.505337  2812 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip17230229.mesosphere.io' server fqdn: 'ip17230229.mesosphere.io' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   [03:36:29]w:  [step 10/10] i0618 03:36:29.505342  2812 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  [03:36:29]w:  [step 10/10] i0618 03:36:29.505347  2812 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  [03:36:29]w:  [step 10/10] i0618 03:36:29.505355  2812 authenticator.cpp:318] authentication success  [03:36:29]w:  [step 10/10] i0618 03:36:29.505399  2813 authenticatee.cpp:299] authentication success  [03:36:29]w:  [step 10/10] i0618 03:36:29.505421  2811 authenticator.cpp:432] authentication session cleanup for crammd5authenticatee(991)@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.505436  2812 master.cpp:5973] successfully authenticated principal 'testprincipal' at scheduler3e992438052b45f0af6a851091145739@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.505534  2816 sched.cpp:484] successfully authenticated with master master@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.505553  2816 sched.cpp:800] sending subscribe call to master@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.505591  2816 sched.cpp:833] will retry registration in 11.319315ms if necessary  [03:36:29]w:  [step 10/10] i0618 03:36:29.505672  2815 master.cpp:2539] received subscribe call for framework 'default' at scheduler3e992438052b45f0af6a851091145739@172.30.2.29:37328  [03:36:29]w:  [step 10/10] i0618 03:36:29.505702  2815 master.cpp:2008] authorizing framework principal 'testprincipal' to receive offers for role ''  [03:36:29]w:  [step 10/10] i0618 03:36:29.505854  2818 master.cpp:2615] subscribing framework default with checkpointing enabled and capabilities [  ]  [03:36:29]w:  [step 10/10] i0618 03:36:29.506031  2818 sched.cpp:723] framework registered with 6d44b7c1ac0b440997dfa53fa2e39d09 0000  [03:36:29]w:  [...",2,test
MESOS-5671,MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.,"  [00:48:29] :  [step 10/10] [ run      ] memorypressuremesostest.cgroupsrootstatistics  [00:48:29]w:  [step 10/10] 10 records in  [00:48:29]w:  [step 10/10] 10 records out  [00:48:29]w:  [step 10/10] 1048576 bytes (1.0 mb) copied, 0.000517638 s, 2.0 gb/s  [00:48:30]w:  [step 10/10] i0617 00:48:30.000998 25413 cluster.cpp:155] creating default 'local' authorizer  [00:48:30]w:  [step 10/10] i0617 00:48:30.020459 25413 leveldb.cpp:174] opened db in 19.338463ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.022897 25413 leveldb.cpp:181] compacted db in 2.416906ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.022919 25413 leveldb.cpp:196] created db iterator in 4037ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.022927 25413 leveldb.cpp:202] seeked to beginning of db in 769ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.022932 25413 leveldb.cpp:271] iterated through 0 keys in the db in 390ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.022944 25413 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  [00:48:30]w:  [step 10/10] i0617 00:48:30.023272 25432 recover.cpp:451] starting replica recovery  [00:48:30]w:  [step 10/10] i0617 00:48:30.023425 25434 recover.cpp:477] replica is in empty status  [00:48:30]w:  [step 10/10] i0617 00:48:30.023748 25434 replica.cpp:673] replica in empty status received a broadcasted recover request from (19361)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.023849 25429 recover.cpp:197] received a recover response from a replica in empty status  [00:48:30]w:  [step 10/10] i0617 00:48:30.024019 25435 recover.cpp:568] updating replica status to starting  [00:48:30]w:  [step 10/10] i0617 00:48:30.024338 25432 master.cpp:382] master 0e92ffa44f264cea84d39c67612de1bd (ip17230256.mesosphere.io) started on 172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.024348 25432 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/jbjy5p/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/tmp/jbjy5p/master"" zksessiontimeout=""10secs""  [00:48:30]w:  [step 10/10] i0617 00:48:30.024502 25432 master.cpp:434] master only allowing authenticated frameworks to register  [00:48:30]w:  [step 10/10] i0617 00:48:30.024508 25432 master.cpp:448] master only allowing authenticated agents to register  [00:48:30]w:  [step 10/10] i0617 00:48:30.024513 25432 master.cpp:461] master only allowing authenticated http frameworks to register  [00:48:30]w:  [step 10/10] i0617 00:48:30.024516 25432 credentials.hpp:37] loading credentials for authentication from '/tmp/jbjy5p/credentials'  [00:48:30]w:  [step 10/10] i0617 00:48:30.024603 25432 master.cpp:506] using default 'crammd5' authenticator  [00:48:30]w:  [step 10/10] i0617 00:48:30.024644 25432 master.cpp:578] using default 'basic' http authenticator  [00:48:30]w:  [step 10/10] i0617 00:48:30.024701 25432 master.cpp:658] using default 'basic' http framework authenticator  [00:48:30]w:  [step 10/10] i0617 00:48:30.024770 25432 master.cpp:705] authorization enabled  [00:48:30]w:  [step 10/10] i0617 00:48:30.024883 25435 whitelistwatcher.cpp:77] no whitelist given  [00:48:30]w:  [step 10/10] i0617 00:48:30.024885 25434 hierarchical.cpp:142] initialized hierarchical allocator process  [00:48:30]w:  [step 10/10] i0617 00:48:30.025539 25433 master.cpp:1969] the newly elected leader is master@172.30.2.56:53790 with id 0e92ffa44f264cea84d39c67612de1bd  [00:48:30]w:  [step 10/10] i0617 00:48:30.025555 25433 master.cpp:1982] elected as the leading master!  [00:48:30]w:  [step 10/10] i0617 00:48:30.025560 25433 master.cpp:1669] recovering from registrar  [00:48:30]w:  [step 10/10] i0617 00:48:30.025611 25432 registrar.cpp:332] recovering registrar  [00:48:30]w:  [step 10/10] i0617 00:48:30.026397 25431 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 2.288187ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.026438 25431 replica.cpp:320] persisted replica status to starting  [00:48:30]w:  [step 10/10] i0617 00:48:30.026486 25431 recover.cpp:477] replica is in starting status  [00:48:30]w:  [step 10/10] i0617 00:48:30.026793 25432 replica.cpp:673] replica in starting status received a broadcasted recover request from (19364)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.026897 25429 recover.cpp:197] received a recover response from a replica in starting status  [00:48:30]w:  [step 10/10] i0617 00:48:30.027031 25428 recover.cpp:568] updating replica status to voting  [00:48:30]w:  [step 10/10] i0617 00:48:30.028960 25432 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.874668ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.028975 25432 replica.cpp:320] persisted replica status to voting  [00:48:30]w:  [step 10/10] i0617 00:48:30.029007 25432 recover.cpp:582] successfully joined the paxos group  [00:48:30]w:  [step 10/10] i0617 00:48:30.029047 25432 recover.cpp:466] recover process terminated  [00:48:30]w:  [step 10/10] i0617 00:48:30.029209 25430 log.cpp:553] attempting to start the writer  [00:48:30]w:  [step 10/10] i0617 00:48:30.029614 25429 replica.cpp:493] replica received implicit promise request from (19365)@172.30.2.56:53790 with proposal 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.031486 25429 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 1.850474ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.031502 25429 replica.cpp:342] persisted promised to 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.031726 25431 coordinator.cpp:238] coordinator attempting to fill missing positions  [00:48:30]w:  [step 10/10] i0617 00:48:30.032245 25428 replica.cpp:388] replica received explicit promise request from (19366)@172.30.2.56:53790 for position 0 with proposal 2  [00:48:30]w:  [step 10/10] i0617 00:48:30.034101 25428 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 1.831441ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.034117 25428 replica.cpp:712] persisted action at 0  [00:48:30]w:  [step 10/10] i0617 00:48:30.034561 25433 replica.cpp:537] replica received write request for position 0 from (19367)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.034589 25433 leveldb.cpp:436] reading position from leveldb took 10586ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.036419 25433 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 1.817267ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.036434 25433 replica.cpp:712] persisted action at 0  [00:48:30]w:  [step 10/10] i0617 00:48:30.036679 25429 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  [00:48:30]w:  [step 10/10] i0617 00:48:30.038661 25429 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.96521ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.038677 25429 replica.cpp:712] persisted action at 0  [00:48:30]w:  [step 10/10] i0617 00:48:30.038682 25429 replica.cpp:697] replica learned nop action at position 0  [00:48:30]w:  [step 10/10] i0617 00:48:30.038839 25435 log.cpp:569] writer started with ending position 0  [00:48:30]w:  [step 10/10] i0617 00:48:30.039198 25433 leveldb.cpp:436] reading position from leveldb took 10572ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.039412 25433 registrar.cpp:365] successfully fetched the registry (0b) in 13.778944ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.039448 25433 registrar.cpp:464] applied 1 operations in 4778ns; attempting to update the 'registry'  [00:48:30]w:  [step 10/10] i0617 00:48:30.039643 25428 log.cpp:577] attempting to append 205 bytes to the log  [00:48:30]w:  [step 10/10] i0617 00:48:30.039696 25432 coordinator.cpp:348] coordinator attempting to write append action at position 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.039945 25430 replica.cpp:537] replica received write request for position 1 from (19368)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.041738 25430 leveldb.cpp:341] persisting action (224 bytes) to leveldb took 1.771112ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.041754 25430 replica.cpp:712] persisted action at 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.041977 25432 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  [00:48:30]w:  [step 10/10] i0617 00:48:30.043805 25432 leveldb.cpp:341] persisting action (226 bytes) to leveldb took 1.810425ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.043820 25432 replica.cpp:712] persisted action at 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.043825 25432 replica.cpp:697] replica learned append action at position 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.044040 25430 registrar.cpp:509] successfully updated the 'registry' in 4.556032ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.044100 25430 registrar.cpp:395] successfully recovered registrar  [00:48:30]w:  [step 10/10] i0617 00:48:30.044124 25428 log.cpp:596] attempting to truncate the log to 1  [00:48:30]w:  [step 10/10] i0617 00:48:30.044215 25431 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  [00:48:30]w:  [step 10/10] i0617 00:48:30.044244 25430 master.cpp:1777] recovered 0 agents from the registry (166b) ; allowing 10mins for agents to reregister  [00:48:30]w:  [step 10/10] i0617 00:48:30.044317 25433 hierarchical.cpp:169] skipping recovery of hierarchical allocator: nothing to recover  [00:48:30]w:  [step 10/10] i0617 00:48:30.044497 25433 replica.cpp:537] replica received write request for position 2 from (19369)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.046368 25433 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 1.851883ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.046383 25433 replica.cpp:712] persisted action at 2  [00:48:30]w:  [step 10/10] i0617 00:48:30.046583 25430 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  [00:48:30]w:  [step 10/10] i0617 00:48:30.048426 25430 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 1.821628ms  [00:48:30]w:  [step 10/10] i0617 00:48:30.048455 25430 leveldb.cpp:399] deleting ~1 keys from leveldb took 14283ns  [00:48:30]w:  [step 10/10] i0617 00:48:30.048463 25430 replica.cpp:712] persisted action at 2  [00:48:30]w:  [step 10/10] i0617 00:48:30.048468 25430 replica.cpp:697] replica learned truncate action at position 2  [00:48:30]w:  [step 10/10] i0617 00:48:30.055145 25413 containerizer.cpp:203] using isolation: cgroups/mem,filesystem/posix,network/cni  [00:48:30]w:  [step 10/10] i0617 00:48:30.058349 25413 linuxlauncher.cpp:101] using /cgroup/freezer as the freezer hierarchy for the linux launcher  [00:48:30]w:  [step 10/10] i0617 00:48:30.069301 25413 cluster.cpp:432] creating default 'local' authorizer  [00:48:30]w:  [step 10/10] i0617 00:48:30.069707 25431 slave.cpp:203] agent started on 485)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.069718 25431 slave.cpp:204] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesostestd7ff4961cb6d4d51bb2110129a5c5572"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""cgroups/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p""  [00:48:30]w:  [step 10/10] i0617 00:48:30.069916 25431 credentials.hpp:86] loading credential for authentication from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/credential'  [00:48:30]w:  [step 10/10] i0617 00:48:30.069967 25431 slave.cpp:341] agent using credential for: testprincipal  [00:48:30]w:  [step 10/10] i0617 00:48:30.069984 25431 credentials.hpp:37] loading credentials for authentication from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/httpcredentials'  [00:48:30]w:  [step 10/10] i0617 00:48:30.070050 25431 slave.cpp:393] using default 'basic' http authenticator  [00:48:30]w:  [step 10/10] i0617 00:48:30.070127 25431 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]  [00:48:30]w:  [step 10/10] trying semicolondelimited string format instead  [00:48:30]w:  [step 10/10] i0617 00:48:30.070282 25431 slave.cpp:592] agent resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  [00:48:30]w:  [step 10/10] i0617 00:48:30.070309 25431 slave.cpp:600] agent attributes: [  ]  [00:48:30]w:  [step 10/10] i0617 00:48:30.070314 25431 slave.cpp:605] agent hostname: ip17230256.mesosphere.io  [00:48:30]w:  [step 10/10] i0617 00:48:30.070484 25413 sched.cpp:224] version: 1.0.0  [00:48:30]w:  [step 10/10] i0617 00:48:30.070667 25433 sched.cpp:328] new master detected at master@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.070711 25429 state.cpp:57] recovering state from '/mnt/teamcity/temp/buildtmp/memorypressuremesostestcgroupsrootstatisticsaf5x0p/meta'  [00:48:30]w:  [step 10/10] i0617 00:48:30.070749 25433 sched.cpp:394] authenticating with master master@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.070758 25433 sched.cpp:401] using default crammd5 authenticatee  [00:48:30]w:  [step 10/10] i0617 00:48:30.070793 25430 statusupdatemanager.cpp:200] recovering status update manager  [00:48:30]w:  [step 10/10] i0617 00:48:30.070904 25432 authenticatee.cpp:121] creating new client sasl connection  [00:48:30]w:  [step 10/10] i0617 00:48:30.070914 25430 containerizer.cpp:518] recovering containerizer  [00:48:30]w:  [step 10/10] i0617 00:48:30.071049 25432 master.cpp:5943] authenticating scheduler21f8a98862884ec19d6ab66ae746896a@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071105 25428 authenticator.cpp:414] starting authentication session for crammd5authenticatee(984)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071164 25434 authenticator.cpp:98] creating new server sasl connection  [00:48:30]w:  [step 10/10] i0617 00:48:30.071241 25434 authenticatee.cpp:213] received sasl authentication mechanisms: crammd5  [00:48:30]w:  [step 10/10] i0617 00:48:30.071254 25434 authenticatee.cpp:239] attempting to authenticate with mechanism 'crammd5'  [00:48:30]w:  [step 10/10] i0617 00:48:30.071292 25434 authenticator.cpp:204] received sasl authentication start  [00:48:30]w:  [step 10/10] i0617 00:48:30.071336 25434 authenticator.cpp:326] authentication requires more steps  [00:48:30]w:  [step 10/10] i0617 00:48:30.071374 25434 authenticatee.cpp:259] received sasl authentication step  [00:48:30]w:  [step 10/10] i0617 00:48:30.071553 25434 authenticator.cpp:232] received sasl authentication step  [00:48:30]w:  [step 10/10] i0617 00:48:30.071574 25434 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip17230256' server fqdn: 'ip17230256' saslauxpropoverride: false saslauxpropauthzid: false   [00:48:30]w:  [step 10/10] i0617 00:48:30.071586 25434 auxprop.cpp:179] looking up auxiliary property 'userpassword'  [00:48:30]w:  [step 10/10] i0617 00:48:30.071594 25434 auxprop.cpp:179] looking up auxiliary property 'cmusaslsecretcrammd5'  [00:48:30]w:  [step 10/10] i0617 00:48:30.071604 25434 auxprop.cpp:107] request to lookup properties for user: 'testprincipal' realm: 'ip17230256' server fqdn: 'ip17230256' saslauxpropoverride: false saslauxpropauthzid: true   [00:48:30]w:  [step 10/10] i0617 00:48:30.071615 25434 auxprop.cpp:129] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  [00:48:30]w:  [step 10/10] i0617 00:48:30.071619 25434 auxprop.cpp:129] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  [00:48:30]w:  [step 10/10] i0617 00:48:30.071630 25434 authenticator.cpp:318] authentication success  [00:48:30]w:  [step 10/10] i0617 00:48:30.071684 25428 authenticator.cpp:432] authentication session cleanup for crammd5authenticatee(984)@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071687 25431 authenticatee.cpp:299] authentication success  [00:48:30]w:  [step 10/10] i0617 00:48:30.071704 25434 master.cpp:5973] successfully authenticated principal 'testprincipal' at scheduler21f8a98862884ec19d6ab66ae746896a@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071826 25431 sched.cpp:484] successfully authenticated with master master@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071841 25431 sched.cpp:800] sending subscribe call to master@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.071954 25431 sched.cpp:833] will retry registration in 731.385085ms if necessary  [00:48:30]w:  [step 10/10] i0617 00:48:30.071996 25434 master.cpp:2539] received subscribe call for framework 'default' at scheduler21f8a98862884ec19d6ab66ae746896a@172.30.2.56:53790  [00:48:30]w:  [step 10/10] i0617 00:48:30.072013 25434 master.cpp:2008] authorizing framework principal 'testprincipal' to receive offers for role ''  [00:48:30]w:  [step 10/10] i0617 00:48:30.072180 25430 master.cpp:2615] subscribing framework default with checkpointing disabled and capabilities [  ]  [00:48:30]w:  [step 10/10] i0617 00:48:30.072305 25429 hierarchical.cpp:264] added framework 0e92ffa44f264cea84d39c67612de1bd 000...",2,test
MESOS-5673,Port mapping isolator may cause segfault if it bind mount root does not exist.,"a check is needed for port mapping isolator for its bind mount root. otherwise, nonexisted portmapping bind mount root may cause segmentation fault for some cases. here is the test log:      [00:57:42] :  [step 10/10] [] 11 tests from portmappingisolatortest  [00:57:42] :  [step 10/10] [ run      ] portmappingisolatortest.rootnccontainertocontainertcp  [00:57:42]w:  [step 10/10] i0604 00:57:42.723029 24841 portmappingtests.cpp:229] using eth0 as the public interface  [00:57:42]w:  [step 10/10] i0604 00:57:42.723348 24841 portmappingtests.cpp:237] using lo as the loopback interface  [00:57:42]w:  [step 10/10] i0604 00:57:42.735090 24841 resources.cpp:572] parsing resources as json failed: cpus:2;mem:1024;disk:1024;ephemeralports:[3000130999];ports:[3100032000]  [00:57:42]w:  [step 10/10] trying semicolondelimited string format instead  [00:57:42]w:  [step 10/10] i0604 00:57:42.736006 24841 portmapping.cpp:1557] using eth0 as the public interface  [00:57:42]w:  [step 10/10] i0604 00:57:42.736331 24841 portmapping.cpp:1582] using lo as the loopback interface  [00:57:42]w:  [step 10/10] i0604 00:57:42.737501 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gcthresh3 = '1024'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737545 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gcthresh1 = '128'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737578 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpwmem = '4096 16384 4194304'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737608 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpsynackretries = '5'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737637 24841 portmapping.cpp:1869] /proc/sys/net/core/rmemmax = '212992'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737666 24841 portmapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737694 24841 portmapping.cpp:1869] /proc/sys/net/core/wmemmax = '212992'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737720 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcprmem = '4096 87380 6291456'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737746 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpkeepalivetime = '7200'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737772 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gcthresh2 = '512'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737798 24841 portmapping.cpp:1869] /proc/sys/net/core/netdevmaxbacklog = '1000'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737828 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpkeepaliveintvl = '75'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737854 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpkeepaliveprobes = '9'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737879 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpmaxsynbacklog = '512'  [00:57:42]w:  [step 10/10] i0604 00:57:42.737905 24841 portmapping.cpp:1869] /proc/sys/net/ipv4/tcpretries2 = '15'  [00:57:42]w:  [step 10/10] f0604 00:57:42.737968 24841 portmappingtests.cpp:448] checksome(isolator): failed to get realpath for bind mount root '/var/run/netns': not found   [00:57:42]w:  [step 10/10]  check failure stack trace:   [00:57:42]w:  [step 10/10]     @     0x7f8bd52583d2  google::logmessage::fail()  [00:57:42]w:  [step 10/10]     @     0x7f8bd525832b  google::logmessage::sendtolog()  [00:57:42]w:  [step 10/10]     @     0x7f8bd5257d21  google::logmessage::flush()  [00:57:42]w:  [step 10/10]     @     0x7f8bd525ab92  google::logmessagefatal::logmessagefatal()  [00:57:42]w:  [step 10/10]     @           0xa62171  checkfatal::checkfatal()  [00:57:42]w:  [step 10/10]     @          0x1931b17  mesos::internal::tests::portmappingisolatortestrootnccontainertocontainertcptest::testbody()  [00:57:42]w:  [step 10/10]     @          0x19e17b6  testing::internal::handlesehexceptionsinmethodifsupported/()  [00:57:42]w:  [step 10/10]     @          0x19dc864  testing::internal::handleexceptionsinmethodifsupported/()  [00:57:42]w:  [step 10/10]     @          0x19bd2ae  testing::test::run()  [00:57:42]w:  [step 10/10]     @          0x19bda66  testing::testinfo::run()  [00:57:42]w:  [step 10/10]     @          0x19be0b7  testing::testcase::run()  [00:57:42]w:  [step 10/10]     @          0x19c4bf5  testing::internal::unittestimpl::runalltests()  [00:57:42]w:  [step 10/10]     @          0x19e247d  testing::internal::handlesehexceptionsinmethodifsupported/()  [00:57:42]w:  [step 10/10]     @          0x19dd3a4  testing::internal::handleexceptionsinmethodifsupported/()  [00:57:42]w:  [step 10/10]     @          0x19c38d1  testing::unittest::run()  [00:57:42]w:  [step 10/10]     @           0xfd28cb  runalltests()  [00:57:42]w:  [step 10/10]     @           0xfd24b1  main  [00:57:42]w:  [step 10/10]     @     0x7f8bceb89580  libcstartmain  [00:57:42]w:  [step 10/10]     @           0xa607c9  start  [00:57:43]w:  [step 10/10] /mnt/teamcity/temp/agenttmp/customscript659125926639545396: line 3: 24841 aborted                 (core dumped) glogv=1 ./bin/mesostests.sh verbose gtestfilter=""$gtest_filter""  [00:57:43]w:  [step 10/10] process exited with code 134  ",3,test
MESOS-5674,Port mapping isolator may fail in 'isolate' method.,"port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that containerid already existed. we should overwrite the symlink if it exist.    this affects a couple test failures:    portmappingisolatortest.roottoomanycontainers  portmappingisolatortest.rootcontainerarpexternal  portmappingisolatortest.rootcontainercmpinternal  portmappingisolatortest.rootnchosttocontainertcp      here is an example failure test log:     clonenewnet  [00:28:37w:  [step 10/10] i0606 00:28:37.084079 24863 portmapping.cpp:2576] bind mounted '/proc/11997/ns/net' to '/run/netns/11997' for container container1  [00:28:37] :  [step 10/10] ../../src/tests/containerizer/portmappingtests.cpp:1438: failure  [00:28:37] :  [step 10/10] (isolator.get()>isolate(containerid1, pid.get())).failure(): failed to symlink the network namespace handle '/var/run/mesos/netns/container1' > '/run/netns/11997': file exists  [00:28:37] :  [step 10/10] [  failed  ] portmappingisolatortest.roottoomanycontainers (57 ms)  ",3,test
MESOS-5677,Provide doc examples for dynamic reservation/persistent volumes,"users have found it difficult to make use of the dynamic reservation and persistent volume features. the api governing use of these features is a bit complicated, and this leads to users having trouble forming correct requests for reservations, volume creation, etc. providing multiple examples of reserve/unreserve/create/destroy requests would make it much easier for users to get started.",3,test
MESOS-5679,Example frameworks should allow setting failover timeout,"the example frameworks do not currently set a framework failover timeout when they register with the master. this means that when these frameworks are used in prolonged testing scenarios, small network outages can lead to flapping frameworks.    we should either set the failover timeout to a reasonable value in the example frameworks, or add command line flags that allow the timeout to be set.",2,test
MESOS-5684,Master captures `this` when creating authorization callback,"when exposing its log file, the master currently installs an authorization callback for the log file which captures the master's this pointer. such captures have previously caused bugs (mesos 5629), and this one should be fixed as well. the callback should be dispatched to the master process, and it should be dispatched via the self() pid.",1,test
MESOS-5685,The /files/download endpoint's authorization can be compromised,"if a forward slash is appended to the path of a file a user wishes to download via /files/download, the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions. this is because we store the authorization callbacks for these paths in a map which is keyed by the path name, so a request to /master/log/ fails to find the callback which is installed for /master/log. when the master fails to find the callback, it assumes authorization is not required for that path and authorizes the action.    consider the following excerpt:    gmann@gmac:/src/mesos/build  http get http:/127.0.0.1:5050/files/download\?path\=/master/log a foo:bar  http/1.1 403 forbidden  contentlength: 0  date: wed, 22 jun 2016 21:28:53 gmt    gmann@gmac:/src/mesos/build  http get http:/127.0.0.1:5050/files/download\?path\=/master/log/ a foo:bar  http/1.1 200 ok  contentdisposition: attachment; filename=mesosmaster.gmac.gmann.log.info.20160622142843.65615  contentlength: 14432  contenttype: application/octetstream  date: wed, 22 jun 2016 21:28:56 gmt    log file created at: 2016/06/22 14:28:43  running on machine: gmac  log line format: [iwef]mmdd hh:mm:ss.uuuuuu threadid file:line] msg  i0622 14:28:43.476925 2080764672 logging.cpp:194] info level logging started!  i0622 14:28:43.477522 2080764672 main.cpp:367] using 'hierarchicaldrf' allocator  i0622 14:28:43.480650 2080764672 leveldb.cpp:174] opened db in 2961us  i0622 14:28:43.481046 2080764672 leveldb.cpp:181] compacted db in 372us  i0622 14:28:43.481078 2080764672 leveldb.cpp:196] created db iterator in 13us  i0622 14:28:43.481096 2080764672 leveldb.cpp:202] seeked to beginning of db in 9us  i0622 14:28:43.481111 2080764672 leveldb.cpp:271] iterated through 0 keys in the db in 8us  i0622 14:28:43.481165 2080764672 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0622 14:28:43.481967 219914240 recover.cpp:451] starting replica recovery  i0622 14:28:43.482193 219914240 recover.cpp:477] replica is in empty status  i0622 14:28:43.482589 2080764672 main.cpp:488] creating default 'local' authorizer  i0622 14:28:43.482719 2080764672 main.cpp:545] starting mesos master  i0622 14:28:43.483085 218841088 replica.cpp:673] replica in empty status received a broadcasted recover request from (4)@127.0.0.1:5050  i0622 14:28:43.487284 218304512 recover.cpp:197] received a recover response from a replica in empty status  i0622 14:28:43.487694 219914240 recover.cpp:568] updating replica status to starting      we could consider disallowing paths which end in trailing slashes.",2,test
MESOS-5691,SSL downgrade support will leak sockets in CLOSE_WAIT status,"repro steps:  1) start a master:    bin/mesosmaster.sh workdir=/tmp/master      2) start an agent with ssl and downgrade enabled:    # taken from http:/mesos.apache.org/documentation/latest/ssl/  openssl genrsa des3 f4 passout pass:somepassword out key.pem 4096  openssl req new x509 passin pass:somepassword days 365 key key.pem out cert.pem    sslkeyfile=key.pem sslcertfile=cert.pem sslenabled=true sslsupportdowngrade=true sudo e bin/mesosagent.sh master=localhost:5050 workdir=/tmp/agent      3) start a framework that launches lots of executors, one after another:    sudo src/balloonframework master=localhost:5050 taskmemory=64mb taskmemoryusagelimit=256mb longrunning      4) check fds, repeatedly    sudo lsof i  grep closewait | wc l      the number of sockets in closewait will increase linearly with the number of launched executors.",5,test
MESOS-5697,Support file volume in mesos containerizer.,"currently in mesos containerizer, the host_path volume (to be bind mounted from a host path) specified in containerinfo can only be a directory. we should also support the volume type as a file.",3,test
MESOS-5698,Quota sorter not updated for resource changes at agent.,"consider this sequence of events:    1. slave connects, with 128mb of disk.  2. master offers resources at slave to framework  3. framework creates a dynamic reservation for 1mb and a persistent volume of the same size on the slave's resources.    => this invokes master::apply, which invokes allocator>updateallocation, which invokes sorter::update() on the framework sorter and role sorter. if the framework's role has a configured quota, it also invokes update on the quota role sorter  in this case, the framework's role has no quota, so the quota role sorter is not updated.    => drfsorter::update updates the total resources at a given slave, among updating other state. new total resources will be 127mb of unreserved disk and 1mb of reserved disk with a volume. note that the quota role sorter still thinks the slave has 128mb of unreserved disk.  4. the slave is removed from the cluster. hierarchicalallocatorprocess::removeslave invokes:        rolesorter>remove(slaveid, slaves[slaveid].total);    quotarolesorter>remove(slaveid, slaves[slaveid].total.nonrevocable());      slaves\[slaveid\].total.nonrevocable() is 127mb of unreserved disk and 1mb of reserved disk with a volume. when we remove this from the quota role sorter, we're left with total resources on the reserved slave of 1mb of unreserved disk, since that is the result of subtracting / from /.    the implications of this can't be good: at minimum, we're leaking resources for removed slaves in the quota role sorter. we're also introducing an inconsistency between total.resources\[slaveid\] and total.scalarquantities, since the latter has already strippedout volume/reservation information.",5,test
MESOS-5699,Create new documentation for Mesos networking.,"with introduction of cni and dockers support docker userdefined networks, there are quite a few options within mesos for ippercontainer solutions for container networking.     we therefore need to rewrite networking documentation for mesos highlighting all the networking support that mesos provides for orchestrating containers on ip networks.",1,test
MESOS-5704,Fine-grained authorization on /frameworks,"even if acls were defined for the actions viewframeworks,  viewexecutors and view_tasks, the data these actions were  supposed to protect, could still leaked through the master's  /frameworks endpoint, since it didn't enable any authorization  mechanism.",3,test
MESOS-5705,ZK credential is exposed in /flags and /state,"mesos allows zk credentials to be embedded in the zk url, but exposes these credentials in the /flags and /state endpoint. even though /state is authorized, it only filters out frameworks/tasks, so the toplevel flags are shown to any authenticated user.    ""zk"": ""zk:/dcosmesosmaster:mysecretpassword@127.0.0.1:2181/mesos"",    we need to find some way to hide this data, or even add a firstclass view_flags acl that applies to any endpoint that exposes flags.",5,test
MESOS-5706,GET_ENDPOINT_WITH_PATH authz doesn't make sense for /flags,"the master or agent flags are exposed in /state as well as /flags, so any user who wants to disable/control access to the flags likely intends to control access to flags no matter what endpoint exposes them. as such, /flags is a poor candidate for getendpointwithpath authz, since we care more about protecting the flag data than the specific endpoint path.  we should remove the getendpoint authz from master and agent /flags until we can come up with a better solution, perhaps a first class view_flags acl.",2,test
MESOS-5707,LocalAuthorizer should error if passed a GET_ENDPOINT ACL with an unhandled path,"since getendpointwithpath doesn't (yet) work with any arbitrary path, we should  a) validate acls and error if getendpointwithpath has a path object that doesn't match an endpoint that uses this authz strategy.  b) document exactly which endpoints support getendpointwith_path",3,test
MESOS-5708,Add authz to /files/debug,"the /files/debug endpoint exposes the attached master/agent log paths and every attached sandbox path, which includes the frameworkid and executorid. even if sandboxes are protected, we still don't want to expose this information to unauthorized users.",3,test
MESOS-5709,Authorization for /roles,"the /roles endpoint exposes the list of all roles and their weights, as well as the list of all frameworkids registered with each role. this is a superset of the information exposed on get /weights, which we already protect. we should protect the data in /roles the same way.   should we reuse viewframework with role (from /state)?   should we add a new viewrole and adapt get_weights to use it?",3,test
MESOS-5710,The /logging/toggle endpoint accepts requests with any http method,"any of a get, post, put, or delete to `/logging/toggle?level=info&duration=5mins` will set the log level and return 200. to be consistent with restlike syntax, delete, get, and even post are wrong and should return a methodnotallowed.    once this endpoint no longer accepts get, it is no longer appropriate to use the getendpoint acl here. instead we could create a new putendpointwithpath acl (which hopefully ignores query params), or add a firstclass toggle_logging acl.",3,test
MESOS-5711,Update AUTHORIZATION strings in endpoint help,"the endpoint help macros support authentication and authorization sections. we added authorization help for some of the newer endpoints, but not the previously authenticated endpoints.    authorization endpoints needing help string updates:  master::http::createvolumeshelp  master::http::destroyvolumeshelp  master::http::reservehelp  master::http::statehelp  master::http::statesummaryhelp  master::http::teardownhelp  master::http::taskshelp  master::http::unreservehelp  slave::http::state_help",2,test
MESOS-5712,Document exactly what is handled by GET_ENDPOINTS_WITH_PATH acl,"users may expect that the getendpointwith_path acl can be used with any mesos endpoint, but that is not (yet) the case. we should clearly document the list of applicable endpoints, in authorization.md and probably even upgrades.md.",1,test
MESOS-5713,Add a __sockets__ diagnostic endpoint to libprocess.,"libprocess exposes a endpoint /processes, which displays some info on the existing actors and messages queued up on each.    it would be nice to inspect the state of libprocess's socketmanager too.  this could be an endpoint like /sockets that exposes information like:   inbound fds: type and source   outbound fds: type and source   temporary and persistent sockets   linkers and linkees.    outgoing messages and their associated socket",3,test
MESOS-5716,Document docker private registry with authentication support in Unified Containerizer.,add documentation for docker private registry with authentication support in unified containerizer. this is the basic support for docker private registry.,3,test
MESOS-5717,Can't autodiscovery GPU resources without '--enable-nvidia-gpu-support' and '--nvidia_gpu_devices' flags,"prerequisite: in mesos\5257  ""by default, with no 'nvidiagpudevices' flag or `gpus` resources flag, the new autodiscovery will simply enumerate all of the gpus on the system"" and in mesos\5630 ""removes this flag(enablenvidiagpusupport) and enables this support for all builds on linux.""    so, i '../configure' without any flag, and start agent without 'resources' or 'nvidiagpudevices' ,  but can not discovery gpu resources, and i also start agent with 'resources' and 'nvidiagpudevices' , it also does not work.    i'm sure the nvidia gpus on my machines are ok, because with 'enablenvidiagpusupport' when './configure' and with 'resources', ' nvidiagpudevices' when starting agents it works well.",2,test
MESOS-5723,SSL-enabled libprocess will leak incoming links to forks,"encountered two different buggy behaviors that can be tracked down to the same underlying problem.    repro #1 (noncrashy):  (1) start a master.  doesn't matter if ssl is enabled or not.  (2) start an agent, with ssl enabled.  downgrade support has the same problem.  the master/agent link to one another.  (3) run a sleep task.  keep this alive.  if you inspect fds at this point, you'll notice the task has inherited the link fd (master > agent).  (4) restart the agent.  due to (3), the master's link stays open.  (5) check master's logs for the agent's reregistration message.  (6) check the agent's logs for reregistration.  the message will not appear.  the master is actually using the old link which is not connected to the agent.        repro #2 (crashy):  (1) start a master.  doesn't matter if ssl is enabled or not.  (2) start an agent, with ssl enabled.  downgrade support has the same problem.  (3) run ~100 sleep task one after the other, keep them all alive.  each task links back to the agent.  due to an fd leak, each task will inherit the incoming links from all other actors...  (4) at some point, the agent will run out of fds and kernel panic.        it appears that the ssl socket accept call is missing os::nonblock and os::cloexec calls:  https:/github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libeventsslsocket.cpp#l794l806    for reference, here's poll socket's accept:  https:/github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#l53l75  ",2,test
MESOS-5727,Command executor health check does not work when the task specifies container image.,"since we launch the task after pivot_root, we no longer has the access to the mesoshealthcheck binary. the solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.    one note here is that we should strive to keep both the command executor and the task in the same mount namespace so that mesos cli tooling does not need to find the mount namespace for the task. it just need to find the corresponding pid for the executor.",5,test
MESOS-5729,Consider allowing the libprocess caller an option to not set CLOEXEC on libprocess sockets,"both implementations of libprocess's socket interface will set the cloexec option on all new sockets (incoming or outgoing).  this assumption is pervasive across mesos, but since libprocess aims to be a general purpose library, the caller should be able to not cloexec sockets when desired.    see todos added here: https:/reviews.apache.org/r/49281/",3,test
MESOS-5740,Consider adding `relink` functionality to libprocess,"currently we don't have the relink functionality in libprocess.  i.e. a way to create a new persistent connection between actors, even if a connection already exists.     this can benefit us in a couple of ways:   the application may have more information on the state of a connection than libprocess does, as libprocess only checks if the connection is alive or not.  for example, a linkee may accept a connection, then fork, pass the connection to a child, and subsequently exit.  as the connection is still active, libprocess may not detect the exit.   sometimes, the exitedevent might be delayed or might be dropped due to the remote instance being unavailable (e.g., partition, network intermediaries not sending rst's etc).   ",3,test
MESOS-5742,"When start an agent with `--resources`, the GPU resource can be fractional","so far, the gpu resource is not fractional, only integer values are allowed. but when starting agents with resources='gpu:1.2', it can also work without any warning or error. and in the webui the gpu resource is `1.2`.",1,test
MESOS-5748,Potential segfault in `link` and `send` when linking to a remote process,"there is a race in the socketmanager, between a remote link and disconnection of the underlying socket.    we potentially segfault here: https:/github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#l1512    \socket dereferences the shared pointer underpinning the socket object.  however, the code above this line actually has ownership of the pointer:  https:/github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#l1494l1499    if the socket dies during the link, the ignorerecvdata may delete the socket underneath link:  https:/github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#l1399l1411      the same race exists for send.    this race was discovered while running a new test in repetition:  https:/reviews.apache.org/r/49175/    on osx, i hit the race consistently every 500800 repetitions:    3rdparty/libprocess/libprocesstests gtestfilter=""processremotelinktest.remotelink""  gtestbreakonfailure gtest_repeat=1000  ",2,test
MESOS-5753,Command executor should use `mesos-containerizer launch` to launch user task.,"currently, command executor and `mesoscontainerizer launch` share a lot of the logic. command executor should in fact, just use `mesoscontainerizer launch` to launch the user task.    potentially, `mesos containerizer launch` can be also used by custom executor to launch user tasks.",8,test
MESOS-5754,CommandInfo.user not honored in docker containerizer,"repro by creating a framework that starts a task with commandinfo.user set, and observe that the dockerized executor is still running as the default (e.g. root).    cc ",3,test
MESOS-5755,NVML headers are not installed as part of 3rdparty install with --enable-install-module-dependencies,review: https:/reviews.apache.org/r/49480/  ,2,test
MESOS-5759,ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky,"processremotelinktest.remoteusestalelink and processremotelinktest.remotestalelinkrelink are failing occasionally with the error:    [ run      ] processremotelinktest.remotestalelinkrelink  warning: logging before initgooglelogging() is written to stderr  i0630 07:42:34.661110 18888 process.cpp:1066] libprocess is initialized on 172.17.0.2:56294 with 16 worker threads  e0630 07:42:34.666393 18765 process.cpp:2104] failed to shutdown socket with fd 7: transport endpoint is not connected  /mesos/3rdparty/libprocess/src/tests/processtests.cpp:1059: failure  value of: exitedpid.ispending()    actual: false  expected: true  [  failed  ] processremotelinktest.remotestalelinkrelink (56 ms)      there appears to be a race between establishing a socket connection and the test calling ::shutdown on the socket.  under some circumstances, the ::shutdown may actually result in failing the future in socketmanager::linkconnect error and thereby trigger socketmanager::close.",1,test
MESOS-5761,Improve the logic of orphan tasks,"right now, a task is called orphaned if an agent reregisters with it but the corresponding framework information is not known to the master. this happens immediately after a master failover.    it would great if the master knows the information about the framework even after a failover, irrespective of whether a framework reregisters, so that we don't have orphan tasks. getting rid of orphan tasks will make the task authorization story easy (see mesos 5757).",5,test
MESOS-5765,Add 'systemGetDriverVersion' to NVML abstraction.,this command returns a string representing the version of the underlying nvidia drivers installed on a host. it will be used by the upcoming nvidiavolume component.,2,test
MESOS-5766,Missing License Information for Bundled NVML headers,see summary,1,test
MESOS-5767,Add ELFIO as bundled Dependency to Mesos,"elfio is a headeronly replacement for parsing elf binaries. previously we were using libelf, which introduced both a new buildtime dependency as well as a runtime dependence even though we only really needed this library when operating on machines that have gpus.    by using this header only library and bundling it with mesos, we can remove this external dependence altogether.",2,test
MESOS-5768,Reimplement the stout ELF abstraction in terms of ELFIO,"with the introduction of the new bundled elfio library, we need to reimplement our stout elf abstraction in terms of it.  as part of this, we need to update the tests that use it (i.e. ldcache_test.cpp)",2,test
MESOS-5769,Add get_abi_version() to ELF abstraction in stout,this function allows us to inspect the .note.abi tag section of an elf binary to determine the abi version of the executable / library.  this is needed for checking soe of the logic in building up an nvidiavolume for injection into a container. ,2,test
MESOS-5779,Allow Docker v1 ImageManifests to be parsed from the output of `docker inspect`,"    the `docker::spec::v1::imagemanifest` protobuf implements the      official v1 image manifest specification found at:            https:/github.com/docker/docker/blob/master/image/spec/v1.md            the field names in this spec are all written in snakecase as are the      field names of the json representing the image manifest when reading      it from disk (for example after performing a `docker save`). as such,      the protobuf for imagemanifest also provides these fields in      snakecase. unfortunately, the `docker inspect` command also provides      a method of retrieving the json for an image manifest, with one major      caveat  it represents all of its top level keys in camelcase.            to allow both representations to be parsed in the same way, we      should intercept the incoming json from either source (disk or `docker      inspect`) and convert it to a canonical snake_case representation.",3,test
MESOS-5782,Renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo.,currently the 'commands' in isolator.proto containerlaunchinfo is somehow confusing. it is a pre executed command (can be any script or shell command) before launch. we should renamed 'commands' to 'preexeccommands' in containerlaunchinfo and add comments.,2,test
MESOS-5787,Add ability to set framework capabilities in 'mesos-execute',"for now, we want to add this so that we can run mesos execute against agents that offer gpu resources. in the future, as we add more framework capabilities, this functionality will become more generally useful.",2,test
MESOS-5788,Consider adding a Java Scheduler Shim/Adapter for the new/old API.,"currently, for existing java based frameworks, moving to try out the new api can be cumbersome. this change intends to introduce a shim/adapter interface that makes this easier by allowing to toggle between the old/new api (driver/new scheduler library) implementation via an environment variable. this would allow framework developers to transition their older frameworks to the new api rather seamlessly.    this would look similar to the work done for the executor shim for c (command/docker executor). ",8,test
MESOS-5792,Add mesos tests to CMake (make check),provide cmakelists.txt and configuration files to build mesos tests using cmake.,8,test
MESOS-5793,Add ability to inject Nvidia devices into a container,nan,3,test
MESOS-5802,SlaveAuthorizerTest/0.ViewFlags is flaky.,"  [15:24:47] :  [step 10/10] [ run      ] slaveauthorizertest/0.viewflags  [15:24:47]w:  [step 10/10] i0707 15:24:47.025609 25322 containerizer.cpp:196] using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni  [15:24:47]w:  [step 10/10] i0707 15:24:47.030421 25322 linuxlauncher.cpp:101] using /sys/fs/cgroup/freezer as the freezer hierarchy for the linux launcher  [15:24:47]w:  [step 10/10] i0707 15:24:47.032060 25339 slave.cpp:205] agent started on 335)@172.30.2.7:43076  [15:24:47]w:  [step 10/10] i0707 15:24:47.032078 25339 slave.cpp:206] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authenticationbackofffactor=""1secs"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""true"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c"" xfsprojectrange=""[500010000]""  [15:24:47]w:  [step 10/10] i0707 15:24:47.032306 25339 credentials.hpp:86] loading credential for authentication from '/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/credential'  [15:24:47]w:  [step 10/10] i0707 15:24:47.032424 25339 slave.cpp:343] agent using credential for: testprincipal  [15:24:47]w:  [step 10/10] i0707 15:24:47.032441 25339 credentials.hpp:37] loading credentials for authentication from '/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/httpcredentials'  [15:24:47]w:  [step 10/10] i0707 15:24:47.032528 25339 slave.cpp:395] using default 'basic' http authenticator  [15:24:47]w:  [step 10/10] i0707 15:24:47.032754 25339 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]  [15:24:47]w:  [step 10/10] trying semicolondelimited string format instead  [15:24:47]w:  [step 10/10] i0707 15:24:47.032838 25339 resources.cpp:572] parsing resources as json failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]  [15:24:47]w:  [step 10/10] trying semicolondelimited string format instead  [15:24:47]w:  [step 10/10] i0707 15:24:47.032968 25339 slave.cpp:594] agent resources: cpus():2; mem():1024; disk():1024; ports( ):[3100032000]  [15:24:47]w:  [step 10/10] i0707 15:24:47.032994 25339 slave.cpp:602] agent attributes: [  ]  [15:24:47]w:  [step 10/10] i0707 15:24:47.032999 25339 slave.cpp:607] agent hostname: ip172302 7.ec2.internal.mesosphere.io  [15:24:47]w:  [step 10/10] i0707 15:24:47.033291 25339 process.cpp:3322] handling http event for process 'slave(335)' with path: '/slave(335)/flags'  [15:24:47]w:  [step 10/10] i0707 15:24:47.033329 25343 state.cpp:57] recovering state from '/mnt/teamcity/temp/buildtmp/slaveauthorizertest0viewflagsosjb5c/meta'  [15:24:47]w:  [step 10/10] i0707 15:24:47.033576 25342 statusupdatemanager.cpp:200] recovering status update manager  [15:24:47] :  [step 10/10] ../../src/tests/slaveauthorizationtests.cpp:316: failure  [15:24:47]w:  [step 10/10] i0707 15:24:47.033604 25340 http.cpp:269] http get for /slave(335)/flags from 172.30.2.7:33866  [15:24:47] :  [step 10/10] value of: (response).get().status  [15:24:47] :  [step 10/10]   actual: ""503 service unavailable""  [15:24:47]w:  [step 10/10] i0707 15:24:47.033687 25340 containerizer.cpp:522] recovering containerizer  [15:24:47] :  [step 10/10] expected: ok().status  [15:24:47] :  [step 10/10] which is: ""200 ok""  [15:24:47]w:  [step 10/10] i0707 15:24:47.034953 25340 process.cpp:3322] handling http event for process 'slave(335)' with path: '/slave(335)/state'  [15:24:47] :  [step 10/10] agent has not finished recovery  [15:24:47] :  [step 10/10] ../../src/tests/slaveauthorizationtests.cpp:320: failure  [15:24:47]w:  [step 10/10] i0707 15:24:47.035152 25343 http.cpp:269] http get for /slave(335)/state from 172.30.2.7:33868  [15:24:47] :  [step 10/10] parse: syntax error at line 1 near: agent has not finished recovery  [15:24:47]w:  [step 10/10] i0707 15:24:47.035768 25341 slave.cpp:841] agent terminating  [15:24:47]w:  [step 10/10] i0707 15:24:47.036150 25337 provisioner.cpp:253] provisioner recovery complete  [15:24:47] :  [step 10/10] [  failed  ] slaveauthorizertest/0.viewflags, where typeparam = mesos::internal::localauthorizer (14 ms)  ",2,test
MESOS-5806,CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.,"currently, the cni isolator will just ignore those containers that want to join the host network (i.e., not specifying networkinfo). however, if the container specifies a container image, we need to make sure that it has access to host /etc/  files. we should perform the bind mount for the container. this is also what docker does when a container is running in host mode.",5,test
MESOS-5812,MasterAPITest.Subscribe is flaky,"this test seems to be flaky, although on mac os x and centos 7 the error a bit different.    on mac os x:  [ run      ] contenttype/masterapitest.subscribe/0  i0708 11:42:48.474665 1927435008 cluster.cpp:155] creating default 'local' authorizer  i0708 11:42:48.480677 1927435008 leveldb.cpp:174] opened db in 5727us  i0708 11:42:48.481494 1927435008 leveldb.cpp:181] compacted db in 722us  i0708 11:42:48.481541 1927435008 leveldb.cpp:196] created db iterator in 19us  i0708 11:42:48.481572 1927435008 leveldb.cpp:202] seeked to beginning of db in 9us  i0708 11:42:48.481587 1927435008 leveldb.cpp:271] iterated through 0 keys in the db in 7us  i0708 11:42:48.481617 1927435008 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0708 11:42:48.482030 350982144 recover.cpp:451] starting replica recovery  i0708 11:42:48.482203 350982144 recover.cpp:477] replica is in empty status  i0708 11:42:48.484107 348299264 replica.cpp:673] replica in empty status received a broadcasted recover request from (3780)@127.0.0.1:50325  i0708 11:42:48.484318 350982144 recover.cpp:197] received a recover response from a replica in empty status  i0708 11:42:48.484750 348835840 master.cpp:382] master e055d60c05ff487e82dad0a43e52605c (localhost) started on 127.0.0.1:50325  i0708 11:42:48.484850 349908992 recover.cpp:568] updating replica status to starting  i0708 11:42:48.484788 348835840 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""1secs"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/private/tmp/sn2kf4/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/usr/local/share/mesos/webui"" workdir=""/private/tmp/sn2kf4/master"" zksessiontimeout=""10secs""  w0708 11:42:48.485263 348835840 master.cpp:387]     master bound to loopback interface! cannot communicate with remote schedulers or agents. you might want to set 'ip' flag to a routable ip address.    i0708 11:42:48.485291 348835840 master.cpp:434] master only allowing authenticated frameworks to register  i0708 11:42:48.485314 348835840 master.cpp:448] master only allowing authenticated agents to register  i0708 11:42:48.485335 348835840 master.cpp:461] master only allowing authenticated http frameworks to register  i0708 11:42:48.485347 348835840 credentials.hpp:37] loading credentials for authentication from '/private/tmp/sn2kf4/credentials'  i0708 11:42:48.485373 349372416 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 397us  i0708 11:42:48.485414 349372416 replica.cpp:320] persisted replica status to starting  i0708 11:42:48.485608 350982144 recover.cpp:477] replica is in starting status  i0708 11:42:48.485749 348835840 master.cpp:506] using default 'crammd5' authenticator  i0708 11:42:48.485852 348835840 master.cpp:578] using default 'basic' http authenticator  i0708 11:42:48.486018 348835840 master.cpp:658] using default 'basic' http framework authenticator  i0708 11:42:48.486140 348835840 master.cpp:705] authorization enabled  i0708 11:42:48.486486 350982144 replica.cpp:673] replica in starting status received a broadcasted recover request from (3783)@127.0.0.1:50325  i0708 11:42:48.486758 352055296 recover.cpp:197] received a recover response from a replica in starting status  i0708 11:42:48.487176 350982144 recover.cpp:568] updating replica status to voting  i0708 11:42:48.487576 352055296 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 300us  i0708 11:42:48.487658 352055296 replica.cpp:320] persisted replica status to voting  i0708 11:42:48.487736 350982144 recover.cpp:582] successfully joined the paxos group  i0708 11:42:48.487951 350982144 recover.cpp:466] recover process terminated  i0708 11:42:48.489441 348835840 master.cpp:1973] the newly elected leader is master@127.0.0.1:50325 with id e055d60c05ff487e82dad0a43e52605c  i0708 11:42:48.489518 348835840 master.cpp:1986] elected as the leading master!  i0708 11:42:48.489545 348835840 master.cpp:1673] recovering from registrar  i0708 11:42:48.489637 350982144 registrar.cpp:332] recovering registrar  i0708 11:42:48.490120 351518720 log.cpp:553] attempting to start the writer  i0708 11:42:48.491161 350445568 replica.cpp:493] replica received implicit promise request from (3784)@127.0.0.1:50325 with proposal 1  i0708 11:42:48.491461 350445568 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 252us  i0708 11:42:48.491528 350445568 replica.cpp:342] persisted promised to 1  i0708 11:42:48.492337 348299264 coordinator.cpp:238] coordinator attempting to fill missing positions  i0708 11:42:48.493482 349372416 replica.cpp:388] replica received explicit promise request from (3785)@127.0.0.1:50325 for position 0 with proposal 2  i0708 11:42:48.493854 349372416 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 283us  i0708 11:42:48.493904 349372416 replica.cpp:712] persisted action at 0  i0708 11:42:48.495302 348299264 replica.cpp:537] replica received write request for position 0 from (3786)@127.0.0.1:50325  i0708 11:42:48.495455 348299264 leveldb.cpp:436] reading position from leveldb took 45us  i0708 11:42:48.495761 348299264 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 261us  i0708 11:42:48.495803 348299264 replica.cpp:712] persisted action at 0  i0708 11:42:48.496484 350445568 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0708 11:42:48.496795 350445568 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 255us  i0708 11:42:48.496857 350445568 replica.cpp:712] persisted action at 0  i0708 11:42:48.496896 350445568 replica.cpp:697] replica learned nop action at position 0  i0708 11:42:48.497445 350982144 log.cpp:569] writer started with ending position 0  i0708 11:42:48.498523 350982144 leveldb.cpp:436] reading position from leveldb took 80us  i0708 11:42:48.499307 349908992 registrar.cpp:365] successfully fetched the registry (0b) in 9.63712ms  i0708 11:42:48.499464 349908992 registrar.cpp:464] applied 1 operations in 36us; attempting to update the 'registry'  i0708 11:42:48.499953 351518720 log.cpp:577] attempting to append 159 bytes to the log  i0708 11:42:48.500088 350982144 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0708 11:42:48.500880 348299264 replica.cpp:537] replica received write request for position 1 from (3787)@127.0.0.1:50325  i0708 11:42:48.501186 348299264 leveldb.cpp:341] persisting action (178 bytes) to leveldb took 259us  i0708 11:42:48.501231 348299264 replica.cpp:712] persisted action at 1  i0708 11:42:48.501786 351518720 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0708 11:42:48.502118 351518720 leveldb.cpp:341] persisting action (180 bytes) to leveldb took 311us  i0708 11:42:48.502260 351518720 replica.cpp:712] persisted action at 1  i0708 11:42:48.502305 351518720 replica.cpp:697] replica learned append action at position 1  i0708 11:42:48.503475 349908992 registrar.cpp:509] successfully updated the 'registry' in 3.944192ms  i0708 11:42:48.503909 349908992 registrar.cpp:395] successfully recovered registrar  i0708 11:42:48.504003 350982144 log.cpp:596] attempting to truncate the log to 1  i0708 11:42:48.504250 349372416 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0708 11:42:48.504546 350445568 master.cpp:1781] recovered 0 agents from the registry (121b) ; allowing 10mins for agents to reregister  i0708 11:42:48.506022 352055296 replica.cpp:537] replica received write request for position 2 from (3788)@127.0.0.1:50325  i0708 11:42:48.506479 352055296 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 320us  i0708 11:42:48.506513 352055296 replica.cpp:712] persisted action at 2  i0708 11:42:48.506978 351518720 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0708 11:42:48.507155 351518720 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 169us  i0708 11:42:48.507237 351518720 leveldb.cpp:399] deleting 1 keys from leveldb took 37us  i0708 11:42:48.507264 351518720 replica.cpp:712] persisted action at 2  i0708 11:42:48.507285 351518720 replica.cpp:697] replica learned truncate action at position 2  i0708 11:42:48.521363 1927435008 cluster.cpp:432] creating default 'local' authorizer  i0708 11:42:48.522498 350982144 slave.cpp:205] agent started on 119)@127.0.0.1:50325  i0708 11:42:48.522538 350982144 slave.cpp:206] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authenticationbackofffactor=""1secs"" authorizer=""local"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/users/zhitao/uber/sync/zhitaomesos1.dev.uber.com/home/uber/mesos/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[3100032000]"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" version=""false"" workdir=""/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx""  w0708 11:42:48.522903 350982144 slave.cpp:209]     agent bound to loopback interface! cannot communicate with remote master(s). you might want to set 'ip' flag to a routable ip address.    i0708 11:42:48.522922 350982144 credentials.hpp:86] loading credential for authentication from '/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/credential'  w0708 11:42:48.522965 1927435008 scheduler.cpp:157]     scheduler driver bound to loopback interface! cannot communicate with remote master(s). you might want to set 'libprocessip' environment variable to use a routable ip address.    i0708 11:42:48.522992 1927435008 scheduler.cpp:172] version: 1.0.0  i0708 11:42:48.523066 350982144 slave.cpp:343] agent using credential for: testprincipal  i0708 11:42:48.523092 350982144 credentials.hpp:37] loading credentials for authentication from '/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/httpcredentials'  i0708 11:42:48.523334 350982144 slave.cpp:395] using default 'basic' http authenticator  i0708 11:42:48.523973 352055296 scheduler.cpp:461] new master detected at master@127.0.0.1:50325  i0708 11:42:48.524050 350982144 slave.cpp:594] agent resources: cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0708 11:42:48.524196 350982144 slave.cpp:602] agent attributes: [  ]  i0708 11:42:48.524224 350982144 slave.cpp:607] agent hostname: localhost  i0708 11:42:48.525522 350445568 state.cpp:57] recovering state from '/var/folders/ny/tcvyblqj43s2gdh2895v9nw0000gp/t/contenttypemasterapitestsubscribe0vapndx/meta'  i0708 11:42:48.525853 350445568 statusupdatemanager.cpp:200] recovering status update manager  i0708 11:42:48.526165 350445568 slave.cpp:4856] finished recovery  i0708 11:42:48.527223 349372416 statusupdatemanager.cpp:174] pausing sending status updates  i0708 11:42:48.527231 352055296 slave.cpp:969] new master detected at master@127.0.0.1:50325  i0708 11:42:48.527276 352055296 slave.cpp:1028] authenticating with master master@127.0.0.1:50325  i0708 11:42:48.527328 352055296 slave.cpp:1039] using default crammd5 authenticatee  i0708 11:42:48.527561 352055296 slave.cpp:1001] detecting new master  i0708 11:42:48.527582 348299264 authenticatee.cpp:121] creating new client sasl connection  i0708 11:42:48.528666 349908992 master.cpp:6006] authenticating slave(119)@127.0.0.1:50325  i0708 11:42:48.528880 352055296 authenticator.cpp:98] creating new server sasl connection  i0708 11:42:48.529089 350445568 http.cpp:381] http post for /master/api/v1/scheduler from 127.0.0.1:50918  i0708 11:42:48.529233 350445568 master.cpp:2272] received subscription request for http framework 'default'  i0708 11:42:48.529261 350445568 master.cpp:2012] authorizing framework principal 'testprincipal' to receive offers for role ''  i0708 11:42:48.529323 352055296 authenticatee.cpp:213] received sasl authentication mechanisms: crammd5  i0708 11:42:48.529357 352055296 authenticatee.cpp:239] attempting to authenticate with mechanism 'crammd5'  i0708 11:42:48.529417 352055296 authenticator.cpp:204] received sasl authentication start  i0708 11:42:48.529503 352055296 authenticator.cpp:326] authentication requires more steps  i0708 11:42:48.529561 352055296 master.cpp:2370] subscribing framework 'default' with checkpointing disabled and capabilities [  ]  i0708 11:42:48.529721 349908992 authenticatee.cpp:259] received sasl authentication step  i0708 11:42:48.530005 348835840 authenticator.cpp:232] received sasl authentication step  i0708 11:42:48.530241 348835840 authenticator.cpp:318] authentication success  i0708 11:42:48.530254 350445568 hierarchical.cpp:271] added framework e055d60c05ff487e82dad0a43e52605c0000  i0708 11:42:48.530900 349908992 authenticatee.cpp:299] authentication success  i0708 11:42:48.531186 350982144 master.cpp:6036] successfully authenticated principal 'testprincipal' at slave(119)@127.0.0.1:50325  i0708 11:42:48.531657 348299264 slave.cpp:1123] successfully authenticated with master master@127.0.0.1:50325  i0708 11:42:48.531935 349372416 master.cpp:4676] registering agent at slave(119)@127.0.0.1:50325 (localhost) with id e055d60c05ff487e82dad0a43e52605cs0  i0708 11:42:48.532304 349908992 registrar.cpp:464] applied 1 operations in 55us; attempting to update the 'registry'  i0708 11:42:48.532908 348835840 log.cpp:577] attempting to append 326 bytes to the log  i0708 11:42:48.533015 352055296 coordinator.cpp:348] coordinator attempting to write append action at position 3  i0708 11:42:48.533641 349372416 replica.cpp:537] replica received write request for position 3 from (3798)@127.0.0.1:50325  i0708 11:42:48.533867 349372416 leveldb.cpp:341] persisting action (345 bytes) to leveldb took 186us  i0708 11:42:48.533917 349372416 replica.cpp:712] persisted action at 3  i0708 11:42:48.537066 349908992 replica.cpp:691] replica received learned notice for position 3 from @0.0.0.0:0  i0708 11:42:48.538169 349908992 leveldb.cpp:341] persisting action (347 bytes) to leveldb took 914us  i0708 11:42:48.538226 349908992 replica.cpp:712] persisted action at 3  i0708 11:42:48.538255 349908992 replica.cpp:697] replica learned append action at position 3  i0708 11:42:48.539247 352055296 registrar.cpp:509] successfully updated the 'registry' in 6.895104ms  i0708 11:42:48.539302 348299264 log.cpp:596] attempting to truncate the log to 3  i0708 11:42:48.539393 348299264 coordinator.cpp:348] coordinator attempting to write truncate action at position 4  i0708 11:42:48.539798 348835840 master.cpp:4745] registered agent e055d60c05ff487e82dad0a43e52605cs0 at slave(119)@127.0.0.1:50325 (localhost) with cpus():2; mem():1024; disk():1024; ports():[3100032000]  i0708 11:42:48.539881 348299264 hierarchical.cpp:478] added agent e055d60c05ff487e82dad0a43e52605cs0 (localhost) with cpus():2; mem():1024; disk():1024; ports():[3100032000] (allocated: )  i0708 11:42:48.539901 349908992 slave.cpp:1169] registered with master master@127.0.0.1:50325; given agent id e055d60c05ff487e82dad0a43e52605cs0  i0708 11:42:48.540287 350445568 statusupdate_manager.cpp:181] resuming sending status updates  i0708 11:42:48.540501 351518720 replica.cpp:537] replica received write request for position 4 from (3799)@127.0.0.1:50325  i0708 11:42:48.540583 352055296 master.cpp:5835] sending 1 offers to framework e055d60c05ff487e82dad0a43e52605c0000 (default)  i0708 11:42:48.540798 351518720 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 247us  i0708 11:42:48.540868 351518720 replica.cpp:712] persisted action at 4  i0708 11:42:48.540895 349908992 slave.cpp:1229] forwarding total oversubscribed resources   i0708 11:42:48.541035 352055296 master.cpp:5128] received update of agent e055d60c05ff487e82dad0a43e52605cs0 at slave(119)@127.0.0.1:50325 (localhost) with total oversubscribed resources   i0708 11:42:48.541291 349908992 hierarchical.cpp:542] agent e055d60c05ff487e82dad0a43e52605cs0 (localhost) updated with oversubscribed resources  (total: cpus():2; mem():1024; disk():1024; ports():[3100032000], allocated: cpus():2; mem():1024; disk():1024; ports():[3100032000])  i0708 11:42:48.541630 350982144 replica.cpp:691] replica received learned notice for position 4 from @0.0.0.0:0  i0708 11:42:48.541911 350982144 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 189us  i0708 11:42:48.541965 350982144 leveldb.cpp:399] deleting 2 keys from leveldb took 28us  i0708 11:42:48.541987 350982144 replica.cpp:712] persisted action at 4  i0708 11:42:48.542006 350982144 replica.cpp:697] replica learned truncate action at position 4  i0708 11:42:48.544836 352055296 http.cpp:381] http post for /master/api/v1 from 127.0.0.1:50920  i0708 11:42:48.544884 352055296 http.cpp:484] processing call subscribe  i0708 11:42:48.545382 352055296 master.cpp:7599] added subscriber: a85e7341ac154f1890211a2efa326442 to the list of active subscribers  i0708 11:42:48.550048 348835840 http.cpp:381] http post for /master/api/v1/scheduler from 127.0.0.1:50919  i0708 11:42:48.550339 348835840 master.cpp:3468] processing accept call for offers: [ e055d60c05ff487e82dad0a43e52605co0 ] on agent e055d60c05ff487e82dad0a43e52605cs0 at slave(119)@127.0.0.1:50325 (localhost) for framework e055d60c05ff487e82dad0a43e52605c 0000 (default)  i0708 11:42:48.550390 348835840 mas...",3,test
MESOS-5822,Add a build script for the Windows CI,"the asf ci for mesos runs a script that lives inside the mesos codebase:  https:/github.com/apache/mesos/blob/1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1/support/docker_build.sh    asf infrastructure have set up a machine that we can use for building mesos on windows.  considering the environment, we will need a separate script to build here.",3,test
MESOS-5824,Include disk source information in stringification,"some frameworks (like kafkamesos) ignore the source field when trying to reserve an offered mount or path persistent volume; the resulting error message is bewildering:      task uses more resources  cpus():4; mem():4096;     ports():[3100031000]; disk(kafka, kafka)[kafka0:data]:960679  than available  cpus():32; mem():256819;  ports():[3100032000]; disk(kafka, kafka)[kafka_0:data]:960679;   disk( ):240169;      the stringification of disk resources should include source information.  ",3,test
MESOS-5825,Support mounting image volume in mesos containerizer.,"mesos containerizer should be able to support mounting image volume type. specifically, both image rootfs and default manifest should be reachable inside container's mount namespace.",5,test
MESOS-5828,Modularize Network in replicated_log,currently replicatedlog relies on zookeeper for coordinator election. this is done through network abstraction zookeepernetwork. we need to modularize this part in order to enable replicatedlog when using master contender/detector modules.,8,test
MESOS-5841,Clean up `FlagsBase::add`,"in the definition for flagsbase, we currently have 20 overloads for the flagsbase::add function. this makes both the flagsbase class definition and the flags.cpp files in mesos difficult to read. we should clean up flagsbase::add so that it does not require so many overloads.",3,test
MESOS-5844,PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove test is flaky,"observed on asf ci: https:/builds.apache.org/job/mesos/buildtool=autotools,compiler=gcc,configuration=verbose,environment=glogv=1%20mesosverbose=1,os=ubuntu%3a14.04,labelexp=(docker%7c%7chadoop)&&(ubuntu6)/2497/changes      [ run      ] persistentvolumeendpointstest.offercreatethenendpointremove  i0713 18:43:55.968503 28220 cluster.cpp:155] creating default 'local' authorizer  i0713 18:43:56.082345 28220 leveldb.cpp:174] opened db in 113.403661ms  i0713 18:43:56.131445 28220 leveldb.cpp:181] compacted db in 49.034774ms  i0713 18:43:56.131533 28220 leveldb.cpp:196] created db iterator in 28012ns  i0713 18:43:56.131552 28220 leveldb.cpp:202] seeked to beginning of db in 3046ns  i0713 18:43:56.131564 28220 leveldb.cpp:271] iterated through 0 keys in the db in 255ns  i0713 18:43:56.131614 28220 replica.cpp:779] replica recovered with log positions 0 > 0 with 1 holes and 0 unlearned  i0713 18:43:56.134064 28246 recover.cpp:451] starting replica recovery  i0713 18:43:56.134627 28246 recover.cpp:477] replica is in empty status  i0713 18:43:56.136396 28252 replica.cpp:673] replica in empty status received a broadcasted recover request from (9553)@172.17.0.8:35418  i0713 18:43:56.136759 28252 recover.cpp:197] received a recover response from a replica in empty status  i0713 18:43:56.137676 28246 recover.cpp:568] updating replica status to starting  i0713 18:43:56.148720 28242 master.cpp:382] master 2258d072b0c94c40874c6cf933ee345a (500c3e866abe) started on 172.17.0.8:35418  i0713 18:43:56.148759 28242 master.cpp:384] flags at startup: acls="""" agentpingtimeout=""15secs"" agentreregistertimeout=""10mins"" allocationinterval=""50ms"" allocator=""hierarchicaldrf"" authenticateagents=""true"" authenticateframeworks=""true"" authenticatehttp=""true"" authenticatehttpframeworks=""true"" authenticators=""crammd5"" authorizers=""local"" credentials=""/tmp/lrwrl4/credentials"" frameworksorter=""drf"" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpframeworkauthenticators=""basic"" initializedriverlogging=""true"" logautoinitialize=""true"" logbufsecs=""0"" logginglevel=""info"" maxagentpingtimeouts=""5"" maxcompletedframeworks=""50"" maxcompletedtasksperframework=""1000"" quiet=""false"" recoveryagentremovallimit=""100%"" registry=""replicatedlog"" registryfetchtimeout=""1mins"" registrystoretimeout=""100secs"" registrystrict=""true"" roles=""role1"" rootsubmissions=""true"" usersorter=""drf"" version=""false"" webuidir=""/mesos/mesos1.1.0/inst/share/mesos/webui"" workdir=""/tmp/lrwrl4/master"" zksessiontimeout=""10secs""  i0713 18:43:56.149247 28242 master.cpp:434] master only allowing authenticated frameworks to register  i0713 18:43:56.149265 28242 master.cpp:448] master only allowing authenticated agents to register  i0713 18:43:56.149273 28242 master.cpp:461] master only allowing authenticated http frameworks to register  i0713 18:43:56.149283 28242 credentials.hpp:37] loading credentials for authentication from '/tmp/lrwrl4/credentials'  i0713 18:43:56.149780 28242 master.cpp:506] using default 'crammd5' authenticator  i0713 18:43:56.149940 28242 master.cpp:578] using default 'basic' http authenticator  i0713 18:43:56.150091 28242 master.cpp:658] using default 'basic' http framework authenticator  i0713 18:43:56.150209 28242 master.cpp:705] authorization enabled  w0713 18:43:56.150233 28242 master.cpp:768] the 'roles' flag is deprecated. this flag will be removed in the future. see the mesos 0.27 upgrade notes for more information  i0713 18:43:56.150760 28240 hierarchical.cpp:151] initialized hierarchical allocator process  i0713 18:43:56.151018 28249 whitelistwatcher.cpp:77] no whitelist given  i0713 18:43:56.155668 28242 master.cpp:1973] the newly elected leader is master@172.17.0.8:35418 with id 2258d072b0c94c40874c6cf933ee345a  i0713 18:43:56.155781 28242 master.cpp:1986] elected as the leading master!  i0713 18:43:56.155848 28242 master.cpp:1673] recovering from registrar  i0713 18:43:56.156065 28254 registrar.cpp:332] recovering registrar  i0713 18:43:56.201568 28245 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.201666 28245 hierarchical.cpp:1172] performed allocation for 0 agents in 167962ns  i0713 18:43:56.218626 28246 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 80.746657ms  i0713 18:43:56.218705 28246 replica.cpp:320] persisted replica status to starting  i0713 18:43:56.219219 28246 recover.cpp:477] replica is in starting status  i0713 18:43:56.221391 28246 replica.cpp:673] replica in starting status received a broadcasted recover request from (9556)@172.17.0.8:35418  i0713 18:43:56.221869 28253 recover.cpp:197] received a recover response from a replica in starting status  i0713 18:43:56.222760 28249 recover.cpp:568] updating replica status to voting  i0713 18:43:56.252303 28254 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.252404 28254 hierarchical.cpp:1172] performed allocation for 0 agents in 167038ns  i0713 18:43:56.270256 28249 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 47.316392ms  i0713 18:43:56.270387 28249 replica.cpp:320] persisted replica status to voting  i0713 18:43:56.270700 28250 recover.cpp:582] successfully joined the paxos group  i0713 18:43:56.271121 28250 recover.cpp:466] recover process terminated  i0713 18:43:56.271503 28248 log.cpp:553] attempting to start the writer  i0713 18:43:56.273140 28240 replica.cpp:493] replica received implicit promise request from (9557)@172.17.0.8:35418 with proposal 1  i0713 18:43:56.303086 28254 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.303175 28254 hierarchical.cpp:1172] performed allocation for 0 agents in 155905ns  i0713 18:43:56.312978 28240 leveldb.cpp:304] persisting metadata (8 bytes) to leveldb took 39.718643ms  i0713 18:43:56.313405 28240 replica.cpp:342] persisted promised to 1  i0713 18:43:56.314775 28245 coordinator.cpp:238] coordinator attempting to fill missing positions  i0713 18:43:56.316547 28250 replica.cpp:388] replica received explicit promise request from (9558)@172.17.0.8:35418 for position 0 with proposal 2  i0713 18:43:56.354794 28239 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.354898 28239 hierarchical.cpp:1172] performed allocation for 0 agents in 178033ns  i0713 18:43:56.363484 28250 leveldb.cpp:341] persisting action (8 bytes) to leveldb took 46.846904ms  i0713 18:43:56.363585 28250 replica.cpp:712] persisted action at 0  i0713 18:43:56.365622 28250 replica.cpp:537] replica received write request for position 0 from (9559)@172.17.0.8:35418  i0713 18:43:56.365727 28250 leveldb.cpp:436] reading position from leveldb took 45172ns  i0713 18:43:56.406314 28252 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.406421 28252 hierarchical.cpp:1172] performed allocation for 0 agents in 177001ns  i0713 18:43:56.421867 28250 leveldb.cpp:341] persisting action (14 bytes) to leveldb took 56.06514ms  i0713 18:43:56.421968 28250 replica.cpp:712] persisted action at 0  i0713 18:43:56.423286 28254 replica.cpp:691] replica received learned notice for position 0 from @0.0.0.0:0  i0713 18:43:56.458665 28250 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.458799 28250 hierarchical.cpp:1172] performed allocation for 0 agents in 250863ns  i0713 18:43:56.470486 28254 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 47.13918ms  i0713 18:43:56.470552 28254 replica.cpp:712] persisted action at 0  i0713 18:43:56.470584 28254 replica.cpp:697] replica learned nop action at position 0  i0713 18:43:56.471782 28247 log.cpp:569] writer started with ending position 0  i0713 18:43:56.475908 28253 leveldb.cpp:436] reading position from leveldb took 79764ns  i0713 18:43:56.479058 28247 registrar.cpp:365] successfully fetched the registry (0b) in 322.939904ms  i0713 18:43:56.479388 28247 registrar.cpp:464] applied 1 operations in 50643ns; attempting to update the 'registry'  i0713 18:43:56.483093 28247 log.cpp:577] attempting to append 168 bytes to the log  i0713 18:43:56.483269 28249 coordinator.cpp:348] coordinator attempting to write append action at position 1  i0713 18:43:56.484673 28245 replica.cpp:537] replica received write request for position 1 from (9560)@172.17.0.8:35418  i0713 18:43:56.509866 28239 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.509959 28239 hierarchical.cpp:1172] performed allocation for 0 agents in 157789ns  i0713 18:43:56.512147 28245 leveldb.cpp:341] persisting action (187 bytes) to leveldb took 27.358967ms  i0713 18:43:56.512193 28245 replica.cpp:712] persisted action at 1  i0713 18:43:56.513278 28250 replica.cpp:691] replica received learned notice for position 1 from @0.0.0.0:0  i0713 18:43:56.537894 28250 leveldb.cpp:341] persisting action (189 bytes) to leveldb took 24.568093ms  i0713 18:43:56.537973 28250 replica.cpp:712] persisted action at 1  i0713 18:43:56.538008 28250 replica.cpp:697] replica learned append action at position 1  i0713 18:43:56.539737 28252 registrar.cpp:509] successfully updated the 'registry' in 60.26496ms  i0713 18:43:56.539949 28252 registrar.cpp:395] successfully recovered registrar  i0713 18:43:56.540544 28252 master.cpp:1781] recovered 0 agents from the registry (129b) ; allowing 10mins for agents to reregister  i0713 18:43:56.540832 28250 hierarchical.cpp:178] skipping recovery of hierarchical allocator: nothing to recover  i0713 18:43:56.541285 28251 log.cpp:596] attempting to truncate the log to 1  i0713 18:43:56.541637 28248 coordinator.cpp:348] coordinator attempting to write truncate action at position 2  i0713 18:43:56.542763 28240 replica.cpp:537] replica received write request for position 2 from (9561)@172.17.0.8:35418  i0713 18:43:56.571691 28240 leveldb.cpp:341] persisting action (16 bytes) to leveldb took 28.798341ms  i0713 18:43:56.571889 28240 replica.cpp:712] persisted action at 2  i0713 18:43:56.573218 28240 replica.cpp:691] replica received learned notice for position 2 from @0.0.0.0:0  i0713 18:43:56.620200 28240 leveldb.cpp:341] persisting action (18 bytes) to leveldb took 46.927607ms  i0713 18:43:56.620338 28240 leveldb.cpp:399] deleting ~1 keys from leveldb took 59898ns  i0713 18:43:56.620512 28240 replica.cpp:712] persisted action at 2  i0713 18:43:56.620630 28240 replica.cpp:697] replica learned truncate action at position 2  i0713 18:43:56.624091 28249 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.624169 28249 hierarchical.cpp:1172] performed allocation for 0 agents in 140818ns  i0713 18:43:56.628180 28220 containerizer.cpp:196] using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni  w0713 18:43:56.629341 28220 backend.cpp:75] failed to create 'aufs' backend: aufsbackend requires root privileges, but is running as user mesos  w0713 18:43:56.629616 28220 backend.cpp:75] failed to create 'bind' backend: bindbackend requires root privileges  i0713 18:43:56.631988 28220 cluster.cpp:432] creating default 'local' authorizer  i0713 18:43:56.635001 28243 slave.cpp:205] agent started on 251)@172.17.0.8:35418  i0713 18:43:56.635308 28220 resources.cpp:572] parsing resources as json failed: disk:512  trying semicolondelimited string format instead  i0713 18:43:56.635026 28243 slave.cpp:206] flags at startup: acls="""" appcsimplediscoveryuriprefix=""http:/"" appcstoredir=""/tmp/mesos/store/appc"" authenticatehttp=""true"" authenticatee=""crammd5"" authenticationbackofffactor=""1secs"" authorizer=""local"" cgroupscpuenablepidsandtidscount=""false"" cgroupsenablecfs=""false"" cgroupshierarchy=""/sys/fs/cgroup"" cgroupslimitswap=""false"" cgroupsroot=""mesos"" containerdiskwatchinterval=""15secs"" containerizers=""mesos"" credential=""/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/credential"" defaultrole="""" diskwatchinterval=""1mins"" docker=""docker"" dockerkillorphans=""true"" dockerregistry=""https:/registry1.docker.io"" dockerremovedelay=""6hrs"" dockersocket=""/var/run/docker.sock"" dockerstoptimeout=""0ns"" dockerstoredir=""/tmp/mesos/store/docker"" dockervolumecheckpointdir=""/var/run/mesos/isolators/docker/volume"" enforcecontainerdiskquota=""false"" executorregistrationtimeout=""1mins"" executorshutdowngraceperiod=""5secs"" fetchercachedir=""/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/fetch"" fetchercachesize=""2gb"" frameworkshome="""" gcdelay=""1weeks"" gcdiskheadroom=""0.1"" hadoophome="""" help=""false"" hostnamelookup=""true"" httpauthenticators=""basic"" httpcommandexecutor=""false"" httpcredentials=""/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/httpcredentials"" imageprovisionerbackend=""copy"" initializedriverlogging=""true"" isolation=""posix/cpu,posix/mem"" launcherdir=""/mesos/mesos1.1.0/build/src"" logbufsecs=""0"" logginglevel=""info"" oversubscribedresourcesinterval=""15secs"" perfduration=""10secs"" perfinterval=""1mins"" qoscorrectionintervalmin=""0ns"" quiet=""false"" recover=""reconnect"" recoverytimeout=""15mins"" registrationbackofffactor=""10ms"" resources=""disk():1024"" revocablecpulowpriority=""true"" sandboxdirectory=""/mnt/mesos/sandbox"" strict=""true"" switchuser=""true"" systemdenablesupport=""true"" systemdruntimedirectory=""/run/systemd/system"" version=""false"" workdir=""/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq""  i0713 18:43:56.635709 28243 credentials.hpp:86] loading credential for authentication from '/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/credential'  i0713 18:43:56.635892 28243 slave.cpp:343] agent using credential for: testprincipal  i0713 18:43:56.635924 28243 credentials.hpp:37] loading credentials for authentication from '/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/httpcredentials'  i0713 18:43:56.636272 28243 slave.cpp:395] using default 'basic' http authenticator  i0713 18:43:56.636615 28243 resources.cpp:572] parsing resources as json failed: disk():1024  trying semicolondelimited string format instead  i0713 18:43:56.636878 28243 resources.cpp:572] parsing resources as json failed: disk():1024  trying semicolondelimited string format instead  i0713 18:43:56.637318 28243 slave.cpp:594] agent resources: disk():1024; cpus():16; mem():47270; ports():[3100032000]  i0713 18:43:56.637859 28243 slave.cpp:602] agent attributes: [  ]  i0713 18:43:56.638073 28220 sched.cpp:226] version: 1.1.0  i0713 18:43:56.638074 28243 slave.cpp:607] agent hostname: 500c3e866abe  i0713 18:43:56.640148 28253 sched.cpp:330] new master detected at master@172.17.0.8:35418  i0713 18:43:56.640650 28253 sched.cpp:396] authenticating with master master@172.17.0.8:35418  i0713 18:43:56.640738 28253 sched.cpp:403] using default crammd5 authenticatee  i0713 18:43:56.640801 28239 state.cpp:57] recovering state from '/tmp/persistentvolumeendpointstestoffercreatethenendpointremovegqstxq/meta'  i0713 18:43:56.640976 28243 authenticatee.cpp:121] creating new client sasl connection  i0713 18:43:56.641319 28253 statusupdatemanager.cpp:200] recovering status update manager  i0713 18:43:56.641477 28243 master.cpp:6006] authenticating scheduler398078e06dae4c028197af69d9eb230a@172.17.0.8:35418  i0713 18:43:56.641636 28239 authenticator.cpp:414] starting authentication session for crammd5authenticatee(554)@172.17.0.8:35418  i0713 18:43:56.641542 28240 containerizer.cpp:522] recovering containerizer  i0713 18:43:56.642201 28239 authenticator.cpp:98] creating new server sasl connection  i0713 18:43:56.642602 28252 authenticatee.cpp:213] received sasl authentication mechanisms: crammd5  i0713 18:43:56.642634 28252 authenticatee.cpp:239] attempting to authenticate with mechanism 'crammd5'  i0713 18:43:56.642714 28239 authenticator.cpp:204] received sasl authentication start  i0713 18:43:56.642792 28239 authenticator.cpp:326] authentication requires more steps  i0713 18:43:56.642882 28239 authenticatee.cpp:259] received sasl authentication step  i0713 18:43:56.642978 28239 authenticator.cpp:232] received sasl authentication step  i0713 18:43:56.643009 28239 auxprop.cpp:109] request to lookup properties for user: 'testprincipal' realm: '500c3e866abe' server fqdn: '500c3e866abe' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: false   i0713 18:43:56.643026 28239 auxprop.cpp:181] looking up auxiliary property 'userpassword'  i0713 18:43:56.643064 28239 auxprop.cpp:181] looking up auxiliary property 'cmusaslsecretcrammd5'  i0713 18:43:56.643091 28239 auxprop.cpp:109] request to lookup properties for user: 'testprincipal' realm: '500c3e866abe' server fqdn: '500c3e866abe' saslauxpropverifyagainsthash: false saslauxpropoverride: false saslauxpropauthzid: true   i0713 18:43:56.643107 28239 auxprop.cpp:131] skipping auxiliary property 'userpassword' since saslauxpropauthzid == true  i0713 18:43:56.643117 28239 auxprop.cpp:131] skipping auxiliary property 'cmusaslsecretcrammd5' since saslauxpropauthzid == true  i0713 18:43:56.643136 28239 authenticator.cpp:318] authentication success  i0713 18:43:56.643290 28239 authenticatee.cpp:299] authentication success  i0713 18:43:56.643379 28239 master.cpp:6036] successfully authenticated principal 'testprincipal' at scheduler398078e06dae4c028197af69d9eb230a@172.17.0.8:35418  i0713 18:43:56.643501 28244 authenticator.cpp:432] authentication session cleanup for crammd5authenticatee(554)@172.17.0.8:35418  i0713 18:43:56.643987 28244 sched.cpp:502] successfully authenticated with master master@172.17.0.8:35418  i0713 18:43:56.644011 28244 sched.cpp:820] sending subscribe call to master@172.17.0.8:35418  i0713 18:43:56.644103 28244 sched.cpp:853] will retry registration in 809.142674ms if necessary  i0713 18:43:56.644287 28244 master.cpp:2550] received subscribe call for framework 'default' at scheduler398078e06dae4c028197af69d9eb230a@172.17.0.8:35418  i0713 18:43:56.644346 28244 master.cpp:2012] authorizing framework principal 'testprincipal' to receive offers for role 'role1'  i0713 18:43:56.644675 28249 provisioner.cpp:253] provisioner recovery complete  i0713 18:43:56.645089 28245 master.cpp:2626] subscribing framework default with checkpointing disabled and capabilities [  ]  i0713 18:43:56.645783 28249 hierarchical.cpp:271] added framework 2258d072b0c94c40874c6cf933ee345a0000  i0713 18:43:56.645916 28249 hierarchical.cpp:1537] no allocations performed  i0713 18:43:56.646000 28249 hierarchical.cpp:1632] no inverse offers to send out!  i0713 18:43:56.646083 28248 sched.cpp:743] framework registered with 2258d072b0c94c40874c6cf933ee345a0000  i0713 18:43:56.646116 28249 hierarchical.cpp:1172] performed allocation for 0 agents in 249850ns  i0713 18:43:56.646163 28248 sched.cpp:757] scheduler::registered took 21831ns  i0713 18:43:56.646317 28246 slave.cpp:4856] finished recovery  i0713 18:43:56.663516 28246 slave.cpp:5028] querying resource estimator for oversubscribable resources  i0713 18:43:56.664029 28254 statusupdatemanager.cpp:174] pausing sending status updates  i0713 18:43:56.664043 28246 slave.cpp:969] new master detected at master@172.17.0.8:35418  i0713 18:43:56.664567 28246 slave.cpp:1028] authenticating with master master@172.17.0.8:35418  i0713 18:43:56.665148 28246 slave.cpp:1039] using default cram md5 authenticatee  i0713 18:43:56.665555 28246 slave.cpp:1001] detecting new master  i0713 18:43:56.665590 28244 authenticatee.cpp:121] creating new client sasl connection  i0713 18:43:56.665889 28246 slave.cpp:5042] received oversubscribable resources  from the resource estimator  i0713 18:43:56.666071 28253 master.cpp:6006] authenticating slave(251)@172.17.0.8:35418  i0713 18:43:56.666316 28244 authenticator.cpp:414] starting authentication session for crammd5_authenticat...",1,test
MESOS-5845,The fetcher can access any local file as root,"the mesos fetcher currently runs as root and does a blind cp+chown of any file:/ uri into the task's sandbox, to be owned by the task user. even if frameworks are restricted from running tasks as root, it seems they can still access root protected files in this way. we should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. one option would be to run the fetcher as the same user that the task will run as.",3,test
MESOS-5848,Docker health checks are malformed.,"when wrapping the health check command into docker exec, docker executor erroneously forms the health check command itself. here is an excerpt from an executor log:    launching health check process: docker exec mesos2070f452212045ada8d2a339d234da41s0.c27d1b78d4aa424b91fa1e91576db9b5 sh c "" true "" /opt/mesosphere/packages/mesos59d45b30116143cb8d9995ca26f9dec5e93dc710/libexec/mesos/mesoshealthcheck executor=(1)@10.0.1.41:40651 healthcheckjson=,""consecutivefailures"":1,""delayseconds"":0.0,""graceperiodseconds"":10.0,""intervalseconds"":5.0,""timeoutseconds"":10.0} task_id=testhc.db69c60b4a7511e6b9b0c254ada9b06d  ",1,test
MESOS-5850,Add a test that runs the 'mesos-local' binary,"the balloon framework test runs the mesos master and agent binaries, but we don't seem to have any tests which run the mesos local binary at the moment. such a test should be added, or one of the existing example framework tests could be modified to accomplish this.",2,test
MESOS-5852,CMake build needs to generate protobufs before building libmesos,"the existing cmake lists place protobufs at the same level as other mesos sources:  https:/github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/cmakelists.txt#l415    this is incorrect, as protobuf changes need to be regenerated before we can build against them.    note: in the autotools build, this is done by compiling protobufs into libmesos, which then builds libmesosno3rdparty:  https:/github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/makefile.am#l1304 l1305",2,test
MESOS-5855,Create a 'Disk (not) full' example framework,"we need example frameworks for verifying the correct behavior of posix/disk isolator when the disk quota enforcement is in place. one framework for verifying that disk quota enforcement is working and that container gets terminated when it goes beyond disk quota, and another one for verifying that container does not get killed if it stays within its disk quota bounds.   ",3,test
MESOS-5856,Logrotate ContainerLogger module does not rotate logs when run as root with --switch_user,"the logrotate containerlogger module runs as the agent's user.  in most cases, this is root.    when logrotate is run as root, there is an additional check the configuration files must pass (because a root logrotate needs to be secured against nonroot modifications to the configuration):  https:/github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#l807l815    log rotation will fail under the following scenario:  1) the agent is run with switch_user (default: true)  2) a task is launched with a nonroot user specified  3) the logrotate module spawns a few companion processes (as root) and this creates the stdout, stderr, stdout.logrotate.conf, and stderr.logrotate.conf files (as root).  this step races with the next step.  4) the mesos containerizer and fetcher will chown the task's sandbox to the nonroot user.  including the files just created.  5) when logrotate is run, it will skip any nonroot configuration files.  this means the files are not rotated.         fix: the logrotate module's companion processes should call setuid and setgid.",1,test
MESOS-5863,Enabling SSL causes fetcher fail to fetch from HTTPS sites.,"this is because curl (which fetcher relies on) also relies on some of the environment variables used by libprocess ssl support. for instance, `sslcertfile`. if the operator sets `sslcertfile` env var for mesos agent, the fetcher will inherit this env var and cause curl to fail:      [centos@ip10100205 ]$ sslcertfile=/run/dcos/pki/tls/certs/mesosslave.crt curl https:/registry1.docker.io:443/v2/library/alpine/manifests/latest  curl: (60) ssl certificate problem: unable to get local issuer certificate  more details here: https:/curl.haxx.se/docs/sslcerts.html    curl performs ssl certificate verification by default, using a ""bundle""   of certificate authority (ca) public keys (ca certs). if the default   bundle file isn't adequate, you can specify an alternate file   using the cacert option.  if this https server uses a certificate signed by a ca represented in   the bundle, the certificate verification probably failed due to a   problem with the certificate (it might be expired, or the name might   not match the domain name in the url).  if you'd like to turn off curl's verification of the certificate, use   the k (or insecure) option.    [centos@ip10100205 ]$ curl https:/registry1.docker.io:443/v2/library/alpine/manifests/latest  ]}]}      to solve this problem, we deprecated the existing `ssl` env variables and used `libprocessssl` instead. to be backward compatible, we still accept `ssl` env variables for the time being.",3,test
MESOS-5864,Document MESOS_SANDBOX executor env variable.,and we should document the difference with mesos_directory.,2,test
MESOS-5874,Only send ShutdownFrameworkMessage to agents associated with framework.,slave.cpp:2079] asked to shut down framework $ by master@$  slave.cpp:2094] cannot shut down unknown framework $     for high framework/churn clusters this saturates agent logs with these messages. when a framework terminates a shutdownframeworkmessage is sent to every registered slave in a for loop. this patch proposes sending this message to agents with executors associated with the framework.     also proposed is moving the logline to vlog(1). ,1,test
MESOS-5878,Strict/RegistrarTest.UpdateQuota/0 is flaky,"observed on asf ci (https:/builds.apache.org/job/mesos/buildtool=autotools,compiler=clang,configuration=verbose%20enablelibevent%20enablessl,environment=glogv=1%20mesosverbose=1,os=ubuntu:14.04,label_exp=(docker%7c%7chadoop)&&(ubuntu6)/2539/consolefull). log file is attached. note that this might have been uncovered due to the recent removal of os::sleep from clock::settle.",3,test
MESOS-5879,cgroups/net_cls isolator causing agent recovery issues,"we run with 'cgroups/netcls' in our isolator list, and when we restart any agent process in a cluster running an experimental custom isolator as well, the agents are unable to recover from checkpoint, because netcls reports that unknown orphan containers have duplicate netcls handles.    while this is a problem that needs to be solved (probably by fixing our custom isolator), it's also a problem that the netcls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery. can this be fixed?",1,test
MESOS-5886,FUTURE_DISPATCH may react on irrelevant dispatch.,"https:/github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#l50 uses https:/github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/gmock.hpp#l350 to figure out whether a processed dispatchevent is the same the user is waiting for. however, comparing std::typeinfo of function pointers is not enough: different class methods with same signatures will be matched. here is the test that proves this:    class dispatchprocess : public process/  ;      test(processtest, dispatchmatch)      the test passes:    [ run      ] processtest.dispatchmatch  [       ok ] processtest.dispatchmatch (1 ms)      this change was introduced in https:/reviews.apache.org/r/28052/.",5,test
MESOS-5887,Enhance DispatchEvent to include demangled method name.,"currently, https:/github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/include/process/event.hpp#l148 does not include any userfriendly information about the actual method being dispatched. this can be helpful in order to simplify triaging and debugging, e.g., using processes endpoint. now we print the https:/github.com/apache/mesos/blob/e8ebbe5fe4189ef7ab046da2276a6abee41deeb2/3rdparty/libprocess/src/process.cpp#l3198l3203.",5,test
MESOS-5891,/help endpoint does not set Content-Type to HTML,"this change added a default contenttype to all responses:  https:/github.com/apache/mesos/commit/b2c5d91addbae609af3791f128c53fb3a26c7d53    unfortunately, this changed the /help endpoint from no contenttype to text/plain.  for a browser to render this page correctly, we need an html content type.",1,test
MESOS-5901,Make the command executor unversioned,"currently, the command executor in src/launcher/executor.cpp is in the v1 namespace. as referenced in the versioning design doc, we had agreed to keep the mesos internal code in the unversioned namespace and use evolve/devolve helpers for requests/responses.     following this pattern, we should bring the command executor in the mesos::internal namespace.",2,test
MESOS-5907,ExamplesTest.DiskFullFramework fails on Arch,"this test fails consistently on recent arch linux, running in a vm.",1,test
MESOS-5909,"Stout ""OsTest.User"" test can fail on some systems","libc call getgrouplist doesn't return the gid list in a sorted manner (in my case, it's returning ""471 100"") ... whereas id  g return a sorted list (""100 471"" in my case) causing the validation inside the loop to fail.    we should sort both lists before comparing the values.",2,test
MESOS-5923,"Ubuntu 14.04 LTS GPU Isolator ""/run"" directory is noexec","in ubuntu 14.04 lts the mount for /run directory is noexec.  it affect the /var/run/mesos/isolators/gpu/nvidia_352.63/bin directory which mesos gpu isolators depended on.    bill@billz:/var/run$ mount | grep noexec  proc on /proc type proc (rw,noexec,nosuid,nodev)  sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)  devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)  tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)    the /var/run is link to /run:  bill@billz:/var$ ll  total 52  drwxrxrx 13 root root     4096 may  5 20:00 ./  drwxrxrx 27 root root     4096 jul 14 17:29 ../  lrwxrwxrwx  1 root root        9 may  5 19:50 lock > /run/lock/  drwxrwxrx 19 root syslog   4096 jul 28 08:00 log/  drwxrxrx  2 root root     4096 aug  4  2015 opt/  lrwxrwxrwx  1 root root        4 may  5 19:50 run > /run/    current the work around is mount without noexec:  sudo mount o remount,exec /run",3,test
MESOS-5924,Fetcher may print logging error when run as unprivileged user,"now that the fetcher performs its fetching as  the framework's/task's user when one is specified, it prints an error message when its user does not have permissions to create the default glog logging file:    i0728 17:29:39.337363 25464 logging.cpp:194] info level logging started!  i0728 17:29:39.337628 25464 fetcher.cpp:498] fetcher info: }],""sandboxdirectory"":""\/var\/lib\/mesos\/slave\/slaves\/57c21e0d487d46688da6005f13401598s0\/frameworks\/57c21e0d487d46688da6005f134015980001\/executors\/nonrootsuccess.dc9e820d54e811e6b08270b3d5120001\/runs\/3cae229f8c6d439e811678bf06ac3731"",""user"":""centos""}  i0728 17:29:39.339738 25464 fetcher.cpp:409] fetching uri 'file:/nonrootdir/nonroottest'  i0728 17:29:39.339758 25464 fetcher.cpp:250] fetching directly into the sandbox directory  i0728 17:29:39.339777 25464 fetcher.cpp:187] fetching uri 'file:/nonrootdir/nonroottest'  i0728 17:29:39.339792 25464 fetcher.cpp:167] copying resource with command:cp '/nonrootdir/nonroottest' '/var/lib/mesos/slave/slaves/57c21e0d487d46688da6005f13401598s0/frameworks/57c21e0d487d46688da6005f134015980001/executors/nonrootsuccess.dc9e820d54e811e6b08270b3d5120001/runs/3cae229f8c6d439e811678bf06ac3731/nonroottest'  could not create logging file: permission denied  could not create a loggingfile 20160728172939.25464!w0728 17:29:39.342435 25464 fetcher.cpp:289] copying instead of extracting resource from uri with 'extract' flag, because it does not seem to be an archive: file:/nonrootdir/nonroottest  i0728 17:29:39.342511 25464 fetcher.cpp:547] fetched 'file:/nonrootdir/nonroottest' to '/var/lib/mesos/slave/slaves/57c21e0d487d46688da6005f13401598s0/frameworks/57c21e0d487d46688da6005f134015980001/executors/nonrootsuccess.dc9e820d54e811e6b08270b3d5120001/runs/3cae229f8c6d439e811678bf06ac3731/nonroottest'  + /opt/mesosphere/packages/mesos6c64154890d6c22595d7d047193773cda8de6a7c/libexec/mesos/mesoscontainerizer mount help=false operation=makerslave path=/  i0728 17:29:39.603394 25433 exec.cpp:161] version: 1.0.0  i0728 17:29:39.699053 25490 exec.cpp:236] executor registered on agent 57c21e0d487d46688da6005f13401598 s0      it seems that the fetcher binary is unable to create the default logging file due to a permissions issue. however, when i set the relevant gloglogtostderr=true flag, which should prevent the creation of this default file, it had no effect.    note that the fetcher's logging output was piped to stdout/stderr as expected, and the task ran and completed successfully, so these errors do not seem to affect the execution of the task.",2,test
MESOS-5928,Agent's '--version' flag doesn't work,"with the removal of the agent's default workdir, the version flag no longer works. instead, the agent complains about the lack of a workdir and prints the usage instructions.",1,test
MESOS-5934,Enable the upgrade test script to run multiple masters/agents,"the script designed to test upgrades between different mesos versions, support/test upgrade.py, should be improved to test upgrades with multiple masters and agents.",3,test
MESOS-5935,Add upgrade testing to the ASF CI,"we should add execution of the support/test upgrade.py script to the asf ci runs. this will require having a build of a previous mesos version to run against latest master; perhaps we could cache builds of the last stable release somewhere, which could be fetched and executed against ci builds.",5,test
MESOS-5943,Incremental http parsing of URLs leads to decoder error,"when requests arrive to the decoder in pieces (e.g. mes followed by a separate chunk of os.apache.org) the http parser is not able to handle this case if the split is within the url component.    this causes the decoder to error out, and can lead to connection invalidation.    the scheduler driver is susceptible to this.",3,test
MESOS-5944,Remove `O_SYNC` from StatusUpdateManager logs,currently the statusupdatemanager uses osync to flush status updates to disk.     we don't need to use osync because we only read this file if the host did not crash. os::write success implies the kernel will have flushed our data to the page cache. this is sufficient for the recovery scenarios we use this data for.,1,test
MESOS-5945,NvidiaVolume::create() should check for root before creating volume,"without root, we cannot create the nvidia volume in /var/run/mesos or mount a tmpfs in cases where we need to override the noexec on the current file system.",2,test
MESOS-5954,Docker executor does not use HealthChecker library.,"https:/github.com/apache/mesos/commit/1556d9a3a02de4e8a90b5b64d268754f95b12d77 refactored health checks into a library. command executor uses the library instead of the ""mesoshealthcheck"" binary, docker executor should do the same for consistency.",3,test
MESOS-5955,"The ""mesos-health-check"" binary is not used anymore.","mesos5727 and mesos5954 refactored the health check code into the healthchecker library, hence the ""mesoshealthcheck"" binary became unused.    while the command and docker executors could just use the library to avoid the subprocess complexity, we may want to consider keeping a binary version that ships with the installation, because the intention of the binary was to allow other executors to re use our implementation. on the other side, this binary is ill suited to this since it uses libprocess message passing, so if we do not have code that requires the binary it seems ok to remove it for now. custom executors may use the healthchecker library directly, it is not much more complex than using the binary.",3,test
MESOS-5959,All non-root tests fail on GPU machine,a recent addition to ensure that nvidiavolume::create() ran as root broke all nonroot tests on gpu machines. the reason is that we unconditionally create this volume so long as we detect nvml.isavailable() which will fail now that we are only allowed to create this volume if we have root permissions.    we should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of containerizer and  isolation flags.,2,test
